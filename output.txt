CONF: TOPICS	 ALPHA	 ETA
 =============================================================== 
10	0.1	0.01
 =============================================================== 
subspace clustering irrelevant features via robust dantzig selector paper considers subspace clustering problem data contains irrelevant corrupted features propose method termed robust dantzig selector successfully identify clustering structure even presence irrelevant features idea simple yet powerful replace inner product robust counterpart insensitive irrelevant features given upper bound number irrelevant features establish theoretical guarantees algorithm identify correct subspace demonstrate effectiveness algorithm via numerical simulations best knowledge first method developed tackle subspace clustering irrelevant features
robust gaussian graphical modeling trimmed graphical lasso gaussian graphical models ggms popular tools studying network structures however many modern applications gene network discovery social interactions analysis often involve high dimensional noisy data outliers heavier tails gaussian distribution paper propose trimmed graphical lasso robust estimation sparse ggms method guards outliers implicit trimming mechanism akin popular least trimmed squares method used linear regression provide rigorous statistical analysis estimator high dimensional setting contrast existing approaches robust sparse ggms estimation lack statistical guarantees theoretical results complemented experiments simulated real gene expression data demonstrate value approach
regularized algorithms unified framework statistical guarantees latent models fundamental modeling tool machine learning applications present significant computational analytical challenges popular algorithm variants much used algorithmic tool yet rigorous understanding performance highly incomplete recently work demonstrated important class problems exhibits linear local convergence high dimensional setting however step well defined address precisely setting unified treatment using regularization regularization high dimensional problems well understood iterative algorithm requires careful balancing making progress towards solution identifying right structure sparsity low rank particular regularizing step using state art high dimensional prescriptions guaranteed provide balance algorithm analysis linked way reveals balance optimization statistical errors specialize general framework sparse gaussian mixture models high dimensional mixed regression regression missing variables obtaining statistical guarantees examples
mixed robust average submodular partitioning fast algorithms guarantees applications investigate novel mixed robust average case submodular data partitioning problems collectively call submodular partitioning problems generalize purely robust instances problem namely max min submodular fair allocation sfa emph min max submodular load balancing slb also average case instances submodular welfare problem swp submodular multiway partition smp robust versions studied theory community existing work focused tight approximation guarantees resultant algorithms generally scalable large real world applications contrasts average case instances algorithms scalable present paper bridge gap proposing several new algorithms including greedy majorization minimization minorization maximization relaxation algorithms scale large datasets also achieve theoretical approximation guarantees comparable state art moreover provide new scalable algorithms apply additive combinations robust average case objectives show problems many applications machine learning including data partitioning load balancing distributed data clustering image segmentation empirically demonstrate efficacy algorithms real world problems involving data partitioning distributed optimization convex deep neural network objectives also purely unsupervised image segmentation
calibrated structured prediction user facing applications displaying calibrated confidence measures probabilities correspond true frequency important obtaining high accuracy interested calibration structured prediction problems speech recognition optical character recognition medical diagnosis structured prediction presents new challenges calibration output space large users issue many types probability queries marginals structured output extend notion calibration handle various subtleties pertaining structured setting provide simple recalibration method trains binary classifier predict probabilities interest explore range features appropriate structured recalibration demonstrate efficacy real world datasets
convergence analysis prediction markets via randomized subspace descent prediction markets economic mechanisms aggregating information future events sequential interactions traders pricing mechanisms markets known related optimization algorithms machine learning connections understanding equilibrium market prices relate beliefs traders market however little known rates guarantees convergence sequential mechanisms recent papers cite important open question paper show previously studied prediction market trading models understood natural generalization randomized coordinate descent call randomized subspace descent rsd establish convergence rates rsd leverage prove rates prediction market models answering open questions results extend beyond standard centralized markets arbitrary trade networks
halting random walk kernels random walk kernels measure graph similarity counting matching walks graphs popular form geometric random walk kernels longer walks length downweighted factor lambda lambda ensure convergence corresponding geometric series know field link prediction downweighting often leads phenomenon referred halting longer walks downweighted much similarity score completely dominated comparison walks length naive kernel edges vertices theoretically show halting occur geometric random walk kernels also empirically quantify impact simulated datasets popular graph classification benchmark datasets findings promise instrumental future graph kernel development applications random walk kernels
class network models recoverable spectral clustering finding communities networks problem remains difficult spite amount attention recently received stochastic block model sbm generative model graphs communities simplicity theoretical understanding advanced fast recent years particular various results showing simple versions spectralclustering using normalized laplacian graph recoverthe communities almost perfectly high probability show essentially algorithm used sbm extension called degree corrected sbm works wider class block models call preference frame models essentially guarantees moreover parametrization introduce clearly exhibits free parameters needed specify class models results bounds expose clarity parameters control recovery error model class
fast accurate inference plackett luce models show maximum likelihood estimate models derived luce choice axiom plackett luce model expressed stationary distribution markov chain conveys insight several recently proposed spectral inference algorithms take advantage perspective formulate new spectral algorithm significantly accurate previous ones plackett luce model simple adaptation algorithm used iteratively producing sequence estimates converges estimate version runs faster competing approaches benchmark datasets algorithms easy implement making relevant practitioners large
nearly optimal private lasso present nearly optimal differentially private version well known lasso estimator algorithm provides privacy protection respect training data item excess risk algorithm compared non private version widetilde assuming input data bounded ell_ infty norm first differentially private algorithm achieves bound without polynomial dependence addition assumption design matrix addition show error bound nearly optimal amongst differentially private algorithms
robust feature sample linear discriminant analysis brain disorders diagnosis wide spectrum discriminative methods increasingly used diverse applications classification regression tasks however many existing discriminative methods assume input data nearly noise free limits applications solve real world problems particularly disease diagnosis data acquired neuroimaging devices always prone different sources noise robust discriminative models somewhat scarce attempts made make robust noise outliers methods focus detecting either sample outliers feature noises moreover usually use unsupervised noising procedures separately noise training testing data factors induce biases learning process thus limit performance paper propose classification method based least squares formulation linear discriminant analysis simultaneously detects sample outliers feature noises proposed method operates semi supervised setting labeled training unlabeled testing data incorporated form intrinsic geometry sample space therefore violating samples feature values identified sample outliers feature noises respectively test algorithm synthetic brain neurodegenerative databases particularly parkinson disease alzheimer disease results demonstrate method outperforms baseline state art methods terms accuracy area roc curve
tractable learning complex probability queries tractable learning aims learn probabilistic models inference guaranteed efficient however particular class queries tractable depends model underlying representation usually class mpe conditional probabilities joint assignments propose tractable learner guarantees efficient inference broader class queries simultaneously learns markov network tractable circuit representation order guarantee measure tractability approach differs earlier work using sentential decision diagrams sdd tractable language instead arithmetic circuits sdds desirable properties general representations acs lack enable basic primitives boolean circuit compilation allows support broader class complex probability queries including counting threshold parity polytime
variational dropout local reparameterization trick explore yet unexploited opportunity drastically improving efficiency stochastic gradient variational bayes sgvb global model parameters regular sgvb estimators rely sampling parameters per minibatch data variance constant minibatch size efficiency estimators drastically improved upon translating uncertainty global parameters local noise independent across datapoints minibatch reparameterizations local noise trivially parallelized variance inversely proportional minibatch size generally leading much faster convergence find important connection regularization dropout original gaussian dropout objective corresponds sgvb local noise scale invariant prior proportionally fixed posterior variance method allows inference flexibly parameterized posteriors specifically propose emph variational dropout generalization gaussian dropout flexibly parameterized posterior often leading better generalization method demonstrated several experiments
fast distributed center clustering outliers massive data clustering large data fundamental problem vast number applications due increasing size data practitioners interested clustering turned distributed computation methods work consider widely used center clustering problem variant used handle noisy data center outliers noise free setting demonstrate previously proposed distributed method actually approximation algorithm accurately explains strong empirical performance additionally noisy setting develop novel distributed algorithm also approximation algorithms highly parallel lend virtually distributed computing framework compare empirically best known noisy sequential clustering methods show distributed algorithms consistently close sequential versions algorithms hope distributed settings fast memory efficient match sequential counterparts
learning transduce unbounded memory recently strong results demonstrated deep recurrent neural networks natural language transduction problems paper explore representational power models using synthetic grammars designed exhibit phenomena similar found real transduction problems machine translation experiments lead propose new memory based recurrent networks implement continuously differentiable analogues traditional data structures stacks queues deques show architectures exhibit superior generalisation performance deep rnns often able learn underlying generating algorithms transduction experiments
robust means theoretical revisit last years many variations quadratic means clustering procedure proposed aiming robustify performance algorithm presence outliers general terms main approaches developed based penalized regularization methods based trimming functions work present theoretical analysis robustness consistency properties variant classical quadratic means algorithm robust means borrows ideas outlier detection regression show outliers dataset enough breakdown clustering procedure however focus well structured datasets robust means recover underlying cluster structure spite outliers finally show slight modifications general non asymptotic results consistency quadratic means remain valid robust variant
monotone submodular function maximization size constraints submodular function generalization submodular function input consists disjoint subsets instead single subset domain many machine learning problems including influence maximization kinds topics sensor placement kinds sensors naturally modeled problem maximizing monotone submodular functions paper give constant factor approximation algorithms maximizing monotone submodular functions subject several size constraints running time algorithms almost linear domain size experimentally demonstrate algorithms outperform baseline algorithms terms solution quality
fast randomized kernel ridge regression statistical guarantees approach improving running time kernel based methods build small sketch kernel matrix use lieu full matrix machine learning task interest describe version approach comes running time guarantees well improved guarantees statistical performance extending notion emph statistical leverage scores setting kernel ridge regression able identify sampling distribution reduces size sketch required number columns sampled emph effective dimensionality problem latter quantity often much smaller previous bounds depend emph maximal degrees freedom give empirical evidence supporting fact second contribution present fast algorithm quickly compute coarse approximations thesescores time linear number samples precisely running time algorithm depending trace kernel matrix regularization parameter obtained via variant squared length sampling adapt kernel setting lastly discuss new notion leverage data point captures fine notion difficulty learning problem
empirical localization homogeneous divergences discrete sample spaces paper propose novel parameter estimator probabilistic models discrete space proposed estimator derived minimization homogeneous divergence constructed without calculation normalization constant frequently infeasible models discrete space investigate statistical properties proposed estimator consistency asymptotic normality reveal relationship alpha divergence small experiments show proposed estimator attains comparable performance mle drastically lower computational cost
bandits unobserved confounders causal approach multi armed bandit problem constitutes archetypal setting sequential decision making permeating multiple domains including engineering business medicine hallmarks bandit setting agent capacity explore environment active intervention contrasts ability collect passive data estimating associational relationships actions payouts existence unobserved confounders namely unmeasured variables affecting action outcome variables implies data collection modes general coincide paper show formalizing distinction conceptual algorithmic implications bandit setting current generation bandit algorithms implicitly try maximize rewards based estimation experimental distribution show always best strategy pursue indeed achieve low regret certain realistic classes bandit problems namely face unobserved confounders experimental observational quantities required rational agent realization propose optimization metric employing experimental observational distributions bandit agents pursue illustrate benefits traditional algorithms
CONF: TOPICS	 ALPHA	 ETA
 =============================================================== 
20	0.1	0.01
 =============================================================== 
rethinking lda moment matching discrete ica consider moment matching techniques estimation latent dirichlet allocation lda drawing explicit links lda discrete versions independent component analysis ica first derive new set cumulant based tensors improved sample complexity moreover reuse standard ica techniques joint diagonalization tensors improve existing methods based tensor power method extensive set experiments synthetic real datasets show new combination tensors orthogonal joint diagonalization techniques outperforms existing moment matching methods
regularized algorithms unified framework statistical guarantees latent models fundamental modeling tool machine learning applications present significant computational analytical challenges popular algorithm variants much used algorithmic tool yet rigorous understanding performance highly incomplete recently work demonstrated important class problems exhibits linear local convergence high dimensional setting however step well defined address precisely setting unified treatment using regularization regularization high dimensional problems well understood iterative algorithm requires careful balancing making progress towards solution identifying right structure sparsity low rank particular regularizing step using state art high dimensional prescriptions guaranteed provide balance algorithm analysis linked way reveals balance optimization statistical errors specialize general framework sparse gaussian mixture models high dimensional mixed regression regression missing variables obtaining statistical guarantees examples
robust gaussian graphical modeling trimmed graphical lasso gaussian graphical models ggms popular tools studying network structures however many modern applications gene network discovery social interactions analysis often involve high dimensional noisy data outliers heavier tails gaussian distribution paper propose trimmed graphical lasso robust estimation sparse ggms method guards outliers implicit trimming mechanism akin popular least trimmed squares method used linear regression provide rigorous statistical analysis estimator high dimensional setting contrast existing approaches robust sparse ggms estimation lack statistical guarantees theoretical results complemented experiments simulated real gene expression data demonstrate value approach
fast accurate inference plackett luce models show maximum likelihood estimate models derived luce choice axiom plackett luce model expressed stationary distribution markov chain conveys insight several recently proposed spectral inference algorithms take advantage perspective formulate new spectral algorithm significantly accurate previous ones plackett luce model simple adaptation algorithm used iteratively producing sequence estimates converges estimate version runs faster competing approaches benchmark datasets algorithms easy implement making relevant practitioners large
robust feature sample linear discriminant analysis brain disorders diagnosis wide spectrum discriminative methods increasingly used diverse applications classification regression tasks however many existing discriminative methods assume input data nearly noise free limits applications solve real world problems particularly disease diagnosis data acquired neuroimaging devices always prone different sources noise robust discriminative models somewhat scarce attempts made make robust noise outliers methods focus detecting either sample outliers feature noises moreover usually use unsupervised noising procedures separately noise training testing data factors induce biases learning process thus limit performance paper propose classification method based least squares formulation linear discriminant analysis simultaneously detects sample outliers feature noises proposed method operates semi supervised setting labeled training unlabeled testing data incorporated form intrinsic geometry sample space therefore violating samples feature values identified sample outliers feature noises respectively test algorithm synthetic brain neurodegenerative databases particularly parkinson disease alzheimer disease results demonstrate method outperforms baseline state art methods terms accuracy area roc curve
variational dropout local reparameterization trick explore yet unexploited opportunity drastically improving efficiency stochastic gradient variational bayes sgvb global model parameters regular sgvb estimators rely sampling parameters per minibatch data variance constant minibatch size efficiency estimators drastically improved upon translating uncertainty global parameters local noise independent across datapoints minibatch reparameterizations local noise trivially parallelized variance inversely proportional minibatch size generally leading much faster convergence find important connection regularization dropout original gaussian dropout objective corresponds sgvb local noise scale invariant prior proportionally fixed posterior variance method allows inference flexibly parameterized posteriors specifically propose emph variational dropout generalization gaussian dropout flexibly parameterized posterior often leading better generalization method demonstrated several experiments
mixed robust average submodular partitioning fast algorithms guarantees applications investigate novel mixed robust average case submodular data partitioning problems collectively call submodular partitioning problems generalize purely robust instances problem namely max min submodular fair allocation sfa emph min max submodular load balancing slb also average case instances submodular welfare problem swp submodular multiway partition smp robust versions studied theory community existing work focused tight approximation guarantees resultant algorithms generally scalable large real world applications contrasts average case instances algorithms scalable present paper bridge gap proposing several new algorithms including greedy majorization minimization minorization maximization relaxation algorithms scale large datasets also achieve theoretical approximation guarantees comparable state art moreover provide new scalable algorithms apply additive combinations robust average case objectives show problems many applications machine learning including data partitioning load balancing distributed data clustering image segmentation empirically demonstrate efficacy algorithms real world problems involving data partitioning distributed optimization convex deep neural network objectives also purely unsupervised image segmentation
class network models recoverable spectral clustering finding communities networks problem remains difficult spite amount attention recently received stochastic block model sbm generative model graphs communities simplicity theoretical understanding advanced fast recent years particular various results showing simple versions spectralclustering using normalized laplacian graph recoverthe communities almost perfectly high probability show essentially algorithm used sbm extension called degree corrected sbm works wider class block models call preference frame models essentially guarantees moreover parametrization introduce clearly exhibits free parameters needed specify class models results bounds expose clarity parameters control recovery error model class
fast randomized kernel ridge regression statistical guarantees approach improving running time kernel based methods build small sketch kernel matrix use lieu full matrix machine learning task interest describe version approach comes running time guarantees well improved guarantees statistical performance extending notion emph statistical leverage scores setting kernel ridge regression able identify sampling distribution reduces size sketch required number columns sampled emph effective dimensionality problem latter quantity often much smaller previous bounds depend emph maximal degrees freedom give empirical evidence supporting fact second contribution present fast algorithm quickly compute coarse approximations thesescores time linear number samples precisely running time algorithm depending trace kernel matrix regularization parameter obtained via variant squared length sampling adapt kernel setting lastly discuss new notion leverage data point captures fine notion difficulty learning problem
learning transduce unbounded memory recently strong results demonstrated deep recurrent neural networks natural language transduction problems paper explore representational power models using synthetic grammars designed exhibit phenomena similar found real transduction problems machine translation experiments lead propose new memory based recurrent networks implement continuously differentiable analogues traditional data structures stacks queues deques show architectures exhibit superior generalisation performance deep rnns often able learn underlying generating algorithms transduction experiments
convergence analysis prediction markets via randomized subspace descent prediction markets economic mechanisms aggregating information future events sequential interactions traders pricing mechanisms markets known related optimization algorithms machine learning connections understanding equilibrium market prices relate beliefs traders market however little known rates guarantees convergence sequential mechanisms recent papers cite important open question paper show previously studied prediction market trading models understood natural generalization randomized coordinate descent call randomized subspace descent rsd establish convergence rates rsd leverage prove rates prediction market models answering open questions results extend beyond standard centralized markets arbitrary trade networks
faster ridge regression via subsampled randomized hadamard transform propose fast algorithm ridge regression number features much larger number observations standard way solve ridge regression setting works dual space gives running time algorithm srht drr runs time log works preconditioning design matrix randomized walsh hadamard transform subsequent subsampling features provide risk bounds srht drr algorithm fixed design setting show experimental results synthetic real datasets
nonconvex optimization framework low rank matrix estimation study estimation low rank matrices via nonconvex optimization compared convex relaxation nonconvex optimization exhibits superior empirical performance large scale instances low rank matrix estimation however understanding theoretical guarantees limited paper define notion projected oracle divergence based establish sufficient conditions success nonconvex optimization illustrate consequences general framework matrix sensing completion particular prove broad class nonconvex optimization algorithms including alternating minimization gradient type methods geometrically converge global optimum exactly recover true low rank matrices standard conditions
dimensionality reduction subspace structure preservation modeling data sampled union independent subspaces widely applied number real world applications however dimensionality reduction approaches theoretically preserve independence assumption well studied key contribution show projection vectors sufficient independence preservation class data sampled union independent subspaces non trivial observation use designing dimensionality reduction technique paper propose novel dimensionality reduction algorithm theoretically preserves structure given dataset support theoretical analysis empirical results synthetic real world data achieving textit state art results compared popular dimensionality reduction techniques
hessian free optimization learning deep multidimensional recurrent neural networks multidimensional recurrent neural networks mdrnns shown remarkable performance area speech handwriting recognition performance mdrnn improved increasing depth difficulty learning deeper network overcome using hessian free optimization given connectionist temporal classification ctc utilized objective learning mdrnn sequence labeling non convexity ctc poses problem applying network solution convex approximation ctc formulated relationship algorithm fisher information matrix discussed mdrnn depth layers successfully trained using resulting improved performance sequence labeling
bandits unobserved confounders causal approach multi armed bandit problem constitutes archetypal setting sequential decision making permeating multiple domains including engineering business medicine hallmarks bandit setting agent capacity explore environment active intervention contrasts ability collect passive data estimating associational relationships actions payouts existence unobserved confounders namely unmeasured variables affecting action outcome variables implies data collection modes general coincide paper show formalizing distinction conceptual algorithmic implications bandit setting current generation bandit algorithms implicitly try maximize rewards based estimation experimental distribution show always best strategy pursue indeed achieve low regret certain realistic classes bandit problems namely face unobserved confounders experimental observational quantities required rational agent realization propose optimization metric employing experimental observational distributions bandit agents pursue illustrate benefits traditional algorithms
fast guaranteed tensor decomposition via sketching tensor candecomp parafac decomposition wide applications statistical learning latent variable models data mining paper propose fast randomized tensor decomposition algorithms based sketching build idea count sketches introduce many novel ideas unique tensors develop novel methods randomized com putation tensor contractions via ffts without explicitly forming tensors tensor contractions encountered decomposition methods sor power iterations alternating least squares also design novel colliding hashes symmetric tensors save time computing sketches combine sketching ideas existing whitening tensor power iter ative techniques obtain fastest algorithm sparse dense tensors quality approximation method depend properties sparsity uniformity elements etc apply method topic mod eling obtain competitive results
spectral representations convolutional neural networks discrete fourier transforms provide significant speedup computation convolutions deep learning work demonstrate beyond advantages efficient computation spectral domain also provides powerful representation model train convolutional neural networks cnns employ spectral representations introduce number innovations cnn design first propose spectral pooling performs dimensionality reduction truncating representation frequency domain approach preserves considerably information per parameter pooling strategies enables flexibility choice pooling output dimensionality representation also enables new form stochastic regularization randomized modification resolution show methods achieve competitive results classification approximation tasks without using dropout max pooling finally demonstrate effectiveness complex coefficient spectral parameterization convolutional filters leaves underlying model unchanged results representation greatly facilitates optimization observe variety popular cnn configurations leads significantly faster convergence training
learning additive exponential family graphical models via ell_ norm regularized estimation investigate subclass exponential family graphical models sufficient statistics defined arbitrary additive forms propose ell_ norm regularized maximum likelihood estimators learn model parameters samples first joint mle estimator estimates parameters simultaneously second node wise conditional mle estimator estimates parameters node individually estimators statistical analysis shows mild conditions extra flexibility gained additive exponential family models comes almost cost statistical efficiency monte carlo approximation method developed efficiently optimize proposed estimators advantages estimators gaussian graphical models nonparanormal estimators demonstrated synthetic real data sets
spals fast alternating least squares via implicit leverage scores sampling tensor candecomp parafac decomposition powerful computationally challenging tool modern data analytics paper show ways sampling intermediate steps alternating minimization algorithms computing low rank tensor decompositions leading sparse alternating least squares spals method specifically sample khatri rao product arises intermediate object iterations alternating least squares product captures interactions different tensor modes form main computational bottleneck solving many tensor related tasks exploiting spectral structures matrix khatri rao product provide efficient access statistical leverage scores applied tensor decomposition method leads first algorithm runs sublinear time per iteration approximates output deterministic alternating least squares algorithms empirical evaluations approach show significantly speedups existing randomized deterministic routines performing decomposition tensor size 92k billion nonzeros formed amazon product reviews routine converges minutes error deterministic als
CONF: TOPICS	 ALPHA	 ETA
 =============================================================== 
20	0.1	0.01
 =============================================================== 
rethinking lda moment matching discrete ica consider moment matching techniques estimation latent dirichlet allocation lda drawing explicit links lda discrete versions independent component analysis ica first derive new set cumulant based tensors improved sample complexity moreover reuse standard ica techniques joint diagonalization tensors improve existing methods based tensor power method extensive set experiments synthetic real datasets show new combination tensors orthogonal joint diagonalization techniques outperforms existing moment matching methods
regularized algorithms unified framework statistical guarantees latent models fundamental modeling tool machine learning applications present significant computational analytical challenges popular algorithm variants much used algorithmic tool yet rigorous understanding performance highly incomplete recently work demonstrated important class problems exhibits linear local convergence high dimensional setting however step well defined address precisely setting unified treatment using regularization regularization high dimensional problems well understood iterative algorithm requires careful balancing making progress towards solution identifying right structure sparsity low rank particular regularizing step using state art high dimensional prescriptions guaranteed provide balance algorithm analysis linked way reveals balance optimization statistical errors specialize general framework sparse gaussian mixture models high dimensional mixed regression regression missing variables obtaining statistical guarantees examples
robust gaussian graphical modeling trimmed graphical lasso gaussian graphical models ggms popular tools studying network structures however many modern applications gene network discovery social interactions analysis often involve high dimensional noisy data outliers heavier tails gaussian distribution paper propose trimmed graphical lasso robust estimation sparse ggms method guards outliers implicit trimming mechanism akin popular least trimmed squares method used linear regression provide rigorous statistical analysis estimator high dimensional setting contrast existing approaches robust sparse ggms estimation lack statistical guarantees theoretical results complemented experiments simulated real gene expression data demonstrate value approach
fast accurate inference plackett luce models show maximum likelihood estimate models derived luce choice axiom plackett luce model expressed stationary distribution markov chain conveys insight several recently proposed spectral inference algorithms take advantage perspective formulate new spectral algorithm significantly accurate previous ones plackett luce model simple adaptation algorithm used iteratively producing sequence estimates converges estimate version runs faster competing approaches benchmark datasets algorithms easy implement making relevant practitioners large
robust feature sample linear discriminant analysis brain disorders diagnosis wide spectrum discriminative methods increasingly used diverse applications classification regression tasks however many existing discriminative methods assume input data nearly noise free limits applications solve real world problems particularly disease diagnosis data acquired neuroimaging devices always prone different sources noise robust discriminative models somewhat scarce attempts made make robust noise outliers methods focus detecting either sample outliers feature noises moreover usually use unsupervised noising procedures separately noise training testing data factors induce biases learning process thus limit performance paper propose classification method based least squares formulation linear discriminant analysis simultaneously detects sample outliers feature noises proposed method operates semi supervised setting labeled training unlabeled testing data incorporated form intrinsic geometry sample space therefore violating samples feature values identified sample outliers feature noises respectively test algorithm synthetic brain neurodegenerative databases particularly parkinson disease alzheimer disease results demonstrate method outperforms baseline state art methods terms accuracy area roc curve
variational dropout local reparameterization trick explore yet unexploited opportunity drastically improving efficiency stochastic gradient variational bayes sgvb global model parameters regular sgvb estimators rely sampling parameters per minibatch data variance constant minibatch size efficiency estimators drastically improved upon translating uncertainty global parameters local noise independent across datapoints minibatch reparameterizations local noise trivially parallelized variance inversely proportional minibatch size generally leading much faster convergence find important connection regularization dropout original gaussian dropout objective corresponds sgvb local noise scale invariant prior proportionally fixed posterior variance method allows inference flexibly parameterized posteriors specifically propose emph variational dropout generalization gaussian dropout flexibly parameterized posterior often leading better generalization method demonstrated several experiments
mixed robust average submodular partitioning fast algorithms guarantees applications investigate novel mixed robust average case submodular data partitioning problems collectively call submodular partitioning problems generalize purely robust instances problem namely max min submodular fair allocation sfa emph min max submodular load balancing slb also average case instances submodular welfare problem swp submodular multiway partition smp robust versions studied theory community existing work focused tight approximation guarantees resultant algorithms generally scalable large real world applications contrasts average case instances algorithms scalable present paper bridge gap proposing several new algorithms including greedy majorization minimization minorization maximization relaxation algorithms scale large datasets also achieve theoretical approximation guarantees comparable state art moreover provide new scalable algorithms apply additive combinations robust average case objectives show problems many applications machine learning including data partitioning load balancing distributed data clustering image segmentation empirically demonstrate efficacy algorithms real world problems involving data partitioning distributed optimization convex deep neural network objectives also purely unsupervised image segmentation
class network models recoverable spectral clustering finding communities networks problem remains difficult spite amount attention recently received stochastic block model sbm generative model graphs communities simplicity theoretical understanding advanced fast recent years particular various results showing simple versions spectralclustering using normalized laplacian graph recoverthe communities almost perfectly high probability show essentially algorithm used sbm extension called degree corrected sbm works wider class block models call preference frame models essentially guarantees moreover parametrization introduce clearly exhibits free parameters needed specify class models results bounds expose clarity parameters control recovery error model class
fast randomized kernel ridge regression statistical guarantees approach improving running time kernel based methods build small sketch kernel matrix use lieu full matrix machine learning task interest describe version approach comes running time guarantees well improved guarantees statistical performance extending notion emph statistical leverage scores setting kernel ridge regression able identify sampling distribution reduces size sketch required number columns sampled emph effective dimensionality problem latter quantity often much smaller previous bounds depend emph maximal degrees freedom give empirical evidence supporting fact second contribution present fast algorithm quickly compute coarse approximations thesescores time linear number samples precisely running time algorithm depending trace kernel matrix regularization parameter obtained via variant squared length sampling adapt kernel setting lastly discuss new notion leverage data point captures fine notion difficulty learning problem
learning transduce unbounded memory recently strong results demonstrated deep recurrent neural networks natural language transduction problems paper explore representational power models using synthetic grammars designed exhibit phenomena similar found real transduction problems machine translation experiments lead propose new memory based recurrent networks implement continuously differentiable analogues traditional data structures stacks queues deques show architectures exhibit superior generalisation performance deep rnns often able learn underlying generating algorithms transduction experiments
convergence analysis prediction markets via randomized subspace descent prediction markets economic mechanisms aggregating information future events sequential interactions traders pricing mechanisms markets known related optimization algorithms machine learning connections understanding equilibrium market prices relate beliefs traders market however little known rates guarantees convergence sequential mechanisms recent papers cite important open question paper show previously studied prediction market trading models understood natural generalization randomized coordinate descent call randomized subspace descent rsd establish convergence rates rsd leverage prove rates prediction market models answering open questions results extend beyond standard centralized markets arbitrary trade networks
faster ridge regression via subsampled randomized hadamard transform propose fast algorithm ridge regression number features much larger number observations standard way solve ridge regression setting works dual space gives running time algorithm srht drr runs time log works preconditioning design matrix randomized walsh hadamard transform subsequent subsampling features provide risk bounds srht drr algorithm fixed design setting show experimental results synthetic real datasets
nonconvex optimization framework low rank matrix estimation study estimation low rank matrices via nonconvex optimization compared convex relaxation nonconvex optimization exhibits superior empirical performance large scale instances low rank matrix estimation however understanding theoretical guarantees limited paper define notion projected oracle divergence based establish sufficient conditions success nonconvex optimization illustrate consequences general framework matrix sensing completion particular prove broad class nonconvex optimization algorithms including alternating minimization gradient type methods geometrically converge global optimum exactly recover true low rank matrices standard conditions
dimensionality reduction subspace structure preservation modeling data sampled union independent subspaces widely applied number real world applications however dimensionality reduction approaches theoretically preserve independence assumption well studied key contribution show projection vectors sufficient independence preservation class data sampled union independent subspaces non trivial observation use designing dimensionality reduction technique paper propose novel dimensionality reduction algorithm theoretically preserves structure given dataset support theoretical analysis empirical results synthetic real world data achieving textit state art results compared popular dimensionality reduction techniques
hessian free optimization learning deep multidimensional recurrent neural networks multidimensional recurrent neural networks mdrnns shown remarkable performance area speech handwriting recognition performance mdrnn improved increasing depth difficulty learning deeper network overcome using hessian free optimization given connectionist temporal classification ctc utilized objective learning mdrnn sequence labeling non convexity ctc poses problem applying network solution convex approximation ctc formulated relationship algorithm fisher information matrix discussed mdrnn depth layers successfully trained using resulting improved performance sequence labeling
bandits unobserved confounders causal approach multi armed bandit problem constitutes archetypal setting sequential decision making permeating multiple domains including engineering business medicine hallmarks bandit setting agent capacity explore environment active intervention contrasts ability collect passive data estimating associational relationships actions payouts existence unobserved confounders namely unmeasured variables affecting action outcome variables implies data collection modes general coincide paper show formalizing distinction conceptual algorithmic implications bandit setting current generation bandit algorithms implicitly try maximize rewards based estimation experimental distribution show always best strategy pursue indeed achieve low regret certain realistic classes bandit problems namely face unobserved confounders experimental observational quantities required rational agent realization propose optimization metric employing experimental observational distributions bandit agents pursue illustrate benefits traditional algorithms
fast guaranteed tensor decomposition via sketching tensor candecomp parafac decomposition wide applications statistical learning latent variable models data mining paper propose fast randomized tensor decomposition algorithms based sketching build idea count sketches introduce many novel ideas unique tensors develop novel methods randomized com putation tensor contractions via ffts without explicitly forming tensors tensor contractions encountered decomposition methods sor power iterations alternating least squares also design novel colliding hashes symmetric tensors save time computing sketches combine sketching ideas existing whitening tensor power iter ative techniques obtain fastest algorithm sparse dense tensors quality approximation method depend properties sparsity uniformity elements etc apply method topic mod eling obtain competitive results
spectral representations convolutional neural networks discrete fourier transforms provide significant speedup computation convolutions deep learning work demonstrate beyond advantages efficient computation spectral domain also provides powerful representation model train convolutional neural networks cnns employ spectral representations introduce number innovations cnn design first propose spectral pooling performs dimensionality reduction truncating representation frequency domain approach preserves considerably information per parameter pooling strategies enables flexibility choice pooling output dimensionality representation also enables new form stochastic regularization randomized modification resolution show methods achieve competitive results classification approximation tasks without using dropout max pooling finally demonstrate effectiveness complex coefficient spectral parameterization convolutional filters leaves underlying model unchanged results representation greatly facilitates optimization observe variety popular cnn configurations leads significantly faster convergence training
learning additive exponential family graphical models via ell_ norm regularized estimation investigate subclass exponential family graphical models sufficient statistics defined arbitrary additive forms propose ell_ norm regularized maximum likelihood estimators learn model parameters samples first joint mle estimator estimates parameters simultaneously second node wise conditional mle estimator estimates parameters node individually estimators statistical analysis shows mild conditions extra flexibility gained additive exponential family models comes almost cost statistical efficiency monte carlo approximation method developed efficiently optimize proposed estimators advantages estimators gaussian graphical models nonparanormal estimators demonstrated synthetic real data sets
spals fast alternating least squares via implicit leverage scores sampling tensor candecomp parafac decomposition powerful computationally challenging tool modern data analytics paper show ways sampling intermediate steps alternating minimization algorithms computing low rank tensor decompositions leading sparse alternating least squares spals method specifically sample khatri rao product arises intermediate object iterations alternating least squares product captures interactions different tensor modes form main computational bottleneck solving many tensor related tasks exploiting spectral structures matrix khatri rao product provide efficient access statistical leverage scores applied tensor decomposition method leads first algorithm runs sublinear time per iteration approximates output deterministic alternating least squares algorithms empirical evaluations approach show significantly speedups existing randomized deterministic routines performing decomposition tensor size 92k billion nonzeros formed amazon product reviews routine converges minutes error deterministic als
