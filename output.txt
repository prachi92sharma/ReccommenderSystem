CONF: TOPICS	 ALPHA	 ETA
 =============================================================== 
10	0.1	0.01
 =============================================================== 
subspace clustering irrelevant features via robust dantzig selector paper considers subspace clustering problem data contains irrelevant corrupted features propose method termed robust dantzig selector successfully identify clustering structure even presence irrelevant features idea simple yet powerful replace inner product robust counterpart insensitive irrelevant features given upper bound number irrelevant features establish theoretical guarantees algorithm identify correct subspace demonstrate effectiveness algorithm via numerical simulations best knowledge first method developed tackle subspace clustering irrelevant features
robust gaussian graphical modeling trimmed graphical lasso gaussian graphical models ggms popular tools studying network structures however many modern applications gene network discovery social interactions analysis often involve high dimensional noisy data outliers heavier tails gaussian distribution paper propose trimmed graphical lasso robust estimation sparse ggms method guards outliers implicit trimming mechanism akin popular least trimmed squares method used linear regression provide rigorous statistical analysis estimator high dimensional setting contrast existing approaches robust sparse ggms estimation lack statistical guarantees theoretical results complemented experiments simulated real gene expression data demonstrate value approach
regularized algorithms unified framework statistical guarantees latent models fundamental modeling tool machine learning applications present significant computational analytical challenges popular algorithm variants much used algorithmic tool yet rigorous understanding performance highly incomplete recently work demonstrated important class problems exhibits linear local convergence high dimensional setting however step well defined address precisely setting unified treatment using regularization regularization high dimensional problems well understood iterative algorithm requires careful balancing making progress towards solution identifying right structure sparsity low rank particular regularizing step using state art high dimensional prescriptions guaranteed provide balance algorithm analysis linked way reveals balance optimization statistical errors specialize general framework sparse gaussian mixture models high dimensional mixed regression regression missing variables obtaining statistical guarantees examples
mixed robust average submodular partitioning fast algorithms guarantees applications investigate novel mixed robust average case submodular data partitioning problems collectively call submodular partitioning problems generalize purely robust instances problem namely max min submodular fair allocation sfa emph min max submodular load balancing slb also average case instances submodular welfare problem swp submodular multiway partition smp robust versions studied theory community existing work focused tight approximation guarantees resultant algorithms generally scalable large real world applications contrasts average case instances algorithms scalable present paper bridge gap proposing several new algorithms including greedy majorization minimization minorization maximization relaxation algorithms scale large datasets also achieve theoretical approximation guarantees comparable state art moreover provide new scalable algorithms apply additive combinations robust average case objectives show problems many applications machine learning including data partitioning load balancing distributed data clustering image segmentation empirically demonstrate efficacy algorithms real world problems involving data partitioning distributed optimization convex deep neural network objectives also purely unsupervised image segmentation
calibrated structured prediction user facing applications displaying calibrated confidence measures probabilities correspond true frequency important obtaining high accuracy interested calibration structured prediction problems speech recognition optical character recognition medical diagnosis structured prediction presents new challenges calibration output space large users issue many types probability queries marginals structured output extend notion calibration handle various subtleties pertaining structured setting provide simple recalibration method trains binary classifier predict probabilities interest explore range features appropriate structured recalibration demonstrate efficacy real world datasets
convergence analysis prediction markets via randomized subspace descent prediction markets economic mechanisms aggregating information future events sequential interactions traders pricing mechanisms markets known related optimization algorithms machine learning connections understanding equilibrium market prices relate beliefs traders market however little known rates guarantees convergence sequential mechanisms recent papers cite important open question paper show previously studied prediction market trading models understood natural generalization randomized coordinate descent call randomized subspace descent rsd establish convergence rates rsd leverage prove rates prediction market models answering open questions results extend beyond standard centralized markets arbitrary trade networks
halting random walk kernels random walk kernels measure graph similarity counting matching walks graphs popular form geometric random walk kernels longer walks length downweighted factor lambda lambda ensure convergence corresponding geometric series know field link prediction downweighting often leads phenomenon referred halting longer walks downweighted much similarity score completely dominated comparison walks length naive kernel edges vertices theoretically show halting occur geometric random walk kernels also empirically quantify impact simulated datasets popular graph classification benchmark datasets findings promise instrumental future graph kernel development applications random walk kernels
class network models recoverable spectral clustering finding communities networks problem remains difficult spite amount attention recently received stochastic block model sbm generative model graphs communities simplicity theoretical understanding advanced fast recent years particular various results showing simple versions spectralclustering using normalized laplacian graph recoverthe communities almost perfectly high probability show essentially algorithm used sbm extension called degree corrected sbm works wider class block models call preference frame models essentially guarantees moreover parametrization introduce clearly exhibits free parameters needed specify class models results bounds expose clarity parameters control recovery error model class
fast accurate inference plackett luce models show maximum likelihood estimate models derived luce choice axiom plackett luce model expressed stationary distribution markov chain conveys insight several recently proposed spectral inference algorithms take advantage perspective formulate new spectral algorithm significantly accurate previous ones plackett luce model simple adaptation algorithm used iteratively producing sequence estimates converges estimate version runs faster competing approaches benchmark datasets algorithms easy implement making relevant practitioners large
nearly optimal private lasso present nearly optimal differentially private version well known lasso estimator algorithm provides privacy protection respect training data item excess risk algorithm compared non private version widetilde assuming input data bounded ell_ infty norm first differentially private algorithm achieves bound without polynomial dependence addition assumption design matrix addition show error bound nearly optimal amongst differentially private algorithms
robust feature sample linear discriminant analysis brain disorders diagnosis wide spectrum discriminative methods increasingly used diverse applications classification regression tasks however many existing discriminative methods assume input data nearly noise free limits applications solve real world problems particularly disease diagnosis data acquired neuroimaging devices always prone different sources noise robust discriminative models somewhat scarce attempts made make robust noise outliers methods focus detecting either sample outliers feature noises moreover usually use unsupervised noising procedures separately noise training testing data factors induce biases learning process thus limit performance paper propose classification method based least squares formulation linear discriminant analysis simultaneously detects sample outliers feature noises proposed method operates semi supervised setting labeled training unlabeled testing data incorporated form intrinsic geometry sample space therefore violating samples feature values identified sample outliers feature noises respectively test algorithm synthetic brain neurodegenerative databases particularly parkinson disease alzheimer disease results demonstrate method outperforms baseline state art methods terms accuracy area roc curve
tractable learning complex probability queries tractable learning aims learn probabilistic models inference guaranteed efficient however particular class queries tractable depends model underlying representation usually class mpe conditional probabilities joint assignments propose tractable learner guarantees efficient inference broader class queries simultaneously learns markov network tractable circuit representation order guarantee measure tractability approach differs earlier work using sentential decision diagrams sdd tractable language instead arithmetic circuits sdds desirable properties general representations acs lack enable basic primitives boolean circuit compilation allows support broader class complex probability queries including counting threshold parity polytime
variational dropout local reparameterization trick explore yet unexploited opportunity drastically improving efficiency stochastic gradient variational bayes sgvb global model parameters regular sgvb estimators rely sampling parameters per minibatch data variance constant minibatch size efficiency estimators drastically improved upon translating uncertainty global parameters local noise independent across datapoints minibatch reparameterizations local noise trivially parallelized variance inversely proportional minibatch size generally leading much faster convergence find important connection regularization dropout original gaussian dropout objective corresponds sgvb local noise scale invariant prior proportionally fixed posterior variance method allows inference flexibly parameterized posteriors specifically propose emph variational dropout generalization gaussian dropout flexibly parameterized posterior often leading better generalization method demonstrated several experiments
fast distributed center clustering outliers massive data clustering large data fundamental problem vast number applications due increasing size data practitioners interested clustering turned distributed computation methods work consider widely used center clustering problem variant used handle noisy data center outliers noise free setting demonstrate previously proposed distributed method actually approximation algorithm accurately explains strong empirical performance additionally noisy setting develop novel distributed algorithm also approximation algorithms highly parallel lend virtually distributed computing framework compare empirically best known noisy sequential clustering methods show distributed algorithms consistently close sequential versions algorithms hope distributed settings fast memory efficient match sequential counterparts
learning transduce unbounded memory recently strong results demonstrated deep recurrent neural networks natural language transduction problems paper explore representational power models using synthetic grammars designed exhibit phenomena similar found real transduction problems machine translation experiments lead propose new memory based recurrent networks implement continuously differentiable analogues traditional data structures stacks queues deques show architectures exhibit superior generalisation performance deep rnns often able learn underlying generating algorithms transduction experiments
robust means theoretical revisit last years many variations quadratic means clustering procedure proposed aiming robustify performance algorithm presence outliers general terms main approaches developed based penalized regularization methods based trimming functions work present theoretical analysis robustness consistency properties variant classical quadratic means algorithm robust means borrows ideas outlier detection regression show outliers dataset enough breakdown clustering procedure however focus well structured datasets robust means recover underlying cluster structure spite outliers finally show slight modifications general non asymptotic results consistency quadratic means remain valid robust variant
monotone submodular function maximization size constraints submodular function generalization submodular function input consists disjoint subsets instead single subset domain many machine learning problems including influence maximization kinds topics sensor placement kinds sensors naturally modeled problem maximizing monotone submodular functions paper give constant factor approximation algorithms maximizing monotone submodular functions subject several size constraints running time algorithms almost linear domain size experimentally demonstrate algorithms outperform baseline algorithms terms solution quality
fast randomized kernel ridge regression statistical guarantees approach improving running time kernel based methods build small sketch kernel matrix use lieu full matrix machine learning task interest describe version approach comes running time guarantees well improved guarantees statistical performance extending notion emph statistical leverage scores setting kernel ridge regression able identify sampling distribution reduces size sketch required number columns sampled emph effective dimensionality problem latter quantity often much smaller previous bounds depend emph maximal degrees freedom give empirical evidence supporting fact second contribution present fast algorithm quickly compute coarse approximations thesescores time linear number samples precisely running time algorithm depending trace kernel matrix regularization parameter obtained via variant squared length sampling adapt kernel setting lastly discuss new notion leverage data point captures fine notion difficulty learning problem
empirical localization homogeneous divergences discrete sample spaces paper propose novel parameter estimator probabilistic models discrete space proposed estimator derived minimization homogeneous divergence constructed without calculation normalization constant frequently infeasible models discrete space investigate statistical properties proposed estimator consistency asymptotic normality reveal relationship alpha divergence small experiments show proposed estimator attains comparable performance mle drastically lower computational cost
bandits unobserved confounders causal approach multi armed bandit problem constitutes archetypal setting sequential decision making permeating multiple domains including engineering business medicine hallmarks bandit setting agent capacity explore environment active intervention contrasts ability collect passive data estimating associational relationships actions payouts existence unobserved confounders namely unmeasured variables affecting action outcome variables implies data collection modes general coincide paper show formalizing distinction conceptual algorithmic implications bandit setting current generation bandit algorithms implicitly try maximize rewards based estimation experimental distribution show always best strategy pursue indeed achieve low regret certain realistic classes bandit problems namely face unobserved confounders experimental observational quantities required rational agent realization propose optimization metric employing experimental observational distributions bandit agents pursue illustrate benefits traditional algorithms
CONF: TOPICS	 ALPHA	 ETA
 =============================================================== 
20	0.1	0.01
 =============================================================== 
rethinking lda moment matching discrete ica consider moment matching techniques estimation latent dirichlet allocation lda drawing explicit links lda discrete versions independent component analysis ica first derive new set cumulant based tensors improved sample complexity moreover reuse standard ica techniques joint diagonalization tensors improve existing methods based tensor power method extensive set experiments synthetic real datasets show new combination tensors orthogonal joint diagonalization techniques outperforms existing moment matching methods
regularized algorithms unified framework statistical guarantees latent models fundamental modeling tool machine learning applications present significant computational analytical challenges popular algorithm variants much used algorithmic tool yet rigorous understanding performance highly incomplete recently work demonstrated important class problems exhibits linear local convergence high dimensional setting however step well defined address precisely setting unified treatment using regularization regularization high dimensional problems well understood iterative algorithm requires careful balancing making progress towards solution identifying right structure sparsity low rank particular regularizing step using state art high dimensional prescriptions guaranteed provide balance algorithm analysis linked way reveals balance optimization statistical errors specialize general framework sparse gaussian mixture models high dimensional mixed regression regression missing variables obtaining statistical guarantees examples
robust gaussian graphical modeling trimmed graphical lasso gaussian graphical models ggms popular tools studying network structures however many modern applications gene network discovery social interactions analysis often involve high dimensional noisy data outliers heavier tails gaussian distribution paper propose trimmed graphical lasso robust estimation sparse ggms method guards outliers implicit trimming mechanism akin popular least trimmed squares method used linear regression provide rigorous statistical analysis estimator high dimensional setting contrast existing approaches robust sparse ggms estimation lack statistical guarantees theoretical results complemented experiments simulated real gene expression data demonstrate value approach
fast accurate inference plackett luce models show maximum likelihood estimate models derived luce choice axiom plackett luce model expressed stationary distribution markov chain conveys insight several recently proposed spectral inference algorithms take advantage perspective formulate new spectral algorithm significantly accurate previous ones plackett luce model simple adaptation algorithm used iteratively producing sequence estimates converges estimate version runs faster competing approaches benchmark datasets algorithms easy implement making relevant practitioners large
robust feature sample linear discriminant analysis brain disorders diagnosis wide spectrum discriminative methods increasingly used diverse applications classification regression tasks however many existing discriminative methods assume input data nearly noise free limits applications solve real world problems particularly disease diagnosis data acquired neuroimaging devices always prone different sources noise robust discriminative models somewhat scarce attempts made make robust noise outliers methods focus detecting either sample outliers feature noises moreover usually use unsupervised noising procedures separately noise training testing data factors induce biases learning process thus limit performance paper propose classification method based least squares formulation linear discriminant analysis simultaneously detects sample outliers feature noises proposed method operates semi supervised setting labeled training unlabeled testing data incorporated form intrinsic geometry sample space therefore violating samples feature values identified sample outliers feature noises respectively test algorithm synthetic brain neurodegenerative databases particularly parkinson disease alzheimer disease results demonstrate method outperforms baseline state art methods terms accuracy area roc curve
variational dropout local reparameterization trick explore yet unexploited opportunity drastically improving efficiency stochastic gradient variational bayes sgvb global model parameters regular sgvb estimators rely sampling parameters per minibatch data variance constant minibatch size efficiency estimators drastically improved upon translating uncertainty global parameters local noise independent across datapoints minibatch reparameterizations local noise trivially parallelized variance inversely proportional minibatch size generally leading much faster convergence find important connection regularization dropout original gaussian dropout objective corresponds sgvb local noise scale invariant prior proportionally fixed posterior variance method allows inference flexibly parameterized posteriors specifically propose emph variational dropout generalization gaussian dropout flexibly parameterized posterior often leading better generalization method demonstrated several experiments
mixed robust average submodular partitioning fast algorithms guarantees applications investigate novel mixed robust average case submodular data partitioning problems collectively call submodular partitioning problems generalize purely robust instances problem namely max min submodular fair allocation sfa emph min max submodular load balancing slb also average case instances submodular welfare problem swp submodular multiway partition smp robust versions studied theory community existing work focused tight approximation guarantees resultant algorithms generally scalable large real world applications contrasts average case instances algorithms scalable present paper bridge gap proposing several new algorithms including greedy majorization minimization minorization maximization relaxation algorithms scale large datasets also achieve theoretical approximation guarantees comparable state art moreover provide new scalable algorithms apply additive combinations robust average case objectives show problems many applications machine learning including data partitioning load balancing distributed data clustering image segmentation empirically demonstrate efficacy algorithms real world problems involving data partitioning distributed optimization convex deep neural network objectives also purely unsupervised image segmentation
class network models recoverable spectral clustering finding communities networks problem remains difficult spite amount attention recently received stochastic block model sbm generative model graphs communities simplicity theoretical understanding advanced fast recent years particular various results showing simple versions spectralclustering using normalized laplacian graph recoverthe communities almost perfectly high probability show essentially algorithm used sbm extension called degree corrected sbm works wider class block models call preference frame models essentially guarantees moreover parametrization introduce clearly exhibits free parameters needed specify class models results bounds expose clarity parameters control recovery error model class
fast randomized kernel ridge regression statistical guarantees approach improving running time kernel based methods build small sketch kernel matrix use lieu full matrix machine learning task interest describe version approach comes running time guarantees well improved guarantees statistical performance extending notion emph statistical leverage scores setting kernel ridge regression able identify sampling distribution reduces size sketch required number columns sampled emph effective dimensionality problem latter quantity often much smaller previous bounds depend emph maximal degrees freedom give empirical evidence supporting fact second contribution present fast algorithm quickly compute coarse approximations thesescores time linear number samples precisely running time algorithm depending trace kernel matrix regularization parameter obtained via variant squared length sampling adapt kernel setting lastly discuss new notion leverage data point captures fine notion difficulty learning problem
learning transduce unbounded memory recently strong results demonstrated deep recurrent neural networks natural language transduction problems paper explore representational power models using synthetic grammars designed exhibit phenomena similar found real transduction problems machine translation experiments lead propose new memory based recurrent networks implement continuously differentiable analogues traditional data structures stacks queues deques show architectures exhibit superior generalisation performance deep rnns often able learn underlying generating algorithms transduction experiments
convergence analysis prediction markets via randomized subspace descent prediction markets economic mechanisms aggregating information future events sequential interactions traders pricing mechanisms markets known related optimization algorithms machine learning connections understanding equilibrium market prices relate beliefs traders market however little known rates guarantees convergence sequential mechanisms recent papers cite important open question paper show previously studied prediction market trading models understood natural generalization randomized coordinate descent call randomized subspace descent rsd establish convergence rates rsd leverage prove rates prediction market models answering open questions results extend beyond standard centralized markets arbitrary trade networks
faster ridge regression via subsampled randomized hadamard transform propose fast algorithm ridge regression number features much larger number observations standard way solve ridge regression setting works dual space gives running time algorithm srht drr runs time log works preconditioning design matrix randomized walsh hadamard transform subsequent subsampling features provide risk bounds srht drr algorithm fixed design setting show experimental results synthetic real datasets
nonconvex optimization framework low rank matrix estimation study estimation low rank matrices via nonconvex optimization compared convex relaxation nonconvex optimization exhibits superior empirical performance large scale instances low rank matrix estimation however understanding theoretical guarantees limited paper define notion projected oracle divergence based establish sufficient conditions success nonconvex optimization illustrate consequences general framework matrix sensing completion particular prove broad class nonconvex optimization algorithms including alternating minimization gradient type methods geometrically converge global optimum exactly recover true low rank matrices standard conditions
dimensionality reduction subspace structure preservation modeling data sampled union independent subspaces widely applied number real world applications however dimensionality reduction approaches theoretically preserve independence assumption well studied key contribution show projection vectors sufficient independence preservation class data sampled union independent subspaces non trivial observation use designing dimensionality reduction technique paper propose novel dimensionality reduction algorithm theoretically preserves structure given dataset support theoretical analysis empirical results synthetic real world data achieving textit state art results compared popular dimensionality reduction techniques
hessian free optimization learning deep multidimensional recurrent neural networks multidimensional recurrent neural networks mdrnns shown remarkable performance area speech handwriting recognition performance mdrnn improved increasing depth difficulty learning deeper network overcome using hessian free optimization given connectionist temporal classification ctc utilized objective learning mdrnn sequence labeling non convexity ctc poses problem applying network solution convex approximation ctc formulated relationship algorithm fisher information matrix discussed mdrnn depth layers successfully trained using resulting improved performance sequence labeling
bandits unobserved confounders causal approach multi armed bandit problem constitutes archetypal setting sequential decision making permeating multiple domains including engineering business medicine hallmarks bandit setting agent capacity explore environment active intervention contrasts ability collect passive data estimating associational relationships actions payouts existence unobserved confounders namely unmeasured variables affecting action outcome variables implies data collection modes general coincide paper show formalizing distinction conceptual algorithmic implications bandit setting current generation bandit algorithms implicitly try maximize rewards based estimation experimental distribution show always best strategy pursue indeed achieve low regret certain realistic classes bandit problems namely face unobserved confounders experimental observational quantities required rational agent realization propose optimization metric employing experimental observational distributions bandit agents pursue illustrate benefits traditional algorithms
fast guaranteed tensor decomposition via sketching tensor candecomp parafac decomposition wide applications statistical learning latent variable models data mining paper propose fast randomized tensor decomposition algorithms based sketching build idea count sketches introduce many novel ideas unique tensors develop novel methods randomized com putation tensor contractions via ffts without explicitly forming tensors tensor contractions encountered decomposition methods sor power iterations alternating least squares also design novel colliding hashes symmetric tensors save time computing sketches combine sketching ideas existing whitening tensor power iter ative techniques obtain fastest algorithm sparse dense tensors quality approximation method depend properties sparsity uniformity elements etc apply method topic mod eling obtain competitive results
spectral representations convolutional neural networks discrete fourier transforms provide significant speedup computation convolutions deep learning work demonstrate beyond advantages efficient computation spectral domain also provides powerful representation model train convolutional neural networks cnns employ spectral representations introduce number innovations cnn design first propose spectral pooling performs dimensionality reduction truncating representation frequency domain approach preserves considerably information per parameter pooling strategies enables flexibility choice pooling output dimensionality representation also enables new form stochastic regularization randomized modification resolution show methods achieve competitive results classification approximation tasks without using dropout max pooling finally demonstrate effectiveness complex coefficient spectral parameterization convolutional filters leaves underlying model unchanged results representation greatly facilitates optimization observe variety popular cnn configurations leads significantly faster convergence training
learning additive exponential family graphical models via ell_ norm regularized estimation investigate subclass exponential family graphical models sufficient statistics defined arbitrary additive forms propose ell_ norm regularized maximum likelihood estimators learn model parameters samples first joint mle estimator estimates parameters simultaneously second node wise conditional mle estimator estimates parameters node individually estimators statistical analysis shows mild conditions extra flexibility gained additive exponential family models comes almost cost statistical efficiency monte carlo approximation method developed efficiently optimize proposed estimators advantages estimators gaussian graphical models nonparanormal estimators demonstrated synthetic real data sets
spals fast alternating least squares via implicit leverage scores sampling tensor candecomp parafac decomposition powerful computationally challenging tool modern data analytics paper show ways sampling intermediate steps alternating minimization algorithms computing low rank tensor decompositions leading sparse alternating least squares spals method specifically sample khatri rao product arises intermediate object iterations alternating least squares product captures interactions different tensor modes form main computational bottleneck solving many tensor related tasks exploiting spectral structures matrix khatri rao product provide efficient access statistical leverage scores applied tensor decomposition method leads first algorithm runs sublinear time per iteration approximates output deterministic alternating least squares algorithms empirical evaluations approach show significantly speedups existing randomized deterministic routines performing decomposition tensor size 92k billion nonzeros formed amazon product reviews routine converges minutes error deterministic als
CONF: TOPICS	 ALPHA	 ETA
 =============================================================== 
20	0.1	0.01
 =============================================================== 
rethinking lda moment matching discrete ica consider moment matching techniques estimation latent dirichlet allocation lda drawing explicit links lda discrete versions independent component analysis ica first derive new set cumulant based tensors improved sample complexity moreover reuse standard ica techniques joint diagonalization tensors improve existing methods based tensor power method extensive set experiments synthetic real datasets show new combination tensors orthogonal joint diagonalization techniques outperforms existing moment matching methods
regularized algorithms unified framework statistical guarantees latent models fundamental modeling tool machine learning applications present significant computational analytical challenges popular algorithm variants much used algorithmic tool yet rigorous understanding performance highly incomplete recently work demonstrated important class problems exhibits linear local convergence high dimensional setting however step well defined address precisely setting unified treatment using regularization regularization high dimensional problems well understood iterative algorithm requires careful balancing making progress towards solution identifying right structure sparsity low rank particular regularizing step using state art high dimensional prescriptions guaranteed provide balance algorithm analysis linked way reveals balance optimization statistical errors specialize general framework sparse gaussian mixture models high dimensional mixed regression regression missing variables obtaining statistical guarantees examples
robust gaussian graphical modeling trimmed graphical lasso gaussian graphical models ggms popular tools studying network structures however many modern applications gene network discovery social interactions analysis often involve high dimensional noisy data outliers heavier tails gaussian distribution paper propose trimmed graphical lasso robust estimation sparse ggms method guards outliers implicit trimming mechanism akin popular least trimmed squares method used linear regression provide rigorous statistical analysis estimator high dimensional setting contrast existing approaches robust sparse ggms estimation lack statistical guarantees theoretical results complemented experiments simulated real gene expression data demonstrate value approach
fast accurate inference plackett luce models show maximum likelihood estimate models derived luce choice axiom plackett luce model expressed stationary distribution markov chain conveys insight several recently proposed spectral inference algorithms take advantage perspective formulate new spectral algorithm significantly accurate previous ones plackett luce model simple adaptation algorithm used iteratively producing sequence estimates converges estimate version runs faster competing approaches benchmark datasets algorithms easy implement making relevant practitioners large
robust feature sample linear discriminant analysis brain disorders diagnosis wide spectrum discriminative methods increasingly used diverse applications classification regression tasks however many existing discriminative methods assume input data nearly noise free limits applications solve real world problems particularly disease diagnosis data acquired neuroimaging devices always prone different sources noise robust discriminative models somewhat scarce attempts made make robust noise outliers methods focus detecting either sample outliers feature noises moreover usually use unsupervised noising procedures separately noise training testing data factors induce biases learning process thus limit performance paper propose classification method based least squares formulation linear discriminant analysis simultaneously detects sample outliers feature noises proposed method operates semi supervised setting labeled training unlabeled testing data incorporated form intrinsic geometry sample space therefore violating samples feature values identified sample outliers feature noises respectively test algorithm synthetic brain neurodegenerative databases particularly parkinson disease alzheimer disease results demonstrate method outperforms baseline state art methods terms accuracy area roc curve
variational dropout local reparameterization trick explore yet unexploited opportunity drastically improving efficiency stochastic gradient variational bayes sgvb global model parameters regular sgvb estimators rely sampling parameters per minibatch data variance constant minibatch size efficiency estimators drastically improved upon translating uncertainty global parameters local noise independent across datapoints minibatch reparameterizations local noise trivially parallelized variance inversely proportional minibatch size generally leading much faster convergence find important connection regularization dropout original gaussian dropout objective corresponds sgvb local noise scale invariant prior proportionally fixed posterior variance method allows inference flexibly parameterized posteriors specifically propose emph variational dropout generalization gaussian dropout flexibly parameterized posterior often leading better generalization method demonstrated several experiments
mixed robust average submodular partitioning fast algorithms guarantees applications investigate novel mixed robust average case submodular data partitioning problems collectively call submodular partitioning problems generalize purely robust instances problem namely max min submodular fair allocation sfa emph min max submodular load balancing slb also average case instances submodular welfare problem swp submodular multiway partition smp robust versions studied theory community existing work focused tight approximation guarantees resultant algorithms generally scalable large real world applications contrasts average case instances algorithms scalable present paper bridge gap proposing several new algorithms including greedy majorization minimization minorization maximization relaxation algorithms scale large datasets also achieve theoretical approximation guarantees comparable state art moreover provide new scalable algorithms apply additive combinations robust average case objectives show problems many applications machine learning including data partitioning load balancing distributed data clustering image segmentation empirically demonstrate efficacy algorithms real world problems involving data partitioning distributed optimization convex deep neural network objectives also purely unsupervised image segmentation
class network models recoverable spectral clustering finding communities networks problem remains difficult spite amount attention recently received stochastic block model sbm generative model graphs communities simplicity theoretical understanding advanced fast recent years particular various results showing simple versions spectralclustering using normalized laplacian graph recoverthe communities almost perfectly high probability show essentially algorithm used sbm extension called degree corrected sbm works wider class block models call preference frame models essentially guarantees moreover parametrization introduce clearly exhibits free parameters needed specify class models results bounds expose clarity parameters control recovery error model class
fast randomized kernel ridge regression statistical guarantees approach improving running time kernel based methods build small sketch kernel matrix use lieu full matrix machine learning task interest describe version approach comes running time guarantees well improved guarantees statistical performance extending notion emph statistical leverage scores setting kernel ridge regression able identify sampling distribution reduces size sketch required number columns sampled emph effective dimensionality problem latter quantity often much smaller previous bounds depend emph maximal degrees freedom give empirical evidence supporting fact second contribution present fast algorithm quickly compute coarse approximations thesescores time linear number samples precisely running time algorithm depending trace kernel matrix regularization parameter obtained via variant squared length sampling adapt kernel setting lastly discuss new notion leverage data point captures fine notion difficulty learning problem
learning transduce unbounded memory recently strong results demonstrated deep recurrent neural networks natural language transduction problems paper explore representational power models using synthetic grammars designed exhibit phenomena similar found real transduction problems machine translation experiments lead propose new memory based recurrent networks implement continuously differentiable analogues traditional data structures stacks queues deques show architectures exhibit superior generalisation performance deep rnns often able learn underlying generating algorithms transduction experiments
convergence analysis prediction markets via randomized subspace descent prediction markets economic mechanisms aggregating information future events sequential interactions traders pricing mechanisms markets known related optimization algorithms machine learning connections understanding equilibrium market prices relate beliefs traders market however little known rates guarantees convergence sequential mechanisms recent papers cite important open question paper show previously studied prediction market trading models understood natural generalization randomized coordinate descent call randomized subspace descent rsd establish convergence rates rsd leverage prove rates prediction market models answering open questions results extend beyond standard centralized markets arbitrary trade networks
faster ridge regression via subsampled randomized hadamard transform propose fast algorithm ridge regression number features much larger number observations standard way solve ridge regression setting works dual space gives running time algorithm srht drr runs time log works preconditioning design matrix randomized walsh hadamard transform subsequent subsampling features provide risk bounds srht drr algorithm fixed design setting show experimental results synthetic real datasets
nonconvex optimization framework low rank matrix estimation study estimation low rank matrices via nonconvex optimization compared convex relaxation nonconvex optimization exhibits superior empirical performance large scale instances low rank matrix estimation however understanding theoretical guarantees limited paper define notion projected oracle divergence based establish sufficient conditions success nonconvex optimization illustrate consequences general framework matrix sensing completion particular prove broad class nonconvex optimization algorithms including alternating minimization gradient type methods geometrically converge global optimum exactly recover true low rank matrices standard conditions
dimensionality reduction subspace structure preservation modeling data sampled union independent subspaces widely applied number real world applications however dimensionality reduction approaches theoretically preserve independence assumption well studied key contribution show projection vectors sufficient independence preservation class data sampled union independent subspaces non trivial observation use designing dimensionality reduction technique paper propose novel dimensionality reduction algorithm theoretically preserves structure given dataset support theoretical analysis empirical results synthetic real world data achieving textit state art results compared popular dimensionality reduction techniques
hessian free optimization learning deep multidimensional recurrent neural networks multidimensional recurrent neural networks mdrnns shown remarkable performance area speech handwriting recognition performance mdrnn improved increasing depth difficulty learning deeper network overcome using hessian free optimization given connectionist temporal classification ctc utilized objective learning mdrnn sequence labeling non convexity ctc poses problem applying network solution convex approximation ctc formulated relationship algorithm fisher information matrix discussed mdrnn depth layers successfully trained using resulting improved performance sequence labeling
bandits unobserved confounders causal approach multi armed bandit problem constitutes archetypal setting sequential decision making permeating multiple domains including engineering business medicine hallmarks bandit setting agent capacity explore environment active intervention contrasts ability collect passive data estimating associational relationships actions payouts existence unobserved confounders namely unmeasured variables affecting action outcome variables implies data collection modes general coincide paper show formalizing distinction conceptual algorithmic implications bandit setting current generation bandit algorithms implicitly try maximize rewards based estimation experimental distribution show always best strategy pursue indeed achieve low regret certain realistic classes bandit problems namely face unobserved confounders experimental observational quantities required rational agent realization propose optimization metric employing experimental observational distributions bandit agents pursue illustrate benefits traditional algorithms
fast guaranteed tensor decomposition via sketching tensor candecomp parafac decomposition wide applications statistical learning latent variable models data mining paper propose fast randomized tensor decomposition algorithms based sketching build idea count sketches introduce many novel ideas unique tensors develop novel methods randomized com putation tensor contractions via ffts without explicitly forming tensors tensor contractions encountered decomposition methods sor power iterations alternating least squares also design novel colliding hashes symmetric tensors save time computing sketches combine sketching ideas existing whitening tensor power iter ative techniques obtain fastest algorithm sparse dense tensors quality approximation method depend properties sparsity uniformity elements etc apply method topic mod eling obtain competitive results
spectral representations convolutional neural networks discrete fourier transforms provide significant speedup computation convolutions deep learning work demonstrate beyond advantages efficient computation spectral domain also provides powerful representation model train convolutional neural networks cnns employ spectral representations introduce number innovations cnn design first propose spectral pooling performs dimensionality reduction truncating representation frequency domain approach preserves considerably information per parameter pooling strategies enables flexibility choice pooling output dimensionality representation also enables new form stochastic regularization randomized modification resolution show methods achieve competitive results classification approximation tasks without using dropout max pooling finally demonstrate effectiveness complex coefficient spectral parameterization convolutional filters leaves underlying model unchanged results representation greatly facilitates optimization observe variety popular cnn configurations leads significantly faster convergence training
learning additive exponential family graphical models via ell_ norm regularized estimation investigate subclass exponential family graphical models sufficient statistics defined arbitrary additive forms propose ell_ norm regularized maximum likelihood estimators learn model parameters samples first joint mle estimator estimates parameters simultaneously second node wise conditional mle estimator estimates parameters node individually estimators statistical analysis shows mild conditions extra flexibility gained additive exponential family models comes almost cost statistical efficiency monte carlo approximation method developed efficiently optimize proposed estimators advantages estimators gaussian graphical models nonparanormal estimators demonstrated synthetic real data sets
spals fast alternating least squares via implicit leverage scores sampling tensor candecomp parafac decomposition powerful computationally challenging tool modern data analytics paper show ways sampling intermediate steps alternating minimization algorithms computing low rank tensor decompositions leading sparse alternating least squares spals method specifically sample khatri rao product arises intermediate object iterations alternating least squares product captures interactions different tensor modes form main computational bottleneck solving many tensor related tasks exploiting spectral structures matrix khatri rao product provide efficient access statistical leverage scores applied tensor decomposition method leads first algorithm runs sublinear time per iteration approximates output deterministic alternating least squares algorithms empirical evaluations approach show significantly speedups existing randomized deterministic routines performing decomposition tensor size 92k billion nonzeros formed amazon product reviews routine converges minutes error deterministic als
<<<<<<< HEAD
=======
CONF: TOPICS	 ALPHA	 ETA
 =============================================================== 
20	0.01	0.01
 =============================================================== 
regularized algorithms unified framework statistical guarantees latent models fundamental modeling tool machine learning applications present significant computational analytical challenges popular algorithm variants much used algorithmic tool yet rigorous understanding performance highly incomplete recently work demonstrated important class problems exhibits linear local convergence high dimensional setting however step well defined address precisely setting unified treatment using regularization regularization high dimensional problems well understood iterative algorithm requires careful balancing making progress towards solution identifying right structure sparsity low rank particular regularizing step using state art high dimensional prescriptions guaranteed provide balance algorithm analysis linked way reveals balance optimization statistical errors specialize general framework sparse gaussian mixture models high dimensional mixed regression regression missing variables obtaining statistical guarantees examples
robust gaussian graphical modeling trimmed graphical lasso gaussian graphical models ggms popular tools studying network structures however many modern applications gene network discovery social interactions analysis often involve high dimensional noisy data outliers heavier tails gaussian distribution paper propose trimmed graphical lasso robust estimation sparse ggms method guards outliers implicit trimming mechanism akin popular least trimmed squares method used linear regression provide rigorous statistical analysis estimator high dimensional setting contrast existing approaches robust sparse ggms estimation lack statistical guarantees theoretical results complemented experiments simulated real gene expression data demonstrate value approach
rethinking lda moment matching discrete ica consider moment matching techniques estimation latent dirichlet allocation lda drawing explicit links lda discrete versions independent component analysis ica first derive new set cumulant based tensors improved sample complexity moreover reuse standard ica techniques joint diagonalization tensors improve existing methods based tensor power method extensive set experiments synthetic real datasets show new combination tensors orthogonal joint diagonalization techniques outperforms existing moment matching methods
fast accurate inference plackett luce models show maximum likelihood estimate models derived luce choice axiom plackett luce model expressed stationary distribution markov chain conveys insight several recently proposed spectral inference algorithms take advantage perspective formulate new spectral algorithm significantly accurate previous ones plackett luce model simple adaptation algorithm used iteratively producing sequence estimates converges estimate version runs faster competing approaches benchmark datasets algorithms easy implement making relevant practitioners large
mixed robust average submodular partitioning fast algorithms guarantees applications investigate novel mixed robust average case submodular data partitioning problems collectively call submodular partitioning problems generalize purely robust instances problem namely max min submodular fair allocation sfa emph min max submodular load balancing slb also average case instances submodular welfare problem swp submodular multiway partition smp robust versions studied theory community existing work focused tight approximation guarantees resultant algorithms generally scalable large real world applications contrasts average case instances algorithms scalable present paper bridge gap proposing several new algorithms including greedy majorization minimization minorization maximization relaxation algorithms scale large datasets also achieve theoretical approximation guarantees comparable state art moreover provide new scalable algorithms apply additive combinations robust average case objectives show problems many applications machine learning including data partitioning load balancing distributed data clustering image segmentation empirically demonstrate efficacy algorithms real world problems involving data partitioning distributed optimization convex deep neural network objectives also purely unsupervised image segmentation
robust feature sample linear discriminant analysis brain disorders diagnosis wide spectrum discriminative methods increasingly used diverse applications classification regression tasks however many existing discriminative methods assume input data nearly noise free limits applications solve real world problems particularly disease diagnosis data acquired neuroimaging devices always prone different sources noise robust discriminative models somewhat scarce attempts made make robust noise outliers methods focus detecting either sample outliers feature noises moreover usually use unsupervised noising procedures separately noise training testing data factors induce biases learning process thus limit performance paper propose classification method based least squares formulation linear discriminant analysis simultaneously detects sample outliers feature noises proposed method operates semi supervised setting labeled training unlabeled testing data incorporated form intrinsic geometry sample space therefore violating samples feature values identified sample outliers feature noises respectively test algorithm synthetic brain neurodegenerative databases particularly parkinson disease alzheimer disease results demonstrate method outperforms baseline state art methods terms accuracy area roc curve
fast randomized kernel ridge regression statistical guarantees approach improving running time kernel based methods build small sketch kernel matrix use lieu full matrix machine learning task interest describe version approach comes running time guarantees well improved guarantees statistical performance extending notion emph statistical leverage scores setting kernel ridge regression able identify sampling distribution reduces size sketch required number columns sampled emph effective dimensionality problem latter quantity often much smaller previous bounds depend emph maximal degrees freedom give empirical evidence supporting fact second contribution present fast algorithm quickly compute coarse approximations thesescores time linear number samples precisely running time algorithm depending trace kernel matrix regularization parameter obtained via variant squared length sampling adapt kernel setting lastly discuss new notion leverage data point captures fine notion difficulty learning problem
variational dropout local reparameterization trick explore yet unexploited opportunity drastically improving efficiency stochastic gradient variational bayes sgvb global model parameters regular sgvb estimators rely sampling parameters per minibatch data variance constant minibatch size efficiency estimators drastically improved upon translating uncertainty global parameters local noise independent across datapoints minibatch reparameterizations local noise trivially parallelized variance inversely proportional minibatch size generally leading much faster convergence find important connection regularization dropout original gaussian dropout objective corresponds sgvb local noise scale invariant prior proportionally fixed posterior variance method allows inference flexibly parameterized posteriors specifically propose emph variational dropout generalization gaussian dropout flexibly parameterized posterior often leading better generalization method demonstrated several experiments
bandits unobserved confounders causal approach multi armed bandit problem constitutes archetypal setting sequential decision making permeating multiple domains including engineering business medicine hallmarks bandit setting agent capacity explore environment active intervention contrasts ability collect passive data estimating associational relationships actions payouts existence unobserved confounders namely unmeasured variables affecting action outcome variables implies data collection modes general coincide paper show formalizing distinction conceptual algorithmic implications bandit setting current generation bandit algorithms implicitly try maximize rewards based estimation experimental distribution show always best strategy pursue indeed achieve low regret certain realistic classes bandit problems namely face unobserved confounders experimental observational quantities required rational agent realization propose optimization metric employing experimental observational distributions bandit agents pursue illustrate benefits traditional algorithms
convergence analysis prediction markets via randomized subspace descent prediction markets economic mechanisms aggregating information future events sequential interactions traders pricing mechanisms markets known related optimization algorithms machine learning connections understanding equilibrium market prices relate beliefs traders market however little known rates guarantees convergence sequential mechanisms recent papers cite important open question paper show previously studied prediction market trading models understood natural generalization randomized coordinate descent call randomized subspace descent rsd establish convergence rates rsd leverage prove rates prediction market models answering open questions results extend beyond standard centralized markets arbitrary trade networks
nonconvex optimization framework low rank matrix estimation study estimation low rank matrices via nonconvex optimization compared convex relaxation nonconvex optimization exhibits superior empirical performance large scale instances low rank matrix estimation however understanding theoretical guarantees limited paper define notion projected oracle divergence based establish sufficient conditions success nonconvex optimization illustrate consequences general framework matrix sensing completion particular prove broad class nonconvex optimization algorithms including alternating minimization gradient type methods geometrically converge global optimum exactly recover true low rank matrices standard conditions
learning transduce unbounded memory recently strong results demonstrated deep recurrent neural networks natural language transduction problems paper explore representational power models using synthetic grammars designed exhibit phenomena similar found real transduction problems machine translation experiments lead propose new memory based recurrent networks implement continuously differentiable analogues traditional data structures stacks queues deques show architectures exhibit superior generalisation performance deep rnns often able learn underlying generating algorithms transduction experiments
multi class svms tighter data dependent generalization bounds novel algorithms paper studies generalization performance multi class classification algorithms obtain first time data dependent generalization error bound logarithmic dependence class size substantially improving state art linear dependence existing data dependent generalization analysis theoretical analysis motivates introduce new multi class classification machine based norm regularization parameter controls complexity corresponding bounds derive efficient optimization algorithm based fenchel duality theory benchmarks several real world datasets show proposed algorithm achieve significant accuracy gains state art
group additive structure identification kernel nonparametric regression additive model popularly used models high dimensional nonparametric regression analysis however main drawback neglects possible interactions predictor variables paper reexamine group additive model proposed literature rigorously define intrinsic group additive structure relationship response variable predictor vector vect develop effective structure penalized kernel method simultaneous identification intrinsic group additive structure nonparametric function estimation method utilizes novel complexity measure derive group additive structures show proposed method consistent identifying intrinsic group additive structure simulation study real data applications demonstrate effectiveness proposed method general tool high dimensional nonparametric regression
dimensionality reduction subspace structure preservation modeling data sampled union independent subspaces widely applied number real world applications however dimensionality reduction approaches theoretically preserve independence assumption well studied key contribution show projection vectors sufficient independence preservation class data sampled union independent subspaces non trivial observation use designing dimensionality reduction technique paper propose novel dimensionality reduction algorithm theoretically preserves structure given dataset support theoretical analysis empirical results synthetic real world data achieving textit state art results compared popular dimensionality reduction techniques
nearly optimal private lasso present nearly optimal differentially private version well known lasso estimator algorithm provides privacy protection respect training data item excess risk algorithm compared non private version widetilde assuming input data bounded ell_ infty norm first differentially private algorithm achieves bound without polynomial dependence addition assumption design matrix addition show error bound nearly optimal amongst differentially private algorithms
general tensor spectral clustering higher order data spectral clustering clustering well known techniques data analysis recent work extended spectral clustering square symmetric tensors hypermatrices derived network develop new tensor spectral clustering method simultaneously clusters rows columns slices nonnegative mode tensor generalizes tensors number modes algorithm based new random walk model call super spacey random surfer show method performs state art clustering methods several synthetic datasets ground truth clusters use algorithm analyze several real world datasets
empirical localization homogeneous divergences discrete sample spaces paper propose novel parameter estimator probabilistic models discrete space proposed estimator derived minimization homogeneous divergence constructed without calculation normalization constant frequently infeasible models discrete space investigate statistical properties proposed estimator consistency asymptotic normality reveal relationship alpha divergence small experiments show proposed estimator attains comparable performance mle drastically lower computational cost
simultaneously leveraging output task structures multiple output regression multiple output regression models require estimating multiple functions output improve parameter estimation models methods based structural regularization model parameters usually needed paper present multiple output regression model leverages covariance structure functions multiple functions related well conditional covariance structure outputs contrast existing methods usually take account structures importantly unlike existing methods none structures need known priori model learned data several previously proposed structural regularization based multiple output regression models turn special cases model moreover addition rich model multiple output regression model also used estimating graphical model structure set variables multivariate outputs conditioned another set variables inputs experimental results synthetic real datasets demonstrate effectiveness method
polynomial time form robust regression despite variety robust regression methods developed current regression formulations either hard allow unbounded response even single leverage point present general formulation robust regression variational estimation unifies number robust regression methods allowing tractable approximation strategy develop estimator requires polynomial time achieving certain robustness consistency guarantees experimental evaluation demonstrates effectiveness new estimation approach compared standard methods
CONF: TOPICS	 ALPHA	 ETA
 =============================================================== 
25	0.01	0.01
 =============================================================== 
calibrated structured prediction user facing applications displaying calibrated confidence measures probabilities correspond true frequency important obtaining high accuracy interested calibration structured prediction problems speech recognition optical character recognition medical diagnosis structured prediction presents new challenges calibration output space large users issue many types probability queries marginals structured output extend notion calibration handle various subtleties pertaining structured setting provide simple recalibration method trains binary classifier predict probabilities interest explore range features appropriate structured recalibration demonstrate efficacy real world datasets
fast accurate inference plackett luce models show maximum likelihood estimate models derived luce choice axiom plackett luce model expressed stationary distribution markov chain conveys insight several recently proposed spectral inference algorithms take advantage perspective formulate new spectral algorithm significantly accurate previous ones plackett luce model simple adaptation algorithm used iteratively producing sequence estimates converges estimate version runs faster competing approaches benchmark datasets algorithms easy implement making relevant practitioners large
mixed robust average submodular partitioning fast algorithms guarantees applications investigate novel mixed robust average case submodular data partitioning problems collectively call submodular partitioning problems generalize purely robust instances problem namely max min submodular fair allocation sfa emph min max submodular load balancing slb also average case instances submodular welfare problem swp submodular multiway partition smp robust versions studied theory community existing work focused tight approximation guarantees resultant algorithms generally scalable large real world applications contrasts average case instances algorithms scalable present paper bridge gap proposing several new algorithms including greedy majorization minimization minorization maximization relaxation algorithms scale large datasets also achieve theoretical approximation guarantees comparable state art moreover provide new scalable algorithms apply additive combinations robust average case objectives show problems many applications machine learning including data partitioning load balancing distributed data clustering image segmentation empirically demonstrate efficacy algorithms real world problems involving data partitioning distributed optimization convex deep neural network objectives also purely unsupervised image segmentation
class network models recoverable spectral clustering finding communities networks problem remains difficult spite amount attention recently received stochastic block model sbm generative model graphs communities simplicity theoretical understanding advanced fast recent years particular various results showing simple versions spectralclustering using normalized laplacian graph recoverthe communities almost perfectly high probability show essentially algorithm used sbm extension called degree corrected sbm works wider class block models call preference frame models essentially guarantees moreover parametrization introduce clearly exhibits free parameters needed specify class models results bounds expose clarity parameters control recovery error model class
regularized algorithms unified framework statistical guarantees latent models fundamental modeling tool machine learning applications present significant computational analytical challenges popular algorithm variants much used algorithmic tool yet rigorous understanding performance highly incomplete recently work demonstrated important class problems exhibits linear local convergence high dimensional setting however step well defined address precisely setting unified treatment using regularization regularization high dimensional problems well understood iterative algorithm requires careful balancing making progress towards solution identifying right structure sparsity low rank particular regularizing step using state art high dimensional prescriptions guaranteed provide balance algorithm analysis linked way reveals balance optimization statistical errors specialize general framework sparse gaussian mixture models high dimensional mixed regression regression missing variables obtaining statistical guarantees examples
convergence analysis prediction markets via randomized subspace descent prediction markets economic mechanisms aggregating information future events sequential interactions traders pricing mechanisms markets known related optimization algorithms machine learning connections understanding equilibrium market prices relate beliefs traders market however little known rates guarantees convergence sequential mechanisms recent papers cite important open question paper show previously studied prediction market trading models understood natural generalization randomized coordinate descent call randomized subspace descent rsd establish convergence rates rsd leverage prove rates prediction market models answering open questions results extend beyond standard centralized markets arbitrary trade networks
variational dropout local reparameterization trick explore yet unexploited opportunity drastically improving efficiency stochastic gradient variational bayes sgvb global model parameters regular sgvb estimators rely sampling parameters per minibatch data variance constant minibatch size efficiency estimators drastically improved upon translating uncertainty global parameters local noise independent across datapoints minibatch reparameterizations local noise trivially parallelized variance inversely proportional minibatch size generally leading much faster convergence find important connection regularization dropout original gaussian dropout objective corresponds sgvb local noise scale invariant prior proportionally fixed posterior variance method allows inference flexibly parameterized posteriors specifically propose emph variational dropout generalization gaussian dropout flexibly parameterized posterior often leading better generalization method demonstrated several experiments
robust gaussian graphical modeling trimmed graphical lasso gaussian graphical models ggms popular tools studying network structures however many modern applications gene network discovery social interactions analysis often involve high dimensional noisy data outliers heavier tails gaussian distribution paper propose trimmed graphical lasso robust estimation sparse ggms method guards outliers implicit trimming mechanism akin popular least trimmed squares method used linear regression provide rigorous statistical analysis estimator high dimensional setting contrast existing approaches robust sparse ggms estimation lack statistical guarantees theoretical results complemented experiments simulated real gene expression data demonstrate value approach
decomposition bounds marginal map marginal map inference involves making map predictions systems defined latent variables missing information significantly difficult pure marginalization map tasks large class efficient convergent variational algorithms dual decomposition exist work generalize dual decomposition generic powered sum inference task includes marginal map along pure marginalization map special cases method based block coordinate descent algorithm new convex decomposition bound guaranteed converge monotonically parallelized efficiently demonstrate approach various inference queries real world problems uai approximate inference challenge showing framework faster reliable previous methods
sum squares lower bounds sparse pca paper establishes statistical versus computational trade offfor solving basic high dimensional machine learning problem via basic convex relaxation method specifically consider sparse principal component analysis sparse pca problem family sum squares sos aka lasserre parillo convex relaxations well known large dimension planted sparse unit vector principle detected using approx log gaussian bernoulli samples efficient polynomial time algorithms known require approx samples also known quadratic gap cannot improved basic semi definite sdp aka spectral relaxation equivalent degree sos algorithms prove also degree sos algorithms cannot improve quadratic gap average case lower bound adds small collection hardness results machine learning powerful family convex relaxation algorithms moreover design moments pseudo expectations lower bound quite different previous lower bounds establishing lower bounds higher degree sos algorithms remains challenging problem
cyclic coordinate descent beats randomized coordinate descent coordinate descent methods seen resurgence recent interest applicability machine learning well large scale data analysis superior empirical performance methods variants cyclic coordinate descent ccd randomized coordinate descent rcd deterministic randomized versions methods light recent results literature common perception rcd always dominates ccd terms performance paper question perception provide examples generally problem classes ccd deterministic order faster rcd terms asymptotic worst case convergence furthermore provide lower upper bounds amount improvement rate deterministic relative rcd amount improvement depend deterministic order used also provide characterization best deterministic order leads maximum improvement convergence rate terms combinatorial properties hessian matrix objective function
robust feature sample linear discriminant analysis brain disorders diagnosis wide spectrum discriminative methods increasingly used diverse applications classification regression tasks however many existing discriminative methods assume input data nearly noise free limits applications solve real world problems particularly disease diagnosis data acquired neuroimaging devices always prone different sources noise robust discriminative models somewhat scarce attempts made make robust noise outliers methods focus detecting either sample outliers feature noises moreover usually use unsupervised noising procedures separately noise training testing data factors induce biases learning process thus limit performance paper propose classification method based least squares formulation linear discriminant analysis simultaneously detects sample outliers feature noises proposed method operates semi supervised setting labeled training unlabeled testing data incorporated form intrinsic geometry sample space therefore violating samples feature values identified sample outliers feature noises respectively test algorithm synthetic brain neurodegenerative databases particularly parkinson disease alzheimer disease results demonstrate method outperforms baseline state art methods terms accuracy area roc curve
spectral representations convolutional neural networks discrete fourier transforms provide significant speedup computation convolutions deep learning work demonstrate beyond advantages efficient computation spectral domain also provides powerful representation model train convolutional neural networks cnns employ spectral representations introduce number innovations cnn design first propose spectral pooling performs dimensionality reduction truncating representation frequency domain approach preserves considerably information per parameter pooling strategies enables flexibility choice pooling output dimensionality representation also enables new form stochastic regularization randomized modification resolution show methods achieve competitive results classification approximation tasks without using dropout max pooling finally demonstrate effectiveness complex coefficient spectral parameterization convolutional filters leaves underlying model unchanged results representation greatly facilitates optimization observe variety popular cnn configurations leads significantly faster convergence training
multi class svms tighter data dependent generalization bounds novel algorithms paper studies generalization performance multi class classification algorithms obtain first time data dependent generalization error bound logarithmic dependence class size substantially improving state art linear dependence existing data dependent generalization analysis theoretical analysis motivates introduce new multi class classification machine based norm regularization parameter controls complexity corresponding bounds derive efficient optimization algorithm based fenchel duality theory benchmarks several real world datasets show proposed algorithm achieve significant accuracy gains state art
feature clustering accelerating parallel coordinate descent large scale ell_1 regularized loss minimization problems arise numerous applications compressed sensing high dimensional supervised learning including classification regression problems high performance algorithms implementations critical efficiently solving problems building upon previous work coordinate descent algorithms ell_1 regularized problems introduce novel family algorithms called block greedy coordinate descent includes special cases several existing algorithms scd greedy shotgun thread greedy give unified convergence analysis family block greedy algorithms analysis suggests block greedy coordinate descent better exploit parallelism features clustered maximum inner product features different blocks small theoretical convergence analysis supported experimental results using data diverse real world applications hope algorithmic approaches convergence analysis provide advance field also encourage researchers systematically explore design space algorithms solving large scale ell_1 regularization problems
nonconvex optimization framework low rank matrix estimation study estimation low rank matrices via nonconvex optimization compared convex relaxation nonconvex optimization exhibits superior empirical performance large scale instances low rank matrix estimation however understanding theoretical guarantees limited paper define notion projected oracle divergence based establish sufficient conditions success nonconvex optimization illustrate consequences general framework matrix sensing completion particular prove broad class nonconvex optimization algorithms including alternating minimization gradient type methods geometrically converge global optimum exactly recover true low rank matrices standard conditions
block coordinate descent approach large scale sparse inverse covariance estimation sparse inverse covariance estimation problem arises many statistical applications machine learning signal processing problem inverse covariance matrix multivariate normal distribution estimated assuming sparse ell_1 regularized log determinant optimization problem typically solved approximate matrices memory limitations existing algorithms unable handle large scale instances problem paper present new block coordinate descent approach solving problem large scale data sets method treats sought matrix block block using quadratic approximations show approach advantages existing methods several aspects numerical experiments synthetic real gene expression data demonstrate approach outperforms existing state art methods especially large scale problems
exploiting tradeoffs exact recovery heterogeneous stochastic block models stochastic block model sbm widely used random graph model networks communities despite recent burst interest community detection sbm statistical computational points view still gaps understanding fundamental limits recovery paper consider sbm full generality restriction number sizes communities grow number nodes well connectivity probabilities inside across communities stochastic block models provide guarantees exact recovery via semidefinite program well upper lower bounds sbm parameters exact recoverability results exploit tradeoffs among various parameters heterogenous sbm provide recovery guarantees many new interesting sbm configurations
large scale canonical correlation analysis iterative least squares canonical correlation analysis cca widely used statistical tool well established theory favorable performance wide range machine learning problems however computing cca huge datasets slow since involves implementing decomposition singular value decomposition huge matrices paper introduce cca iterative algorithm compute cca fast huge sparse datasets theory asymptotic convergence finite time accuracy cca established experiments also show cca outperform fast cca approximation schemes real datasets
regression tree tuning streaming setting consider problem maintaining data structures partition based regression procedure setting training data arrives sequentially time prove possible maintain structure time log time step achieving nearly optimal regression rate tilde terms unknown metric dimension finally prove new regression lower bound independent given data size hence appropriate streaming setting
CONF: TOPICS	 ALPHA	 ETA
 =============================================================== 
25	0.01	0.01
 =============================================================== 
robust spectral inference joint stochastic matrix factorization spectral inference provides fast algorithms provable optimality latent topic analysis real data algorithms require additional hoc heuristics even often produce unusable results explain poor performance casting problem topic inference framework joint stochastic matrix factorization jsmf showing previous methods violate theoretical conditions necessary good solution exist propose novel rectification method learns high quality topics interactions even small noisy data method achieves results comparable probabilistic techniques several domains maintaining scalability provable optimality
orthogonal nmf subspace exploration orthogonal nonnegative matrix factorization onmf aims approximate nonnegative matrix product dimensional nonnegative factors orthonormal columns yields potentially useful data representations superposition disjoint parts shown work well clustering tasks traditional methods underperform existing algorithms rely mostly heuristics despite good empirical performance lack provable performance guarantees present new onmf algorithm provable approximation guarantees constant dimension obtain additive eptas without assumptions input algorithm relies novel approximation related nonnegative principal component analysis nnpca problem given arbitrary data matrix nnpca seeks nonnegative components jointly capture variance nnpca algorithm independent interest generalizes previous work could obtain guarantees single component evaluate algorithms several real synthetic datasets show performance matches outperforms state art
lifted inference rules constraints lifted inference rules exploit symmetries fast reasoning statistical rela tional models computational complexity rules highly dependent onthe choice constraint language operate therefore coming upwith right kind representation critical success lifted inference paper propose new constraint language called setineq allowssubset equality inequality constraints represent substitutions vari ables theory constraint formulation strictly expressive thanexisting representations yet easy operate reformulate mainlifting rules decomposer generalized binomial recently proposed singleoccurrence map inference work constraint representation exper iments benchmark mlns exact sampling based inference demonstratethe effectiveness approach several existing techniques
fast distributed center clustering outliers massive data clustering large data fundamental problem vast number applications due increasing size data practitioners interested clustering turned distributed computation methods work consider widely used center clustering problem variant used handle noisy data center outliers noise free setting demonstrate previously proposed distributed method actually approximation algorithm accurately explains strong empirical performance additionally noisy setting develop novel distributed algorithm also approximation algorithms highly parallel lend virtually distributed computing framework compare empirically best known noisy sequential clustering methods show distributed algorithms consistently close sequential versions algorithms hope distributed settings fast memory efficient match sequential counterparts
precision recall gain curves analysis done right precision recall analysis abounds applications binary classification true negatives add value hence affect assessment classifier performance perhaps inspired many advantages receiver operating characteristic roc curves area curves accuracy based performance assessment many researchers taken report precision recall curves associated areas performance metric demonstrate paper practice fraught difficulties mainly incoherent scale assumptions area curve takes arithmetic mean precision values whereas beta score applies harmonic mean show fix plotting curves different coordinate system demonstrate new precision recall gain curves inherit key advantages roc curves particular area precision recall gain curves conveys expected f_1 score harmonic scale convex hull precision recall gain curve allows calibrate classifier scores determine operating point convex hull interval beta values point optimises beta demonstrate experimentally area traditional curves easily favour models lower expected f_1 score others use precision recall gain curves result better model selection
spherical random features polynomial kernels compact explicit feature maps provide practical framework scale kernel methods large scale learning deriving maps many types kernels remains challenging open problem among commonly used kernels nonlinear classification polynomial kernels low approximation error thus far necessitated explicit feature maps large dimensionality especially higher order polynomials meanwhile polynomial kernels unbounded frequently applied data normalized unit norm question address work know priori data normalized devise compact map show putative affirmative answer question based random fourier features impossible setting introduce new approximation paradigm spherical random fourier srf features circumvents issues delivers compact approximation polynomial kernels data unit sphere compared prior work srf features less rank deficient compact achieve better kernel approximation especially higher order polynomials resulting predictions lower variance typically yield better classification accuracy
learning spatiotemporal trajectories manifold valued longitudinal data propose bayesian mixed effects model learn typical scenarios changes longitudinal manifold valued data namely repeated measurements objects individuals several points time model allows estimate group average trajectory space measurements random variations trajectory result spatiotemporal transformations allow changes direction trajectory pace trajectories followed use tools riemannian geometry allows derive generic algorithm kind data smooth constraints lie therefore riemannian manifold stochastic approximations expectation maximization algorithm used estimate model parameters highly non linear setting method used estimate data driven model progressive impairments cognitive functions onset alzheimer disease experimental results show model correctly put correspondence age individual diagnosed disease thus validating fact effectively estimated normative scenario disease progression random effects provide unique insights variations ordering timing succession cognitive impairments across different individuals
distributed means median clustering general communication topologies paper provides new algorithms distributed clustering popular center based objectives median means algorithms provable guarantees improve communication complexity existing approaches following classic approach clustering cite har2004coresets reduce problem finding clustering low cost problem finding coreset small size provide distributed method constructing global coreset improves previous methods reducing communication complexity works general communication topologies provide experimental evidence approach synthetic real data sets
principal geodesic analysis probability measures optimal transport metric consider work space probability measures hilbert space endowed wasserstein metric given finite family probability measures propose iterative approach compute geodesic principal components summarize efficiently dataset wasserstein metric provides riemannian structure associated concepts echet mean geodesics tangent vectors prove crucial follow intuitive approach laid standard principal component analysis make approach feasible propose use alternative parameterization geodesics proposed citet ambrosio2006gradient textit generalized geodesics parameterized velocity fields defined support wasserstein mean data pointing towards ending point generalized geodesic resulting optimization problem finding principal components solved adapting projected gradient descend method experiment results show ability computed principal components capture axes variability histograms probability measures data
algorithms sparse multi factor nmf nonnegative matrix factorization nmf popular data analysis method objective decompose matrix nonnegative components product nonnegative matrices work describe new simple efficient algorithm multi factor nonnegative matrix factorization problem mfnmf generalizes original nmf problem factors furthermore extend mfnmf algorithm incorporate regularizer based dirichlet distribution normalized columns encourage sparsity obtained factors sparse nmf algorithm affords closed form intuitive interpretation efficient comparison previous works use fix point iterations demonstrate effectiveness efficiency algorithms synthetic real data sets
distributed submodular cover succinctly summarizing massive data find subset ideally small possible well represents massive dataset corresponding utility measured according suitable utility function comparable whole dataset paper formalize challenge submodular cover problem utility assumed exhibit submodularity natural diminishing returns condition preva lent many data summarization applications classical greedy algorithm known provide solutions logarithmic approximation guarantees compared optimum solution however sequential centralized approach imprac tical truly large scale problems work develop first distributed algorithm discover submodular set cover easily implementable using mapreduce style computations theoretically analyze approach present approximation guarantees solutions returned discover also study natural trade communication cost num ber rounds required obtain solution extensive experiments demonstrate effectiveness approach several applications includ ing active set selection exemplar based clustering vertex cover tens millions data points using spark
dimensionality reduction massive sparse datasets using coresets paper present practical solution performance guarantees problem dimensionality reduction large scale sparse matrices show applications approach computing principle component analysis pca times matrix using pass stream rows solution uses coresets scaled subset rows approximates sum squared distances emph every dimensional emph affine subspace open theoretical problem compute coreset independent open practical problem compute non trivial approximation pca large sparse databases wikipedia document term matrix reasonable time answer questions affirmatively main technical result new framework deterministic coreset constructions based reduction problem counting items stream
large scale probabilistic predictors without guarantees validity paper studies theoretically empirically method turning machine learning algorithms probabilistic predictors automatically enjoys property validity perfect calibration computationally efficient price pay perfect calibration probabilistic predictors produce imprecise practice almost precise large data sets probabilities imprecise probabilities merged precise probabilities resulting predictors losing theoretical property perfect calibration consistently accurate existing methods empirical studies
bootstrap model aggregation distributed statistical learning distributed privacy preserving learning often given set probabilistic models estimated different local repositories asked combine single model gives efficient statistical estimation simple method linearly average parameters local models however tends degenerate applicable non convex models models different parameter dimensions practical strategy generate bootstrap samples local models learn joint model based combined bootstrap set unfortunately bootstrap procedure introduces additional noise significantly deteriorate performance work propose variance reduction methods correct bootstrap noise including weighted estimator statistically efficient practically powerful theoretical empirical analysis provided demonstrate methods
model parallelization scheduling strategies distributed machine learning distributed machine learning typically approached data parallel perspective big data partitioned multiple workers algorithm executed concurrently different data subsets various synchronization schemes ensure speed correctness sibling problem received relatively less attention ensure efficient correct model parallel execution algorithms parameters program partitioned different workers undergone concurrent iterative updates argue model data parallelisms impose rather different challenges system design algorithmic adjustment theoretical analysis paper develop system model parallelism strads provides programming abstraction scheduling parameter updates discovering leveraging changing structural properties programs strads enables flexible tradeoff scheduling efficiency fidelity intrinsic dependencies within models improves memory efficiency distributed demonstrate efficacy model parallel algorithms implemented strads versus popular implementations topic modeling matrix factorization lasso
fast lifted map inference via partitioning recently growing interest lifting map inference algorithms markov logic networks mlns key advantage lifted algorithms much smaller computational complexity propositional algorithms symmetries present mln symmetries detected using lifted inference rules unfortunately lifted inference rules sound complete often miss many symmetries problematic symmetries cannot exploited lifted inference algorithms ground mln search solutions much larger propositional space paper present novel approach cleverly introduces new symmetries time grounding main idea partition ground atoms force inference algorithm treat atoms part indistinguishable show systematically carefully refining growing partitions build advanced time space map inference algorithms experiments several real world datasets clearly show new algorithm superior previous approaches often finds useful symmetries search space existing lifted inference rules unable detect
next system real world development evaluation application active learning active learning methods automatically adapt data collection selecting informative samples order accelerate machine learning real world testing comparing active learning algorithms requires collecting new datasets adaptively rather simply applying algorithms benchmark datasets norm passive machine learning research facilitate development testing deployment active learning real applications built open source software system large scale active learning research experimentation system called next provides unique platform real world reproducible active learning research paper details challenges building system demonstrates capabilities several experiments results show experimentation help expose strengths weaknesses active learning algorithms sometimes unexpected enlightening ways
efficient robust automated machine learning success machine learning broad range applications led ever growing demand machine learning systems used shelf non experts effective practice systems need automatically choose good algorithm feature preprocessing steps new dataset hand also set respective hyperparameters recent work started tackle automated machine learning automl problem help efficient bayesian optimization methods work introduce robust new automl system based scikit learn using classifiers feature preprocessing methods data preprocessing methods giving rise structured hypothesis space 110 hyperparameters system dub auto sklearn improves existing automl methods automatically taking account past performance similar datasets constructing ensembles models evaluated optimization system first phase ongoing chalearn automl challenge comprehensive analysis 100 diverse datasets shows substantially outperforms previous state art automl also demonstrate performance gains due contributions derive insights effectiveness individual components auto sklearn
sparse pca via bipartite matchings consider following multi component sparse pca problem given set data points seek extract small number sparse components emph disjoint supports jointly capture maximum possible variance components computed repeatedly solving single component problem deflating input data matrix greedy procedure suboptimal present novel algorithm sparse pca jointly optimizes multiple disjoint components extracted features capture variance lies within multiplicative factor arbitrarily close optimal algorithm combinatorial computes desired components solving multiple instances bipartite maximum weight matching problem complexity grows low order polynomial ambient dimension input data exponentially rank however effectively applied low dimensional sketch input data evaluate algorithm real datasets empirically demonstrate many cases outperforms existing deflation based approaches
randomized dependence coefficient introduce randomized dependence coefficient rdc measure non linear dependence random variables arbitrary dimension based hirschfeld gebelein nyi maximum correlation coefficient rdc defined terms correlation random non linear copula projections invariant respect marginal distribution transformations low computational cost easy implement lines code included end paper
CONF: TOPICS	 ALPHA	 ETA
 =============================================================== 
30	0.01	0.01
 =============================================================== 
mixed robust average submodular partitioning fast algorithms guarantees applications investigate novel mixed robust average case submodular data partitioning problems collectively call submodular partitioning problems generalize purely robust instances problem namely max min submodular fair allocation sfa emph min max submodular load balancing slb also average case instances submodular welfare problem swp submodular multiway partition smp robust versions studied theory community existing work focused tight approximation guarantees resultant algorithms generally scalable large real world applications contrasts average case instances algorithms scalable present paper bridge gap proposing several new algorithms including greedy majorization minimization minorization maximization relaxation algorithms scale large datasets also achieve theoretical approximation guarantees comparable state art moreover provide new scalable algorithms apply additive combinations robust average case objectives show problems many applications machine learning including data partitioning load balancing distributed data clustering image segmentation empirically demonstrate efficacy algorithms real world problems involving data partitioning distributed optimization convex deep neural network objectives also purely unsupervised image segmentation
fast randomized kernel ridge regression statistical guarantees approach improving running time kernel based methods build small sketch kernel matrix use lieu full matrix machine learning task interest describe version approach comes running time guarantees well improved guarantees statistical performance extending notion emph statistical leverage scores setting kernel ridge regression able identify sampling distribution reduces size sketch required number columns sampled emph effective dimensionality problem latter quantity often much smaller previous bounds depend emph maximal degrees freedom give empirical evidence supporting fact second contribution present fast algorithm quickly compute coarse approximations thesescores time linear number samples precisely running time algorithm depending trace kernel matrix regularization parameter obtained via variant squared length sampling adapt kernel setting lastly discuss new notion leverage data point captures fine notion difficulty learning problem
class network models recoverable spectral clustering finding communities networks problem remains difficult spite amount attention recently received stochastic block model sbm generative model graphs communities simplicity theoretical understanding advanced fast recent years particular various results showing simple versions spectralclustering using normalized laplacian graph recoverthe communities almost perfectly high probability show essentially algorithm used sbm extension called degree corrected sbm works wider class block models call preference frame models essentially guarantees moreover parametrization introduce clearly exhibits free parameters needed specify class models results bounds expose clarity parameters control recovery error model class
matrix completion fewer entries spectral detectability rank estimation completion low rank matrices entries task many practical applications consider aspects problem detectability ability estimate rank reliably fewest possible random entries performance achieving small reconstruction error propose spectral algorithm tasks called macbeth matrix completion bethe hessian rank estimated number negative eigenvalues bethe hessian matrix corresponding eigenvectors used initial condition minimization discrepancy estimated matrix revealed entries analyze performance random matrix setting using results statistical mechanics hopfield neural network show particular macbeth efficiently detects rank large times matrix sqrt entries constant close also evaluate corresponding root mean square error empirically show macbeth compares favorably existing approaches
monotone submodular function maximization size constraints submodular function generalization submodular function input consists disjoint subsets instead single subset domain many machine learning problems including influence maximization kinds topics sensor placement kinds sensors naturally modeled problem maximizing monotone submodular functions paper give constant factor approximation algorithms maximizing monotone submodular functions subject several size constraints running time algorithms almost linear domain size experimentally demonstrate algorithms outperform baseline algorithms terms solution quality
regularized algorithms unified framework statistical guarantees latent models fundamental modeling tool machine learning applications present significant computational analytical challenges popular algorithm variants much used algorithmic tool yet rigorous understanding performance highly incomplete recently work demonstrated important class problems exhibits linear local convergence high dimensional setting however step well defined address precisely setting unified treatment using regularization regularization high dimensional problems well understood iterative algorithm requires careful balancing making progress towards solution identifying right structure sparsity low rank particular regularizing step using state art high dimensional prescriptions guaranteed provide balance algorithm analysis linked way reveals balance optimization statistical errors specialize general framework sparse gaussian mixture models high dimensional mixed regression regression missing variables obtaining statistical guarantees examples
learning transduce unbounded memory recently strong results demonstrated deep recurrent neural networks natural language transduction problems paper explore representational power models using synthetic grammars designed exhibit phenomena similar found real transduction problems machine translation experiments lead propose new memory based recurrent networks implement continuously differentiable analogues traditional data structures stacks queues deques show architectures exhibit superior generalisation performance deep rnns often able learn underlying generating algorithms transduction experiments
variational dropout local reparameterization trick explore yet unexploited opportunity drastically improving efficiency stochastic gradient variational bayes sgvb global model parameters regular sgvb estimators rely sampling parameters per minibatch data variance constant minibatch size efficiency estimators drastically improved upon translating uncertainty global parameters local noise independent across datapoints minibatch reparameterizations local noise trivially parallelized variance inversely proportional minibatch size generally leading much faster convergence find important connection regularization dropout original gaussian dropout objective corresponds sgvb local noise scale invariant prior proportionally fixed posterior variance method allows inference flexibly parameterized posteriors specifically propose emph variational dropout generalization gaussian dropout flexibly parameterized posterior often leading better generalization method demonstrated several experiments
nonconvex optimization framework low rank matrix estimation study estimation low rank matrices via nonconvex optimization compared convex relaxation nonconvex optimization exhibits superior empirical performance large scale instances low rank matrix estimation however understanding theoretical guarantees limited paper define notion projected oracle divergence based establish sufficient conditions success nonconvex optimization illustrate consequences general framework matrix sensing completion particular prove broad class nonconvex optimization algorithms including alternating minimization gradient type methods geometrically converge global optimum exactly recover true low rank matrices standard conditions
robust feature sample linear discriminant analysis brain disorders diagnosis wide spectrum discriminative methods increasingly used diverse applications classification regression tasks however many existing discriminative methods assume input data nearly noise free limits applications solve real world problems particularly disease diagnosis data acquired neuroimaging devices always prone different sources noise robust discriminative models somewhat scarce attempts made make robust noise outliers methods focus detecting either sample outliers feature noises moreover usually use unsupervised noising procedures separately noise training testing data factors induce biases learning process thus limit performance paper propose classification method based least squares formulation linear discriminant analysis simultaneously detects sample outliers feature noises proposed method operates semi supervised setting labeled training unlabeled testing data incorporated form intrinsic geometry sample space therefore violating samples feature values identified sample outliers feature noises respectively test algorithm synthetic brain neurodegenerative databases particularly parkinson disease alzheimer disease results demonstrate method outperforms baseline state art methods terms accuracy area roc curve
multi class svms tighter data dependent generalization bounds novel algorithms paper studies generalization performance multi class classification algorithms obtain first time data dependent generalization error bound logarithmic dependence class size substantially improving state art linear dependence existing data dependent generalization analysis theoretical analysis motivates introduce new multi class classification machine based norm regularization parameter controls complexity corresponding bounds derive efficient optimization algorithm based fenchel duality theory benchmarks several real world datasets show proposed algorithm achieve significant accuracy gains state art
hessian free optimization learning deep multidimensional recurrent neural networks multidimensional recurrent neural networks mdrnns shown remarkable performance area speech handwriting recognition performance mdrnn improved increasing depth difficulty learning deeper network overcome using hessian free optimization given connectionist temporal classification ctc utilized objective learning mdrnn sequence labeling non convexity ctc poses problem applying network solution convex approximation ctc formulated relationship algorithm fisher information matrix discussed mdrnn depth layers successfully trained using resulting improved performance sequence labeling
algorithms terms optimizing training error reducing test error quickly
greedy subspace clustering consider problem subspace clustering given points lie near union many low dimensional linear subspaces recover subspaces end first identifies sets points close subspace uses sets estimate subspaces geometric structure clusters linear subspaces forbids proper performance general distance based approaches means many model specific methods proposed paper provide new simple efficient algorithms problem statistical analysis shows algorithms guaranteed exact perfect clustering performance certain conditions number points affinity tween subspaces conditions weaker considered standard statistical literature experimental results synthetic data generated standard unions subspaces model demonstrate theory also show algorithm performs competitively state art algorithms real world applications motion segmentation face clustering much simpler implementation lower computational cost
submodular optimization submodular cover submodular knapsack constraints investigate new optimization problems minimizing submodular function subject submodular lower bound constraint submodular cover maximizing submodular function subject submodular upper bound constraint submodular knapsack motivated number real world applications machine learning including sensor placement data subset selection require maximizing certain submodular function like coverage diversity simultaneously minimizing another like cooperative cost problems often posed minimizing difference submodular functions worst case inapproximable show however phrasing problems constrained optimization natural many applications achieve number bounded approximation guarantees also show problems closely related approximation algorithm solving used obtain approximation guarantee provide hardness results problems thus showing approximation factors tight log factors finally empirically demonstrate performance good scalability properties algorithms
joint best diverse labelings parametric submodular minimization consider problem jointly inferring best diverse labelings binary high order submodular energy graphical model recently shown problem solved global optimum many practically interesting diversity measures noted labelings called nested nestedness property also holds labelings class parametric submodular minimization problems different values global parameter gamma give rise different solutions popular example parametric submodular minimization monotonic parametric max flow problem also widely used computing multiple labelings main contribution work establish close relationship diversity submodular energies parametric submodular minimization particular joint best diverse labelings obtained running non parametric submodular minimization special case max flow solver different values gamma parallel certain diversity measures importantly values gamma computed closed form advance prior optimization theoretical results suggest simple yet efficient algorithms joint best diverse problem outperform competitors terms runtime quality results particular show paper new methods compute exact best diverse labelings faster popular method batra sense obtains approximate solutions
sketching structured matrices faster nonlinear regression motivated desire extend fast randomized techniques nonlinear l_p regression consider class structured regression problems problems involve vandermonde matrices arise naturally various statistical modeling settings including classical polynomial fitting problems recently developed randomized techniques scalable kernel methods show structure exploited accelerate solution regression problem achieving running times faster input sparsity present empirical results confirming practical value modeling framework well speedup benefits randomized regression
exploiting tradeoffs exact recovery heterogeneous stochastic block models stochastic block model sbm widely used random graph model networks communities despite recent burst interest community detection sbm statistical computational points view still gaps understanding fundamental limits recovery paper consider sbm full generality restriction number sizes communities grow number nodes well connectivity probabilities inside across communities stochastic block models provide guarantees exact recovery via semidefinite program well upper lower bounds sbm parameters exact recoverability results exploit tradeoffs among various parameters heterogenous sbm provide recovery guarantees many new interesting sbm configurations
subset selection pareto optimization selecting optimal subset large set variables fundamental problem various learning tasks feature selection sparse regression dictionary learning etc paper propose poss approach employs evolutionary pareto optimization find small sized subset good performance prove sparse regression poss able achieve best far theoretically guaranteed approximation performance efficiently particularly emph exponential decay subclass poss proven achieve optimal solution empirical study verifies theoretical results exhibits superior performance poss greedy convex relaxation methods
graph clustering block models model free results clustering graphs stochastic block model sbm extensions well studied guarantees correctness exist assumption data sampled model paper propose framework obtain correctness guarantees without assuming data comes model guarantees obtain depend instead statistics data checked also show framework ties existing model based framework exploit results model based recovery well strengthen results existing area research
CONF: TOPICS	 ALPHA	 ETA
 =============================================================== 
40	0.01	0.01
 =============================================================== 
fast accurate inference plackett luce models show maximum likelihood estimate models derived luce choice axiom plackett luce model expressed stationary distribution markov chain conveys insight several recently proposed spectral inference algorithms take advantage perspective formulate new spectral algorithm significantly accurate previous ones plackett luce model simple adaptation algorithm used iteratively producing sequence estimates converges estimate version runs faster competing approaches benchmark datasets algorithms easy implement making relevant practitioners large
learning transduce unbounded memory recently strong results demonstrated deep recurrent neural networks natural language transduction problems paper explore representational power models using synthetic grammars designed exhibit phenomena similar found real transduction problems machine translation experiments lead propose new memory based recurrent networks implement continuously differentiable analogues traditional data structures stacks queues deques show architectures exhibit superior generalisation performance deep rnns often able learn underlying generating algorithms transduction experiments
spectral representations convolutional neural networks discrete fourier transforms provide significant speedup computation convolutions deep learning work demonstrate beyond advantages efficient computation spectral domain also provides powerful representation model train convolutional neural networks cnns employ spectral representations introduce number innovations cnn design first propose spectral pooling performs dimensionality reduction truncating representation frequency domain approach preserves considerably information per parameter pooling strategies enables flexibility choice pooling output dimensionality representation also enables new form stochastic regularization randomized modification resolution show methods achieve competitive results classification approximation tasks without using dropout max pooling finally demonstrate effectiveness complex coefficient spectral parameterization convolutional filters leaves underlying model unchanged results representation greatly facilitates optimization observe variety popular cnn configurations leads significantly faster convergence training
robust feature sample linear discriminant analysis brain disorders diagnosis wide spectrum discriminative methods increasingly used diverse applications classification regression tasks however many existing discriminative methods assume input data nearly noise free limits applications solve real world problems particularly disease diagnosis data acquired neuroimaging devices always prone different sources noise robust discriminative models somewhat scarce attempts made make robust noise outliers methods focus detecting either sample outliers feature noises moreover usually use unsupervised noising procedures separately noise training testing data factors induce biases learning process thus limit performance paper propose classification method based least squares formulation linear discriminant analysis simultaneously detects sample outliers feature noises proposed method operates semi supervised setting labeled training unlabeled testing data incorporated form intrinsic geometry sample space therefore violating samples feature values identified sample outliers feature noises respectively test algorithm synthetic brain neurodegenerative databases particularly parkinson disease alzheimer disease results demonstrate method outperforms baseline state art methods terms accuracy area roc curve
convergence stochastic gradient mcmc algorithms high order integrators recent advances bayesian learning large scale data witnessed emergence stochastic gradient mcmc algorithms mcmc stochastic gradient langevin dynamics sgld stochastic gradient hamiltonian mcmc sghmc stochastic gradient thermostat finite time convergence properties sgld 1st order euler integrator recently studied corresponding theory general mcmcs explored paper consider general mcmcs high order integrators develop theory analyze finite time convergence properties asymptotic invariant measures theoretical results show faster convergence rates accurate invariant measures mcmcs higher order integrators example proposed efficient 2nd order symmetric splitting integrator mean square error mse posterior average sghmc achieves optimal convergence rate iterations compared sghmc sgld 1st order euler integrators furthermore convergence results decreasing step size mcmcs also developed convergence rates fixed step size counterparts specific decreasing sequence experiments synthetic real datasets verify theory show advantages proposed method large scale real applications
lifted symmetry detection breaking map inference symmetry breaking technique speeding propositional satisfiability testing adding constraints theory restrict search space preserving satisfiability work extend symmetry breaking problem model finding weighted unweighted relational theories class problems includes map inference markov logic similar statistical relational languages introduce term symmetries induced evidence set extend symmetries relational theory provide important special case term equivalent symmetries showing symmetries found low degree polynomial time show break exponential number symmetries added constraints linear size domain demonstrate effectiveness techniques experiments relational domains also discuss connections relational symmetry breaking work lifted inference statistical relational reasoning
class network models recoverable spectral clustering finding communities networks problem remains difficult spite amount attention recently received stochastic block model sbm generative model graphs communities simplicity theoretical understanding advanced fast recent years particular various results showing simple versions spectralclustering using normalized laplacian graph recoverthe communities almost perfectly high probability show essentially algorithm used sbm extension called degree corrected sbm works wider class block models call preference frame models essentially guarantees moreover parametrization introduce clearly exhibits free parameters needed specify class models results bounds expose clarity parameters control recovery error model class
faster cnn towards real time object detection region proposal networks state art object detection networks depend region proposal algorithms hypothesize object locations advances like sppnet fast cnn reduced running time detection networks exposing region proposal computation bottleneck work introduce region proposal network rpn shares full image convolutional features detection network thus enabling nearly cost free region proposals rpn fully convolutional network simultaneously predicts object bounds objectness scores position rpns trained end end generate high quality region proposals used fast cnn detection simple alternating optimization rpn fast cnn trained share convolutional features deep vgg model detection system frame rate 5fps including steps gpu achieving state art object detection accuracy pascal voc 2007 map 2012 map using 300 proposals per image code available https github com shaoqingren faster_rcnn
variational dropout local reparameterization trick explore yet unexploited opportunity drastically improving efficiency stochastic gradient variational bayes sgvb global model parameters regular sgvb estimators rely sampling parameters per minibatch data variance constant minibatch size efficiency estimators drastically improved upon translating uncertainty global parameters local noise independent across datapoints minibatch reparameterizations local noise trivially parallelized variance inversely proportional minibatch size generally leading much faster convergence find important connection regularization dropout original gaussian dropout objective corresponds sgvb local noise scale invariant prior proportionally fixed posterior variance method allows inference flexibly parameterized posteriors specifically propose emph variational dropout generalization gaussian dropout flexibly parameterized posterior often leading better generalization method demonstrated several experiments
multi class svms tighter data dependent generalization bounds novel algorithms paper studies generalization performance multi class classification algorithms obtain first time data dependent generalization error bound logarithmic dependence class size substantially improving state art linear dependence existing data dependent generalization analysis theoretical analysis motivates introduce new multi class classification machine based norm regularization parameter controls complexity corresponding bounds derive efficient optimization algorithm based fenchel duality theory benchmarks several real world datasets show proposed algorithm achieve significant accuracy gains state art
differentiable learning logical rules knowledge base reasoning study problem learning probabilistic first order logical rules knowledge base reasoning learning problem difficult requires learning parameters continuous space well structure discrete space propose framework neural logic programming combines parameter structure learning first order logical rules end end differentiable model approach inspired recently developed differentiable logic called tensorlog inference tasks compiled sequences differentiable operations design neural controller system learns compose operations empirically method obtains state art results multiple knowledge base benchmark datasets including freebase wikimovies
rapid distance based outlier detection via sampling distance based approaches outlier detection popular data mining require model underlying probability distribution particularly challenging high dimensional data present empirical comparison various approaches distance based outlier detection across large number datasets report surprising observation simple sampling based scheme outperforms state art techniques terms efficiency effectiveness better understand phenomenon provide theoretical analysis sampling based approach outperforms alternative methods based nearest neighbor search
fcn object detection via region based fully convolutional networks present region based fully convolutional networks accurate efficient object detection contrast previous region based detectors fast faster cnn apply costly per region subnetwork hundreds times region based detector fully convolutional almost computation shared entire image achieve goal propose position sensitive score maps address dilemma translation invariance image classification translation variance object detection method thus naturally adopt fully convolutional image classifier backbones latest residual networks resnets object detection show competitive results pascal voc datasets map 2007 set 101 layer resnet meanwhile result achieved test time speed 170ms per image times faster faster cnn counterpart code made publicly available https github com daijifeng001 fcn
nice adversarial training mcmc existing markov chain monte carlo mcmc methods either based general purpose domain agnostic schemes lead slow convergence require hand crafting problem specific proposals expert propose nice novel method train flexible parametric markov chain kernels produce samples desired properties first propose efficient likelihood free adversarial training method train markov chain mimic given data distribution leverage flexible volume preserving flows obtain parametric kernels mcmc using bootstrap approach show train efficient markov chains sample prescribed posterior distribution iteratively improving quality model samples nice provides first framework automatically design efficient domain specific mcmc proposals empirical results demonstrate nice combines strong guarantees mcmc expressiveness deep neural networks able significantly outperform competing methods hamiltonian monte carlo
adversarial invariant feature learning learning meaningful representations maintain content necessary particular task filtering away detrimental variations problem great interest machine learning paper tackle problem learning representations invariant specific factor trait data leading better generalization representation learning process formulated adversarial minimax game analyze optimal equilibrium game benchmark tasks namely fair classifications bias free language independent generation lighting independent image classification show proposed framework induces invariant representation leads better generalization evidenced improved test performance
analysis learning positive unlabeled data learning classifier positive unlabeled data important class classification problems conceivable many practical applications paper first show problem solved cost sensitive learning positive unlabeled data show convex surrogate loss functions hinge loss lead wrong classification boundary due intrinsic bias problem avoided using non convex loss functions ramp loss next analyze excess risk class prior estimated data show classification accuracy sensitive class prior estimation unlabeled data dominated positive data naturally satisfied inlier based outlier detection inliers dominant unlabeled dataset finally provide generalization error bounds show equal number labeled unlabeled samples generalization error learning positive unlabeled samples worse sqrt times fully supervised case theoretical findings also validated experiments
efficient use limited memory resources accelerate linear learning work propose generic approach efficiently use compute accelerators gpus fpgas training large scale machine learning models training data exceeds memory capacity technique builds upon primal dual coordinate selection uses duality gaps selection criteria dynamically decide part data made available fast processing provide strong theoretical guarantees motivating gap based selection scheme provide efficient practical implementation thereof illustrate power approach demonstrate performance training generalized linear models large scale datasets exceeding memory size modern gpu showing order magnitude speedup existing approaches
non linear domain adaptation boosting common assumption machine vision training test samples drawn distribution however many problems assumption grossly violated bio medical applications different acquisitions generate drastic variations appearance data due changing experimental conditions problem accentuated data annotation time consuming limiting amount data labeled new acquisitions training paper present multi task learning algorithm domain adaptation based boosting unlike previous approaches learn task specific decision boundaries method learns single decision boundary shared feature space common tasks use boosting trick learn non linear mapping observations task need specific priori knowledge global analytical form yields parameter free domain adaptation approach successfully leverages learning new tasks labeled data scarce evaluate approach challenging bio medical datasets achieve significant improvement state art
converge sublinear rates problem proposed method incorporates memory previous gradient values order achieve linear convergence
new liftable classes first order probabilistic inference statistical relational models provide compact encodings probabilistic dependencies relational domains result highly intractable graphical models goal lifted inference carry probabilistic inference without needing reason individual separately instead treating exchangeable undistinguished objects whole paper study domain recursion inference rule despite central role early theoretical results domain lifted inference later believed redundant show rule powerful expected fact significantly extends range models lifted inference runs time polynomial number individuals domain includes open problem called symmetric transitivity model first order logic encoding birthday paradox identify new classes s2fo2 s2ru domain liftable theories respectively subsume fo2 recursively unary theories largest classes domain liftable theories known far show using domain recursion achieve exponential speedup even theories cannot fully lifted existing set inference rules
CONF: TOPICS	 ALPHA	 ETA
 =============================================================== 
50	0.01	0.01
 =============================================================== 
fast accurate inference plackett luce models show maximum likelihood estimate models derived luce choice axiom plackett luce model expressed stationary distribution markov chain conveys insight several recently proposed spectral inference algorithms take advantage perspective formulate new spectral algorithm significantly accurate previous ones plackett luce model simple adaptation algorithm used iteratively producing sequence estimates converges estimate version runs faster competing approaches benchmark datasets algorithms easy implement making relevant practitioners large
robust feature sample linear discriminant analysis brain disorders diagnosis wide spectrum discriminative methods increasingly used diverse applications classification regression tasks however many existing discriminative methods assume input data nearly noise free limits applications solve real world problems particularly disease diagnosis data acquired neuroimaging devices always prone different sources noise robust discriminative models somewhat scarce attempts made make robust noise outliers methods focus detecting either sample outliers feature noises moreover usually use unsupervised noising procedures separately noise training testing data factors induce biases learning process thus limit performance paper propose classification method based least squares formulation linear discriminant analysis simultaneously detects sample outliers feature noises proposed method operates semi supervised setting labeled training unlabeled testing data incorporated form intrinsic geometry sample space therefore violating samples feature values identified sample outliers feature noises respectively test algorithm synthetic brain neurodegenerative databases particularly parkinson disease alzheimer disease results demonstrate method outperforms baseline state art methods terms accuracy area roc curve
bandits unobserved confounders causal approach multi armed bandit problem constitutes archetypal setting sequential decision making permeating multiple domains including engineering business medicine hallmarks bandit setting agent capacity explore environment active intervention contrasts ability collect passive data estimating associational relationships actions payouts existence unobserved confounders namely unmeasured variables affecting action outcome variables implies data collection modes general coincide paper show formalizing distinction conceptual algorithmic implications bandit setting current generation bandit algorithms implicitly try maximize rewards based estimation experimental distribution show always best strategy pursue indeed achieve low regret certain realistic classes bandit problems namely face unobserved confounders experimental observational quantities required rational agent realization propose optimization metric employing experimental observational distributions bandit agents pursue illustrate benefits traditional algorithms
spectral representations convolutional neural networks discrete fourier transforms provide significant speedup computation convolutions deep learning work demonstrate beyond advantages efficient computation spectral domain also provides powerful representation model train convolutional neural networks cnns employ spectral representations introduce number innovations cnn design first propose spectral pooling performs dimensionality reduction truncating representation frequency domain approach preserves considerably information per parameter pooling strategies enables flexibility choice pooling output dimensionality representation also enables new form stochastic regularization randomized modification resolution show methods achieve competitive results classification approximation tasks without using dropout max pooling finally demonstrate effectiveness complex coefficient spectral parameterization convolutional filters leaves underlying model unchanged results representation greatly facilitates optimization observe variety popular cnn configurations leads significantly faster convergence training
robust gaussian graphical modeling trimmed graphical lasso gaussian graphical models ggms popular tools studying network structures however many modern applications gene network discovery social interactions analysis often involve high dimensional noisy data outliers heavier tails gaussian distribution paper propose trimmed graphical lasso robust estimation sparse ggms method guards outliers implicit trimming mechanism akin popular least trimmed squares method used linear regression provide rigorous statistical analysis estimator high dimensional setting contrast existing approaches robust sparse ggms estimation lack statistical guarantees theoretical results complemented experiments simulated real gene expression data demonstrate value approach
variational dropout local reparameterization trick explore yet unexploited opportunity drastically improving efficiency stochastic gradient variational bayes sgvb global model parameters regular sgvb estimators rely sampling parameters per minibatch data variance constant minibatch size efficiency estimators drastically improved upon translating uncertainty global parameters local noise independent across datapoints minibatch reparameterizations local noise trivially parallelized variance inversely proportional minibatch size generally leading much faster convergence find important connection regularization dropout original gaussian dropout objective corresponds sgvb local noise scale invariant prior proportionally fixed posterior variance method allows inference flexibly parameterized posteriors specifically propose emph variational dropout generalization gaussian dropout flexibly parameterized posterior often leading better generalization method demonstrated several experiments
hessian free optimization learning deep multidimensional recurrent neural networks multidimensional recurrent neural networks mdrnns shown remarkable performance area speech handwriting recognition performance mdrnn improved increasing depth difficulty learning deeper network overcome using hessian free optimization given connectionist temporal classification ctc utilized objective learning mdrnn sequence labeling non convexity ctc poses problem applying network solution convex approximation ctc formulated relationship algorithm fisher information matrix discussed mdrnn depth layers successfully trained using resulting improved performance sequence labeling
dimensionality reduction subspace structure preservation modeling data sampled union independent subspaces widely applied number real world applications however dimensionality reduction approaches theoretically preserve independence assumption well studied key contribution show projection vectors sufficient independence preservation class data sampled union independent subspaces non trivial observation use designing dimensionality reduction technique paper propose novel dimensionality reduction algorithm theoretically preserves structure given dataset support theoretical analysis empirical results synthetic real world data achieving textit state art results compared popular dimensionality reduction techniques
nonconvex optimization framework low rank matrix estimation study estimation low rank matrices via nonconvex optimization compared convex relaxation nonconvex optimization exhibits superior empirical performance large scale instances low rank matrix estimation however understanding theoretical guarantees limited paper define notion projected oracle divergence based establish sufficient conditions success nonconvex optimization illustrate consequences general framework matrix sensing completion particular prove broad class nonconvex optimization algorithms including alternating minimization gradient type methods geometrically converge global optimum exactly recover true low rank matrices standard conditions
online learning nonparametric mixture models via sequential variational approximation reliance computationally expensive algorithms inference limiting use bayesian nonparametric models large scale applications tackle problem propose bayesian learning algorithm mixture models instead following conventional paradigm random initialization plus iterative update take progressive approach starting given prior method recursively transforms approximate posterior sequential variational approximation process new components incorporated fly needed algorithm reliably estimate mixture model pass making particularly suited applications massive data experiments synthetic data real datasets demonstrate remarkable improvement efficiency orders magnitude speed compared state art
discriminative state space models paper introduce analyze discriminative state space models forecasting non stationary time series provide data dependent generalization guarantees learning models based recently introduced notion discrepancy provide depth analysis complexity models finally also study generalization guarantees several structural risk minimization approaches problem provide efficient implementation based convex objective
multi class svms tighter data dependent generalization bounds novel algorithms paper studies generalization performance multi class classification algorithms obtain first time data dependent generalization error bound logarithmic dependence class size substantially improving state art linear dependence existing data dependent generalization analysis theoretical analysis motivates introduce new multi class classification machine based norm regularization parameter controls complexity corresponding bounds derive efficient optimization algorithm based fenchel duality theory benchmarks several real world datasets show proposed algorithm achieve significant accuracy gains state art
translating embeddings modeling multi relational data consider problem embedding entities relationships multi relational data low dimensional vector spaces objective propose canonical model easy train contains reduced number parameters scale large databases hence propose transe method models relationships interpreting translations operating low dimensional embeddings entities despite simplicity assumption proves powerful since extensive experiments show transe significantly outperforms state art methods link prediction knowledge bases besides successfully trained large scale data set entities 25k relationships 17m training samples
fast robust least squares estimation corrupted linear models subsampling methods recently proposed speed least squares estimation large scale settings however algorithms typically robust outliers corruptions observed covariates concept influence developed regression diagnostics used detect corrupted observations shown paper property influence also develop randomized approximation motivates proposed subsampling algorithm large scale corrupted linear regression limits influence data points since highly influential points contribute residual error general model corrupted observations show theoretically empirically variety simulated real datasets algorithm improves current state art approximation schemes ordinary least squares
rapid distance based outlier detection via sampling distance based approaches outlier detection popular data mining require model underlying probability distribution particularly challenging high dimensional data present empirical comparison various approaches distance based outlier detection across large number datasets report surprising observation simple sampling based scheme outperforms state art techniques terms efficiency effectiveness better understand phenomenon provide theoretical analysis sampling based approach outperforms alternative methods based nearest neighbor search
general table completion using bayesian nonparametric model even though heterogeneous databases found broad variety applications exists lack tools estimating missing data databases paper provide efficient robust table completion tool based bayesian nonparametric latent feature model particular propose general observation model indian buffet process ibp adapted mixed continuous real valued positive real valued discrete categorical ordinal count observations propose inference algorithm scales linearly number observations finally experiments real databases show proposed approach provides robust accurate estimates standard ibp bayesian probabilistic matrix factorization gaussian observations
estimating accuracy unlabeled data probabilistic logic approach propose efficient method estimate accuracy classifiers using unlabeled data consider setting multiple classification problems target classes tied together logical constraints example set classes mutually exclusive meaning data instance belong proposed method based intuition classifiers agree likely correct classifiers make prediction violates constraints least classifier must making error experiments real world data sets produce accuracy estimates within percent true accuracy using solely unlabeled data models also outperform existing state art solutions estimating accuracies combining multiple classifier outputs results emphasize utility logical constraints estimating accuracy thus validating intuition
hierarchical methods moments spectral methods moments provide powerful tool learning parameters latent variable models despite theoretical appeal applicability methods real data still limited due lack robustness model misspecification paper present hierarchical approach methods moments circumvent limitations method based replacing tensor decomposition step used previous algorithms approximate joint diagonalization experiments topic modeling show method outperforms previous tensor decomposition methods terms speed model quality
scale adaptive blind deblurring presence noise small scale structures usually leads large kernel estimation errors blind image deblurring empirically total failure present scale space perspective blind deblurring algorithms introduce cascaded scale space formulation blind deblurring new formulation suggests natural approach robust noise small scale structures tying estimation across multiple scales balancing contributions different scales automatically learning data proposed formulation also allows handle non uniform blur straightforward extension experiments conducted benchmark dataset real world images validate effectiveness proposed method surprising finding based approach blur kernel estimation necessarily best finest scale
estimating class prior posterior noisy positives unlabeled data develop classification algorithm estimating posterior distributions positive unlabeled data robust noise positive labels effective high dimensional data recent years several algorithms proposed learn positive unlabeled data however many contributions remain theoretical performing poorly real high dimensional data typically contaminated noise build previous work develop practical classification algorithms explicitly model noise positive labels utilize univariate transforms built discriminative classifiers prove univariate transforms preserve class prior enabling estimation univariate space avoiding kernel density estimation high dimensional data theoretical development parametric nonparametric algorithms proposed constitute important step towards wide spread use robust classification algorithms positive unlabeled data
CONF: TOPICS	 ALPHA	 ETA
 =============================================================== 
60	0.01	0.01
 =============================================================== 
standard stochastic gradient methods
variational dropout local reparameterization trick explore yet unexploited opportunity drastically improving efficiency stochastic gradient variational bayes sgvb global model parameters regular sgvb estimators rely sampling parameters per minibatch data variance constant minibatch size efficiency estimators drastically improved upon translating uncertainty global parameters local noise independent across datapoints minibatch reparameterizations local noise trivially parallelized variance inversely proportional minibatch size generally leading much faster convergence find important connection regularization dropout original gaussian dropout objective corresponds sgvb local noise scale invariant prior proportionally fixed posterior variance method allows inference flexibly parameterized posteriors specifically propose emph variational dropout generalization gaussian dropout flexibly parameterized posterior often leading better generalization method demonstrated several experiments
regularized algorithms unified framework statistical guarantees latent models fundamental modeling tool machine learning applications present significant computational analytical challenges popular algorithm variants much used algorithmic tool yet rigorous understanding performance highly incomplete recently work demonstrated important class problems exhibits linear local convergence high dimensional setting however step well defined address precisely setting unified treatment using regularization regularization high dimensional problems well understood iterative algorithm requires careful balancing making progress towards solution identifying right structure sparsity low rank particular regularizing step using state art high dimensional prescriptions guaranteed provide balance algorithm analysis linked way reveals balance optimization statistical errors specialize general framework sparse gaussian mixture models high dimensional mixed regression regression missing variables obtaining statistical guarantees examples
stochastic gradient method exponential convergence
spectral representations convolutional neural networks discrete fourier transforms provide significant speedup computation convolutions deep learning work demonstrate beyond advantages efficient computation spectral domain also provides powerful representation model train convolutional neural networks cnns employ spectral representations introduce number innovations cnn design first propose spectral pooling performs dimensionality reduction truncating representation frequency domain approach preserves considerably information per parameter pooling strategies enables flexibility choice pooling output dimensionality representation also enables new form stochastic regularization randomized modification resolution show methods achieve competitive results classification approximation tasks without using dropout max pooling finally demonstrate effectiveness complex coefficient spectral parameterization convolutional filters leaves underlying model unchanged results representation greatly facilitates optimization observe variety popular cnn configurations leads significantly faster convergence training
nonconvex optimization framework low rank matrix estimation study estimation low rank matrices via nonconvex optimization compared convex relaxation nonconvex optimization exhibits superior empirical performance large scale instances low rank matrix estimation however understanding theoretical guarantees limited paper define notion projected oracle divergence based establish sufficient conditions success nonconvex optimization illustrate consequences general framework matrix sensing completion particular prove broad class nonconvex optimization algorithms including alternating minimization gradient type methods geometrically converge global optimum exactly recover true low rank matrices standard conditions
convergence stochastic gradient mcmc algorithms high order integrators recent advances bayesian learning large scale data witnessed emergence stochastic gradient mcmc algorithms mcmc stochastic gradient langevin dynamics sgld stochastic gradient hamiltonian mcmc sghmc stochastic gradient thermostat finite time convergence properties sgld 1st order euler integrator recently studied corresponding theory general mcmcs explored paper consider general mcmcs high order integrators develop theory analyze finite time convergence properties asymptotic invariant measures theoretical results show faster convergence rates accurate invariant measures mcmcs higher order integrators example proposed efficient 2nd order symmetric splitting integrator mean square error mse posterior average sghmc achieves optimal convergence rate iterations compared sghmc sgld 1st order euler integrators furthermore convergence results decreasing step size mcmcs also developed convergence rates fixed step size counterparts specific decreasing sequence experiments synthetic real datasets verify theory show advantages proposed method large scale real applications
learning incremental iterative regularization within statistical learning setting propose study iterative regularization algorithm least squares defined incremental gradient method particular show parameters fixed priori number passes data epochs acts regularization parameter prove strong universal consistency almost sure convergence risk well sharp finite sample bounds iterates results step towards understanding effect multiple epochs stochastic gradient techniques machine learning rely integrating statistical optimizationresults
variance reduction stochastic gradient langevin dynamics stochastic gradient based monte carlo methods stochastic gradient langevin dynamics useful tools posterior inference large scale datasets many machine learning applications methods scale large datasets using noisy gradients calculated using mini batch subset dataset however high variance inherent noisy gradients degrades performance leads slower mixing paper present techniques reducing variance stochastic gradient langevin dynamics yielding novel stochastic monte carlo methods improve performance reducing variance stochastic gradient show proposed method better theoretical guarantees convergence rate stochastic langevin dynamics complemented impressive empirical results obtained variety real world datasets different machine learning tasks regression classification independent component analysis mixture modeling theoretical empirical contributions combine make compelling case using variance reduction stochastic monte carlo methods
multi class svms tighter data dependent generalization bounds novel algorithms paper studies generalization performance multi class classification algorithms obtain first time data dependent generalization error bound logarithmic dependence class size substantially improving state art linear dependence existing data dependent generalization analysis theoretical analysis motivates introduce new multi class classification machine based norm regularization parameter controls complexity corresponding bounds derive efficient optimization algorithm based fenchel duality theory benchmarks several real world datasets show proposed algorithm achieve significant accuracy gains state art
fast sample testing analytic representations probability measures propose class nonparametric sample tests cost linear sample size tests given based ensemble distances analytic functions representing distributions first test uses smoothed empirical characteristic functions represent distributions second uses distribution embeddings reproducing kernel hilbert space analyticity implies differences distributions detected almost surely finite number randomly chosen locations frequencies new tests consistent larger class alternatives previous linear time tests based non smoothed empirical characteristic functions much faster current state art quadratic time kernel based energy distance based tests experiments artificial benchmarks challenging real world testing problems demonstrate tests give better power time tradeoff competing approaches cases better outright power even expensive quadratic time tests performance advantage retained even high dimensions cases difference distributions observable low order statistics
differentially private learning structured discrete distributions investigate problem learning unknown probability distribution discrete population random samples goal design efficient algorithms simultaneously achieve low error total variation norm guaranteeing differential privacy individuals population describe general approach yields near sample optimal computationally efficient differentially private estimators wide range well studied natural distribution families theoretical results show wide variety structured distributions exist private estimation algorithms nearly efficient terms sample size running time non private counterparts complement theoretical guarantees experimental evaluation experiments illustrate speed accuracy private estimators synthetic mixture models large public data set
communication efficient distributed machine learning parameter server paper describes third generation parameter server framework distributed machine learning framework offers relaxations balance system performance algorithm efficiency propose new algorithm takes advantage framework solve non convex non smooth problems convergence guarantees present depth analysis large scale machine learning problems ranging ell_1 regularized logistic regression cpus reconstruction ica gpus using 636tb real data hundreds billions samples dimensions demonstrate using examples parameter server framework effective straightforward way scale machine learning larger problems systems previously achieved
learning supervised pagerank gradient based gradient free optimization methods paper consider non convex loss minimization problem learning supervised pagerank models account features nodes edges propose gradient based random gradient free methods solve problem algorithms based concept inexact oracle unlike state art gradient based method manage provide theoretically convergence rate guarantees finally compare performance proposed optimization methods state art applied ranking task
variance based regularization convex objectives develop approach risk minimization stochastic optimization provides convex surrogate variance allowing near optimal computationally efficient trading approximation estimation error approach builds techniques distributionally robust optimization owen empirical likelihood provide number finite sample asymptotic results characterizing theoretical performance estimator particular show procedure comes certificates optimality achieving scenarios faster rates convergence empirical risk minimization virtue automatically balancing bias variance give corroborating empirical evidence showing practice estimator indeed trades variance absolute performance training sample improving sample test performance standard empirical risk minimization number classification problems
variance reduction stochastic gradient optimization stochastic gradient optimization class widely used algorithms training machine learning models optimize objective uses noisy gradient computed random data samples instead true gradient computed entire dataset however variance noisy gradient large algorithm might spend much time bouncing around leading slower convergence worse performance paper develop general approach using control variate variance reduction stochastic gradient data statistics low order moments pre computed estimated online used form control variate demonstrate construct control variate practical problems using stochastic gradient optimization convex map estimation logistic regression non convex stochastic variational inference latent dirichlet allocation problems approach shows faster convergence better performance classical approach
discriminative state space models paper introduce analyze discriminative state space models forecasting non stationary time series provide data dependent generalization guarantees learning models based recently introduced notion discrepancy provide depth analysis complexity models finally also study generalization guarantees several structural risk minimization approaches problem provide efficient implementation based convex objective
stochastic gradient riemannian langevin dynamics probability simplex paper investigate use langevin monte carlo methods probability simplex propose new method stochastic gradient riemannian langevin dynamics simple implement applied online apply method latent dirichlet allocation online setting demonstrate achieves substantial performance improvements state art online variational bayesian methods
stochastic gradient richardson romberg markov chain monte carlo stochastic gradient markov chain monte carlo mcmc algorithms become increasingly popular bayesian inference large scale applications even though methods proved useful several scenarios performance often limited bias study propose novel sampling algorithm aims reduce bias mcmc keeping variance reasonable level approach based numerical sequence acceleration method namely richardson romberg extrapolation simply boils running almost mcmc algorithm twice parallel different step sizes illustrate framework popular stochastic gradient langevin dynamics sgld algorithm propose novel mcmc algorithm referred stochastic gradient richardson romberg langevin dynamics sgrrld provide formal theoretical analysis show sgrrld asymptotically consistent satisfies central limit theorem non asymptotic bias mean squared error bounded results show sgrrld attains higher rates convergence sgld finite time asymptotically achieves theoretical accuracy methods based higher order integrators support findings using synthetic real data experiments
dimensionality reduction subspace structure preservation modeling data sampled union independent subspaces widely applied number real world applications however dimensionality reduction approaches theoretically preserve independence assumption well studied key contribution show projection vectors sufficient independence preservation class data sampled union independent subspaces non trivial observation use designing dimensionality reduction technique paper propose novel dimensionality reduction algorithm theoretically preserves structure given dataset support theoretical analysis empirical results synthetic real world data achieving textit state art results compared popular dimensionality reduction techniques
CONF: TOPICS	 ALPHA	 ETA
 =============================================================== 
60	0.01	0.01
 =============================================================== 
CONF: TOPICS	 ALPHA	 ETA
 =============================================================== 
20	0.01	0.01
 =============================================================== 
CONF: TOPICS	 ALPHA	 ETA
 =============================================================== 
20	0.01	0.01
 =============================================================== 
CONF: TOPICS	 ALPHA	 ETA
 =============================================================== 
20	0.01	0.01
 =============================================================== 
deep knowledge tracing knowledge tracing machine models knowledge student interact coursework established significantly unsolved problem computer supported education paper explore benefit using recurrent neural networks model student learning family models important advantages current state art methods require explicit encoding human domain knowledge far flexible functional form capture substantially complex student interactions show neural networks outperform current state art prediction real student data allowing straightforward interpretation discovery structure curriculum results suggest promising new line research knowledge tracing
poisson gamma belief network infer multilayer representation high dimensional count vectors propose poisson gamma belief network pgbn factorizes layers product connection weight matrix nonnegative real hidden units next layer pgbn hidden layers jointly trained upward downward gibbs sampler iteration upward samples dirichlet distributed connection weight vectors starting first layer bottom data layer downward samples gamma distributed hidden units starting top hidden layer gamma negative binomial process combined layer wise training strategy allows pgbn infer width layer given fixed budget width first layer pgbn single hidden layer reduces poisson factor analysis example results text analysis illustrate interesting relationships width first layer inferred network structure demonstrate pgbn whose hidden units imposed correlated gamma priors add layers increase performance gains poisson factor analysis given limit width first layer
gap safe screening rules sparse multi task multi class models high dimensional regression benefits sparsity promoting regularizations screening rules leverage known sparsity solution ignoring variables optimization hence speeding solvers procedure proven discard features wrongly rules said safe paper derive new safe rules generalized linear models regularized norms rules based duality gap computations spherical safe regions whose diameters converge allows discard safely variables particular low regularization parameters gap safe rule cope iterative solver illustrate performance coordinate descent multi task lasso binary multinomial logistic regression demonstrating significant speed ups tested datasets respect previous safe rules
halting random walk kernels random walk kernels measure graph similarity counting matching walks graphs popular form geometric random walk kernels longer walks length downweighted factor lambda lambda ensure convergence corresponding geometric series know field link prediction downweighting often leads phenomenon referred halting longer walks downweighted much similarity score completely dominated comparison walks length naive kernel edges vertices theoretically show halting occur geometric random walk kernels also empirically quantify impact simulated datasets popular graph classification benchmark datasets findings promise instrumental future graph kernel development applications random walk kernels
discrete nyi classifiers consider binary classification problem predicting target variable discrete feature vector probability distribution known optimal classifier leading minimum misclassification rate given maximum posteriori probability map decision rule however practice estimating complete joint distribution computationally statistically impossible large values therefore alternative approach first estimate low order marginals joint probability distribution design classifier based estimated low order marginals approach also helpful complete training data instances available due privacy concerns work consider problem designing optimum classifier based estimated low order marginals prove given set marginals minimum hirschfeld gebelein enyi hgr correlation principle introduced leads randomized classification rule shown misclassification rate larger twice misclassification rate optimal classifier show separability condition proposed algorithm equivalent randomized linear regression approach naturally results robust feature selection method selecting subset features maximum worst case hgr correlation target variable theoretical upper bound similar recent discrete chebyshev classifier dcc approach proposed algorithm significant computational advantages since requires solving least square optimization problem finally numerically compare proposed algorithm dcc classifier show proposed algorithm results better misclassification rate various uci data repository datasets
binaryconnect training deep neural networks binary weights propagations deep neural networks dnn achieved state art results wide range tasks best results obtained large training sets large models past gpus enabled breakthroughs greater computational speed future faster computation training test time likely crucial progress consumer applications low power devices result much interest research development dedicated hardware deep learning binary weights weights constrained possible values would bring great benefits specialized hardware replacing many multiply accumulate operations simple accumulations multipliers space power hungry components digital implementation neural networks introduce binaryconnect method consists training dnn binary weights forward backward propagations retaining precision stored weights gradients accumulated like dropout schemes show binaryconnect acts regularizer obtain near state art results binaryconnect permutation invariant mnist cifar svhn
texture synthesis using convolutional neural networks introduce new model natural textures based feature spaces convolutional neural networks optimised object recognition samples model high perceptual quality demonstrating generative power neural networks trained purely discriminative fashion within model textures represented correlations feature maps several layers network show across layers texture representations increasingly capture statistical properties natural images making object information explicit model provides new tool generate stimuli neuroscience might offer insights deep representations learned convolutional neural networks
estimating mixture models via mixtures polynomials mixture modeling general technique making simple model expressive weighted combination generality simplicity part explains success expectation maximization algorithm updates easy derive wide class mixture models however likelihood mixture model non convex known global convergence guarantees recently method moments approaches offer global guarantees mixture models extend easily range mixture models exist work present polymom unifying framework based method moments estimation procedures easily derivable polymom applicable moments single mixture component polynomials parameters key observation moments mixture model mixture polynomials allows cast estimation generalized moment problem solve relaxations using semidefinite optimization extract parameters using ideas computer algebra framework allows draw insights apply tools convex optimization computer algebra theory moments study problems statistical estimation simulations show good empirical performance several models
generative image modeling using spatial lstms modeling distribution natural images challenging partly strong statistical dependencies extend hundreds pixels recurrent neural networks successful capturing long range dependencies number problems recently found way generative image models introduce recurrent image model based multi dimensional long short term memory units particularly suited image modeling due spatial structure model scales images arbitrary size likelihood computationally tractable find outperforms state art quantitative comparisons several image datasets produces promising results used texture synthesis inpainting
multi task averaging present multi task learning approach jointly estimate means multiple independent data sets proposed multi task averaging mta algorithm results convex combination single task averages derive optimal amount regularization show effectively estimated simulations real data experiments demonstrate mta maximum likelihood james stein estimators approach estimating amount regularization rivals cross validation performance computationally efficient
spectral representations convolutional neural networks discrete fourier transforms provide significant speedup computation convolutions deep learning work demonstrate beyond advantages efficient computation spectral domain also provides powerful representation model train convolutional neural networks cnns employ spectral representations introduce number innovations cnn design first propose spectral pooling performs dimensionality reduction truncating representation frequency domain approach preserves considerably information per parameter pooling strategies enables flexibility choice pooling output dimensionality representation also enables new form stochastic regularization randomized modification resolution show methods achieve competitive results classification approximation tasks without using dropout max pooling finally demonstrate effectiveness complex coefficient spectral parameterization convolutional filters leaves underlying model unchanged results representation greatly facilitates optimization observe variety popular cnn configurations leads significantly faster convergence training
exploring models data image question answering work aims address problem image based question answering new models datasets work propose use neural networks visual semantic embeddings without intermediate stages object detection image segmentation predict answers simple questions images model performs times better published results existing image dataset also present question generation algorithm converts image descriptions widely available form used algorithm produce order magnitude larger dataset evenly distributed answers suite baseline results new dataset also presented
recurrent latent variable model sequential data paper explore inclusion latent random variables hidden state recurrent neural network rnn combining elements variational autoencoder argue use high level latent random variables variational rnn vrnn model kind variability observed highly structured sequential data natural speech empirically evaluate proposed model related sequential models speech datasets handwriting dataset results show important roles latent random variables play rnn dynamics
good semi supervised learning requires bad gan semi supervised learning methods based generative adversarial networks gans obtained strong empirical results clear discriminator benefits joint training generator good semi supervised classification performance good generator cannot obtained time theoretically show given discriminator objective good semi supervised learning indeed requires bad generator propose definition preferred generator empirically derive novel formulation based analysis substantially improves feature matching gans obtaining state art results multiple benchmark datasets
learning kernels random features randomized features provide computationally efficient way approximate kernel machines machine learning tasks however methods require user defined kernel input extend randomized feature approach task learning kernel via associated random features specifically present efficient optimization problem learns kernel supervised manner prove consistency estimated kernel well generalization bounds class estimators induced optimized kernel experimentally evaluate technique several datasets approach efficient highly scalable attain competitive results fraction training cost techniques
discriminative transfer learning tree based priors paper proposes way improving classification performance classes training examples key idea discover classes similar transfer knowledge among method organizes classes tree hierarchy tree structure used impose generative prior classification parameters show priors combined discriminative models deep neural networks method benefits power discriminative training deep neural networks time using tree based generative priors classification parameters also propose algorithm learning underlying tree structure gives model flexibility tune tree tree pertinent task solved show model transfer knowledge across related classes using fixed semantic trees moreover learn new meaningful trees usually leading improved performance method achieves state art classification results cifar 100 image data set mir flickr multimodal data set
powerful generative model using random weights deep image representation extent success deep visualization due training could deep visualization using untrained random weight networks address issue explore new powerful generative models popular deep visualization tasks using untrained random weight convolutional neural networks first invert representations feature spaces reconstruct images white noise inputs reconstruction quality statistically higher method applied well trained networks architecture next synthesize textures using scaled correlations representations multiple layers results almost indistinguishable original natural texture synthesized textures based trained network third recasting content image style various artworks create artistic images high perceptual quality highly competitive prior work gatys pretrained networks knowledge first demonstration image representations using untrained deep neural networks work provides new fascinating tool study representation deep network architecture sheds light new understandings deep visualization possibly lead way compare network architectures without training
introspective classification convolutional nets propose introspective convolutional networks icn emphasize importance convolutional neural networks empowered generative capabilities employ reclassification synthesis algorithm perform training using formulation stemmed bayes theory icn tries iteratively synthesize pseudo negative samples enhance improving classification single cnn classifier learned time generative able directly synthesize new samples within discriminative model conduct experiments benchmark datasets including mnist cifar svhn using state art cnn architectures observe improved classification results
predicting parameters deep learning demonstrate significant redundancy parameterization several deep learning models given weight values feature possible accurately predict remaining values moreover show parameter values predicted many need learned train several different architectures learning small number weights predicting rest best case able predict weights network without drop accuracy
efficient high order interaction aware feature selection based conditional mutual information study introduces novel feature selection approach cmicot evolution filter methods sequential forward selection sfs whose scoring functions based conditional mutual information state study novel saddle point max min optimization problem build scoring function able identify joint interactions several features method fills gap based sfs techniques high order dependencies high dimensional case estimation prohibitively high sample complexity mitigate cost using greedy approximation binary representatives makes technique able effectively used superiority approach demonstrated comparison recently proposed interaction aware filters several interaction agnostic state art ones publicly available benchmark datasets
CONF: TOPICS	 ALPHA	 ETA
 =============================================================== 
20	0.01	0.01
 =============================================================== 
online prediction limit temperature design online algorithm classify vertices graph underpinning algorithm probability distribution ising model isomorphic graph classification based predicting label maximum marginal probability limit temperature respect labels vertices seen far computing classifications unfortunately based complete problem motivates develop algorithm give sequential guarantee online mistake bound framework algorithm optimal graph tree matching prior results general graph algorithm exploits additional connectivity tree provide per cluster bound algorithm efficient cumulative time sequentially predict vertices graph quadratic size graph
online gradient boosting extend theory boosting regression problems online learning setting generalizing batch setting boosting notion weak learning algorithm modeled online learning algorithm linear loss functions competes base class regression functions strong learning algorithm online learning algorithm smooth convex loss functions competes larger class regression functions main result online gradient boosting algorithm converts weak online learning algorithm strong larger class functions linear span base class also give simpler boosting algorithm converts weak online learning algorithm strong larger class functions convex hull base class prove optimality
active learning framework using sparse graph codes sparse polynomials graph sketching let rightarrow mathbb variate polynomial consisting monomials coefficients non goal learn polynomial querying values introduce active learning framework associated low query cost computational runtime significant savings enabled leveraging sampling strategies based modern coding theory specifically design analysis sparse graph codes low density parity check ldpc codes represent state art modern packet communications significantly show design perspective leads exciting best knowledge largely unexplored intellectual connections learning coding key relax worst case assumption ensemble average setting polynomial assumed drawn uniformly random ensemble polynomials given size sparsity framework succeeds high probability respect polynomial ensemble sparsity delta delta exactly learned using queries time log even queries perturbed gaussian noise apply proposed framework graph sketching problem inferring sparse graphs querying graph cuts writing cut function polynomial exploiting graph structure propose sketching algorithm learn arbitrary node unknown graph using cut queries scales almost linearly number edges sub linearly graph size experiments real datasets show significant reductions runtime query complexity compared competitive schemes
spectral norm regularization orthonormal representations graph transduction recent literature cite ando suggests embedding graph unit sphere leads better generalization graph transduction however choice optimal embedding efficient algorithm compute remains open paper show orthonormal representations class unit sphere graph embeddings pac learnable existing pac based analysis apply dimension function class infinite propose alternative pac based bound depend dimension underlying function class related famous lov vartheta function main contribution paper spore spectral regularized orthonormal embedding graph transduction derived pac bound spore posed non smooth convex function emph elliptope problems usually solved semi definite programs sdps time complexity present infeasible inexact proximal iip inexact proximal method performs subgradient procedure approximate projection necessarily feasible iip scalable sdp frac sqrt convergence generally applicable whenever suitable approximate projection available use iip compute spore approximate projection step computed fista accelerated gradient descent procedure show method convergence rate frac sqrt proposed algorithm easily scales 1000 vertices standard sdp computation scale beyond hundred vertices furthermore analysis presented easily extends multiple graph setting
discrete nyi classifiers consider binary classification problem predicting target variable discrete feature vector probability distribution known optimal classifier leading minimum misclassification rate given maximum posteriori probability map decision rule however practice estimating complete joint distribution computationally statistically impossible large values therefore alternative approach first estimate low order marginals joint probability distribution design classifier based estimated low order marginals approach also helpful complete training data instances available due privacy concerns work consider problem designing optimum classifier based estimated low order marginals prove given set marginals minimum hirschfeld gebelein enyi hgr correlation principle introduced leads randomized classification rule shown misclassification rate larger twice misclassification rate optimal classifier show separability condition proposed algorithm equivalent randomized linear regression approach naturally results robust feature selection method selecting subset features maximum worst case hgr correlation target variable theoretical upper bound similar recent discrete chebyshev classifier dcc approach proposed algorithm significant computational advantages since requires solving least square optimization problem finally numerically compare proposed algorithm dcc classifier show proposed algorithm results better misclassification rate various uci data repository datasets
decomposition bounds marginal map marginal map inference involves making map predictions systems defined latent variables missing information significantly difficult pure marginalization map tasks large class efficient convergent variational algorithms dual decomposition exist work generalize dual decomposition generic powered sum inference task includes marginal map along pure marginalization map special cases method based block coordinate descent algorithm new convex decomposition bound guaranteed converge monotonically parallelized efficiently demonstrate approach various inference queries real world problems uai approximate inference challenge showing framework faster reliable previous methods
boosting abstention present new boosting algorithm key scenario binary classification abstention algorithm abstain predicting label point price fixed cost round algorithm selects pair functions base predictor base abstention function define convex upper bounds natural loss function associated problem prove calibrated respect bayes solution algorithm benefits general margin based learning guarantees derive ensembles pairs base predictor abstention functions terms rademacher complexities corresponding function classes give convergence guarantees algorithm along linear time weak learning algorithm abstention stumps also report results several experiments suggesting algorithm provides significant improvement practice confidence based algorithms
beyond convexity stochastic quasi convex optimization poster moved monday thursday 101
model based relative entropy stochastic search stochastic search algorithms general black box optimizers due ease use generality recently also gained lot attention operations research machine learning policy search yet algorithms require lot evaluations objective scale poorly problem dimension affected highly noisy objective functions converge prematurely alleviate problems introduce new surrogate based stochastic search approach learn simple quadratic surrogate models objective function quality quadratic approximation limited greedily exploit learned models algorithm misled inaccurate optimum introduced surrogate instead use information theoretic constraints bound distance new old data distribution maximizing objective function additionally new method able sustain exploration search distribution avoid premature convergence compare method state art black box optimization methods standard uni modal multi modal optimization functions simulated planar robot tasks complex robot ball throwing task proposed method considerably outperforms existing approaches
finite set smooth functions sum strongly convex
hardness online sleeping combinatorial optimization problems show several online combinatorial optimization problems admit efficient regret algorithms become computationally hard sleeping setting subset actions becomes unavailable round specifically show sleeping versions problems least hard pac learning dnf expressions long standing open problem show hardness sleeping versions online shortest paths online minimum spanning tree online subsets online truncated permutations online minimum cut online bipartite matching hardness result sleeping version online shortest paths problem resolves open problem presented colt 2015 koolen 2015
learning kernels using local rademacher complexity use notion local rademacher complexity design new algorithms learning kernels algorithms thereby benefit sharper learning bounds based notion certain general conditions guarantee faster convergence rate devise new learning kernel algorithms based convex optimization problem give efficient solution using existing learning kernel techniques another formulated programming problem describe solution detail also report results experiments algorithms binary multi class classification tasks
boosting framework grounds online learning exploiting duality boosting online learning present boosting framework proves extremely powerful thanks employing vast knowledge available online learning area using framework develop various algorithms address multiple practically theoretically interesting questions including sparse boosting smooth distribution boosting agnostic learning product generalization double projection online learning algorithms
online multiclass boosting recent work extended theoretical analysis boosting algorithms multiclass problems online settings however multiclass extension batch setting online extensions consider binary classification fill gap literature defining justifying weak learning condition online multiclass boosting condition leads optimal boosting algorithm requires minimal number weak learners achieve certain accuracy additionally propose adaptive algorithm near optimal enjoys excellent performance real data due adaptive property
convex optimization procedure clustering theoretical revisit paper present theoretical analysis son convex optimization procedure clustering using sum norms son regularization recently proposed cite icml2011hocking_419 son lindsten650707 pelckmans2005convex particular show samples drawn cubes cluster son provably identify cluster membership provided distance cubes larger threshold linearly depends size cube ratio numbers samples cluster best knowledge paper first provide rigorous analysis understand son works believe provide important insights develop novel convex optimization based algorithms clustering
clustering cluster queries propose framework semi supervised active clustering framework ssac learner allowed interact domain expert asking whether given instances belong cluster study query computational complexity clustering framework consider setting expert conforms center based clustering notion margin show trade computational complexity query complexity prove case means clustering expert conforms solution means access relatively queries allows efficient solutions otherwise hard problems particular provide probabilistic polynomial time bpp algorithm clustering setting asks big log log cluster queries runs time complexity big log number clusters number instances success algorithm guaranteed data satisfying margin condition without queries show problem hard also prove lower bound number queries needed computationally efficient clustering algorithm setting
online convex optimization unconstrained domains losses propose online convex optimization algorithm rescaledexp achieves optimal regret unconstrained setting without prior knowledge bounds loss functions prove lower bound showing exponential separation regret existing algorithms require known bound loss functions algorithm require knowledge rescaledexp matches lower bound asymptotically number iterations rescaledexp naturally hyperparameter free demonstrate empirically matches prior optimization algorithms require hyperparameter optimization
weighted theta functions embeddings applications max cut clustering summarization introduce unifying generalization lov theta function associated geometric embedding graphs weights nodes edges show computed exactly semidefinite programming approximate using svm computations show theta function interpreted measure diversity graphs use idea graph embedding algorithms max cut correlation clustering document summarization well represented problems weighted graphs
revisiting perceptron efficient label optimal learning halfspaces long standing problem efficiently learn linear separator using labels possible presence noise work propose efficient perceptron based algorithm actively learning homogeneous linear separators uniform distribution bounded noise label flipped probability eta algorithm achieves near optimal tilde frac 2eta frac epsilon label complexity time tilde frac epsilon 2eta adversarial noise tilde omega epsilon fraction labels flipped algorithm achieves near optimal tilde frac epsilon label complexity time tilde frac epsilon furthermore show active learning algorithm converted efficient passive learning algorithm near optimal sample complexity respect epsilon
active learning weak strong labelers active learner given hypothesis class large set unlabeled examples ability interactively query labels oracle subset examples goal learner learn hypothesis class fits data well making label queries possible work addresses active learning labels obtained strong weak labelers addition standard active learning setting extra weak labeler occasionally provide incorrect labels example learning classify medical images either expensive labels obtained physician oracle strong labeler cheaper occasionally incorrect labels obtained medical resident weak labeler goal learn classifier low error data labeled oracle using weak labeler reduce number label queries made labeler provide active learning algorithm setting establish statistical consistency analyze label complexity characterize provide label savings using strong labeler alone
texture synthesis using convolutional neural networks introduce new model natural textures based feature spaces convolutional neural networks optimised object recognition samples model high perceptual quality demonstrating generative power neural networks trained purely discriminative fashion within model textures represented correlations feature maps several layers network show across layers texture representations increasingly capture statistical properties natural images making object information explicit model provides new tool generate stimuli neuroscience might offer insights deep representations learned convolutional neural networks
convolutional neural networks intra layer recurrent connections scene labeling scene labeling challenging computer vision task requires use local discriminative features global context information adopt deep recurrent convolutional neural network rcnn task originally proposed object recognition different traditional convolutional neural networks cnn model intra layer recurrent connections convolutional layers therefore convolutional layer becomes dimensional recurrent neural network units receive constant feed forward inputs previous layer recurrent inputs neighborhoods recurrent iterations proceed region context captured unit expands way feature extraction context modulation seamlessly integrated different typical methods entail separate modules steps utilize context multi scale rcnn proposed benchmark datasets standford background sift flow model outperforms many state art models accuracy efficiency
exploring models data image question answering work aims address problem image based question answering new models datasets work propose use neural networks visual semantic embeddings without intermediate stages object detection image segmentation predict answers simple questions images model performs times better published results existing image dataset also present question generation algorithm converts image descriptions widely available form used algorithm produce order magnitude larger dataset evenly distributed answers suite baseline results new dataset also presented
hierarchical question image attention visual question answering number recent works proposed attention models visual question answering vqa generate spatial maps highlighting image regions relevant answering question paper argue addition modeling look visual attention equally important model words listen question attention present novel attention model vqa jointly reasons image question attention addition model reasons question consequently image via attention mechanism hierarchical fashion via novel dimensional convolution neural networks cnn model improves state art vqa dataset coco dataset using resnet performance improved vqa coco
powerful generative model using random weights deep image representation extent success deep visualization due training could deep visualization using untrained random weight networks address issue explore new powerful generative models popular deep visualization tasks using untrained random weight convolutional neural networks first invert representations feature spaces reconstruct images white noise inputs reconstruction quality statistically higher method applied well trained networks architecture next synthesize textures using scaled correlations representations multiple layers results almost indistinguishable original natural texture synthesized textures based trained network third recasting content image style various artworks create artistic images high perceptual quality highly competitive prior work gatys pretrained networks knowledge first demonstration image representations using untrained deep neural networks work provides new fascinating tool study representation deep network architecture sheds light new understandings deep visualization possibly lead way compare network architectures without training
recursive training convolutional networks neuronal boundary prediction efforts automate reconstruction neural circuits electron microscopic brain images critical field connectomics important computation reconstruction detection neuronal boundaries images acquired serial section leading technique highly anisotropic inferior quality along third dimension images max pooling convolutional network set standard performance boundary detection achieve substantial gain accuracy innovations following trend towards deeper networks object recognition use much deeper network previously employed boundary detection second incorporate well filters enable computations use context finally adopt recursively trained architecture first network generates preliminary boundary map provided input along original image second network generates final boundary map backpropagation training accelerated znn new implementation convolutional networks uses multicore cpu parallelism speed hybrid architecture could generally applicable types anisotropic images including video recursive framework image labeling problem
gated recurrent convolution neural network ocr optical character recognition ocr aims recognize text natural images widely researched computer vision community paper present new architecture named gated recurrent convolution layer grcl challenge grcl constructed adding gate recurrent convolution layer rcl find equipped gate control context modulation rcl balancing feed forward component well recurrent component addition build bidirectional long short term memory blstm sequence modelling test several variants blstm find suitable architecture ocr finally combine gated recurrent convolution neural network grcnn blstm recognize text natural image grcnn blstm trained end end outperforms benchmark datasets terms state art results including iiit street view text svt icdar
class network models recoverable spectral clustering finding communities networks problem remains difficult spite amount attention recently received stochastic block model sbm generative model graphs communities simplicity theoretical understanding advanced fast recent years particular various results showing simple versions spectralclustering using normalized laplacian graph recoverthe communities almost perfectly high probability show essentially algorithm used sbm extension called degree corrected sbm works wider class block models call preference frame models essentially guarantees moreover parametrization introduce clearly exhibits free parameters needed specify class models results bounds expose clarity parameters control recovery error model class
visalogy answering visual analogy questions paper study problem answering visual analogy questions questions take form image image image answering questions entails discovering mapping image image extending mapping image searching image relation holds pose problem learning embedding encourages pairs analogous images similar transformations close together using convolutional neural networks quadruple siamese architecture introduce dataset visual analogy questions natural images show first results kind solving analogy questions natural images
deep nets really need deep currently deep neural networks state art problems speech recognition computer vision paper empirically demonstrate shallow feed forward nets learn complex functions previously learned deep nets achieve accuracies previously achievable deep models moreover cases shallow nets learn deep functions using number parameters original deep models timit phoneme recognition cifar image recognition tasks shallow nets trained perform similarly complex well engineered deeper convolutional models
visual question answering question representation update qru method aims reasoning natural language questions visual images given natural language question image model updates question representation iteratively selecting image regions relevant query learns give correct answer model contains several reasoning layers exploiting complex visual relations visual question answering vqa task proposed network end end trainable back propagation weights initialized using pre trained convolutional neural network cnn gated recurrent unit gru method evaluated challenging datasets coco vqa yields state art performance
deep generative image models using laplacian pyramid adversarial networks paper introduce generative model capable producing high quality samples natural images approach uses cascade convolutional networks convnets within laplacian pyramid framework generate images coarse fine fashion level pyramid separate generative convnet model trained using generative adversarial nets gan approach samples drawn model significantly higher quality existing models quantitive assessment human evaluators cifar10 samples mistaken real images around time compared gan samples also show samples diverse datasets stl10 lsun
return gating network combining generative models discriminative training natural image priors recent years approaches based machine learning achieved state art performance image restoration problems successful approaches include generative models natural images well discriminative training deep neural networks discriminative training feed forward architectures allows explicit control computational cost performing restoration therefore often leads better performance cost run time contrast generative models advantage trained adapted image restoration task simple use bayes rule paper show combine strengths approaches training discriminative feed forward architecture predict state latent variables generative model natural images apply idea successful gaussian mixture model gmm natural images show possible achieve comparable performance original gmm orders magnitude improvement run time maintaining advantage generative models
variational dropout local reparameterization trick explore yet unexploited opportunity drastically improving efficiency stochastic gradient variational bayes sgvb global model parameters regular sgvb estimators rely sampling parameters per minibatch data variance constant minibatch size efficiency estimators drastically improved upon translating uncertainty global parameters local noise independent across datapoints minibatch reparameterizations local noise trivially parallelized variance inversely proportional minibatch size generally leading much faster convergence find important connection regularization dropout original gaussian dropout objective corresponds sgvb local noise scale invariant prior proportionally fixed posterior variance method allows inference flexibly parameterized posteriors specifically propose emph variational dropout generalization gaussian dropout flexibly parameterized posterior often leading better generalization method demonstrated several experiments
unsupervised learning spoken language visual context humans learn speak read write computers paper present deep neural network model capable rudimentary spoken language acquisition using untranscribed audio training data whose supervision comes form contextually relevant visual images describe collection data comprised 120 000 spoken audio captions places image dataset evaluate model image search annotation task also provide visualizations suggest model learning recognize meaningful words within caption spectrograms
combinatorial energy learning image segmentation introduce new machine learning approach image segmentation uses neural network model conditional energy segmentation given image approach combinatorial energy learning image segmentation celis places particular emphasis modeling inherent combinatorial nature dense image segmentation problems propose efficient algorithms learning deep neural networks model energy function local optimization energy space supervoxel agglomerations extensively evaluate method publicly available microscopy dataset billion voxels ground truth data billion voxel test set find method improves volumetric reconstruction accuracy compared state art baseline methods graph based segmentation output convolutional neural network trained predict boundaries well random forest classifier trained agglomerate supervoxels generated convolutional neural network
deep neural networks suffer crowding crowding visual effect suffered humans object recognized isolation longer recognized objects called clutter placed close work study effect crowding artificial deep neural networks dnns object recognition analyze deep convolutional neural networks dcnns well extension dcnns multi scale change receptive field size convolution filters position image called eccentricity dependent models latter networks recently proposed modeling feedforward path primate visual cortex results reveal incorporating clutter images training set learning dnns lead robustness clutter seen training also dnns trained objects isolation find recognition accuracy dnns falls closer clutter target object clutter find visual similarity target clutter also plays role pooling early layers dnn leads crowding finally show eccentricity dependent model trained objects isolation recognize target objects clutter objects near center image whereas dcnn cannot
learning deep features scene recognition using places database scene recognition hallmark tasks computer vision allowing definition context object recognition whereas tremendous recent progress object recognition tasks due availability large datasets like imagenet rise convolutional neural networks cnns learning high level features performance scene recognition attained level success current deep features trained imagenet competitive enough tasks introduce new scene centric database called places million labeled pictures scenes propose new methods compare density diversity image datasets show places dense scene datasets diversity using cnn learn deep features scene recognition tasks establish new state art results several scene centric datasets visualization cnn layers responses allows show differences internal representations object centric scene centric networks
training deep networks theoretical empirical evidence indicates depth neural networks crucial success however training becomes difficult depth increases training deep networks remains open problem introduce new architecture designed overcome called highway networks allow unimpeded information flow across many layers information highways inspired long short term memory recurrent networks use adaptive gating units regulate information flow even hundreds layers highway networks trained directly simple gradient descent enables study extremely deep efficient architectures
multimodal learning reasoning visual question answering reasoning entities relationships multimodal data key goal artificial general intelligence visual question answering vqa problem excellent way test reasoning capabilities model multimodal representation learning however current vqa models oversimplified deep neural networks comprised long short term memory lstm unit question comprehension convolutional neural network cnn learning single image representation argue single visual representation contains limited general information image contents thus limit model reasoning capabilities work introduce modular neural network model learns multimodal multifaceted representation image question proposed model learns use multimodal representation reason image entities achieves new state art performance vqa benchmark datasets vqa wide margin
texture synthesis using convolutional neural networks introduce new model natural textures based feature spaces convolutional neural networks optimised object recognition samples model high perceptual quality demonstrating generative power neural networks trained purely discriminative fashion within model textures represented correlations feature maps several layers network show across layers texture representations increasingly capture statistical properties natural images making object information explicit model provides new tool generate stimuli neuroscience might offer insights deep representations learned convolutional neural networks
convolutional neural networks intra layer recurrent connections scene labeling scene labeling challenging computer vision task requires use local discriminative features global context information adopt deep recurrent convolutional neural network rcnn task originally proposed object recognition different traditional convolutional neural networks cnn model intra layer recurrent connections convolutional layers therefore convolutional layer becomes dimensional recurrent neural network units receive constant feed forward inputs previous layer recurrent inputs neighborhoods recurrent iterations proceed region context captured unit expands way feature extraction context modulation seamlessly integrated different typical methods entail separate modules steps utilize context multi scale rcnn proposed benchmark datasets standford background sift flow model outperforms many state art models accuracy efficiency
exploring models data image question answering work aims address problem image based question answering new models datasets work propose use neural networks visual semantic embeddings without intermediate stages object detection image segmentation predict answers simple questions images model performs times better published results existing image dataset also present question generation algorithm converts image descriptions widely available form used algorithm produce order magnitude larger dataset evenly distributed answers suite baseline results new dataset also presented
hierarchical question image attention visual question answering number recent works proposed attention models visual question answering vqa generate spatial maps highlighting image regions relevant answering question paper argue addition modeling look visual attention equally important model words listen question attention present novel attention model vqa jointly reasons image question attention addition model reasons question consequently image via attention mechanism hierarchical fashion via novel dimensional convolution neural networks cnn model improves state art vqa dataset coco dataset using resnet performance improved vqa coco
powerful generative model using random weights deep image representation extent success deep visualization due training could deep visualization using untrained random weight networks address issue explore new powerful generative models popular deep visualization tasks using untrained random weight convolutional neural networks first invert representations feature spaces reconstruct images white noise inputs reconstruction quality statistically higher method applied well trained networks architecture next synthesize textures using scaled correlations representations multiple layers results almost indistinguishable original natural texture synthesized textures based trained network third recasting content image style various artworks create artistic images high perceptual quality highly competitive prior work gatys pretrained networks knowledge first demonstration image representations using untrained deep neural networks work provides new fascinating tool study representation deep network architecture sheds light new understandings deep visualization possibly lead way compare network architectures without training
recursive training convolutional networks neuronal boundary prediction efforts automate reconstruction neural circuits electron microscopic brain images critical field connectomics important computation reconstruction detection neuronal boundaries images acquired serial section leading technique highly anisotropic inferior quality along third dimension images max pooling convolutional network set standard performance boundary detection achieve substantial gain accuracy innovations following trend towards deeper networks object recognition use much deeper network previously employed boundary detection second incorporate well filters enable computations use context finally adopt recursively trained architecture first network generates preliminary boundary map provided input along original image second network generates final boundary map backpropagation training accelerated znn new implementation convolutional networks uses multicore cpu parallelism speed hybrid architecture could generally applicable types anisotropic images including video recursive framework image labeling problem
gated recurrent convolution neural network ocr optical character recognition ocr aims recognize text natural images widely researched computer vision community paper present new architecture named gated recurrent convolution layer grcl challenge grcl constructed adding gate recurrent convolution layer rcl find equipped gate control context modulation rcl balancing feed forward component well recurrent component addition build bidirectional long short term memory blstm sequence modelling test several variants blstm find suitable architecture ocr finally combine gated recurrent convolution neural network grcnn blstm recognize text natural image grcnn blstm trained end end outperforms benchmark datasets terms state art results including iiit street view text svt icdar
class network models recoverable spectral clustering finding communities networks problem remains difficult spite amount attention recently received stochastic block model sbm generative model graphs communities simplicity theoretical understanding advanced fast recent years particular various results showing simple versions spectralclustering using normalized laplacian graph recoverthe communities almost perfectly high probability show essentially algorithm used sbm extension called degree corrected sbm works wider class block models call preference frame models essentially guarantees moreover parametrization introduce clearly exhibits free parameters needed specify class models results bounds expose clarity parameters control recovery error model class
visalogy answering visual analogy questions paper study problem answering visual analogy questions questions take form image image image answering questions entails discovering mapping image image extending mapping image searching image relation holds pose problem learning embedding encourages pairs analogous images similar transformations close together using convolutional neural networks quadruple siamese architecture introduce dataset visual analogy questions natural images show first results kind solving analogy questions natural images
deep nets really need deep currently deep neural networks state art problems speech recognition computer vision paper empirically demonstrate shallow feed forward nets learn complex functions previously learned deep nets achieve accuracies previously achievable deep models moreover cases shallow nets learn deep functions using number parameters original deep models timit phoneme recognition cifar image recognition tasks shallow nets trained perform similarly complex well engineered deeper convolutional models
visual question answering question representation update qru method aims reasoning natural language questions visual images given natural language question image model updates question representation iteratively selecting image regions relevant query learns give correct answer model contains several reasoning layers exploiting complex visual relations visual question answering vqa task proposed network end end trainable back propagation weights initialized using pre trained convolutional neural network cnn gated recurrent unit gru method evaluated challenging datasets coco vqa yields state art performance
deep generative image models using laplacian pyramid adversarial networks paper introduce generative model capable producing high quality samples natural images approach uses cascade convolutional networks convnets within laplacian pyramid framework generate images coarse fine fashion level pyramid separate generative convnet model trained using generative adversarial nets gan approach samples drawn model significantly higher quality existing models quantitive assessment human evaluators cifar10 samples mistaken real images around time compared gan samples also show samples diverse datasets stl10 lsun
return gating network combining generative models discriminative training natural image priors recent years approaches based machine learning achieved state art performance image restoration problems successful approaches include generative models natural images well discriminative training deep neural networks discriminative training feed forward architectures allows explicit control computational cost performing restoration therefore often leads better performance cost run time contrast generative models advantage trained adapted image restoration task simple use bayes rule paper show combine strengths approaches training discriminative feed forward architecture predict state latent variables generative model natural images apply idea successful gaussian mixture model gmm natural images show possible achieve comparable performance original gmm orders magnitude improvement run time maintaining advantage generative models
variational dropout local reparameterization trick explore yet unexploited opportunity drastically improving efficiency stochastic gradient variational bayes sgvb global model parameters regular sgvb estimators rely sampling parameters per minibatch data variance constant minibatch size efficiency estimators drastically improved upon translating uncertainty global parameters local noise independent across datapoints minibatch reparameterizations local noise trivially parallelized variance inversely proportional minibatch size generally leading much faster convergence find important connection regularization dropout original gaussian dropout objective corresponds sgvb local noise scale invariant prior proportionally fixed posterior variance method allows inference flexibly parameterized posteriors specifically propose emph variational dropout generalization gaussian dropout flexibly parameterized posterior often leading better generalization method demonstrated several experiments
unsupervised learning spoken language visual context humans learn speak read write computers paper present deep neural network model capable rudimentary spoken language acquisition using untranscribed audio training data whose supervision comes form contextually relevant visual images describe collection data comprised 120 000 spoken audio captions places image dataset evaluate model image search annotation task also provide visualizations suggest model learning recognize meaningful words within caption spectrograms
combinatorial energy learning image segmentation introduce new machine learning approach image segmentation uses neural network model conditional energy segmentation given image approach combinatorial energy learning image segmentation celis places particular emphasis modeling inherent combinatorial nature dense image segmentation problems propose efficient algorithms learning deep neural networks model energy function local optimization energy space supervoxel agglomerations extensively evaluate method publicly available microscopy dataset billion voxels ground truth data billion voxel test set find method improves volumetric reconstruction accuracy compared state art baseline methods graph based segmentation output convolutional neural network trained predict boundaries well random forest classifier trained agglomerate supervoxels generated convolutional neural network
deep neural networks suffer crowding crowding visual effect suffered humans object recognized isolation longer recognized objects called clutter placed close work study effect crowding artificial deep neural networks dnns object recognition analyze deep convolutional neural networks dcnns well extension dcnns multi scale change receptive field size convolution filters position image called eccentricity dependent models latter networks recently proposed modeling feedforward path primate visual cortex results reveal incorporating clutter images training set learning dnns lead robustness clutter seen training also dnns trained objects isolation find recognition accuracy dnns falls closer clutter target object clutter find visual similarity target clutter also plays role pooling early layers dnn leads crowding finally show eccentricity dependent model trained objects isolation recognize target objects clutter objects near center image whereas dcnn cannot
learning deep features scene recognition using places database scene recognition hallmark tasks computer vision allowing definition context object recognition whereas tremendous recent progress object recognition tasks due availability large datasets like imagenet rise convolutional neural networks cnns learning high level features performance scene recognition attained level success current deep features trained imagenet competitive enough tasks introduce new scene centric database called places million labeled pictures scenes propose new methods compare density diversity image datasets show places dense scene datasets diversity using cnn learn deep features scene recognition tasks establish new state art results several scene centric datasets visualization cnn layers responses allows show differences internal representations object centric scene centric networks
training deep networks theoretical empirical evidence indicates depth neural networks crucial success however training becomes difficult depth increases training deep networks remains open problem introduce new architecture designed overcome called highway networks allow unimpeded information flow across many layers information highways inspired long short term memory recurrent networks use adaptive gating units regulate information flow even hundreds layers highway networks trained directly simple gradient descent enables study extremely deep efficient architectures
multimodal learning reasoning visual question answering reasoning entities relationships multimodal data key goal artificial general intelligence visual question answering vqa problem excellent way test reasoning capabilities model multimodal representation learning however current vqa models oversimplified deep neural networks comprised long short term memory lstm unit question comprehension convolutional neural network cnn learning single image representation argue single visual representation contains limited general information image contents thus limit model reasoning capabilities work introduce modular neural network model learns multimodal multifaceted representation image question proposed model learns use multimodal representation reason image entities achieves new state art performance vqa benchmark datasets vqa wide margin
texture synthesis using convolutional neural networks introduce new model natural textures based feature spaces convolutional neural networks optimised object recognition samples model high perceptual quality demonstrating generative power neural networks trained purely discriminative fashion within model textures represented correlations feature maps several layers network show across layers texture representations increasingly capture statistical properties natural images making object information explicit model provides new tool generate stimuli neuroscience might offer insights deep representations learned convolutional neural networks
convolutional neural networks intra layer recurrent connections scene labeling scene labeling challenging computer vision task requires use local discriminative features global context information adopt deep recurrent convolutional neural network rcnn task originally proposed object recognition different traditional convolutional neural networks cnn model intra layer recurrent connections convolutional layers therefore convolutional layer becomes dimensional recurrent neural network units receive constant feed forward inputs previous layer recurrent inputs neighborhoods recurrent iterations proceed region context captured unit expands way feature extraction context modulation seamlessly integrated different typical methods entail separate modules steps utilize context multi scale rcnn proposed benchmark datasets standford background sift flow model outperforms many state art models accuracy efficiency
exploring models data image question answering work aims address problem image based question answering new models datasets work propose use neural networks visual semantic embeddings without intermediate stages object detection image segmentation predict answers simple questions images model performs times better published results existing image dataset also present question generation algorithm converts image descriptions widely available form used algorithm produce order magnitude larger dataset evenly distributed answers suite baseline results new dataset also presented
hierarchical question image attention visual question answering number recent works proposed attention models visual question answering vqa generate spatial maps highlighting image regions relevant answering question paper argue addition modeling look visual attention equally important model words listen question attention present novel attention model vqa jointly reasons image question attention addition model reasons question consequently image via attention mechanism hierarchical fashion via novel dimensional convolution neural networks cnn model improves state art vqa dataset coco dataset using resnet performance improved vqa coco
powerful generative model using random weights deep image representation extent success deep visualization due training could deep visualization using untrained random weight networks address issue explore new powerful generative models popular deep visualization tasks using untrained random weight convolutional neural networks first invert representations feature spaces reconstruct images white noise inputs reconstruction quality statistically higher method applied well trained networks architecture next synthesize textures using scaled correlations representations multiple layers results almost indistinguishable original natural texture synthesized textures based trained network third recasting content image style various artworks create artistic images high perceptual quality highly competitive prior work gatys pretrained networks knowledge first demonstration image representations using untrained deep neural networks work provides new fascinating tool study representation deep network architecture sheds light new understandings deep visualization possibly lead way compare network architectures without training
recursive training convolutional networks neuronal boundary prediction efforts automate reconstruction neural circuits electron microscopic brain images critical field connectomics important computation reconstruction detection neuronal boundaries images acquired serial section leading technique highly anisotropic inferior quality along third dimension images max pooling convolutional network set standard performance boundary detection achieve substantial gain accuracy innovations following trend towards deeper networks object recognition use much deeper network previously employed boundary detection second incorporate well filters enable computations use context finally adopt recursively trained architecture first network generates preliminary boundary map provided input along original image second network generates final boundary map backpropagation training accelerated znn new implementation convolutional networks uses multicore cpu parallelism speed hybrid architecture could generally applicable types anisotropic images including video recursive framework image labeling problem
gated recurrent convolution neural network ocr optical character recognition ocr aims recognize text natural images widely researched computer vision community paper present new architecture named gated recurrent convolution layer grcl challenge grcl constructed adding gate recurrent convolution layer rcl find equipped gate control context modulation rcl balancing feed forward component well recurrent component addition build bidirectional long short term memory blstm sequence modelling test several variants blstm find suitable architecture ocr finally combine gated recurrent convolution neural network grcnn blstm recognize text natural image grcnn blstm trained end end outperforms benchmark datasets terms state art results including iiit street view text svt icdar
class network models recoverable spectral clustering finding communities networks problem remains difficult spite amount attention recently received stochastic block model sbm generative model graphs communities simplicity theoretical understanding advanced fast recent years particular various results showing simple versions spectralclustering using normalized laplacian graph recoverthe communities almost perfectly high probability show essentially algorithm used sbm extension called degree corrected sbm works wider class block models call preference frame models essentially guarantees moreover parametrization introduce clearly exhibits free parameters needed specify class models results bounds expose clarity parameters control recovery error model class
visalogy answering visual analogy questions paper study problem answering visual analogy questions questions take form image image image answering questions entails discovering mapping image image extending mapping image searching image relation holds pose problem learning embedding encourages pairs analogous images similar transformations close together using convolutional neural networks quadruple siamese architecture introduce dataset visual analogy questions natural images show first results kind solving analogy questions natural images
deep nets really need deep currently deep neural networks state art problems speech recognition computer vision paper empirically demonstrate shallow feed forward nets learn complex functions previously learned deep nets achieve accuracies previously achievable deep models moreover cases shallow nets learn deep functions using number parameters original deep models timit phoneme recognition cifar image recognition tasks shallow nets trained perform similarly complex well engineered deeper convolutional models
visual question answering question representation update qru method aims reasoning natural language questions visual images given natural language question image model updates question representation iteratively selecting image regions relevant query learns give correct answer model contains several reasoning layers exploiting complex visual relations visual question answering vqa task proposed network end end trainable back propagation weights initialized using pre trained convolutional neural network cnn gated recurrent unit gru method evaluated challenging datasets coco vqa yields state art performance
deep generative image models using laplacian pyramid adversarial networks paper introduce generative model capable producing high quality samples natural images approach uses cascade convolutional networks convnets within laplacian pyramid framework generate images coarse fine fashion level pyramid separate generative convnet model trained using generative adversarial nets gan approach samples drawn model significantly higher quality existing models quantitive assessment human evaluators cifar10 samples mistaken real images around time compared gan samples also show samples diverse datasets stl10 lsun
return gating network combining generative models discriminative training natural image priors recent years approaches based machine learning achieved state art performance image restoration problems successful approaches include generative models natural images well discriminative training deep neural networks discriminative training feed forward architectures allows explicit control computational cost performing restoration therefore often leads better performance cost run time contrast generative models advantage trained adapted image restoration task simple use bayes rule paper show combine strengths approaches training discriminative feed forward architecture predict state latent variables generative model natural images apply idea successful gaussian mixture model gmm natural images show possible achieve comparable performance original gmm orders magnitude improvement run time maintaining advantage generative models
variational dropout local reparameterization trick explore yet unexploited opportunity drastically improving efficiency stochastic gradient variational bayes sgvb global model parameters regular sgvb estimators rely sampling parameters per minibatch data variance constant minibatch size efficiency estimators drastically improved upon translating uncertainty global parameters local noise independent across datapoints minibatch reparameterizations local noise trivially parallelized variance inversely proportional minibatch size generally leading much faster convergence find important connection regularization dropout original gaussian dropout objective corresponds sgvb local noise scale invariant prior proportionally fixed posterior variance method allows inference flexibly parameterized posteriors specifically propose emph variational dropout generalization gaussian dropout flexibly parameterized posterior often leading better generalization method demonstrated several experiments
unsupervised learning spoken language visual context humans learn speak read write computers paper present deep neural network model capable rudimentary spoken language acquisition using untranscribed audio training data whose supervision comes form contextually relevant visual images describe collection data comprised 120 000 spoken audio captions places image dataset evaluate model image search annotation task also provide visualizations suggest model learning recognize meaningful words within caption spectrograms
combinatorial energy learning image segmentation introduce new machine learning approach image segmentation uses neural network model conditional energy segmentation given image approach combinatorial energy learning image segmentation celis places particular emphasis modeling inherent combinatorial nature dense image segmentation problems propose efficient algorithms learning deep neural networks model energy function local optimization energy space supervoxel agglomerations extensively evaluate method publicly available microscopy dataset billion voxels ground truth data billion voxel test set find method improves volumetric reconstruction accuracy compared state art baseline methods graph based segmentation output convolutional neural network trained predict boundaries well random forest classifier trained agglomerate supervoxels generated convolutional neural network
deep neural networks suffer crowding crowding visual effect suffered humans object recognized isolation longer recognized objects called clutter placed close work study effect crowding artificial deep neural networks dnns object recognition analyze deep convolutional neural networks dcnns well extension dcnns multi scale change receptive field size convolution filters position image called eccentricity dependent models latter networks recently proposed modeling feedforward path primate visual cortex results reveal incorporating clutter images training set learning dnns lead robustness clutter seen training also dnns trained objects isolation find recognition accuracy dnns falls closer clutter target object clutter find visual similarity target clutter also plays role pooling early layers dnn leads crowding finally show eccentricity dependent model trained objects isolation recognize target objects clutter objects near center image whereas dcnn cannot
learning deep features scene recognition using places database scene recognition hallmark tasks computer vision allowing definition context object recognition whereas tremendous recent progress object recognition tasks due availability large datasets like imagenet rise convolutional neural networks cnns learning high level features performance scene recognition attained level success current deep features trained imagenet competitive enough tasks introduce new scene centric database called places million labeled pictures scenes propose new methods compare density diversity image datasets show places dense scene datasets diversity using cnn learn deep features scene recognition tasks establish new state art results several scene centric datasets visualization cnn layers responses allows show differences internal representations object centric scene centric networks
training deep networks theoretical empirical evidence indicates depth neural networks crucial success however training becomes difficult depth increases training deep networks remains open problem introduce new architecture designed overcome called highway networks allow unimpeded information flow across many layers information highways inspired long short term memory recurrent networks use adaptive gating units regulate information flow even hundreds layers highway networks trained directly simple gradient descent enables study extremely deep efficient architectures
multimodal learning reasoning visual question answering reasoning entities relationships multimodal data key goal artificial general intelligence visual question answering vqa problem excellent way test reasoning capabilities model multimodal representation learning however current vqa models oversimplified deep neural networks comprised long short term memory lstm unit question comprehension convolutional neural network cnn learning single image representation argue single visual representation contains limited general information image contents thus limit model reasoning capabilities work introduce modular neural network model learns multimodal multifaceted representation image question proposed model learns use multimodal representation reason image entities achieves new state art performance vqa benchmark datasets vqa wide margin
texture synthesis using convolutional neural networks introduce new model natural textures based feature spaces convolutional neural networks optimised object recognition samples model high perceptual quality demonstrating generative power neural networks trained purely discriminative fashion within model textures represented correlations feature maps several layers network show across layers texture representations increasingly capture statistical properties natural images making object information explicit model provides new tool generate stimuli neuroscience might offer insights deep representations learned convolutional neural networks
convolutional neural networks intra layer recurrent connections scene labeling scene labeling challenging computer vision task requires use local discriminative features global context information adopt deep recurrent convolutional neural network rcnn task originally proposed object recognition different traditional convolutional neural networks cnn model intra layer recurrent connections convolutional layers therefore convolutional layer becomes dimensional recurrent neural network units receive constant feed forward inputs previous layer recurrent inputs neighborhoods recurrent iterations proceed region context captured unit expands way feature extraction context modulation seamlessly integrated different typical methods entail separate modules steps utilize context multi scale rcnn proposed benchmark datasets standford background sift flow model outperforms many state art models accuracy efficiency
exploring models data image question answering work aims address problem image based question answering new models datasets work propose use neural networks visual semantic embeddings without intermediate stages object detection image segmentation predict answers simple questions images model performs times better published results existing image dataset also present question generation algorithm converts image descriptions widely available form used algorithm produce order magnitude larger dataset evenly distributed answers suite baseline results new dataset also presented
hierarchical question image attention visual question answering number recent works proposed attention models visual question answering vqa generate spatial maps highlighting image regions relevant answering question paper argue addition modeling look visual attention equally important model words listen question attention present novel attention model vqa jointly reasons image question attention addition model reasons question consequently image via attention mechanism hierarchical fashion via novel dimensional convolution neural networks cnn model improves state art vqa dataset coco dataset using resnet performance improved vqa coco
recursive training convolutional networks neuronal boundary prediction efforts automate reconstruction neural circuits electron microscopic brain images critical field connectomics important computation reconstruction detection neuronal boundaries images acquired serial section leading technique highly anisotropic inferior quality along third dimension images max pooling convolutional network set standard performance boundary detection achieve substantial gain accuracy innovations following trend towards deeper networks object recognition use much deeper network previously employed boundary detection second incorporate well filters enable computations use context finally adopt recursively trained architecture first network generates preliminary boundary map provided input along original image second network generates final boundary map backpropagation training accelerated znn new implementation convolutional networks uses multicore cpu parallelism speed hybrid architecture could generally applicable types anisotropic images including video recursive framework image labeling problem
powerful generative model using random weights deep image representation extent success deep visualization due training could deep visualization using untrained random weight networks address issue explore new powerful generative models popular deep visualization tasks using untrained random weight convolutional neural networks first invert representations feature spaces reconstruct images white noise inputs reconstruction quality statistically higher method applied well trained networks architecture next synthesize textures using scaled correlations representations multiple layers results almost indistinguishable original natural texture synthesized textures based trained network third recasting content image style various artworks create artistic images high perceptual quality highly competitive prior work gatys pretrained networks knowledge first demonstration image representations using untrained deep neural networks work provides new fascinating tool study representation deep network architecture sheds light new understandings deep visualization possibly lead way compare network architectures without training
gated recurrent convolution neural network ocr optical character recognition ocr aims recognize text natural images widely researched computer vision community paper present new architecture named gated recurrent convolution layer grcl challenge grcl constructed adding gate recurrent convolution layer rcl find equipped gate control context modulation rcl balancing feed forward component well recurrent component addition build bidirectional long short term memory blstm sequence modelling test several variants blstm find suitable architecture ocr finally combine gated recurrent convolution neural network grcnn blstm recognize text natural image grcnn blstm trained end end outperforms benchmark datasets terms state art results including iiit street view text svt icdar
class network models recoverable spectral clustering finding communities networks problem remains difficult spite amount attention recently received stochastic block model sbm generative model graphs communities simplicity theoretical understanding advanced fast recent years particular various results showing simple versions spectralclustering using normalized laplacian graph recoverthe communities almost perfectly high probability show essentially algorithm used sbm extension called degree corrected sbm works wider class block models call preference frame models essentially guarantees moreover parametrization introduce clearly exhibits free parameters needed specify class models results bounds expose clarity parameters control recovery error model class
deep nets really need deep currently deep neural networks state art problems speech recognition computer vision paper empirically demonstrate shallow feed forward nets learn complex functions previously learned deep nets achieve accuracies previously achievable deep models moreover cases shallow nets learn deep functions using number parameters original deep models timit phoneme recognition cifar image recognition tasks shallow nets trained perform similarly complex well engineered deeper convolutional models
visalogy answering visual analogy questions paper study problem answering visual analogy questions questions take form image image image answering questions entails discovering mapping image image extending mapping image searching image relation holds pose problem learning embedding encourages pairs analogous images similar transformations close together using convolutional neural networks quadruple siamese architecture introduce dataset visual analogy questions natural images show first results kind solving analogy questions natural images
deep generative image models using laplacian pyramid adversarial networks paper introduce generative model capable producing high quality samples natural images approach uses cascade convolutional networks convnets within laplacian pyramid framework generate images coarse fine fashion level pyramid separate generative convnet model trained using generative adversarial nets gan approach samples drawn model significantly higher quality existing models quantitive assessment human evaluators cifar10 samples mistaken real images around time compared gan samples also show samples diverse datasets stl10 lsun
visual question answering question representation update qru method aims reasoning natural language questions visual images given natural language question image model updates question representation iteratively selecting image regions relevant query learns give correct answer model contains several reasoning layers exploiting complex visual relations visual question answering vqa task proposed network end end trainable back propagation weights initialized using pre trained convolutional neural network cnn gated recurrent unit gru method evaluated challenging datasets coco vqa yields state art performance
variational dropout local reparameterization trick explore yet unexploited opportunity drastically improving efficiency stochastic gradient variational bayes sgvb global model parameters regular sgvb estimators rely sampling parameters per minibatch data variance constant minibatch size efficiency estimators drastically improved upon translating uncertainty global parameters local noise independent across datapoints minibatch reparameterizations local noise trivially parallelized variance inversely proportional minibatch size generally leading much faster convergence find important connection regularization dropout original gaussian dropout objective corresponds sgvb local noise scale invariant prior proportionally fixed posterior variance method allows inference flexibly parameterized posteriors specifically propose emph variational dropout generalization gaussian dropout flexibly parameterized posterior often leading better generalization method demonstrated several experiments
return gating network combining generative models discriminative training natural image priors recent years approaches based machine learning achieved state art performance image restoration problems successful approaches include generative models natural images well discriminative training deep neural networks discriminative training feed forward architectures allows explicit control computational cost performing restoration therefore often leads better performance cost run time contrast generative models advantage trained adapted image restoration task simple use bayes rule paper show combine strengths approaches training discriminative feed forward architecture predict state latent variables generative model natural images apply idea successful gaussian mixture model gmm natural images show possible achieve comparable performance original gmm orders magnitude improvement run time maintaining advantage generative models
unsupervised learning spoken language visual context humans learn speak read write computers paper present deep neural network model capable rudimentary spoken language acquisition using untranscribed audio training data whose supervision comes form contextually relevant visual images describe collection data comprised 120 000 spoken audio captions places image dataset evaluate model image search annotation task also provide visualizations suggest model learning recognize meaningful words within caption spectrograms
combinatorial energy learning image segmentation introduce new machine learning approach image segmentation uses neural network model conditional energy segmentation given image approach combinatorial energy learning image segmentation celis places particular emphasis modeling inherent combinatorial nature dense image segmentation problems propose efficient algorithms learning deep neural networks model energy function local optimization energy space supervoxel agglomerations extensively evaluate method publicly available microscopy dataset billion voxels ground truth data billion voxel test set find method improves volumetric reconstruction accuracy compared state art baseline methods graph based segmentation output convolutional neural network trained predict boundaries well random forest classifier trained agglomerate supervoxels generated convolutional neural network
training deep networks theoretical empirical evidence indicates depth neural networks crucial success however training becomes difficult depth increases training deep networks remains open problem introduce new architecture designed overcome called highway networks allow unimpeded information flow across many layers information highways inspired long short term memory recurrent networks use adaptive gating units regulate information flow even hundreds layers highway networks trained directly simple gradient descent enables study extremely deep efficient architectures
learning deep features scene recognition using places database scene recognition hallmark tasks computer vision allowing definition context object recognition whereas tremendous recent progress object recognition tasks due availability large datasets like imagenet rise convolutional neural networks cnns learning high level features performance scene recognition attained level success current deep features trained imagenet competitive enough tasks introduce new scene centric database called places million labeled pictures scenes propose new methods compare density diversity image datasets show places dense scene datasets diversity using cnn learn deep features scene recognition tasks establish new state art results several scene centric datasets visualization cnn layers responses allows show differences internal representations object centric scene centric networks
deep neural networks suffer crowding crowding visual effect suffered humans object recognized isolation longer recognized objects called clutter placed close work study effect crowding artificial deep neural networks dnns object recognition analyze deep convolutional neural networks dcnns well extension dcnns multi scale change receptive field size convolution filters position image called eccentricity dependent models latter networks recently proposed modeling feedforward path primate visual cortex results reveal incorporating clutter images training set learning dnns lead robustness clutter seen training also dnns trained objects isolation find recognition accuracy dnns falls closer clutter target object clutter find visual similarity target clutter also plays role pooling early layers dnn leads crowding finally show eccentricity dependent model trained objects isolation recognize target objects clutter objects near center image whereas dcnn cannot
multimodal learning reasoning visual question answering reasoning entities relationships multimodal data key goal artificial general intelligence visual question answering vqa problem excellent way test reasoning capabilities model multimodal representation learning however current vqa models oversimplified deep neural networks comprised long short term memory lstm unit question comprehension convolutional neural network cnn learning single image representation argue single visual representation contains limited general information image contents thus limit model reasoning capabilities work introduce modular neural network model learns multimodal multifaceted representation image question proposed model learns use multimodal representation reason image entities achieves new state art performance vqa benchmark datasets vqa wide margin
texture synthesis using convolutional neural networks introduce new model natural textures based feature spaces convolutional neural networks optimised object recognition samples model high perceptual quality demonstrating generative power neural networks trained purely discriminative fashion within model textures represented correlations feature maps several layers network show across layers texture representations increasingly capture statistical properties natural images making object information explicit model provides new tool generate stimuli neuroscience might offer insights deep representations learned convolutional neural networks
convolutional neural networks intra layer recurrent connections scene labeling scene labeling challenging computer vision task requires use local discriminative features global context information adopt deep recurrent convolutional neural network rcnn task originally proposed object recognition different traditional convolutional neural networks cnn model intra layer recurrent connections convolutional layers therefore convolutional layer becomes dimensional recurrent neural network units receive constant feed forward inputs previous layer recurrent inputs neighborhoods recurrent iterations proceed region context captured unit expands way feature extraction context modulation seamlessly integrated different typical methods entail separate modules steps utilize context multi scale rcnn proposed benchmark datasets standford background sift flow model outperforms many state art models accuracy efficiency
exploring models data image question answering work aims address problem image based question answering new models datasets work propose use neural networks visual semantic embeddings without intermediate stages object detection image segmentation predict answers simple questions images model performs times better published results existing image dataset also present question generation algorithm converts image descriptions widely available form used algorithm produce order magnitude larger dataset evenly distributed answers suite baseline results new dataset also presented
hierarchical question image attention visual question answering number recent works proposed attention models visual question answering vqa generate spatial maps highlighting image regions relevant answering question paper argue addition modeling look visual attention equally important model words listen question attention present novel attention model vqa jointly reasons image question attention addition model reasons question consequently image via attention mechanism hierarchical fashion via novel dimensional convolution neural networks cnn model improves state art vqa dataset coco dataset using resnet performance improved vqa coco
recursive training convolutional networks neuronal boundary prediction efforts automate reconstruction neural circuits electron microscopic brain images critical field connectomics important computation reconstruction detection neuronal boundaries images acquired serial section leading technique highly anisotropic inferior quality along third dimension images max pooling convolutional network set standard performance boundary detection achieve substantial gain accuracy innovations following trend towards deeper networks object recognition use much deeper network previously employed boundary detection second incorporate well filters enable computations use context finally adopt recursively trained architecture first network generates preliminary boundary map provided input along original image second network generates final boundary map backpropagation training accelerated znn new implementation convolutional networks uses multicore cpu parallelism speed hybrid architecture could generally applicable types anisotropic images including video recursive framework image labeling problem
powerful generative model using random weights deep image representation extent success deep visualization due training could deep visualization using untrained random weight networks address issue explore new powerful generative models popular deep visualization tasks using untrained random weight convolutional neural networks first invert representations feature spaces reconstruct images white noise inputs reconstruction quality statistically higher method applied well trained networks architecture next synthesize textures using scaled correlations representations multiple layers results almost indistinguishable original natural texture synthesized textures based trained network third recasting content image style various artworks create artistic images high perceptual quality highly competitive prior work gatys pretrained networks knowledge first demonstration image representations using untrained deep neural networks work provides new fascinating tool study representation deep network architecture sheds light new understandings deep visualization possibly lead way compare network architectures without training
gated recurrent convolution neural network ocr optical character recognition ocr aims recognize text natural images widely researched computer vision community paper present new architecture named gated recurrent convolution layer grcl challenge grcl constructed adding gate recurrent convolution layer rcl find equipped gate control context modulation rcl balancing feed forward component well recurrent component addition build bidirectional long short term memory blstm sequence modelling test several variants blstm find suitable architecture ocr finally combine gated recurrent convolution neural network grcnn blstm recognize text natural image grcnn blstm trained end end outperforms benchmark datasets terms state art results including iiit street view text svt icdar
class network models recoverable spectral clustering finding communities networks problem remains difficult spite amount attention recently received stochastic block model sbm generative model graphs communities simplicity theoretical understanding advanced fast recent years particular various results showing simple versions spectralclustering using normalized laplacian graph recoverthe communities almost perfectly high probability show essentially algorithm used sbm extension called degree corrected sbm works wider class block models call preference frame models essentially guarantees moreover parametrization introduce clearly exhibits free parameters needed specify class models results bounds expose clarity parameters control recovery error model class
deep nets really need deep currently deep neural networks state art problems speech recognition computer vision paper empirically demonstrate shallow feed forward nets learn complex functions previously learned deep nets achieve accuracies previously achievable deep models moreover cases shallow nets learn deep functions using number parameters original deep models timit phoneme recognition cifar image recognition tasks shallow nets trained perform similarly complex well engineered deeper convolutional models
visalogy answering visual analogy questions paper study problem answering visual analogy questions questions take form image image image answering questions entails discovering mapping image image extending mapping image searching image relation holds pose problem learning embedding encourages pairs analogous images similar transformations close together using convolutional neural networks quadruple siamese architecture introduce dataset visual analogy questions natural images show first results kind solving analogy questions natural images
deep generative image models using laplacian pyramid adversarial networks paper introduce generative model capable producing high quality samples natural images approach uses cascade convolutional networks convnets within laplacian pyramid framework generate images coarse fine fashion level pyramid separate generative convnet model trained using generative adversarial nets gan approach samples drawn model significantly higher quality existing models quantitive assessment human evaluators cifar10 samples mistaken real images around time compared gan samples also show samples diverse datasets stl10 lsun
visual question answering question representation update qru method aims reasoning natural language questions visual images given natural language question image model updates question representation iteratively selecting image regions relevant query learns give correct answer model contains several reasoning layers exploiting complex visual relations visual question answering vqa task proposed network end end trainable back propagation weights initialized using pre trained convolutional neural network cnn gated recurrent unit gru method evaluated challenging datasets coco vqa yields state art performance
variational dropout local reparameterization trick explore yet unexploited opportunity drastically improving efficiency stochastic gradient variational bayes sgvb global model parameters regular sgvb estimators rely sampling parameters per minibatch data variance constant minibatch size efficiency estimators drastically improved upon translating uncertainty global parameters local noise independent across datapoints minibatch reparameterizations local noise trivially parallelized variance inversely proportional minibatch size generally leading much faster convergence find important connection regularization dropout original gaussian dropout objective corresponds sgvb local noise scale invariant prior proportionally fixed posterior variance method allows inference flexibly parameterized posteriors specifically propose emph variational dropout generalization gaussian dropout flexibly parameterized posterior often leading better generalization method demonstrated several experiments
return gating network combining generative models discriminative training natural image priors recent years approaches based machine learning achieved state art performance image restoration problems successful approaches include generative models natural images well discriminative training deep neural networks discriminative training feed forward architectures allows explicit control computational cost performing restoration therefore often leads better performance cost run time contrast generative models advantage trained adapted image restoration task simple use bayes rule paper show combine strengths approaches training discriminative feed forward architecture predict state latent variables generative model natural images apply idea successful gaussian mixture model gmm natural images show possible achieve comparable performance original gmm orders magnitude improvement run time maintaining advantage generative models
unsupervised learning spoken language visual context humans learn speak read write computers paper present deep neural network model capable rudimentary spoken language acquisition using untranscribed audio training data whose supervision comes form contextually relevant visual images describe collection data comprised 120 000 spoken audio captions places image dataset evaluate model image search annotation task also provide visualizations suggest model learning recognize meaningful words within caption spectrograms
combinatorial energy learning image segmentation introduce new machine learning approach image segmentation uses neural network model conditional energy segmentation given image approach combinatorial energy learning image segmentation celis places particular emphasis modeling inherent combinatorial nature dense image segmentation problems propose efficient algorithms learning deep neural networks model energy function local optimization energy space supervoxel agglomerations extensively evaluate method publicly available microscopy dataset billion voxels ground truth data billion voxel test set find method improves volumetric reconstruction accuracy compared state art baseline methods graph based segmentation output convolutional neural network trained predict boundaries well random forest classifier trained agglomerate supervoxels generated convolutional neural network
training deep networks theoretical empirical evidence indicates depth neural networks crucial success however training becomes difficult depth increases training deep networks remains open problem introduce new architecture designed overcome called highway networks allow unimpeded information flow across many layers information highways inspired long short term memory recurrent networks use adaptive gating units regulate information flow even hundreds layers highway networks trained directly simple gradient descent enables study extremely deep efficient architectures
learning deep features scene recognition using places database scene recognition hallmark tasks computer vision allowing definition context object recognition whereas tremendous recent progress object recognition tasks due availability large datasets like imagenet rise convolutional neural networks cnns learning high level features performance scene recognition attained level success current deep features trained imagenet competitive enough tasks introduce new scene centric database called places million labeled pictures scenes propose new methods compare density diversity image datasets show places dense scene datasets diversity using cnn learn deep features scene recognition tasks establish new state art results several scene centric datasets visualization cnn layers responses allows show differences internal representations object centric scene centric networks
deep neural networks suffer crowding crowding visual effect suffered humans object recognized isolation longer recognized objects called clutter placed close work study effect crowding artificial deep neural networks dnns object recognition analyze deep convolutional neural networks dcnns well extension dcnns multi scale change receptive field size convolution filters position image called eccentricity dependent models latter networks recently proposed modeling feedforward path primate visual cortex results reveal incorporating clutter images training set learning dnns lead robustness clutter seen training also dnns trained objects isolation find recognition accuracy dnns falls closer clutter target object clutter find visual similarity target clutter also plays role pooling early layers dnn leads crowding finally show eccentricity dependent model trained objects isolation recognize target objects clutter objects near center image whereas dcnn cannot
multimodal learning reasoning visual question answering reasoning entities relationships multimodal data key goal artificial general intelligence visual question answering vqa problem excellent way test reasoning capabilities model multimodal representation learning however current vqa models oversimplified deep neural networks comprised long short term memory lstm unit question comprehension convolutional neural network cnn learning single image representation argue single visual representation contains limited general information image contents thus limit model reasoning capabilities work introduce modular neural network model learns multimodal multifaceted representation image question proposed model learns use multimodal representation reason image entities achieves new state art performance vqa benchmark datasets vqa wide margin
limitation spectral methods gaussian hidden clique problem rank perturbations gaussian tensors consider following detection problem given realization asymmetric matrix dimension distinguish hypothesisthat upper triangular variables gaussians variableswith mean variance hypothesis aplanted principal submatrix dimension upper triangularvariables gaussians mean variance whereasall upper triangular elements gaussians variables mean variance refer asthe gaussian hidden clique problem epsilon sqrt epsilon possible solve thisdetection problem probability o_n computing thespectrum considering largest eigenvalue prove epsilon sqrt algorithm examines theeigenvalues detect existence hiddengaussian clique error probability vanishing infty result immediate consequence general result rank oneperturbations dimensional gaussian tensors context establish lower bound criticalsignal noise ratio rank signal cannot detected
explore improved high probability regret bounds non stochastic bandits work addresses problem regret minimization non stochastic multi armed bandit problems focusing performance guarantees hold high probability results rather scarce literature since proving requires large deal technical effort significant modifications standard intuitive algorithms come guarantees hold expectation modifications forcing learner sample arms uniform distribution least omega sqrt times rounds adversely affect performance many arms suboptimal widely conjectured property essential proving high probability regret bounds show paper possible achieve strong results without undesirable exploration component result relies simple intuitive loss estimation strategy called implicit exploration allows remarkably clean analysis demonstrate flexibility technique derive several improved high probability bounds various extensions standard multi armed bandit framework finally conduct simple experiment illustrates robustness implicit exploration technique
minimax time series prediction consider adversarial formulation problem ofpredicting time series square loss aim predictan arbitrary sequence vectors almost well bestsmooth comparator sequence retrospect approach allowsnatural measures smoothness squared norm ofincrements generally consider linear time seriesmodel penalize comparator sequence energy ofthe implied driving noise terms derive minimax strategyfor problems type show implementedefficiently optimal predictions linear previousobservations obtain explicit expression regret interms parameters defining problem typical simple definitions smoothness computation optimalpredictions involves sparse matrices case ofnorm constrained data smoothness defined termsof squared norm comparator increments show thatthe regret grows sqrt lambda_t lengthof game lambda_t increasing limit comparatorsmoothness
rethinking lda moment matching discrete ica consider moment matching techniques estimation latent dirichlet allocation lda drawing explicit links lda discrete versions independent component analysis ica first derive new set cumulant based tensors improved sample complexity moreover reuse standard ica techniques joint diagonalization tensors improve existing methods based tensor power method extensive set experiments synthetic real datasets show new combination tensors orthogonal joint diagonalization techniques outperforms existing moment matching methods
bayesian optimization exponential convergence paper presents bayesian optimization method exponential convergence without need auxiliary optimization without delta cover sampling bayesian optimization methods require auxiliary optimization additional non convex global optimization problem time consuming hard implement practice also existing bayesian optimization method exponential convergence requires access delta cover sampling considered impractical approach eliminates requirements achieves exponential convergence rate
recognizing retinal ganglion cells dark many neural circuits composed numerous distinct cell types perform different operations inputs send outputs distinct targets therefore key step understanding neural systems reliably distinguish cell types important example retina present day techniques identifying cell types accurate labor intensive develop automated classifiers functional identification retinal ganglion cells output neurons retina based solely recorded voltage patterns large scale array use per cell classifiers based features extracted electrophysiological images spatiotemporal voltage waveforms interspike intervals autocorrelations classifiers achieve high performance distinguishing major ganglion cell classes primate retina fail achieving accuracy predicting cell polarities show use indicators functional coupling within populations ganglion cells cross correlation infer cell polarities matrix completion algorithm result accurate fully automated methods cell type classification
estimating mixture models via mixtures polynomials mixture modeling general technique making simple model expressive weighted combination generality simplicity part explains success expectation maximization algorithm updates easy derive wide class mixture models however likelihood mixture model non convex known global convergence guarantees recently method moments approaches offer global guarantees mixture models extend easily range mixture models exist work present polymom unifying framework based method moments estimation procedures easily derivable polymom applicable moments single mixture component polynomials parameters key observation moments mixture model mixture polynomials allows cast estimation generalized moment problem solve relaxations using semidefinite optimization extract parameters using ideas computer algebra framework allows draw insights apply tools convex optimization computer algebra theory moments study problems statistical estimation simulations show good empirical performance several models
accelerated proximal gradient methods nonconvex programming nonconvex nonsmooth problems recently received considerable attention signal image processing statistics machine learning however solving nonconvex nonsmooth optimization problems remains big challenge accelerated proximal gradient apg excellent method convex programming however still unknown whether usual apg ensure convergence critical point nonconvex programming address issue introduce monitor corrector step extend apg general nonconvex nonsmooth programs accordingly propose monotone apg non monotone apg latter waives requirement monotonic reduction objective function needs less computation iteration best knowledge first provide apg type algorithms general nonconvex nonsmooth problems ensuring every accumulation point critical point convergence rates remain problems convex number iterations numerical results testify advantage algorithms speed
adaptive primal dual splitting methods statistical learning image processing alternating direction method multipliers admm important tool solving complex optimization problems involves minimization sub steps often difficult solve efficiently primal dual hybrid gradient pdhg method powerful alternative often simpler substeps admm thus producing lower complexity solvers despite flexibility method pdhg often impractical requires careful choice multiple stepsize parameters often intuitive way choose parameters maximize efficiency even achieve convergence propose self adaptive stepsize rules automatically tune pdhg parameters optimal convergence rigorously analyze methods identify convergence rates numerical experiments show adaptive pdhg strong advantages non adaptive methods terms efficiency simplicity user
sublinear time orthogonal tensor decomposition recent work wang nips 2015 gives fastest known algorithms orthogonal tensor decomposition provable guarantees algorithm based computing sketches input tensor requires reading entire input show number cases achieve theoretical guarantees sublinear time even without reading input tensor instead using sketches estimate inner products tensor decomposition algorithms use importance sampling achieve sublinear time need know norms tensor slices show number important cases symmetric tensors sum_ lambda_i u_i otimes lambda_i estimate norms sublinear time whenever even important case small values also estimate norms asymmetric tensors sublinear time possible general show tensor slice norms slightly sublinear time possible main strengths work empirical number cases algorithm orders magnitude faster existing methods accuracy
submultiplicative glivenko cantelli uniform convergence revenues work derive variant classic glivenko cantelli theorem asserts uniform convergence empirical cumulative distribution function cdf cdf underlying distribution variant allows tighter convergence bounds extreme values cdf apply bound context revenue learning well studied problem economics algorithmic game theory derive sample complexity bounds uniform convergence rate empirical revenues true revenues assuming bound moment valuations possibly fractional uniform convergence limit give complete characterization law first moment valuations finite uniform convergence almost surely occurs conversely first moment infinite uniform convergence almost never occurs
online prediction limit temperature design online algorithm classify vertices graph underpinning algorithm probability distribution ising model isomorphic graph classification based predicting label maximum marginal probability limit temperature respect labels vertices seen far computing classifications unfortunately based complete problem motivates develop algorithm give sequential guarantee online mistake bound framework algorithm optimal graph tree matching prior results general graph algorithm exploits additional connectivity tree provide per cluster bound algorithm efficient cumulative time sequentially predict vertices graph quadratic size graph
online differentially private tensor decomposition tensor decomposition positioned pervasive tool era big data paper resolve many key algorithmic questions regarding robustness memory efficiency differential privacy tensor decomposition propose simple variants tensor power method enjoy strong properties propose first streaming method linear memory requirement moreover present noise calibrated tensor power method efficient privacy guarantees heart guarantees lies careful perturbation analysis derived paper improves existing results significantly
model based reinforcement learning eluder dimension consider problem learning optimize unknown markov decision process mdp show mdp parameterized within known function class obtain regret bounds scale dimensionality rather cardinality system characterize dependence explicitly tilde sqrt d_k d_e time elapsed d_k kolmogorov dimension d_e emph eluder dimension represent first unified regret bounds model based reinforcement learning provide state art guarantees several important settings moreover present simple computationally efficient algorithm emph posterior sampling reinforcement learning psrl satisfies bounds
efficient streaming algorithm submodular cover problem initiate study classical submodular cover problem data streaming model refer streaming submodular cover ssc show single pass streaming algorithm using sublinear memory size stream fail provide non trivial approximation guarantees ssc hence consider relaxed version ssc seek find partial cover design first efficient bicriteria submodular cover streaming esc streaming algorithm problem provide theoretical guarantees performance supported numerical evidence algorithm finds solutions competitive near optimal offline greedy algorithm despite requiring single pass data stream numerical experiments evaluate performance esc streaming active set selection large scale graph cover problems
sum squares lower bounds sparse pca paper establishes statistical versus computational trade offfor solving basic high dimensional machine learning problem via basic convex relaxation method specifically consider sparse principal component analysis sparse pca problem family sum squares sos aka lasserre parillo convex relaxations well known large dimension planted sparse unit vector principle detected using approx log gaussian bernoulli samples efficient polynomial time algorithms known require approx samples also known quadratic gap cannot improved basic semi definite sdp aka spectral relaxation equivalent degree sos algorithms prove also degree sos algorithms cannot improve quadratic gap average case lower bound adds small collection hardness results machine learning powerful family convex relaxation algorithms moreover design moments pseudo expectations lower bound quite different previous lower bounds establishing lower bounds higher degree sos algorithms remains challenging problem
nestt nonconvex primal dual splitting method distributed stochastic optimization study stochastic distributed algorithm nonconvex problems whose objective consists sum nonconvex l_i smooth functions plus nonsmooth regularizer proposed nonconvex primal dual splitting nestt algorithm splits problem subproblems utilizes augmented lagrangian based primal dual scheme solve distributed stochastic manner special non uniform sampling version nestt achieves epsilon stationary solution using mathcal sum_ sqrt l_i epsilon gradient evaluations mathcal times better proximal gradient descent methods also achieves linear convergence rate nonconvex ell_1 penalized quadratic problems polyhedral constraints reveal fundamental connection primal dual based methods primal methods iag sag saga
converge sublinear rates problem proposed method incorporates memory previous gradient values order achieve linear convergence
accelerating stochastic composition optimization consider stochastic composition optimization problem objective composition expected value functions propose new stochastic first order method namely accelerated stochastic compositional proximal gradient asc method updates based queries sampling oracle using different timescales asc first proximal gradient method stochastic composition problem deal nonsmooth regularization penalty show asc exhibits faster convergence best known algorithms achieves optimal sample error complexity several important special cases demonstrate application asc reinforcement learning conduct numerical experiments
combinatorial cascading bandits propose combinatorial cascading bandits class partial monitoring problems step learning agent chooses tuple ground items subject constraints receives reward weights chosen items weights items binary stochastic drawn independently agent observes index first chosen item whose weight observation model arises network routing instance learning agent observe first link routing path blocks path propose ucb like algorithm solving problems combcascade prove gap dependent gap free upper bounds step regret proofs build recent work stochastic combinatorial semi bandits also address novel challenges setting non linear reward function partial observability evaluate combcascade real world problems show performs well even modeling assumptions violated also demonstrate setting requires new learning algorithm
>>>>>>> 1f5001768549b051f4df79909c7feaa20cbd84c0
limitation spectral methods gaussian hidden clique problem rank perturbations gaussian tensors consider following detection problem given realization asymmetric matrix dimension distinguish hypothesisthat upper triangular variables gaussians variableswith mean variance hypothesis aplanted principal submatrix dimension upper triangularvariables gaussians mean variance whereasall upper triangular elements gaussians variables mean variance refer asthe gaussian hidden clique problem epsilon sqrt epsilon possible solve thisdetection problem probability o_n computing thespectrum considering largest eigenvalue prove epsilon sqrt algorithm examines theeigenvalues detect existence hiddengaussian clique error probability vanishing infty result immediate consequence general result rank oneperturbations dimensional gaussian tensors context establish lower bound criticalsignal noise ratio rank signal cannot detected
explore improved high probability regret bounds non stochastic bandits work addresses problem regret minimization non stochastic multi armed bandit problems focusing performance guarantees hold high probability results rather scarce literature since proving requires large deal technical effort significant modifications standard intuitive algorithms come guarantees hold expectation modifications forcing learner sample arms uniform distribution least omega sqrt times rounds adversely affect performance many arms suboptimal widely conjectured property essential proving high probability regret bounds show paper possible achieve strong results without undesirable exploration component result relies simple intuitive loss estimation strategy called implicit exploration allows remarkably clean analysis demonstrate flexibility technique derive several improved high probability bounds various extensions standard multi armed bandit framework finally conduct simple experiment illustrates robustness implicit exploration technique
minimax time series prediction consider adversarial formulation problem ofpredicting time series square loss aim predictan arbitrary sequence vectors almost well bestsmooth comparator sequence retrospect approach allowsnatural measures smoothness squared norm ofincrements generally consider linear time seriesmodel penalize comparator sequence energy ofthe implied driving noise terms derive minimax strategyfor problems type show implementedefficiently optimal predictions linear previousobservations obtain explicit expression regret interms parameters defining problem typical simple definitions smoothness computation optimalpredictions involves sparse matrices case ofnorm constrained data smoothness defined termsof squared norm comparator increments show thatthe regret grows sqrt lambda_t lengthof game lambda_t increasing limit comparatorsmoothness
rethinking lda moment matching discrete ica consider moment matching techniques estimation latent dirichlet allocation lda drawing explicit links lda discrete versions independent component analysis ica first derive new set cumulant based tensors improved sample complexity moreover reuse standard ica techniques joint diagonalization tensors improve existing methods based tensor power method extensive set experiments synthetic real datasets show new combination tensors orthogonal joint diagonalization techniques outperforms existing moment matching methods
bayesian optimization exponential convergence paper presents bayesian optimization method exponential convergence without need auxiliary optimization without delta cover sampling bayesian optimization methods require auxiliary optimization additional non convex global optimization problem time consuming hard implement practice also existing bayesian optimization method exponential convergence requires access delta cover sampling considered impractical approach eliminates requirements achieves exponential convergence rate
recognizing retinal ganglion cells dark many neural circuits composed numerous distinct cell types perform different operations inputs send outputs distinct targets therefore key step understanding neural systems reliably distinguish cell types important example retina present day techniques identifying cell types accurate labor intensive develop automated classifiers functional identification retinal ganglion cells output neurons retina based solely recorded voltage patterns large scale array use per cell classifiers based features extracted electrophysiological images spatiotemporal voltage waveforms interspike intervals autocorrelations classifiers achieve high performance distinguishing major ganglion cell classes primate retina fail achieving accuracy predicting cell polarities show use indicators functional coupling within populations ganglion cells cross correlation infer cell polarities matrix completion algorithm result accurate fully automated methods cell type classification
estimating mixture models via mixtures polynomials mixture modeling general technique making simple model expressive weighted combination generality simplicity part explains success expectation maximization algorithm updates easy derive wide class mixture models however likelihood mixture model non convex known global convergence guarantees recently method moments approaches offer global guarantees mixture models extend easily range mixture models exist work present polymom unifying framework based method moments estimation procedures easily derivable polymom applicable moments single mixture component polynomials parameters key observation moments mixture model mixture polynomials allows cast estimation generalized moment problem solve relaxations using semidefinite optimization extract parameters using ideas computer algebra framework allows draw insights apply tools convex optimization computer algebra theory moments study problems statistical estimation simulations show good empirical performance several models
accelerated proximal gradient methods nonconvex programming nonconvex nonsmooth problems recently received considerable attention signal image processing statistics machine learning however solving nonconvex nonsmooth optimization problems remains big challenge accelerated proximal gradient apg excellent method convex programming however still unknown whether usual apg ensure convergence critical point nonconvex programming address issue introduce monitor corrector step extend apg general nonconvex nonsmooth programs accordingly propose monotone apg non monotone apg latter waives requirement monotonic reduction objective function needs less computation iteration best knowledge first provide apg type algorithms general nonconvex nonsmooth problems ensuring every accumulation point critical point convergence rates remain problems convex number iterations numerical results testify advantage algorithms speed
adaptive primal dual splitting methods statistical learning image processing alternating direction method multipliers admm important tool solving complex optimization problems involves minimization sub steps often difficult solve efficiently primal dual hybrid gradient pdhg method powerful alternative often simpler substeps admm thus producing lower complexity solvers despite flexibility method pdhg often impractical requires careful choice multiple stepsize parameters often intuitive way choose parameters maximize efficiency even achieve convergence propose self adaptive stepsize rules automatically tune pdhg parameters optimal convergence rigorously analyze methods identify convergence rates numerical experiments show adaptive pdhg strong advantages non adaptive methods terms efficiency simplicity user
sublinear time orthogonal tensor decomposition recent work wang nips 2015 gives fastest known algorithms orthogonal tensor decomposition provable guarantees algorithm based computing sketches input tensor requires reading entire input show number cases achieve theoretical guarantees sublinear time even without reading input tensor instead using sketches estimate inner products tensor decomposition algorithms use importance sampling achieve sublinear time need know norms tensor slices show number important cases symmetric tensors sum_ lambda_i u_i otimes lambda_i estimate norms sublinear time whenever even important case small values also estimate norms asymmetric tensors sublinear time possible general show tensor slice norms slightly sublinear time possible main strengths work empirical number cases algorithm orders magnitude faster existing methods accuracy
submultiplicative glivenko cantelli uniform convergence revenues work derive variant classic glivenko cantelli theorem asserts uniform convergence empirical cumulative distribution function cdf cdf underlying distribution variant allows tighter convergence bounds extreme values cdf apply bound context revenue learning well studied problem economics algorithmic game theory derive sample complexity bounds uniform convergence rate empirical revenues true revenues assuming bound moment valuations possibly fractional uniform convergence limit give complete characterization law first moment valuations finite uniform convergence almost surely occurs conversely first moment infinite uniform convergence almost never occurs
online prediction limit temperature design online algorithm classify vertices graph underpinning algorithm probability distribution ising model isomorphic graph classification based predicting label maximum marginal probability limit temperature respect labels vertices seen far computing classifications unfortunately based complete problem motivates develop algorithm give sequential guarantee online mistake bound framework algorithm optimal graph tree matching prior results general graph algorithm exploits additional connectivity tree provide per cluster bound algorithm efficient cumulative time sequentially predict vertices graph quadratic size graph
online differentially private tensor decomposition tensor decomposition positioned pervasive tool era big data paper resolve many key algorithmic questions regarding robustness memory efficiency differential privacy tensor decomposition propose simple variants tensor power method enjoy strong properties propose first streaming method linear memory requirement moreover present noise calibrated tensor power method efficient privacy guarantees heart guarantees lies careful perturbation analysis derived paper improves existing results significantly
model based reinforcement learning eluder dimension consider problem learning optimize unknown markov decision process mdp show mdp parameterized within known function class obtain regret bounds scale dimensionality rather cardinality system characterize dependence explicitly tilde sqrt d_k d_e time elapsed d_k kolmogorov dimension d_e emph eluder dimension represent first unified regret bounds model based reinforcement learning provide state art guarantees several important settings moreover present simple computationally efficient algorithm emph posterior sampling reinforcement learning psrl satisfies bounds
efficient streaming algorithm submodular cover problem initiate study classical submodular cover problem data streaming model refer streaming submodular cover ssc show single pass streaming algorithm using sublinear memory size stream fail provide non trivial approximation guarantees ssc hence consider relaxed version ssc seek find partial cover design first efficient bicriteria submodular cover streaming esc streaming algorithm problem provide theoretical guarantees performance supported numerical evidence algorithm finds solutions competitive near optimal offline greedy algorithm despite requiring single pass data stream numerical experiments evaluate performance esc streaming active set selection large scale graph cover problems
sum squares lower bounds sparse pca paper establishes statistical versus computational trade offfor solving basic high dimensional machine learning problem via basic convex relaxation method specifically consider sparse principal component analysis sparse pca problem family sum squares sos aka lasserre parillo convex relaxations well known large dimension planted sparse unit vector principle detected using approx log gaussian bernoulli samples efficient polynomial time algorithms known require approx samples also known quadratic gap cannot improved basic semi definite sdp aka spectral relaxation equivalent degree sos algorithms prove also degree sos algorithms cannot improve quadratic gap average case lower bound adds small collection hardness results machine learning powerful family convex relaxation algorithms moreover design moments pseudo expectations lower bound quite different previous lower bounds establishing lower bounds higher degree sos algorithms remains challenging problem
nestt nonconvex primal dual splitting method distributed stochastic optimization study stochastic distributed algorithm nonconvex problems whose objective consists sum nonconvex l_i smooth functions plus nonsmooth regularizer proposed nonconvex primal dual splitting nestt algorithm splits problem subproblems utilizes augmented lagrangian based primal dual scheme solve distributed stochastic manner special non uniform sampling version nestt achieves epsilon stationary solution using mathcal sum_ sqrt l_i epsilon gradient evaluations mathcal times better proximal gradient descent methods also achieves linear convergence rate nonconvex ell_1 penalized quadratic problems polyhedral constraints reveal fundamental connection primal dual based methods primal methods iag sag saga
converge sublinear rates problem proposed method incorporates memory previous gradient values order achieve linear convergence
accelerating stochastic composition optimization consider stochastic composition optimization problem objective composition expected value functions propose new stochastic first order method namely accelerated stochastic compositional proximal gradient asc method updates based queries sampling oracle using different timescales asc first proximal gradient method stochastic composition problem deal nonsmooth regularization penalty show asc exhibits faster convergence best known algorithms achieves optimal sample error complexity several important special cases demonstrate application asc reinforcement learning conduct numerical experiments
combinatorial cascading bandits propose combinatorial cascading bandits class partial monitoring problems step learning agent chooses tuple ground items subject constraints receives reward weights chosen items weights items binary stochastic drawn independently agent observes index first chosen item whose weight observation model arises network routing instance learning agent observe first link routing path blocks path propose ucb like algorithm solving problems combcascade prove gap dependent gap free upper bounds step regret proofs build recent work stochastic combinatorial semi bandits also address novel challenges setting non linear reward function partial observability evaluate combcascade real world problems show performs well even modeling assumptions violated also demonstrate setting requires new learning algorithm
online prediction limit temperature design online algorithm classify vertices graph underpinning algorithm probability distribution ising model isomorphic graph classification based predicting label maximum marginal probability limit temperature respect labels vertices seen far computing classifications unfortunately based complete problem motivates develop algorithm give sequential guarantee online mistake bound framework algorithm optimal graph tree matching prior results general graph algorithm exploits additional connectivity tree provide per cluster bound algorithm efficient cumulative time sequentially predict vertices graph quadratic size graph
online gradient boosting extend theory boosting regression problems online learning setting generalizing batch setting boosting notion weak learning algorithm modeled online learning algorithm linear loss functions competes base class regression functions strong learning algorithm online learning algorithm smooth convex loss functions competes larger class regression functions main result online gradient boosting algorithm converts weak online learning algorithm strong larger class functions linear span base class also give simpler boosting algorithm converts weak online learning algorithm strong larger class functions convex hull base class prove optimality
active learning framework using sparse graph codes sparse polynomials graph sketching let rightarrow mathbb variate polynomial consisting monomials coefficients non goal learn polynomial querying values introduce active learning framework associated low query cost computational runtime significant savings enabled leveraging sampling strategies based modern coding theory specifically design analysis sparse graph codes low density parity check ldpc codes represent state art modern packet communications significantly show design perspective leads exciting best knowledge largely unexplored intellectual connections learning coding key relax worst case assumption ensemble average setting polynomial assumed drawn uniformly random ensemble polynomials given size sparsity framework succeeds high probability respect polynomial ensemble sparsity delta delta exactly learned using queries time log even queries perturbed gaussian noise apply proposed framework graph sketching problem inferring sparse graphs querying graph cuts writing cut function polynomial exploiting graph structure propose sketching algorithm learn arbitrary node unknown graph using cut queries scales almost linearly number edges sub linearly graph size experiments real datasets show significant reductions runtime query complexity compared competitive schemes
spectral norm regularization orthonormal representations graph transduction recent literature cite ando suggests embedding graph unit sphere leads better generalization graph transduction however choice optimal embedding efficient algorithm compute remains open paper show orthonormal representations class unit sphere graph embeddings pac learnable existing pac based analysis apply dimension function class infinite propose alternative pac based bound depend dimension underlying function class related famous lov vartheta function main contribution paper spore spectral regularized orthonormal embedding graph transduction derived pac bound spore posed non smooth convex function emph elliptope problems usually solved semi definite programs sdps time complexity present infeasible inexact proximal iip inexact proximal method performs subgradient procedure approximate projection necessarily feasible iip scalable sdp frac sqrt convergence generally applicable whenever suitable approximate projection available use iip compute spore approximate projection step computed fista accelerated gradient descent procedure show method convergence rate frac sqrt proposed algorithm easily scales 1000 vertices standard sdp computation scale beyond hundred vertices furthermore analysis presented easily extends multiple graph setting
discrete nyi classifiers consider binary classification problem predicting target variable discrete feature vector probability distribution known optimal classifier leading minimum misclassification rate given maximum posteriori probability map decision rule however practice estimating complete joint distribution computationally statistically impossible large values therefore alternative approach first estimate low order marginals joint probability distribution design classifier based estimated low order marginals approach also helpful complete training data instances available due privacy concerns work consider problem designing optimum classifier based estimated low order marginals prove given set marginals minimum hirschfeld gebelein enyi hgr correlation principle introduced leads randomized classification rule shown misclassification rate larger twice misclassification rate optimal classifier show separability condition proposed algorithm equivalent randomized linear regression approach naturally results robust feature selection method selecting subset features maximum worst case hgr correlation target variable theoretical upper bound similar recent discrete chebyshev classifier dcc approach proposed algorithm significant computational advantages since requires solving least square optimization problem finally numerically compare proposed algorithm dcc classifier show proposed algorithm results better misclassification rate various uci data repository datasets
decomposition bounds marginal map marginal map inference involves making map predictions systems defined latent variables missing information significantly difficult pure marginalization map tasks large class efficient convergent variational algorithms dual decomposition exist work generalize dual decomposition generic powered sum inference task includes marginal map along pure marginalization map special cases method based block coordinate descent algorithm new convex decomposition bound guaranteed converge monotonically parallelized efficiently demonstrate approach various inference queries real world problems uai approximate inference challenge showing framework faster reliable previous methods
boosting abstention present new boosting algorithm key scenario binary classification abstention algorithm abstain predicting label point price fixed cost round algorithm selects pair functions base predictor base abstention function define convex upper bounds natural loss function associated problem prove calibrated respect bayes solution algorithm benefits general margin based learning guarantees derive ensembles pairs base predictor abstention functions terms rademacher complexities corresponding function classes give convergence guarantees algorithm along linear time weak learning algorithm abstention stumps also report results several experiments suggesting algorithm provides significant improvement practice confidence based algorithms
beyond convexity stochastic quasi convex optimization poster moved monday thursday 101
model based relative entropy stochastic search stochastic search algorithms general black box optimizers due ease use generality recently also gained lot attention operations research machine learning policy search yet algorithms require lot evaluations objective scale poorly problem dimension affected highly noisy objective functions converge prematurely alleviate problems introduce new surrogate based stochastic search approach learn simple quadratic surrogate models objective function quality quadratic approximation limited greedily exploit learned models algorithm misled inaccurate optimum introduced surrogate instead use information theoretic constraints bound distance new old data distribution maximizing objective function additionally new method able sustain exploration search distribution avoid premature convergence compare method state art black box optimization methods standard uni modal multi modal optimization functions simulated planar robot tasks complex robot ball throwing task proposed method considerably outperforms existing approaches
finite set smooth functions sum strongly convex
hardness online sleeping combinatorial optimization problems show several online combinatorial optimization problems admit efficient regret algorithms become computationally hard sleeping setting subset actions becomes unavailable round specifically show sleeping versions problems least hard pac learning dnf expressions long standing open problem show hardness sleeping versions online shortest paths online minimum spanning tree online subsets online truncated permutations online minimum cut online bipartite matching hardness result sleeping version online shortest paths problem resolves open problem presented colt 2015 koolen 2015
learning kernels using local rademacher complexity use notion local rademacher complexity design new algorithms learning kernels algorithms thereby benefit sharper learning bounds based notion certain general conditions guarantee faster convergence rate devise new learning kernel algorithms based convex optimization problem give efficient solution using existing learning kernel techniques another formulated programming problem describe solution detail also report results experiments algorithms binary multi class classification tasks
boosting framework grounds online learning exploiting duality boosting online learning present boosting framework proves extremely powerful thanks employing vast knowledge available online learning area using framework develop various algorithms address multiple practically theoretically interesting questions including sparse boosting smooth distribution boosting agnostic learning product generalization double projection online learning algorithms
online multiclass boosting recent work extended theoretical analysis boosting algorithms multiclass problems online settings however multiclass extension batch setting online extensions consider binary classification fill gap literature defining justifying weak learning condition online multiclass boosting condition leads optimal boosting algorithm requires minimal number weak learners achieve certain accuracy additionally propose adaptive algorithm near optimal enjoys excellent performance real data due adaptive property
convex optimization procedure clustering theoretical revisit paper present theoretical analysis son convex optimization procedure clustering using sum norms son regularization recently proposed cite icml2011hocking_419 son lindsten650707 pelckmans2005convex particular show samples drawn cubes cluster son provably identify cluster membership provided distance cubes larger threshold linearly depends size cube ratio numbers samples cluster best knowledge paper first provide rigorous analysis understand son works believe provide important insights develop novel convex optimization based algorithms clustering
clustering cluster queries propose framework semi supervised active clustering framework ssac learner allowed interact domain expert asking whether given instances belong cluster study query computational complexity clustering framework consider setting expert conforms center based clustering notion margin show trade computational complexity query complexity prove case means clustering expert conforms solution means access relatively queries allows efficient solutions otherwise hard problems particular provide probabilistic polynomial time bpp algorithm clustering setting asks big log log cluster queries runs time complexity big log number clusters number instances success algorithm guaranteed data satisfying margin condition without queries show problem hard also prove lower bound number queries needed computationally efficient clustering algorithm setting
online convex optimization unconstrained domains losses propose online convex optimization algorithm rescaledexp achieves optimal regret unconstrained setting without prior knowledge bounds loss functions prove lower bound showing exponential separation regret existing algorithms require known bound loss functions algorithm require knowledge rescaledexp matches lower bound asymptotically number iterations rescaledexp naturally hyperparameter free demonstrate empirically matches prior optimization algorithms require hyperparameter optimization
weighted theta functions embeddings applications max cut clustering summarization introduce unifying generalization lov theta function associated geometric embedding graphs weights nodes edges show computed exactly semidefinite programming approximate using svm computations show theta function interpreted measure diversity graphs use idea graph embedding algorithms max cut correlation clustering document summarization well represented problems weighted graphs
revisiting perceptron efficient label optimal learning halfspaces long standing problem efficiently learn linear separator using labels possible presence noise work propose efficient perceptron based algorithm actively learning homogeneous linear separators uniform distribution bounded noise label flipped probability eta algorithm achieves near optimal tilde frac 2eta frac epsilon label complexity time tilde frac epsilon 2eta adversarial noise tilde omega epsilon fraction labels flipped algorithm achieves near optimal tilde frac epsilon label complexity time tilde frac epsilon furthermore show active learning algorithm converted efficient passive learning algorithm near optimal sample complexity respect epsilon
active learning weak strong labelers active learner given hypothesis class large set unlabeled examples ability interactively query labels oracle subset examples goal learner learn hypothesis class fits data well making label queries possible work addresses active learning labels obtained strong weak labelers addition standard active learning setting extra weak labeler occasionally provide incorrect labels example learning classify medical images either expensive labels obtained physician oracle strong labeler cheaper occasionally incorrect labels obtained medical resident weak labeler goal learn classifier low error data labeled oracle using weak labeler reduce number label queries made labeler provide active learning algorithm setting establish statistical consistency analyze label complexity characterize provide label savings using strong labeler alone
online prediction limit temperature design online algorithm classify vertices graph underpinning algorithm probability distribution ising model isomorphic graph classification based predicting label maximum marginal probability limit temperature respect labels vertices seen far computing classifications unfortunately based complete problem motivates develop algorithm give sequential guarantee online mistake bound framework algorithm optimal graph tree matching prior results general graph algorithm exploits additional connectivity tree provide per cluster bound algorithm efficient cumulative time sequentially predict vertices graph quadratic size graph
online gradient boosting extend theory boosting regression problems online learning setting generalizing batch setting boosting notion weak learning algorithm modeled online learning algorithm linear loss functions competes base class regression functions strong learning algorithm online learning algorithm smooth convex loss functions competes larger class regression functions main result online gradient boosting algorithm converts weak online learning algorithm strong larger class functions linear span base class also give simpler boosting algorithm converts weak online learning algorithm strong larger class functions convex hull base class prove optimality
active learning framework using sparse graph codes sparse polynomials graph sketching let rightarrow mathbb variate polynomial consisting monomials coefficients non goal learn polynomial querying values introduce active learning framework associated low query cost computational runtime significant savings enabled leveraging sampling strategies based modern coding theory specifically design analysis sparse graph codes low density parity check ldpc codes represent state art modern packet communications significantly show design perspective leads exciting best knowledge largely unexplored intellectual connections learning coding key relax worst case assumption ensemble average setting polynomial assumed drawn uniformly random ensemble polynomials given size sparsity framework succeeds high probability respect polynomial ensemble sparsity delta delta exactly learned using queries time log even queries perturbed gaussian noise apply proposed framework graph sketching problem inferring sparse graphs querying graph cuts writing cut function polynomial exploiting graph structure propose sketching algorithm learn arbitrary node unknown graph using cut queries scales almost linearly number edges sub linearly graph size experiments real datasets show significant reductions runtime query complexity compared competitive schemes
spectral norm regularization orthonormal representations graph transduction recent literature cite ando suggests embedding graph unit sphere leads better generalization graph transduction however choice optimal embedding efficient algorithm compute remains open paper show orthonormal representations class unit sphere graph embeddings pac learnable existing pac based analysis apply dimension function class infinite propose alternative pac based bound depend dimension underlying function class related famous lov vartheta function main contribution paper spore spectral regularized orthonormal embedding graph transduction derived pac bound spore posed non smooth convex function emph elliptope problems usually solved semi definite programs sdps time complexity present infeasible inexact proximal iip inexact proximal method performs subgradient procedure approximate projection necessarily feasible iip scalable sdp frac sqrt convergence generally applicable whenever suitable approximate projection available use iip compute spore approximate projection step computed fista accelerated gradient descent procedure show method convergence rate frac sqrt proposed algorithm easily scales 1000 vertices standard sdp computation scale beyond hundred vertices furthermore analysis presented easily extends multiple graph setting
discrete nyi classifiers consider binary classification problem predicting target variable discrete feature vector probability distribution known optimal classifier leading minimum misclassification rate given maximum posteriori probability map decision rule however practice estimating complete joint distribution computationally statistically impossible large values therefore alternative approach first estimate low order marginals joint probability distribution design classifier based estimated low order marginals approach also helpful complete training data instances available due privacy concerns work consider problem designing optimum classifier based estimated low order marginals prove given set marginals minimum hirschfeld gebelein enyi hgr correlation principle introduced leads randomized classification rule shown misclassification rate larger twice misclassification rate optimal classifier show separability condition proposed algorithm equivalent randomized linear regression approach naturally results robust feature selection method selecting subset features maximum worst case hgr correlation target variable theoretical upper bound similar recent discrete chebyshev classifier dcc approach proposed algorithm significant computational advantages since requires solving least square optimization problem finally numerically compare proposed algorithm dcc classifier show proposed algorithm results better misclassification rate various uci data repository datasets
decomposition bounds marginal map marginal map inference involves making map predictions systems defined latent variables missing information significantly difficult pure marginalization map tasks large class efficient convergent variational algorithms dual decomposition exist work generalize dual decomposition generic powered sum inference task includes marginal map along pure marginalization map special cases method based block coordinate descent algorithm new convex decomposition bound guaranteed converge monotonically parallelized efficiently demonstrate approach various inference queries real world problems uai approximate inference challenge showing framework faster reliable previous methods
boosting abstention present new boosting algorithm key scenario binary classification abstention algorithm abstain predicting label point price fixed cost round algorithm selects pair functions base predictor base abstention function define convex upper bounds natural loss function associated problem prove calibrated respect bayes solution algorithm benefits general margin based learning guarantees derive ensembles pairs base predictor abstention functions terms rademacher complexities corresponding function classes give convergence guarantees algorithm along linear time weak learning algorithm abstention stumps also report results several experiments suggesting algorithm provides significant improvement practice confidence based algorithms
beyond convexity stochastic quasi convex optimization poster moved monday thursday 101
model based relative entropy stochastic search stochastic search algorithms general black box optimizers due ease use generality recently also gained lot attention operations research machine learning policy search yet algorithms require lot evaluations objective scale poorly problem dimension affected highly noisy objective functions converge prematurely alleviate problems introduce new surrogate based stochastic search approach learn simple quadratic surrogate models objective function quality quadratic approximation limited greedily exploit learned models algorithm misled inaccurate optimum introduced surrogate instead use information theoretic constraints bound distance new old data distribution maximizing objective function additionally new method able sustain exploration search distribution avoid premature convergence compare method state art black box optimization methods standard uni modal multi modal optimization functions simulated planar robot tasks complex robot ball throwing task proposed method considerably outperforms existing approaches
finite set smooth functions sum strongly convex
hardness online sleeping combinatorial optimization problems show several online combinatorial optimization problems admit efficient regret algorithms become computationally hard sleeping setting subset actions becomes unavailable round specifically show sleeping versions problems least hard pac learning dnf expressions long standing open problem show hardness sleeping versions online shortest paths online minimum spanning tree online subsets online truncated permutations online minimum cut online bipartite matching hardness result sleeping version online shortest paths problem resolves open problem presented colt 2015 koolen 2015
learning kernels using local rademacher complexity use notion local rademacher complexity design new algorithms learning kernels algorithms thereby benefit sharper learning bounds based notion certain general conditions guarantee faster convergence rate devise new learning kernel algorithms based convex optimization problem give efficient solution using existing learning kernel techniques another formulated programming problem describe solution detail also report results experiments algorithms binary multi class classification tasks
boosting framework grounds online learning exploiting duality boosting online learning present boosting framework proves extremely powerful thanks employing vast knowledge available online learning area using framework develop various algorithms address multiple practically theoretically interesting questions including sparse boosting smooth distribution boosting agnostic learning product generalization double projection online learning algorithms
online multiclass boosting recent work extended theoretical analysis boosting algorithms multiclass problems online settings however multiclass extension batch setting online extensions consider binary classification fill gap literature defining justifying weak learning condition online multiclass boosting condition leads optimal boosting algorithm requires minimal number weak learners achieve certain accuracy additionally propose adaptive algorithm near optimal enjoys excellent performance real data due adaptive property
convex optimization procedure clustering theoretical revisit paper present theoretical analysis son convex optimization procedure clustering using sum norms son regularization recently proposed cite icml2011hocking_419 son lindsten650707 pelckmans2005convex particular show samples drawn cubes cluster son provably identify cluster membership provided distance cubes larger threshold linearly depends size cube ratio numbers samples cluster best knowledge paper first provide rigorous analysis understand son works believe provide important insights develop novel convex optimization based algorithms clustering
clustering cluster queries propose framework semi supervised active clustering framework ssac learner allowed interact domain expert asking whether given instances belong cluster study query computational complexity clustering framework consider setting expert conforms center based clustering notion margin show trade computational complexity query complexity prove case means clustering expert conforms solution means access relatively queries allows efficient solutions otherwise hard problems particular provide probabilistic polynomial time bpp algorithm clustering setting asks big log log cluster queries runs time complexity big log number clusters number instances success algorithm guaranteed data satisfying margin condition without queries show problem hard also prove lower bound number queries needed computationally efficient clustering algorithm setting
online convex optimization unconstrained domains losses propose online convex optimization algorithm rescaledexp achieves optimal regret unconstrained setting without prior knowledge bounds loss functions prove lower bound showing exponential separation regret existing algorithms require known bound loss functions algorithm require knowledge rescaledexp matches lower bound asymptotically number iterations rescaledexp naturally hyperparameter free demonstrate empirically matches prior optimization algorithms require hyperparameter optimization
weighted theta functions embeddings applications max cut clustering summarization introduce unifying generalization lov theta function associated geometric embedding graphs weights nodes edges show computed exactly semidefinite programming approximate using svm computations show theta function interpreted measure diversity graphs use idea graph embedding algorithms max cut correlation clustering document summarization well represented problems weighted graphs
revisiting perceptron efficient label optimal learning halfspaces long standing problem efficiently learn linear separator using labels possible presence noise work propose efficient perceptron based algorithm actively learning homogeneous linear separators uniform distribution bounded noise label flipped probability eta algorithm achieves near optimal tilde frac 2eta frac epsilon label complexity time tilde frac epsilon 2eta adversarial noise tilde omega epsilon fraction labels flipped algorithm achieves near optimal tilde frac epsilon label complexity time tilde frac epsilon furthermore show active learning algorithm converted efficient passive learning algorithm near optimal sample complexity respect epsilon
active learning weak strong labelers active learner given hypothesis class large set unlabeled examples ability interactively query labels oracle subset examples goal learner learn hypothesis class fits data well making label queries possible work addresses active learning labels obtained strong weak labelers addition standard active learning setting extra weak labeler occasionally provide incorrect labels example learning classify medical images either expensive labels obtained physician oracle strong labeler cheaper occasionally incorrect labels obtained medical resident weak labeler goal learn classifier low error data labeled oracle using weak labeler reduce number label queries made labeler provide active learning algorithm setting establish statistical consistency analyze label complexity characterize provide label savings using strong labeler alone
regularized algorithms unified framework statistical guarantees latent models fundamental modeling tool machine learning applications present significant computational analytical challenges popular algorithm variants much used algorithmic tool yet rigorous understanding performance highly incomplete recently work demonstrated important class problems exhibits linear local convergence high dimensional setting however step well defined address precisely setting unified treatment using regularization regularization high dimensional problems well understood iterative algorithm requires careful balancing making progress towards solution identifying right structure sparsity low rank particular regularizing step using state art high dimensional prescriptions guaranteed provide balance algorithm analysis linked way reveals balance optimization statistical errors specialize general framework sparse gaussian mixture models high dimensional mixed regression regression missing variables obtaining statistical guarantees examples
robust gaussian graphical modeling trimmed graphical lasso gaussian graphical models ggms popular tools studying network structures however many modern applications gene network discovery social interactions analysis often involve high dimensional noisy data outliers heavier tails gaussian distribution paper propose trimmed graphical lasso robust estimation sparse ggms method guards outliers implicit trimming mechanism akin popular least trimmed squares method used linear regression provide rigorous statistical analysis estimator high dimensional setting contrast existing approaches robust sparse ggms estimation lack statistical guarantees theoretical results complemented experiments simulated real gene expression data demonstrate value approach
rethinking lda moment matching discrete ica consider moment matching techniques estimation latent dirichlet allocation lda drawing explicit links lda discrete versions independent component analysis ica first derive new set cumulant based tensors improved sample complexity moreover reuse standard ica techniques joint diagonalization tensors improve existing methods based tensor power method extensive set experiments synthetic real datasets show new combination tensors orthogonal joint diagonalization techniques outperforms existing moment matching methods
fast accurate inference plackett luce models show maximum likelihood estimate models derived luce choice axiom plackett luce model expressed stationary distribution markov chain conveys insight several recently proposed spectral inference algorithms take advantage perspective formulate new spectral algorithm significantly accurate previous ones plackett luce model simple adaptation algorithm used iteratively producing sequence estimates converges estimate version runs faster competing approaches benchmark datasets algorithms easy implement making relevant practitioners large
robust feature sample linear discriminant analysis brain disorders diagnosis wide spectrum discriminative methods increasingly used diverse applications classification regression tasks however many existing discriminative methods assume input data nearly noise free limits applications solve real world problems particularly disease diagnosis data acquired neuroimaging devices always prone different sources noise robust discriminative models somewhat scarce attempts made make robust noise outliers methods focus detecting either sample outliers feature noises moreover usually use unsupervised noising procedures separately noise training testing data factors induce biases learning process thus limit performance paper propose classification method based least squares formulation linear discriminant analysis simultaneously detects sample outliers feature noises proposed method operates semi supervised setting labeled training unlabeled testing data incorporated form intrinsic geometry sample space therefore violating samples feature values identified sample outliers feature noises respectively test algorithm synthetic brain neurodegenerative databases particularly parkinson disease alzheimer disease results demonstrate method outperforms baseline state art methods terms accuracy area roc curve
variational dropout local reparameterization trick explore yet unexploited opportunity drastically improving efficiency stochastic gradient variational bayes sgvb global model parameters regular sgvb estimators rely sampling parameters per minibatch data variance constant minibatch size efficiency estimators drastically improved upon translating uncertainty global parameters local noise independent across datapoints minibatch reparameterizations local noise trivially parallelized variance inversely proportional minibatch size generally leading much faster convergence find important connection regularization dropout original gaussian dropout objective corresponds sgvb local noise scale invariant prior proportionally fixed posterior variance method allows inference flexibly parameterized posteriors specifically propose emph variational dropout generalization gaussian dropout flexibly parameterized posterior often leading better generalization method demonstrated several experiments
mixed robust average submodular partitioning fast algorithms guarantees applications investigate novel mixed robust average case submodular data partitioning problems collectively call submodular partitioning problems generalize purely robust instances problem namely max min submodular fair allocation sfa emph min max submodular load balancing slb also average case instances submodular welfare problem swp submodular multiway partition smp robust versions studied theory community existing work focused tight approximation guarantees resultant algorithms generally scalable large real world applications contrasts average case instances algorithms scalable present paper bridge gap proposing several new algorithms including greedy majorization minimization minorization maximization relaxation algorithms scale large datasets also achieve theoretical approximation guarantees comparable state art moreover provide new scalable algorithms apply additive combinations robust average case objectives show problems many applications machine learning including data partitioning load balancing distributed data clustering image segmentation empirically demonstrate efficacy algorithms real world problems involving data partitioning distributed optimization convex deep neural network objectives also purely unsupervised image segmentation
fast randomized kernel ridge regression statistical guarantees approach improving running time kernel based methods build small sketch kernel matrix use lieu full matrix machine learning task interest describe version approach comes running time guarantees well improved guarantees statistical performance extending notion emph statistical leverage scores setting kernel ridge regression able identify sampling distribution reduces size sketch required number columns sampled emph effective dimensionality problem latter quantity often much smaller previous bounds depend emph maximal degrees freedom give empirical evidence supporting fact second contribution present fast algorithm quickly compute coarse approximations thesescores time linear number samples precisely running time algorithm depending trace kernel matrix regularization parameter obtained via variant squared length sampling adapt kernel setting lastly discuss new notion leverage data point captures fine notion difficulty learning problem
bandits unobserved confounders causal approach multi armed bandit problem constitutes archetypal setting sequential decision making permeating multiple domains including engineering business medicine hallmarks bandit setting agent capacity explore environment active intervention contrasts ability collect passive data estimating associational relationships actions payouts existence unobserved confounders namely unmeasured variables affecting action outcome variables implies data collection modes general coincide paper show formalizing distinction conceptual algorithmic implications bandit setting current generation bandit algorithms implicitly try maximize rewards based estimation experimental distribution show always best strategy pursue indeed achieve low regret certain realistic classes bandit problems namely face unobserved confounders experimental observational quantities required rational agent realization propose optimization metric employing experimental observational distributions bandit agents pursue illustrate benefits traditional algorithms
convergence analysis prediction markets via randomized subspace descent prediction markets economic mechanisms aggregating information future events sequential interactions traders pricing mechanisms markets known related optimization algorithms machine learning connections understanding equilibrium market prices relate beliefs traders market however little known rates guarantees convergence sequential mechanisms recent papers cite important open question paper show previously studied prediction market trading models understood natural generalization randomized coordinate descent call randomized subspace descent rsd establish convergence rates rsd leverage prove rates prediction market models answering open questions results extend beyond standard centralized markets arbitrary trade networks
nonconvex optimization framework low rank matrix estimation study estimation low rank matrices via nonconvex optimization compared convex relaxation nonconvex optimization exhibits superior empirical performance large scale instances low rank matrix estimation however understanding theoretical guarantees limited paper define notion projected oracle divergence based establish sufficient conditions success nonconvex optimization illustrate consequences general framework matrix sensing completion particular prove broad class nonconvex optimization algorithms including alternating minimization gradient type methods geometrically converge global optimum exactly recover true low rank matrices standard conditions
learning transduce unbounded memory recently strong results demonstrated deep recurrent neural networks natural language transduction problems paper explore representational power models using synthetic grammars designed exhibit phenomena similar found real transduction problems machine translation experiments lead propose new memory based recurrent networks implement continuously differentiable analogues traditional data structures stacks queues deques show architectures exhibit superior generalisation performance deep rnns often able learn underlying generating algorithms transduction experiments
multi class svms tighter data dependent generalization bounds novel algorithms paper studies generalization performance multi class classification algorithms obtain first time data dependent generalization error bound logarithmic dependence class size substantially improving state art linear dependence existing data dependent generalization analysis theoretical analysis motivates introduce new multi class classification machine based norm regularization parameter controls complexity corresponding bounds derive efficient optimization algorithm based fenchel duality theory benchmarks several real world datasets show proposed algorithm achieve significant accuracy gains state art
group additive structure identification kernel nonparametric regression additive model popularly used models high dimensional nonparametric regression analysis however main drawback neglects possible interactions predictor variables paper reexamine group additive model proposed literature rigorously define intrinsic group additive structure relationship response variable predictor vector vect develop effective structure penalized kernel method simultaneous identification intrinsic group additive structure nonparametric function estimation method utilizes novel complexity measure derive group additive structures show proposed method consistent identifying intrinsic group additive structure simulation study real data applications demonstrate effectiveness proposed method general tool high dimensional nonparametric regression
polynomial time form robust regression despite variety robust regression methods developed current regression formulations either hard allow unbounded response even single leverage point present general formulation robust regression variational estimation unifies number robust regression methods allowing tractable approximation strategy develop estimator requires polynomial time achieving certain robustness consistency guarantees experimental evaluation demonstrates effectiveness new estimation approach compared standard methods
dimensionality reduction subspace structure preservation modeling data sampled union independent subspaces widely applied number real world applications however dimensionality reduction approaches theoretically preserve independence assumption well studied key contribution show projection vectors sufficient independence preservation class data sampled union independent subspaces non trivial observation use designing dimensionality reduction technique paper propose novel dimensionality reduction algorithm theoretically preserves structure given dataset support theoretical analysis empirical results synthetic real world data achieving textit state art results compared popular dimensionality reduction techniques
nearly optimal private lasso present nearly optimal differentially private version well known lasso estimator algorithm provides privacy protection respect training data item excess risk algorithm compared non private version widetilde assuming input data bounded ell_ infty norm first differentially private algorithm achieves bound without polynomial dependence addition assumption design matrix addition show error bound nearly optimal amongst differentially private algorithms
general tensor spectral clustering higher order data spectral clustering clustering well known techniques data analysis recent work extended spectral clustering square symmetric tensors hypermatrices derived network develop new tensor spectral clustering method simultaneously clusters rows columns slices nonnegative mode tensor generalizes tensors number modes algorithm based new random walk model call super spacey random surfer show method performs state art clustering methods several synthetic datasets ground truth clusters use algorithm analyze several real world datasets
hessian free optimization learning deep multidimensional recurrent neural networks multidimensional recurrent neural networks mdrnns shown remarkable performance area speech handwriting recognition performance mdrnn improved increasing depth difficulty learning deeper network overcome using hessian free optimization given connectionist temporal classification ctc utilized objective learning mdrnn sequence labeling non convexity ctc poses problem applying network solution convex approximation ctc formulated relationship algorithm fisher information matrix discussed mdrnn depth layers successfully trained using resulting improved performance sequence labeling
empirical localization homogeneous divergences discrete sample spaces paper propose novel parameter estimator probabilistic models discrete space proposed estimator derived minimization homogeneous divergence constructed without calculation normalization constant frequently infeasible models discrete space investigate statistical properties proposed estimator consistency asymptotic normality reveal relationship alpha divergence small experiments show proposed estimator attains comparable performance mle drastically lower computational cost
