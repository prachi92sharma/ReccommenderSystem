texture synthesis using convolutional neural networks introduce new model natural textures based feature spaces convolutional neural networks optimised object recognition samples model high perceptual quality demonstrating generative power neural networks trained purely discriminative fashion within model textures represented correlations feature maps several layers network show across layers texture representations increasingly capture statistical properties natural images making object information explicit model provides new tool generate stimuli neuroscience might offer insights deep representations learned convolutional neural networks
convolutional neural networks intra layer recurrent connections scene labeling scene labeling challenging computer vision task requires use local discriminative features global context information adopt deep recurrent convolutional neural network rcnn task originally proposed object recognition different traditional convolutional neural networks cnn model intra layer recurrent connections convolutional layers therefore convolutional layer becomes dimensional recurrent neural network units receive constant feed forward inputs previous layer recurrent inputs neighborhoods recurrent iterations proceed region context captured unit expands way feature extraction context modulation seamlessly integrated different typical methods entail separate modules steps utilize context multi scale rcnn proposed benchmark datasets standford background sift flow model outperforms many state art models accuracy efficiency
grammar foreign language syntactic constituency parsing fundamental problem naturallanguage processing subject intensive researchand engineering decades result accurate parsersare domain specific complex inefficient paper showthat domain agnostic attention enhanced sequence sequence modelachieves state art results widely used syntacticconstituency parsing dataset trained large synthetic corpusthat annotated using existing parsers also matches theperformance standard parsers trained smallhuman annotated dataset shows model highlydata efficient contrast sequence sequence models without theattention mechanism parser also fast processing ahundred sentences per second unoptimized cpu implementation
recursive training convolutional networks neuronal boundary prediction efforts automate reconstruction neural circuits electron microscopic brain images critical field connectomics important computation reconstruction detection neuronal boundaries images acquired serial section leading technique highly anisotropic inferior quality along third dimension images max pooling convolutional network set standard performance boundary detection achieve substantial gain accuracy innovations following trend towards deeper networks object recognition use much deeper network previously employed boundary detection second incorporate well filters enable computations use context finally adopt recursively trained architecture first network generates preliminary boundary map provided input along original image second network generates final boundary map backpropagation training accelerated znn new implementation convolutional networks uses multicore cpu parallelism speed hybrid architecture could generally applicable types anisotropic images including video recursive framework image labeling problem
generative image modeling using spatial lstms modeling distribution natural images challenging partly strong statistical dependencies extend hundreds pixels recurrent neural networks successful capturing long range dependencies number problems recently found way generative image models introduce recurrent image model based multi dimensional long short term memory units particularly suited image modeling due spatial structure model scales images arbitrary size likelihood computationally tractable find outperforms state art quantitative comparisons several image datasets produces promising results used texture synthesis inpainting
faster cnn towards real time object detection region proposal networks state art object detection networks depend region proposal algorithms hypothesize object locations advances like sppnet fast cnn reduced running time detection networks exposing region proposal computation bottleneck work introduce region proposal network rpn shares full image convolutional features detection network thus enabling nearly cost free region proposals rpn fully convolutional network simultaneously predicts object bounds objectness scores position rpns trained end end generate high quality region proposals used fast cnn detection simple alternating optimization rpn fast cnn trained share convolutional features deep vgg model detection system frame rate 5fps including steps gpu achieving state art object detection accuracy pascal voc 2007 map 2012 map using 300 proposals per image code available https github com shaoqingren faster_rcnn
weakly supervised disentangling recurrent transformations view synthesis important problem graphics vision synthesize novel views object single image particular challenging due partial observability inherent projecting object onto image space ill posedness inferring object shape pose however train neural network address problem restrict attention specific object classes case faces chairs gather ample training data paper propose novel recurrent convolutional encoder decoder network trained end end task rendering rotated objects starting single image recurrent structure allows model capture long term dependencies along sequence transformations demonstrate quality predictions human faces multi pie dataset dataset chair models also show ability disentangling latent data factors without using object class labels
exploring models data image question answering work aims address problem image based question answering new models datasets work propose use neural networks visual semantic embeddings without intermediate stages object detection image segmentation predict answers simple questions images model performs times better published results existing image dataset also present question generation algorithm converts image descriptions widely available form used algorithm produce order magnitude larger dataset evenly distributed answers suite baseline results new dataset also presented
talking machine dataset methods multilingual image question paper present mqa model able answer questions content image answer sentence phrase single word model contains components long short term memory lstm extract question representation convolutional neural network cnn extract visual representation lstm storing linguistic context answer fusing component combine information first components generate answer construct freestyle multilingual image question answering iqa dataset train evaluate mqa model contains 150 000 images 310 000 freestyle chinese question answer pairs english translations quality generated answers mqa model dataset evaluated human judges turing test specifically mix answers provided humans model human judges need distinguish model human also provide score larger better indicating quality answer propose strategies monitor quality evaluation process experiments show cases human judges cannot distinguish model humans average score 454 918 human details work including iqa dataset found project page url http idl baidu com iqa html
parallel multi dimensional lstm application fast biomedical volumetric image segmentation convolutional neural networks cnns shifted across images videos segment fixed input size typically perceive small local contexts pixels classified foreground background contrast multi dimensional recurrent nns rnns perceive entire spatio temporal context pixel sweeps pixels especially rnn long short term memory lstm despite theoretical advantages however unlike cnns previous lstm variants hard parallelise gpus arrange traditional cuboid order computations lstm pyramidal fashion resulting pyramid lstm easy parallelise especially data stacks brain slice images pyramid lstm achieved best known pixel wise brain image segmentation results mrbrains13 competitive results isbi12
learning small samples analysis simple decision heuristics simple decision heuristics models human animal behavior use pieces information perhaps single piece information integrate pieces simple ways example considering sequentially time giving equal weight unknown quickly heuristics learned experience show analytically empirically training samples lead substantial progress learning focus families heuristics single cue decision making lexicographic decision making tallying empirical analysis extensive date employing natural data sets diverse subjects
object proposals accurate object class detection goal paper generate high quality object proposals context autonomous driving method exploits stereo imagery place proposals form bounding boxes formulate problem minimizing energy function encoding object size priors ground plane well several depth informed features reason free space point cloud densities distance ground experiments show significant performance gains existing rgb rgb object proposal methods challenging kitti benchmark combined convolutional neural net cnn scoring approach outperforms existing results kitti object classes
poisson gamma belief network infer multilayer representation high dimensional count vectors propose poisson gamma belief network pgbn factorizes layers product connection weight matrix nonnegative real hidden units next layer pgbn hidden layers jointly trained upward downward gibbs sampler iteration upward samples dirichlet distributed connection weight vectors starting first layer bottom data layer downward samples gamma distributed hidden units starting top hidden layer gamma negative binomial process combined layer wise training strategy allows pgbn infer width layer given fixed budget width first layer pgbn single hidden layer reduces poisson factor analysis example results text analysis illustrate interesting relationships width first layer inferred network structure demonstrate pgbn whose hidden units imposed correlated gamma priors add layers increase performance gains poisson factor analysis given limit width first layer
semi supervised factored logistic regression high dimensional neuroimaging data imaging neuroscience links human behavior aspects brain biology ever increasing datasets existing neuroimaging methods typically perform either discovery unknown neural structure testing neural structure associated mental tasks however testing hypotheses neural correlates underlying larger sets mental tasks necessitates adequate representations observations therefore propose blend representation modelling task classification unified statistical learning problem multinomial logistic regression introduced constrained factored coefficients coupled autoencoder show approach yields accurate interpretable neural models psychological tasks reference dataset well better generalization datasets
binaryconnect training deep neural networks binary weights propagations deep neural networks dnn achieved state art results wide range tasks best results obtained large training sets large models past gpus enabled breakthroughs greater computational speed future faster computation training test time likely crucial progress consumer applications low power devices result much interest research development dedicated hardware deep learning binary weights weights constrained possible values would bring great benefits specialized hardware replacing many multiply accumulate operations simple accumulations multipliers space power hungry components digital implementation neural networks introduce binaryconnect method consists training dnn binary weights forward backward propagations retaining precision stored weights gradients accumulated like dropout schemes show binaryconnect acts regularizer obtain near state art results binaryconnect permutation invariant mnist cifar svhn
learning transduce unbounded memory recently strong results demonstrated deep recurrent neural networks natural language transduction problems paper explore representational power models using synthetic grammars designed exhibit phenomena similar found real transduction problems machine translation experiments lead propose new memory based recurrent networks implement continuously differentiable analogues traditional data structures stacks queues deques show architectures exhibit superior generalisation performance deep rnns often able learn underlying generating algorithms transduction experiments
spectral representations convolutional neural networks discrete fourier transforms provide significant speedup computation convolutions deep learning work demonstrate beyond advantages efficient computation spectral domain also provides powerful representation model train convolutional neural networks cnns employ spectral representations introduce number innovations cnn design first propose spectral pooling performs dimensionality reduction truncating representation frequency domain approach preserves considerably information per parameter pooling strategies enables flexibility choice pooling output dimensionality representation also enables new form stochastic regularization randomized modification resolution show methods achieve competitive results classification approximation tasks without using dropout max pooling finally demonstrate effectiveness complex coefficient spectral parameterization convolutional filters leaves underlying model unchanged results representation greatly facilitates optimization observe variety popular cnn configurations leads significantly faster convergence training
theory decision making dynamic context dynamics simple decisions well understood modeled class random walk models laming 1968 ratcliff 1978 busemeyer townsend 1993 usher mcclelland 2001 bogacz 2006 however real life decisions include rich dynamically changing influence additional information call context work describe computational theory decision making dynamically shifting context show model generalizes dominant existing model fixed context decision making ratcliff 1978 built weighted combination fixed context decisions evolving simultaneously also show model generalizes cent work control attention flanker task 2009 finally show model recovers qualitative data patterns another task longstanding psychological interest continuous performance test servan schreiber 1996 using model parameters
bidirectional recurrent neural networks generative models bidirectional recurrent neural networks rnn trained predict positive negative time directions simultaneously used commonly unsupervised tasks probabilistic interpretation model difficult recently different frameworks gsn nade provide connection reconstruction probabilistic modeling makes interpretation possible far know neither gsn nade studied context time series example unsupervised task study problem filling gaps high dimensional time series complex dynamics although unidirectional rnns recently trained successfully model time series inference negative time direction non trivial propose probabilistic interpretations bidirectional rnns used reconstruct missing gaps efficiently experiments text data show proposed methods much accurate unidirectional reconstructions although bit less accurate computationally complex bidirectional bayesian inference unidirectional rnn also provide results music data bayesian inference computationally infeasible demonstrating scalability proposed methods
recognizing retinal ganglion cells dark many neural circuits composed numerous distinct cell types perform different operations inputs send outputs distinct targets therefore key step understanding neural systems reliably distinguish cell types important example retina present day techniques identifying cell types accurate labor intensive develop automated classifiers functional identification retinal ganglion cells output neurons retina based solely recorded voltage patterns large scale array use per cell classifiers based features extracted electrophysiological images spatiotemporal voltage waveforms interspike intervals autocorrelations classifiers achieve high performance distinguishing major ganglion cell classes primate retina fail achieving accuracy predicting cell polarities show use indicators functional coupling within populations ganglion cells cross correlation infer cell polarities matrix completion algorithm result accurate fully automated methods cell type classification
recurrent latent variable model sequential data paper explore inclusion latent random variables hidden state recurrent neural network rnn combining elements variational autoencoder argue use high level latent random variables variational rnn vrnn model kind variability observed highly structured sequential data natural speech empirically evaluate proposed model related sequential models speech datasets handwriting dataset results show important roles latent random variables play rnn dynamics
deep knowledge tracing knowledge tracing machine models knowledge student interact coursework established significantly unsolved problem computer supported education paper explore benefit using recurrent neural networks model student learning family models important advantages current state art methods require explicit encoding human domain knowledge far flexible functional form capture substantially complex student interactions show neural networks outperform current state art prediction real student data allowing straightforward interpretation discovery structure curriculum results suggest promising new line research knowledge tracing
deep temporal sigmoid belief networks sequence modeling deep dynamic generative models developed learn sequential dependencies time series data multi layered model designed constructing hierarchy temporal sigmoid belief networks tsbns defined sequential stack sigmoid belief networks sbns sbn contextual hidden state inherited previous sbns sequence used regulate hidden bias scalable learning inference algorithms derived introducing recognition model yields fast sampling variational posterior recognition model trained jointly generative model maximizing variational lower bound log likelihood experimental results bouncing balls polyphonic music motion capture text streams show proposed approach achieves state art predictive performance capacity synthesize various sequences
hidden technical debt machine learning systems machine learning offers fantastically powerful toolkit building useful complexprediction systems quickly paper argues dangerous think ofthese quick wins coming free using software engineering frameworkof technical debt find common incur massive ongoing maintenancecosts real world systems explore several specific risk factors toaccount system design include boundary erosion entanglement hidden feedback loops undeclared consumers data dependencies configurationissues changes external world variety system level anti patterns
statistical model criticism using kernel sample tests propose exploratory approach statistical model criticism using maximum mean discrepancy mmd sample tests typical approaches model criticism require practitioner select statistic measure discrepancies data statistical model mmd sample tests instead constructed analytic maximisation large space possible statistics therefore automatically select statistic shows discrepancy demonstrate synthetic data selected statistic called witness function used identify statistical model misrepresents data trained apply procedure real data models assessed restricted boltzmann machines deep belief networks gaussian process regression demonstrate ways models fail capture properties data trained
calibrated structured prediction user facing applications displaying calibrated confidence measures probabilities correspond true frequency important obtaining high accuracy interested calibration structured prediction problems speech recognition optical character recognition medical diagnosis structured prediction presents new challenges calibration output space large users issue many types probability queries marginals structured output extend notion calibration handle various subtleties pertaining structured setting provide simple recalibration method trains binary classifier predict probabilities interest explore range features appropriate structured recalibration demonstrate efficacy real world datasets
bayesian framework modeling confidence perceptual decision making degree confidence choice decision critical aspect perceptual decision making attempts quantify decision maker confidence measuring accuracy task yielded limited success confidence accuracy typically equal paper introduce bayesian framework model confidence perceptual decision making show model based partially observable markov decision processes pomdps able predict confidence decision maker based data available experimenter test model experiments confidence based decision making involving well known random dots motion discrimination task experiments show model predictions closely match experimental data additionally model also consistent phenomena hard easy effect perceptual decision making
dependent multinomial models made easy stick breaking polya gamma augmentation many practical modeling problems involve discrete data best represented draws multinomial categorical distributions example nucleotides dna sequence children names given state year text documents commonly modeled multinomial distributions cases expect form dependency draws nucleotide position dna strand depend preceding nucleotides children names highly correlated year year topics text correlated dynamic dependencies naturally captured typical dirichlet multinomial formulation leverage logistic stick breaking representation recent innovations lya gamma augmentation reformulate multinomial distribution terms latent variables jointly gaussian likelihoods enabling take advantage host bayesian inference techniques gaussian models minimal overhead
scalable adaptation state complexity nonparametric hidden markov models bayesian nonparametric hidden markov models typically learned via fixed truncations infinite state space local monte carlo proposals make small changes state space develop inference algorithm sticky hierarchical dirichlet process hidden markov model scales big datasets processing sequences time yet allows rapid adaptation state space cardinality unlike previous point estimate methods novel variational bound penalizes redundant irrelevant states thus enables optimization state space birth proposals use observed data statistics create useful new states escape local optima merge delete proposals remove ineffective states yield simpler models affordable future computations experiments speaker diarization motion capture epigenetic chromatin datasets discover models compact interpretable better aligned ground truth segmentations competitors released open source python implementation parallelize local inference steps across sequences
robust feature sample linear discriminant analysis brain disorders diagnosis wide spectrum discriminative methods increasingly used diverse applications classification regression tasks however many existing discriminative methods assume input data nearly noise free limits applications solve real world problems particularly disease diagnosis data acquired neuroimaging devices always prone different sources noise robust discriminative models somewhat scarce attempts made make robust noise outliers methods focus detecting either sample outliers feature noises moreover usually use unsupervised noising procedures separately noise training testing data factors induce biases learning process thus limit performance paper propose classification method based least squares formulation linear discriminant analysis simultaneously detects sample outliers feature noises proposed method operates semi supervised setting labeled training unlabeled testing data incorporated form intrinsic geometry sample space therefore violating samples feature values identified sample outliers feature noises respectively test algorithm synthetic brain neurodegenerative databases particularly parkinson disease alzheimer disease results demonstrate method outperforms baseline state art methods terms accuracy area roc curve
learning spatiotemporal trajectories manifold valued longitudinal data propose bayesian mixed effects model learn typical scenarios changes longitudinal manifold valued data namely repeated measurements objects individuals several points time model allows estimate group average trajectory space measurements random variations trajectory result spatiotemporal transformations allow changes direction trajectory pace trajectories followed use tools riemannian geometry allows derive generic algorithm kind data smooth constraints lie therefore riemannian manifold stochastic approximations expectation maximization algorithm used estimate model parameters highly non linear setting method used estimate data driven model progressive impairments cognitive functions onset alzheimer disease experimental results show model correctly put correspondence age individual diagnosed disease thus validating fact effectively estimated normative scenario disease progression random effects provide unique insights variations ordering timing succession cognitive impairments across different individuals
hessian free optimization learning deep multidimensional recurrent neural networks multidimensional recurrent neural networks mdrnns shown remarkable performance area speech handwriting recognition performance mdrnn improved increasing depth difficulty learning deeper network overcome using hessian free optimization given connectionist temporal classification ctc utilized objective learning mdrnn sequence labeling non convexity ctc poses problem applying network solution convex approximation ctc formulated relationship algorithm fisher information matrix discussed mdrnn depth layers successfully trained using resulting improved performance sequence labeling
scalable inference gaussian process models black box likelihoods propose sparse method scalable automated variational inference avi large class models gaussian process priors multiple latent functions multiple outputs non linear likelihoods approach maintains statistical efficiency property original avi method requiring expectations univariate gaussian distributions approximate posterior mixture gaussians experiments small datasets various problems including regression classification log gaussian cox processes warped gps show method perform well full method high levels sparsity larger experiments using mnist sarcos datasets show method provide superior performance previously published scalable approaches handcrafted specific likelihood models
variational dropout local reparameterization trick explore yet unexploited opportunity drastically improving efficiency stochastic gradient variational bayes sgvb global model parameters regular sgvb estimators rely sampling parameters per minibatch data variance constant minibatch size efficiency estimators drastically improved upon translating uncertainty global parameters local noise independent across datapoints minibatch reparameterizations local noise trivially parallelized variance inversely proportional minibatch size generally leading much faster convergence find important connection regularization dropout original gaussian dropout objective corresponds sgvb local noise scale invariant prior proportionally fixed posterior variance method allows inference flexibly parameterized posteriors specifically propose emph variational dropout generalization gaussian dropout flexibly parameterized posterior often leading better generalization method demonstrated several experiments
infinite factorial dynamical model propose infinite factorial dynamic model ifdm general bayesian nonparametric model source separation model builds markov indian buffet process consider potentially unbounded number hidden markov chains sources evolve independently according dynamics state space either discrete continuous posterior inference develop algorithm based particle gibbs ancestor sampling efficiently applied wide range source separation problems evaluate performance ifdm well known applications multitarget tracking cocktail party power disaggregation multiuser detection experimental results show approach source separation outperform previous approaches also handle problems computationally intractable existing approaches
variational information maximisation intrinsically motivated reinforcement learning mutual information core statistical quantity applications areas machine learning whether training density models multiple data modalities maximising efficiency noisy transmission channels learning behaviour policies exploration artificial agents learning algorithms involve optimisation mutual information rely blahut arimoto algorithm enumerative algorithm exponential complexity suitable modern machine learning applications paper provides new approach scalable optimisation mutual information merging techniques variational inference deep learning develop approach focusing problem intrinsically motivated learning mutual information forms definition well known internal drive known empowerment using variational lower bound mutual information combined convolutional networks handling visual input streams develop stochastic optimisation algorithm allows scalable information maximisation empowerment based reasoning directly pixels actions
copula variational inference develop general variational inference method preserves dependency among latent variables method uses copulas augment families distributions used mean field structured approximations copulas model dependency captured original variational distribution thus augmented variational family guarantees better approximations posterior stochastic optimization inference augmented distribution scalable furthermore strategy generic applied inference procedure currently uses mean field structured approach copula variational inference many advantages reduces bias less sensitive local optima less sensitive hyperparameters helps characterize interpret dependency among latent variables
fast second order stochastic backpropagation variational inference propose second order hessian hessian free based optimization method variational inference inspired gaussian backpropagation argue quasi newton optimization developed well accomplished generalizing gradient computation stochastic backpropagation via reparametrization trick lower complexity illustrative example apply approach problems bayesian logistic regression variational auto encoder vae additionally compute bounds estimator variance intractable expectations family lipschitz continuous function method practical scalable model free demonstrate method several real world datasets provide comparisons stochastic gradient methods show substantial enhancement convergence rates
rethinking lda moment matching discrete ica consider moment matching techniques estimation latent dirichlet allocation lda drawing explicit links lda discrete versions independent component analysis ica first derive new set cumulant based tensors improved sample complexity moreover reuse standard ica techniques joint diagonalization tensors improve existing methods based tensor power method extensive set experiments synthetic real datasets show new combination tensors orthogonal joint diagonalization techniques outperforms existing moment matching methods
model based relative entropy stochastic search stochastic search algorithms general black box optimizers due ease use generality recently also gained lot attention operations research machine learning policy search yet algorithms require lot evaluations objective scale poorly problem dimension affected highly noisy objective functions converge prematurely alleviate problems introduce new surrogate based stochastic search approach learn simple quadratic surrogate models objective function quality quadratic approximation limited greedily exploit learned models algorithm misled inaccurate optimum introduced surrogate instead use information theoretic constraints bound distance new old data distribution maximizing objective function additionally new method able sustain exploration search distribution avoid premature convergence compare method state art black box optimization methods standard uni modal multi modal optimization functions simulated planar robot tasks complex robot ball throwing task proposed method considerably outperforms existing approaches
supervised learning dynamical system learning recently substantial interest spectral methods learning dynamical systems methods popular since often offer good tradeoffbetween computational statistical efficiency unfortunately difficult use extend practice make difficult incorporateprior information sparsity structure address problem presenta new view dynamical system learning show learn dynamical systems solving sequence ordinary supervised learning problems therebyallowing users incorporate prior knowledge via standard techniques asl regularization many existing spectral methods special cases newframework using linear regression supervised learner demonstrate theeffectiveness framework showing examples nonlinear regressionor lasso let learn better state representations plain linear regression correctness instances follows directly general analysis
expectation particle belief propagation propose original particle based implementation loopy belief propagation lpb algorithm pairwise markov random fields mrf continuous state space algorithm constructs adaptively efficient proposal distributions approximating local beliefs note mrf achieved considering proposal distributions exponential family whose parameters updated iterately expectation propagation framework proposed particle scheme provides consistent estimation lbp marginals number particles increases demonstrate provides accurate results particle belief propagation pbp algorithm ihler mcallester 2009 fraction computational cost additionally robust empirically computational complexity algorithm iteration quadratic number particles also propose accelerated implementation sub quadratic computational complexity still provides consistent estimates loopy marginal distributions performs almost well original procedure
embedding inference structured multilabel prediction key bottleneck structured output prediction need inference training testing usually requiring form dynamic programming rather using approximate inference tailoring specialized inference method particular structure standard responses scaling challenge propose embed prediction constraints directly learned representation eliminating need explicit inference scalable approach structured output prediction achieved particularly test time demonstrate idea multi label prediction subsumption mutual exclusion constraints relationship maximum margin structured output prediction established experiments demonstrate benefits structured output training still realized even inference eliminated
tractable learning complex probability queries tractable learning aims learn probabilistic models inference guaranteed efficient however particular class queries tractable depends model underlying representation usually class mpe conditional probabilities joint assignments propose tractable learner guarantees efficient inference broader class queries simultaneously learns markov network tractable circuit representation order guarantee measure tractability approach differs earlier work using sentential decision diagrams sdd tractable language instead arithmetic circuits sdds desirable properties general representations acs lack enable basic primitives boolean circuit compilation allows support broader class complex probability queries including counting threshold parity polytime
double nothing multiplicative incentive mechanisms crowdsourcing crowdsourcing gained immense popularity machine learning applications obtaining large amounts labeled data crowdsourcing cheap fast suffers problem low quality data address fundamental challenge crowdsourcing propose simple payment mechanism incentivize workers answer questions sure skip rest show surprisingly mild natural free lunch requirement mechanism incentive compatible payment mechanism possible also show among possible incentive compatible mechanisms satisfy free lunch mechanism makes smallest possible payment spammers interestingly unique mechanism takes multiplicative form simplicity mechanism added benefit preliminary experiments involving several hundred workers observe significant reduction error rates unique mechanism lower monetary expenditure
local expectation gradients black box variational inference introduce local expectation gradients general purpose stochastic variational inference algorithm constructing stochastic gradients sampling variational distribution algorithm divides problem estimating stochastic gradients multiple variational parameters smaller sub tasks sub task explores intelligently relevant part variational distribution achieved performing exact expectation single random variable correlates variational parameter interest resulting rao blackwellized estimate low variance method works efficiently continuous discrete random variables furthermore proposed algorithm interesting similarities gibbs sampling time unlike gibbs sampling trivially parallelized
learning wasserstein loss learning predict multi label outputs challenging many problems natural metric outputs used improve predictions paper develop loss function multi label learning based wasserstein distance wasserstein distance provides natural notion dissimilarity probability measures although optimizing respect exact wasserstein distance costly recent work described regularized approximation efficiently computed describe efficient learning algorithm based regularization well novel extension wasserstein distance probability measures unnormalized measures also describe statistical learning bound loss wasserstein loss encourage smoothness predictions respect chosen metric output space demonstrate property real data tag prediction problem using yahoo flickr creative commons dataset outperforming baseline use metric
principal geodesic analysis probability measures optimal transport metric consider work space probability measures hilbert space endowed wasserstein metric given finite family probability measures propose iterative approach compute geodesic principal components summarize efficiently dataset wasserstein metric provides riemannian structure associated concepts echet mean geodesics tangent vectors prove crucial follow intuitive approach laid standard principal component analysis make approach feasible propose use alternative parameterization geodesics proposed citet ambrosio2006gradient textit generalized geodesics parameterized velocity fields defined support wasserstein mean data pointing towards ending point generalized geodesic resulting optimization problem finding principal components solved adapting projected gradient descend method experiment results show ability computed principal components capture axes variability histograms probability measures data
fast accurate inference plackett luce models show maximum likelihood estimate models derived luce choice axiom plackett luce model expressed stationary distribution markov chain conveys insight several recently proposed spectral inference algorithms take advantage perspective formulate new spectral algorithm significantly accurate previous ones plackett luce model simple adaptation algorithm used iteratively producing sequence estimates converges estimate version runs faster competing approaches benchmark datasets algorithms easy implement making relevant practitioners large
backshift learning causal cyclic graphs unknown shift interventions propose simple method learn linear causal cyclic models presence latent variables method relies equilibrium data model recorded specific kind interventions shift interventions location strength interventions known estimated data method called backshift uses second moments data performs simple joint matrix diagonalization applied differences covariance matrices give sufficient necessary condition identifiability system fulfilled almost surely quite general assumptions least distinct experimental settings pure observational data demonstrate performance simulated data applications flow cytometry financial time series
learning relaxed supervision weakly supervised problems deterministic constraints latent variables observed output learning necessitates performing inference latent variables conditioned output intractable matter simple model family even finding single latent variable setting satisfies constraints could difficult instance observed output result latent database query graphics program must inferred difficulty lies model supervision poor approximations stage could lead following wrong learning signal entirely paper develop rigorous approach relaxing supervision yields asymptotically consistent parameter estimates despite altering supervision approach parameterizes family increasingly accurate relaxations jointly optimizes model relaxation parameters formulating constraints parameters ensure efficient inference efficiency constraints allow learn otherwise intractable settings asymptotic consistency ensures always follow valid learning signal
statistic kernel change point detection detecting emergence abrupt change point classic problem statistics machine learning kernel based nonparametric statistics proposed task make fewer assumptions distributions traditional parametric approach however none existing kernel statistics provided computationally efficient way characterize extremal behavior statistic characterization crucial setting detection threshold control significance level offline case well average run length online case paper propose related computationally efficient statistics kernel based change point detection amount background data large novel theoretical result paper characterization tail probability statistics using new technique based change measure characterization provides accurate detection thresholds offline online cases computationally efficient manner without need resort expensive simulations bootstrapping show methods perform well synthetic real world data
fast sample testing analytic representations probability measures propose class nonparametric sample tests cost linear sample size tests given based ensemble distances analytic functions representing distributions first test uses smoothed empirical characteristic functions represent distributions second uses distribution embeddings reproducing kernel hilbert space analyticity implies differences distributions detected almost surely finite number randomly chosen locations frequencies new tests consistent larger class alternatives previous linear time tests based non smoothed empirical characteristic functions much faster current state art quadratic time kernel based energy distance based tests experiments artificial benchmarks challenging real world testing problems demonstrate tests give better power time tradeoff competing approaches cases better outright power even expensive quadratic time tests performance advantage retained even high dimensions cases difference distributions observable low order statistics
adversarial prediction games multivariate losses multivariate loss functions used assess performance many modern prediction tasks including information retrieval ranking applications convex approximations typically optimized place avoid hard empirical risk minimization problems propose approximate training data instead loss function posing multivariate prediction adversarial game loss minimizing prediction player loss maximizing evaluation player constrained match specified properties training data avoids non convexity empirical risk minimization game sizes exponential number predicted variables overcome intractability using double oracle constraint generation method demonstrate efficiency predictive performance approach tasks evaluated using precision score discounted cumulative gain
regressive virtual metric learning interested supervised metric learning mahalanobis like distances existing approaches mainly focus learning new distance using similarity dissimilarity constraints examples paper instead bringing closer examples class pushing far away examples different classes propose move examples respect virtual points hence example brought closer priori defined virtual point reducing number constraints satisfy show approach admits closed form solution kernelized provide theoretical analysis showing consistency approach establishing links classical metric learning methods furthermore propose efficient solution difficult problem selecting virtual points based part recent works optimal transport lastly evaluate approach several state art datasets
halting random walk kernels random walk kernels measure graph similarity counting matching walks graphs popular form geometric random walk kernels longer walks length downweighted factor lambda lambda ensure convergence corresponding geometric series know field link prediction downweighting often leads phenomenon referred halting longer walks downweighted much similarity score completely dominated comparison walks length naive kernel edges vertices theoretically show halting occur geometric random walk kernels also empirically quantify impact simulated datasets popular graph classification benchmark datasets findings promise instrumental future graph kernel development applications random walk kernels
rate agnostic causal structure learning causal structure learning time series data major scientific challenge existing algorithms assume measurements occur sufficiently quickly precisely assume system measurement timescales approximately equal many scientific domains however measurements occur significantly slower rate underlying system changes moreover size mismatch timescales often unknown paper provides distinct causal structure learning algorithms discover dynamic graphs could explain observed measurement data arising undersampling rate algorithms learn causal structure without assuming particular relation measurement system timescales thus rate agnostic apply algorithms data simulations results provide insight challenge undersampling
online prediction limit temperature design online algorithm classify vertices graph underpinning algorithm probability distribution ising model isomorphic graph classification based predicting label maximum marginal probability limit temperature respect labels vertices seen far computing classifications unfortunately based complete problem motivates develop algorithm give sequential guarantee online mistake bound framework algorithm optimal graph tree matching prior results general graph algorithm exploits additional connectivity tree provide per cluster bound algorithm efficient cumulative time sequentially predict vertices graph quadratic size graph
lifted symmetry detection breaking map inference symmetry breaking technique speeding propositional satisfiability testing adding constraints theory restrict search space preserving satisfiability work extend symmetry breaking problem model finding weighted unweighted relational theories class problems includes map inference markov logic similar statistical relational languages introduce term symmetries induced evidence set extend symmetries relational theory provide important special case term equivalent symmetries showing symmetries found low degree polynomial time show break exponential number symmetries added constraints linear size domain demonstrate effectiveness techniques experiments relational domains also discuss connections relational symmetry breaking work lifted inference statistical relational reasoning
bandits unobserved confounders causal approach multi armed bandit problem constitutes archetypal setting sequential decision making permeating multiple domains including engineering business medicine hallmarks bandit setting agent capacity explore environment active intervention contrasts ability collect passive data estimating associational relationships actions payouts existence unobserved confounders namely unmeasured variables affecting action outcome variables implies data collection modes general coincide paper show formalizing distinction conceptual algorithmic implications bandit setting current generation bandit algorithms implicitly try maximize rewards based estimation experimental distribution show always best strategy pursue indeed achieve low regret certain realistic classes bandit problems namely face unobserved confounders experimental observational quantities required rational agent realization propose optimization metric employing experimental observational distributions bandit agents pursue illustrate benefits traditional algorithms
sample complexity bounds iterative stochastic policy optimization paper concerned robustness analysis decision making uncertainty consider class iterative stochastic policy optimization problems analyze resulting expected performance newly updated policy iteration particular employ concentration measure inequalities compute future expected cost probability constraint violation using empirical runs novel inequality bound derived accounts possibly unbounded change measure likelihood ratio resulting iterative policy adaptation bound serves high confidence certificate providing future performance safety guarantees approach illustrated simple robot control scenario initial steps towards applications challenging aerial vehicle navigation problems presented
basis refinement strategies linear value function approximation mdps provide theoretical framework analyzing basis function construction linear value function approximation markov decision processes mdps show important existing methods krylov bases bellman error based methods special case general framework develop provide general algorithmic framework computing basis function refinements respect dynamics environment derive approximation error bounds apply algorithm respecting general framework also show using ideas related bisimulation metrics translate basis refinement process finding prototypes diverse enough represent given mdp
convergence stochastic gradient mcmc algorithms high order integrators recent advances bayesian learning large scale data witnessed emergence stochastic gradient mcmc algorithms mcmc stochastic gradient langevin dynamics sgld stochastic gradient hamiltonian mcmc sghmc stochastic gradient thermostat finite time convergence properties sgld 1st order euler integrator recently studied corresponding theory general mcmcs explored paper consider general mcmcs high order integrators develop theory analyze finite time convergence properties asymptotic invariant measures theoretical results show faster convergence rates accurate invariant measures mcmcs higher order integrators example proposed efficient 2nd order symmetric splitting integrator mean square error mse posterior average sghmc achieves optimal convergence rate iterations compared sghmc sgld 1st order euler integrators furthermore convergence results decreasing step size mcmcs also developed convergence rates fixed step size counterparts specific decreasing sequence experiments synthetic real datasets verify theory show advantages proposed method large scale real applications
active learning framework using sparse graph codes sparse polynomials graph sketching let rightarrow mathbb variate polynomial consisting monomials coefficients non goal learn polynomial querying values introduce active learning framework associated low query cost computational runtime significant savings enabled leveraging sampling strategies based modern coding theory specifically design analysis sparse graph codes low density parity check ldpc codes represent state art modern packet communications significantly show design perspective leads exciting best knowledge largely unexplored intellectual connections learning coding key relax worst case assumption ensemble average setting polynomial assumed drawn uniformly random ensemble polynomials given size sparsity framework succeeds high probability respect polynomial ensemble sparsity delta delta exactly learned using queries time log even queries perturbed gaussian noise apply proposed framework graph sketching problem inferring sparse graphs querying graph cuts writing cut function polynomial exploiting graph structure propose sketching algorithm learn arbitrary node unknown graph using cut queries scales almost linearly number edges sub linearly graph size experiments real datasets show significant reductions runtime query complexity compared competitive schemes
discrete nyi classifiers consider binary classification problem predicting target variable discrete feature vector probability distribution known optimal classifier leading minimum misclassification rate given maximum posteriori probability map decision rule however practice estimating complete joint distribution computationally statistically impossible large values therefore alternative approach first estimate low order marginals joint probability distribution design classifier based estimated low order marginals approach also helpful complete training data instances available due privacy concerns work consider problem designing optimum classifier based estimated low order marginals prove given set marginals minimum hirschfeld gebelein enyi hgr correlation principle introduced leads randomized classification rule shown misclassification rate larger twice misclassification rate optimal classifier show separability condition proposed algorithm equivalent randomized linear regression approach naturally results robust feature selection method selecting subset features maximum worst case hgr correlation target variable theoretical upper bound similar recent discrete chebyshev classifier dcc approach proposed algorithm significant computational advantages since requires solving least square optimization problem finally numerically compare proposed algorithm dcc classifier show proposed algorithm results better misclassification rate various uci data repository datasets
gap safe screening rules sparse multi task multi class models high dimensional regression benefits sparsity promoting regularizations screening rules leverage known sparsity solution ignoring variables optimization hence speeding solvers procedure proven discard features wrongly rules said safe paper derive new safe rules generalized linear models regularized norms rules based duality gap computations spherical safe regions whose diameters converge allows discard safely variables particular low regularization parameters gap safe rule cope iterative solver illustrate performance coordinate descent multi task lasso binary multinomial logistic regression demonstrating significant speed ups tested datasets respect previous safe rules
decomposition bounds marginal map marginal map inference involves making map predictions systems defined latent variables missing information significantly difficult pure marginalization map tasks large class efficient convergent variational algorithms dual decomposition exist work generalize dual decomposition generic powered sum inference task includes marginal map along pure marginalization map special cases method based block coordinate descent algorithm new convex decomposition bound guaranteed converge monotonically parallelized efficiently demonstrate approach various inference queries real world problems uai approximate inference challenge showing framework faster reliable previous methods
anytime influence bounds explosive behavior continuous time diffusion networks paper studies transition phenomena information cascades observed along diffusion process graph introduce laplace hazard matrix show spectral radius fully characterizes dynamics contagion terms influence explosion time using concept prove tight non asymptotic bounds influence set nodes also provide depth analysis critical time contagion becomes super critical contributions include formal definitions tight lower bounds critical explosion time illustrate relevance theoretical results several examples information cascades used epidemiology viral marketing models finally provide series numerical experiments various types networks confirm tightness theoretical bounds
estimating mixture models via mixtures polynomials mixture modeling general technique making simple model expressive weighted combination generality simplicity part explains success expectation maximization algorithm updates easy derive wide class mixture models however likelihood mixture model non convex known global convergence guarantees recently method moments approaches offer global guarantees mixture models extend easily range mixture models exist work present polymom unifying framework based method moments estimation procedures easily derivable polymom applicable moments single mixture component polynomials parameters key observation moments mixture model mixture polynomials allows cast estimation generalized moment problem solve relaxations using semidefinite optimization extract parameters using ideas computer algebra framework allows draw insights apply tools convex optimization computer algebra theory moments study problems statistical estimation simulations show good empirical performance several models
robust gaussian graphical modeling trimmed graphical lasso gaussian graphical models ggms popular tools studying network structures however many modern applications gene network discovery social interactions analysis often involve high dimensional noisy data outliers heavier tails gaussian distribution paper propose trimmed graphical lasso robust estimation sparse ggms method guards outliers implicit trimming mechanism akin popular least trimmed squares method used linear regression provide rigorous statistical analysis estimator high dimensional setting contrast existing approaches robust sparse ggms estimation lack statistical guarantees theoretical results complemented experiments simulated real gene expression data demonstrate value approach
matrix completion fewer entries spectral detectability rank estimation completion low rank matrices entries task many practical applications consider aspects problem detectability ability estimate rank reliably fewest possible random entries performance achieving small reconstruction error propose spectral algorithm tasks called macbeth matrix completion bethe hessian rank estimated number negative eigenvalues bethe hessian matrix corresponding eigenvectors used initial condition minimization discrepancy estimated matrix revealed entries analyze performance random matrix setting using results statistical mechanics hopfield neural network show particular macbeth efficiently detects rank large times matrix sqrt entries constant close also evaluate corresponding root mean square error empirically show macbeth compares favorably existing approaches
robust pca compressed data robust principal component analysis rpca problem seeks separate low rank trends sparse outlierswithin data matrix approximate times matrix sum low rank matrix sparse matrix examine robust principal component analysis rpca problem data compression wherethe data approximately given cdot low rank sparse data matrix compressed size times substantially smaller original dimension via multiplication witha compression matrix give convex program recovering sparse component along compressed low rank component cdot along upper bounds error reconstructionthat scales naturally compression dimension coincides existing results uncompressedsetting results also handle error introduced additive noise missing data scaling dimension compression signal complexity theoretical results verified empirically simulations also apply method data set measuring chlorine concentration acrossa network sensors test performance practice
mixed robust average submodular partitioning fast algorithms guarantees applications investigate novel mixed robust average case submodular data partitioning problems collectively call submodular partitioning problems generalize purely robust instances problem namely max min submodular fair allocation sfa emph min max submodular load balancing slb also average case instances submodular welfare problem swp submodular multiway partition smp robust versions studied theory community existing work focused tight approximation guarantees resultant algorithms generally scalable large real world applications contrasts average case instances algorithms scalable present paper bridge gap proposing several new algorithms including greedy majorization minimization minorization maximization relaxation algorithms scale large datasets also achieve theoretical approximation guarantees comparable state art moreover provide new scalable algorithms apply additive combinations robust average case objectives show problems many applications machine learning including data partitioning load balancing distributed data clustering image segmentation empirically demonstrate efficacy algorithms real world problems involving data partitioning distributed optimization convex deep neural network objectives also purely unsupervised image segmentation
subspace clustering irrelevant features via robust dantzig selector paper considers subspace clustering problem data contains irrelevant corrupted features propose method termed robust dantzig selector successfully identify clustering structure even presence irrelevant features idea simple yet powerful replace inner product robust counterpart insensitive irrelevant features given upper bound number irrelevant features establish theoretical guarantees algorithm identify correct subspace demonstrate effectiveness algorithm via numerical simulations best knowledge first method developed tackle subspace clustering irrelevant features
class network models recoverable spectral clustering finding communities networks problem remains difficult spite amount attention recently received stochastic block model sbm generative model graphs communities simplicity theoretical understanding advanced fast recent years particular various results showing simple versions spectralclustering using normalized laplacian graph recoverthe communities almost perfectly high probability show essentially algorithm used sbm extension called degree corrected sbm works wider class block models call preference frame models essentially guarantees moreover parametrization introduce clearly exhibits free parameters needed specify class models results bounds expose clarity parameters control recovery error model class
monotone submodular function maximization size constraints submodular function generalization submodular function input consists disjoint subsets instead single subset domain many machine learning problems including influence maximization kinds topics sensor placement kinds sensors naturally modeled problem maximizing monotone submodular functions paper give constant factor approximation algorithms maximizing monotone submodular functions subject several size constraints running time algorithms almost linear domain size experimentally demonstrate algorithms outperform baseline algorithms terms solution quality
smooth strong map inference linear convergence maximum posteriori map inference important task many applications although standard formulation gives rise hard combinatorial optimization problem several effective approximations proposed studied recent years focus linear programming relaxations achieved state art performance many applications however optimization resulting program general challenging due non smoothness complex non separable constraints therefore work study benefits augmenting objective function relaxation strong convexity specifically introduce strong convexity adding quadratic term relaxation objective provide theoretical guarantees resulting programs bounding difference optimal value original optimum propose suitable optimization algorithms analyze convergence
stopwasting gradients practical svrg present analyze several strategies improving performance ofstochastic variance reduced gradient svrg methods first show theconvergence rate methods preserved decreasing sequenceof errors control variate use derive variants svrg usegrowing batch strategies reduce number gradient calculations requiredin early iterations show exploit support vectors reducethe number gradient computations later iterations prove thecommonly used regularized svrg iteration justified improves convergencerate iii consider alternate mini batch selection strategies considerthe generalization error method
spectral norm regularization orthonormal representations graph transduction recent literature cite ando suggests embedding graph unit sphere leads better generalization graph transduction however choice optimal embedding efficient algorithm compute remains open paper show orthonormal representations class unit sphere graph embeddings pac learnable existing pac based analysis apply dimension function class infinite propose alternative pac based bound depend dimension underlying function class related famous lov vartheta function main contribution paper spore spectral regularized orthonormal embedding graph transduction derived pac bound spore posed non smooth convex function emph elliptope problems usually solved semi definite programs sdps time complexity present infeasible inexact proximal iip inexact proximal method performs subgradient procedure approximate projection necessarily feasible iip scalable sdp frac sqrt convergence generally applicable whenever suitable approximate projection available use iip compute spore approximate projection step computed fista accelerated gradient descent procedure show method convergence rate frac sqrt proposed algorithm easily scales 1000 vertices standard sdp computation scale beyond hundred vertices furthermore analysis presented easily extends multiple graph setting
differentially private learning structured discrete distributions investigate problem learning unknown probability distribution discrete population random samples goal design efficient algorithms simultaneously achieve low error total variation norm guaranteeing differential privacy individuals population describe general approach yields near sample optimal computationally efficient differentially private estimators wide range well studied natural distribution families theoretical results show wide variety structured distributions exist private estimation algorithms nearly efficient terms sample size running time non private counterparts complement theoretical guarantees experimental evaluation experiments illustrate speed accuracy private estimators synthetic mixture models large public data set
robust portfolio optimization propose robust portfolio optimization approach based quantile statistics proposed method robust extreme events asset returns accommodates large portfolios limited historical data specifically show risk estimated portfolio converges oracle optimal risk parametric rate weakly dependent asset returns theory rely higher order moment assumptions thus allowing heavy tailed asset returns moreover rate convergence quantifies size portfolio management allowed scale exponentially sample size historical data empirical effectiveness proposed method demonstrated synthetic real stock data work extends existing ones achieving robustness high dimensions allowing serial dependence
bayesian optimization exponential convergence paper presents bayesian optimization method exponential convergence without need auxiliary optimization without delta cover sampling bayesian optimization methods require auxiliary optimization additional non convex global optimization problem time consuming hard implement practice also existing bayesian optimization method exponential convergence requires access delta cover sampling considered impractical approach eliminates requirements achieves exponential convergence rate
fast randomized kernel ridge regression statistical guarantees approach improving running time kernel based methods build small sketch kernel matrix use lieu full matrix machine learning task interest describe version approach comes running time guarantees well improved guarantees statistical performance extending notion emph statistical leverage scores setting kernel ridge regression able identify sampling distribution reduces size sketch required number columns sampled emph effective dimensionality problem latter quantity often much smaller previous bounds depend emph maximal degrees freedom give empirical evidence supporting fact second contribution present fast algorithm quickly compute coarse approximations thesescores time linear number samples precisely running time algorithm depending trace kernel matrix regularization parameter obtained via variant squared length sampling adapt kernel setting lastly discuss new notion leverage data point captures fine notion difficulty learning problem
taming wild unified analysis hogwild style algorithms stochastic gradient descent sgd ubiquitous algorithm variety machine learning problems researchers industry developed several techniques optimize sgd runtime performance including asynchronous execution reduced precision main result martingale based analysis enables capture rich noise models arise techniques specifically useour new analysis ways derive convergence rates convex case hogwild relaxed assumptions sparsity problem analyze asynchronous sgd algorithms non convex matrix problems including matrix completion design analyze asynchronous sgd algorithm called buckwild uses lower precision arithmetic show experimentally algorithms run efficiently variety problems modern hardware
beyond convexity stochastic quasi convex optimization poster moved monday thursday 101
limitation spectral methods gaussian hidden clique problem rank perturbations gaussian tensors consider following detection problem given realization asymmetric matrix dimension distinguish hypothesisthat upper triangular variables gaussians variableswith mean variance hypothesis aplanted principal submatrix dimension upper triangularvariables gaussians mean variance whereasall upper triangular elements gaussians variables mean variance refer asthe gaussian hidden clique problem epsilon sqrt epsilon possible solve thisdetection problem probability o_n computing thespectrum considering largest eigenvalue prove epsilon sqrt algorithm examines theeigenvalues detect existence hiddengaussian clique error probability vanishing infty result immediate consequence general result rank oneperturbations dimensional gaussian tensors context establish lower bound criticalsignal noise ratio rank signal cannot detected
regularized algorithms unified framework statistical guarantees latent models fundamental modeling tool machine learning applications present significant computational analytical challenges popular algorithm variants much used algorithmic tool yet rigorous understanding performance highly incomplete recently work demonstrated important class problems exhibits linear local convergence high dimensional setting however step well defined address precisely setting unified treatment using regularization regularization high dimensional problems well understood iterative algorithm requires careful balancing making progress towards solution identifying right structure sparsity low rank particular regularizing step using state art high dimensional prescriptions guaranteed provide balance algorithm analysis linked way reveals balance optimization statistical errors specialize general framework sparse gaussian mixture models high dimensional mixed regression regression missing variables obtaining statistical guarantees examples
black box optimization noisy functions unknown smoothness study problem black box optimization function dimension given function evaluations perturbed noise function assumed locally smooth around global optima smoothness unknown contribution adaptive optimization algorithm poo parallel optimistic optimization able deal setting poo performs almost well best known algorithms requiring knowledge smoothness furthermore poo works larger class functions previously considered especially functions difficult optimize precise sense provide finite time analysis poo performance shows error evaluations factor sqrt away error best known optimization algorithms using knowledge smoothness
combinatorial cascading bandits propose combinatorial cascading bandits class partial monitoring problems step learning agent chooses tuple ground items subject constraints receives reward weights chosen items weights items binary stochastic drawn independently agent observes index first chosen item whose weight observation model arises network routing instance learning agent observe first link routing path blocks path propose ucb like algorithm solving problems combcascade prove gap dependent gap free upper bounds step regret proofs build recent work stochastic combinatorial semi bandits also address novel challenges setting non linear reward function partial observability evaluate combcascade real world problems show performs well even modeling assumptions violated also demonstrate setting requires new learning algorithm
adaptive primal dual splitting methods statistical learning image processing alternating direction method multipliers admm important tool solving complex optimization problems involves minimization sub steps often difficult solve efficiently primal dual hybrid gradient pdhg method powerful alternative often simpler substeps admm thus producing lower complexity solvers despite flexibility method pdhg often impractical requires careful choice multiple stepsize parameters often intuitive way choose parameters maximize efficiency even achieve convergence propose self adaptive stepsize rules automatically tune pdhg parameters optimal convergence rigorously analyze methods identify convergence rates numerical experiments show adaptive pdhg strong advantages non adaptive methods terms efficiency simplicity user
sum squares lower bounds sparse pca paper establishes statistical versus computational trade offfor solving basic high dimensional machine learning problem via basic convex relaxation method specifically consider sparse principal component analysis sparse pca problem family sum squares sos aka lasserre parillo convex relaxations well known large dimension planted sparse unit vector principle detected using approx log gaussian bernoulli samples efficient polynomial time algorithms known require approx samples also known quadratic gap cannot improved basic semi definite sdp aka spectral relaxation equivalent degree sos algorithms prove also degree sos algorithms cannot improve quadratic gap average case lower bound adds small collection hardness results machine learning powerful family convex relaxation algorithms moreover design moments pseudo expectations lower bound quite different previous lower bounds establishing lower bounds higher degree sos algorithms remains challenging problem
online gradient boosting extend theory boosting regression problems online learning setting generalizing batch setting boosting notion weak learning algorithm modeled online learning algorithm linear loss functions competes base class regression functions strong learning algorithm online learning algorithm smooth convex loss functions competes larger class regression functions main result online gradient boosting algorithm converts weak online learning algorithm strong larger class functions linear span base class also give simpler boosting algorithm converts weak online learning algorithm strong larger class functions convex hull base class prove optimality
regularization free estimation trace regression symmetric positive semidefinite matrices trace regression models received considerable attention context matrix completion quantum state tomography compressed sensing estimation underlying matrix regularization based approaches promoting low rankedness notably nuclear norm regularization enjoyed great popularity paper argue regularization longer necessary underlying matrix symmetric positive semidefinite spd design satisfies certain conditions situation simple least squares estimation subject spd constraint perform well regularization based approaches proper choice regularization parameter entails knowledge noise level tuning contrast constrained least squaresestimation comes without tuning parameter hence preferred due simplicity
convergence analysis prediction markets via randomized subspace descent prediction markets economic mechanisms aggregating information future events sequential interactions traders pricing mechanisms markets known related optimization algorithms machine learning connections understanding equilibrium market prices relate beliefs traders market however little known rates guarantees convergence sequential mechanisms recent papers cite important open question paper show previously studied prediction market trading models understood natural generalization randomized coordinate descent call randomized subspace descent rsd establish convergence rates rsd leverage prove rates prediction market models answering open questions results extend beyond standard centralized markets arbitrary trade networks
accelerated proximal gradient methods nonconvex programming nonconvex nonsmooth problems recently received considerable attention signal image processing statistics machine learning however solving nonconvex nonsmooth optimization problems remains big challenge accelerated proximal gradient apg excellent method convex programming however still unknown whether usual apg ensure convergence critical point nonconvex programming address issue introduce monitor corrector step extend apg general nonconvex nonsmooth programs accordingly propose monotone apg non monotone apg latter waives requirement monotonic reduction objective function needs less computation iteration best knowledge first provide apg type algorithms general nonconvex nonsmooth problems ensuring every accumulation point critical point convergence rates remain problems convex number iterations numerical results testify advantage algorithms speed
nearly optimal private lasso present nearly optimal differentially private version well known lasso estimator algorithm provides privacy protection respect training data item excess risk algorithm compared non private version widetilde assuming input data bounded ell_ infty norm first differentially private algorithm achieves bound without polynomial dependence addition assumption design matrix addition show error bound nearly optimal amongst differentially private algorithms
minimax time series prediction consider adversarial formulation problem ofpredicting time series square loss aim predictan arbitrary sequence vectors almost well bestsmooth comparator sequence retrospect approach allowsnatural measures smoothness squared norm ofincrements generally consider linear time seriesmodel penalize comparator sequence energy ofthe implied driving noise terms derive minimax strategyfor problems type show implementedefficiently optimal predictions linear previousobservations obtain explicit expression regret interms parameters defining problem typical simple definitions smoothness computation optimalpredictions involves sparse matrices case ofnorm constrained data smoothness defined termsof squared norm comparator increments show thatthe regret grows sqrt lambda_t lengthof game lambda_t increasing limit comparatorsmoothness
communication complexity distributed convex learning optimization study fundamental limits communication efficient distributed methods convex learning optimization different assumptions information available individual machines types functions considered identify cases existing algorithms already worst case optimal well cases room improvement still possible among things results indicate without similarity local objective functions due statistical data similarity otherwise many communication rounds required even machines unbounded computational power
explore improved high probability regret bounds non stochastic bandits work addresses problem regret minimization non stochastic multi armed bandit problems focusing performance guarantees hold high probability results rather scarce literature since proving requires large deal technical effort significant modifications standard intuitive algorithms come guarantees hold expectation modifications forcing learner sample arms uniform distribution least omega sqrt times rounds adversely affect performance many arms suboptimal widely conjectured property essential proving high probability regret bounds show paper possible achieve strong results without undesirable exploration component result relies simple intuitive loss estimation strategy called implicit exploration allows remarkably clean analysis demonstrate flexibility technique derive several improved high probability bounds various extensions standard multi armed bandit framework finally conduct simple experiment illustrates robustness implicit exploration technique
nonconvex optimization framework low rank matrix estimation study estimation low rank matrices via nonconvex optimization compared convex relaxation nonconvex optimization exhibits superior empirical performance large scale instances low rank matrix estimation however understanding theoretical guarantees limited paper define notion projected oracle divergence based establish sufficient conditions success nonconvex optimization illustrate consequences general framework matrix sensing completion particular prove broad class nonconvex optimization algorithms including alternating minimization gradient type methods geometrically converge global optimum exactly recover true low rank matrices standard conditions
individual planning infinite horizon multiagent settings inference structure scalability paper provides first formalization self interested planning multiagent settings using expectation maximization formalization context infinite horizon finitely nested interactive pomdps pomdp distinct formulations pomdps cooperative multiagent planning frameworks exploit graphical model structure specific pomdps present new approach based block coordinate descent speed forward filtering backward sampling combination exact filtering sampling explored exploit problem structure
deep generative image models using laplacian pyramid adversarial networks paper introduce generative model capable producing high quality samples natural images approach uses cascade convolutional networks convnets within laplacian pyramid framework generate images coarse fine fashion level pyramid separate generative convnet model trained using generative adversarial nets gan approach samples drawn model significantly higher quality existing models quantitive assessment human evaluators cifar10 samples mistaken real images around time compared gan samples also show samples diverse datasets stl10 lsun
shepard convolutional neural networks deep learning recently introduced field low level computer vision image processing promising results obtained number tasks including super resolution inpainting deconvolution filtering etc however previously adopted neural network approaches convolutional neural networks sparse auto encoders inherently translation invariant operators found property prevents deep learning approaches outperforming state art task requires translation variant interpolation tvi paper draw shepard interpolation design shepard convolutional neural networks shcnn efficiently realizes end end trainable tvi operators network show adding feature maps new shepard layers network able achieve stronger results much deeper architecture superior performance image inpainting super resolution obtained system outperforms previous ones keeping running time competitive
learning structured output representation using deep conditional generative models supervised deep learning successfully applied many recognition problems machine learning computer vision although approximate complex many function well large number training data provided lack probabilistic inference current supervised deep learning methods makes difficult model complex structured output representations work develop scalable deep conditional generative model structured output variables using gaussian latent variables model trained efficiently framework stochastic gradient variational bayes allows fast prediction using stochastic feed forward inference addition provide novel strategies build robust structured prediction algorithms recurrent prediction network architecture input noise injection multi scale prediction training methods experiments demonstrate effectiveness proposed algorithm comparison deterministic deep neural network counterparts generating diverse realistic output representations using stochastic inference furthermore proposed schemes training methods architecture design complimentary leads achieve strong pixel level object segmentation semantic labeling performance caltech ucsd birds 200 subset labeled faces wild dataset
expressing image stream sequence natural sentences propose approach generating sequence natural sentences image stream since general users usually take series pictures special moments much online visual information exists form image streams would better take consideration whole set generate natural language descriptions almost previous studies dealt relation single image single natural sentence work extends input output dimension sequence images sequence sentences end design novel architecture called coherent recurrent convolutional network crcn consists convolutional networks bidirectional recurrent networks entity based local coherence model approach directly learns vast user generated resource blog posts text image parallel training data demonstrate approach outperforms state art candidate methods using quantitative measures bleu top recall user studies via amazon mechanical turk
visalogy answering visual analogy questions paper study problem answering visual analogy questions questions take form image image image answering questions entails discovering mapping image image extending mapping image searching image relation holds pose problem learning embedding encourages pairs analogous images similar transformations close together using convolutional neural networks quadruple siamese architecture introduce dataset visual analogy questions natural images show first results kind solving analogy questions natural images
bidirectional recurrent convolutional networks multi frame super resolution super resolving low resolution video usually handled either single image super resolution multi frame single image deals video frame independently ignores intrinsic temporal dependency video frames actually plays important role video super resolution multi frame generally extracts motion information optical flow model temporal dependency often shows high computational cost considering recurrent neural network rnn model long term contextual information temporal sequences well propose bidirectional recurrent convolutional network efficient multi frame different vanilla rnn commonly used recurrent full connections replaced weight sharing convolutional connections conditional convolutional connections previous input layers current hidden layer added enhancing visual temporal dependency modelling powerful temporal dependency modelling model super resolve videos complex motions achieve state art performance due cheap convolution operations model low computational complexity runs orders magnitude faster multi frame methods
submodboxes near optimal search set diverse object proposals paper formulates search set bounding boxes needed object proposal generation monotone submodular maximization problem space possible bounding boxes image since number possible bounding boxes image large pixels even single linear scan perform greedy augmentation submodular maximization intractable thus formulate greedy augmentation step branch bound scheme order speed repeated application propose novel generalization minoux lazy greedy algorithm tree theoretically proposed formulation provides new understanding problem contains classic heuristic approaches sliding window non maximal suppression nms efficient subwindow search ess special cases empirically show approach leads state art performance object proposal generation via novel diversity measure
galileo perceiving physical object properties integrating physics engine deep learning humans demonstrate remarkable abilities predict physical events dynamic scenes infer physical properties objects static images propose generative model solving problems physical scene understanding real world videos images core generative model physics engine operating object based representation physical properties including mass position shape friction infer latent properties using relatively brief runs mcmc drive simulations physics engine fit key features visual observations explore directly mapping visual inputs physical properties inverting part generative process using deep learning name model galileo evaluate video dataset simple yet physically rich scenarios results show galileo able infer physical properties objects predict outcome variety physical events accuracy comparable human subjects study points towards account human vision generative physical knowledge core various recognition models helpers leading efficient inference
learning visual biases human imagination although human visual system recognize many concepts challengingconditions still biases paper investigate whether wecan extract biases transfer machine recognition system introduce novel method inspired well known tools humanpsychophysics estimates biases human visual system might use forrecognition computer vision feature spaces experiments aresurprising suggest classifiers human visual system betransferred machine success since classifiers seem tocapture favorable biases human visual system present svmformulation constrains orientation svm hyperplane agree withthe bias human visual system results suggest transferring thishuman bias machines help object recognition systems generalize acrossdatasets perform better little training data available
character level convolutional networks text classification article offers empirical exploration use character level convolutional networks convnets text classification constructed several large scale datasets show character level convolutional networks could achieve state art competitive results comparisons offered traditional models bag words grams tfidf variants deep learning models word based convnets recurrent neural networks
winner take autoencoders paper propose winner take method learning hierarchical sparse representations unsupervised fashion first introduce fully connected winner take autoencoders use mini batch statistics directly enforce lifetime sparsity activations hidden units propose convolutional winner take autoencoder combines benefits convolutional architectures autoencoders learning shift invariant sparse representations describe way train convolutional autoencoders layer layer addition lifetime sparsity spatial sparsity within feature map achieved using winner take activation functions show winner take autoencoders used learn deep sparse representations mnist cifar imagenet street view house numbers toronto face datasets achieve competitive classification performance
learning weights connections efficient neural network neural networks computationally intensive memory intensive making difficult deploy embedded systems also conventional networks fix architecture training starts result training cannot improve architecture address limitations describe method reduce storage computation required neural networks order magnitude without affecting accuracy learning important connections method prunes redundant connections using step method first train network learn connections important next prune unimportant connections finally retrain network fine tune weights remaining connections imagenet dataset method reduced number parameters alexnet factor million million without incurring accuracy loss similar experiments vgg found total number parameters reduced 138 million million loss accuracy
interactive control diverse complex characters neural networks present method training recurrent neural networks act near optimal feedback controllers able generate stable realistic behaviors range dynamical systems tasks swimming flying biped quadruped walking different body morphologies require motion capture task specific features state machines controller neural network large number feed forward units learn elaborate state action mappings small number recurrent units implement memory states beyond physical system state action generated network defined velocity thus network learning control policy rather dynamics implicit policy essential features method include interleaving supervised learning trajectory optimization injecting noise training training unexpected changes task specification using trajectory optimizer obtain optimal feedback gains addition optimal actions
biologically inspired dynamic textures probing motion perception perception often described predictive process based optimal inference respect generative model study principled construction generative model specifically crafted probe motion perception context first provide axiomatic biologically driven derivation model model synthesizes random dynamic textures defined stationary gaussian distributions obtained random aggregation warped patterns importantly show model equivalently described stochastic partial differential equation using characterization motion images allows recast motion energy models principled bayesian inference framework finally apply textures order psychophysically probe speed perception humans framework likelihood derived generative model prior estimated observed results accounts perceptual bias principled fashion
unsupervised learning program synthesis introduce unsupervised learning algorithmthat combines probabilistic modeling solver based techniques program synthesis apply techniques visual learning domain language learning problem showing algorithm learn many visual concepts examplesand recover english inflectional morphology taken together results give new approach unsupervised learning symbolic compositional structures technique applying program synthesis tools noisy data
deep poisson factor modeling propose new deep architecture topic modeling based poisson factor analysis pfa modules model composed poisson distribution model observed vectors counts well deep hierarchy hidden binary units rather using logistic functions characterize probability latent binary unit employ bernoulli poisson link allows pfa modules used repeatedly deep architecture also describe approach build discriminative topic models adapting pfa modules derive efficient inference via mcmc stochastic variational methods scale number non zeros data binary units yielding significant efficiency relative models based logistic links experiments several corpora demonstrate advantages model compared related deep models
large scale bayesian multi label learning via topic based label embeddings present scalable bayesian multi label learning model based learning low dimensional label embeddings model assumes label vector generated weighted combination set topics topic distribution labels combination weights embeddings label vector conditioned observed feature vector construction coupled bernoulli poisson link function label binary label vector leads model computational cost scales number positive labels label matrix makes model particularly appealing real world multi label learning problems label matrix usually massive highly sparse using data augmentation strategy leads full local conjugacy model facilitating simple efficient gibbs sampling well expectation maximization algorithm inference also predicting label vector test time require inference label embeddings done closed form report results several benchmark data sets comparing model various state art methods
tensorizing neural networks deep neural networks currently demonstrate state art performance several domains time models class demanding terms computational resources particular large amount memory required commonly used fully connected layers making hard use models low end devices stopping increase model size paper convert dense weight matrices fully connected layers tensor train format number parameters reduced huge factor time expressive power layer preserved particular deep vgg networks report compression factor dense weight matrix fully connected layer 200000 times leading compression factor whole network times
training restricted boltzmann machine via thouless anderson palmer free energy restricted boltzmann machines undirected neural networks shown tobe effective many applications including serving initializations fortraining deep multi layer neural networks main reasons success theexistence efficient practical stochastic algorithms contrastive divergence unsupervised training propose alternative deterministic iterative procedure based improved mean field method statistical physics known thouless anderson palmer approach demonstrate algorithm provides performance equal sometimes superior persistent contrastive divergence also providing clear easy evaluate objective function believe strategycan easily generalized models well accurate higher order approximations paving way systematic improvements training boltzmann machineswith hidden units
brain uses reliability stimulus information making perceptual decisions simple perceptual decisions brain identify stimulus based noisy sensory samples stimulus basic statistical considerations state reliability stimulus information amount noise samples taken account decision made however perceptual decision making experiments questioned whether brain indeed uses reliability making decisions confronted unpredictable changes stimulus reliability show even basic drift diffusion model frequently used explain experimental findings perceptual decision making implicitly relies estimates stimulus reliability show variants drift diffusion model allow stimulus specific reliabilities consistent neurophysiological findings analysis suggests brain estimates reliability stimulus short time scale hundred milliseconds
unlocking neural population non stationarities using hierarchical dynamics models neural population activity often exhibits rich variability variability thought arise single neuron stochasticity neural dynamics short time scales well modulations neural firing properties long time scales often referred non stationarity better understand nature variability neural circuits impact cortical information processing introduce hierarchical dynamics model able capture inter trial modulations firing rates well neural population dynamics derive algorithm bayesian laplace propagation fast posterior inference demonstrate model provides better account structure neural firing existing stationary dynamics models applied neural population recordings primary visual cortex
deeply learning messages message passing inference deep structured output learning shows great promise tasks like semantic image segmentation proffer new efficient deep structured model learning scheme show deep convolutional neural networks cnns used directly estimate messages message passing inference structured prediction conditional random fields crfs cnn message estimators obviate need learn evaluate potential functions message calculation confers significant efficiency learning since otherwise performing structured learning crf cnn potentials necessary undertake expensive inference every stochastic gradient iteration network output dimension message estimators number classes rather exponentially growing order potentials hence scalable cases large number classes involved apply method semantic image segmentation achieve impressive performance demonstrates effectiveness usefulness cnn message learning method
coevolve joint point process model information diffusion network evolution information diffusion online social networks affected underlying network topology also power change online users constantly creating new links exposed new information sources turn links alternating way information spreads however highly intertwined stochastic processes information diffusion network evolution predominantly studied separately ignoring evolutionary dynamics propose temporal point process model coevolve joint dynamics allowing intensity process modulated model allows efficiently simulate interleaved diffusion network events generate traces obeying common diffusion network patterns observed real world networks furthermore also develop convex optimization framework learn parameters model historical diffusion network evolution traces experimented synthetic data data gathered twitter show model provides good fit data well accurate predictions alternatives
human kernel bayesian nonparametric models gaussian processes provide compelling framework automatic statistical modelling models high degree flexibility automatically calibrated complexity however automating human expertise remains elusive example gaussian processes standard kernels struggle function extrapolation problems trivial human learners paper create function extrapolation problems acquire human responses design kernel learning framework reverse engineer inductive biases human learners across set behavioral experiments use learned kernels gain psychological insights extrapolate human like ways beyond traditional stationary polynomial kernels finally investigate occam razor human gaussian process based function learning
latent bayesian melding integrating individual population models many statistical problems coarse grained model suitable population level behaviour whereas detailed model appropriate accurate modelling individual behaviour raises question integrate types models methods posterior regularization follow idea generalized moment matching allow matchingexpectations models sometimes models conveniently expressed latent variable models propose latent bayesian melding motivated averaging distributions populations statistics individual level population level models logarithmic opinion pool framework case study electricity disaggregation type single channel blind source separation problem show latent bayesian melding leads significantly accurate predictions approach based solely generalized moment matching
high dimensional neural spike train analysis generalized count linear dynamical systems latent factor models widely used analyze simultaneous recordings spike trains large heterogeneous neural populations models assume signal interest population low dimensional latent intensity evolves time observed high dimension via noisy point process observations techniques well used capture neural correlations across population provide smooth denoised concise representation high dimensional spiking data limitation many current models observation model assumed poisson lacks flexibility capture dispersion common recorded neural data thereby introducing bias estimates covariance develop generalized count linear dynamical system relaxes poisson assumption using general exponential family count data addition containing poisson bernoulli negative binomial common count distributions special cases show model tractably learned extending recent advances variational inference techniques apply model data primate motor cortex demonstrate performance improvements state art methods capturing variance structure data held prediction
efficient learning continuous time hidden markov models disease progression continuous time hidden markov model hmm attractive approach modeling disease progression due ability describe noisy observations arriving irregularly time however lack efficient parameter learning algorithm hmm restricts use small models requires unrealistic constraints state transitions paper present first complete characterization efficient based learning methods hmm models demonstrate learning problem consists challenges estimation posterior state probabilities computation end state conditioned statistics solve first challenge reformulating estimation problem terms equivalent discrete time inhomogeneous hidden markov model second challenge addressed adapting approaches continuous time markov chain literature hmm domain demonstrate use hmms 100 states visualize predict disease progression using glaucoma dataset alzheimer disease dataset
population posterior bayesian modeling streams many modern data analysis problems involve inferences streaming data however streaming data easily amenable standard probabilistic modeling approaches assume condition finite data develop population variational bayes new approach using bayesian modeling analyze streams data approximates new type distribution population posterior combines notion population distribution data bayesian inference probabilistic model study method latent dirichlet allocation dirichlet process mixtures several large scale data sets
probabilistic curve learning coulomb repulsion electrostatic gaussian process learning low dimensional structure multidimensional data canonical problem machine learning common approach suppose observed data close lower dimensional smooth manifold rich variety manifold learning methods available allow mapping data points manifold however clear lack probabilistic methods allow learning manifold along generative distribution observed data best attempt gaussian process latent variable model lvm identifiability issues lead poor performance solve issues proposing novel coulomb repulsive process corp locations points manifold inspired physical models electrostatic interactions among particles combining process prior mapping function yields novel electrostatic electrogp process focusing simple case dimensional manifold develop efficient inference algorithms illustrate substantially improved performance variety experiments including filling missing frames video
preconditioned spectral descent deep learning deep learning presents notorious computational challenges challenges include limited non convexity learning objectives estimating quantities needed optimization algorithms gradients address non convexity present optimization solution ploits far unused geometry objective function order best make use estimated gradients previous work attempted similar goals preconditioned methods euclidean space bfgs rmsprop ada grad stark contrast approach combines non euclidean gradient method preconditioning provide evidence combination accurately captures geometry objective function compared prior work theoretically formalize arguments derive novel preconditioned non euclidean algorithms results promising computational time quality applied restricted boltzmann machines feedforward neural nets convolutional neural nets
learning continuous control policies stochastic value gradients present unified framework learning continuous control policies usingbackpropagation supports stochastic control treating stochasticity thebellman equation deterministic function exogenous noise productis spectrum general policy gradient algorithms range model freemethods value functions model based methods without value functions use learned models require observations environment insteadof observations model predicted trajectories minimizing impactof compounded model errors apply algorithms first toy stochasticcontrol problem several physics based control problems simulation variants svg shows effectiveness learning models valuefunctions policies simultaneously continuous domains
learning stationary time series using gaussian processes nonparametric kernels introduce gaussian process convolution model gpcm stage nonparametric generative procedure model stationary signals convolution continuous time white noise process continuous time linear filter drawn gaussian process gpcm continuous time nonparametric window moving average process conditionally gaussian process nonparametric kernel defined probabilistic fashion generative model equivalently considered frequency domain power spectral density signal specified using gaussian process main contributions paper develop novel variational free energy approach based inter domain inducing variables efficiently learns continuous time linear filter infers driving white noise process turn scheme provides closed form probabilistic estimates covariance kernel noise free signal denoising prediction scenarios additionally variational inference procedure provides closed form expressions approximate posterior spectral density given observed data leading new bayesian nonparametric approaches spectrum estimation proposed gpcm validated using synthetic real world signals
path sgd path normalized optimization deep neural networks revisit choice sgd training deep neural networks reconsidering appropriate geometry optimize weights argue geometry invariant rescaling weights affect output network suggest path sgd approximate steepest descent method respect path wise regularizer related max norm regularization path sgd easy efficient implement leads empirical gains sgd adagrad
automatic variational inference stan variational inference scalable technique approximate bayesian inference deriving variational inference algorithms requires tedious model specific calculations makes difficult non experts use propose automatic variational inference algorithm automatic differentiation variational inference advi implement stan code available probabilistic programming system advi user provides bayesian model dataset nothing else make conjugacy assumptions support broad class models algorithm automatically determines appropriate variational family optimizes variational objective compare advi mcmc sampling across hierarchical generalized linear models nonconjugate matrix factorization mixture model train mixture model quarter million images advi use variational inference model write stan
data generation sequential decision making connect broad class generative models shared reliance sequential decision making motivated view develop extensions existing model explore idea context data imputation perhaps simplest setting investigate relation unconditional conditional generative modelling formulate data imputation mdp develop models capable representing effective policies construct models using neural networks train using form guided policy search models generate predictions iterative process feedback refinement show approach learn effective policies imputation problems varying difficulty across multiple datasets
stochastic expectation propagation expectation propagation deterministic approximation algorithm often used perform approximate bayesian parameter learning approximates full intractable posterior distribution set local approximations iteratively refined datapoint offer analytic computational advantages approximations variational inference method choice number models local nature appears make ideal candidate performing bayesian learning large models large scale datasets settings however crucial limitation context number approximating factors needs increase number data points often entails prohibitively large memory overhead paper presents extension called stochastic expectation propagation sep maintains global posterior approximation like updates local way like experiments number canonical learning problems using synthetic real world datasets indicate sep performs almost well full reduces memory consumption factor sep therefore ideally suited performing approximate bayesian learning large model large dataset setting
deep learning elastic averaging sgd study problem stochastic optimization deep learning parallel computing environment communication constraints new algorithm proposed setting communication coordination work among concurrent processes local workers based elastic force links parameters compute center variable stored parameter server master algorithm enables local workers perform exploration algorithm allows local variables fluctuate center variable reducing amount communication local workers master empirically demonstrate deep learning setting due existence many local optima allowing exploration lead improved performance propose synchronous asynchronous variants new algorithm provide stability analysis asynchronous variant round robin scheme compare common parallelized method admm show stability easgd guaranteed simple stability condition satisfied case admm additionally propose momentum based version algorithm applied synchronous asynchronous settings asynchronous variant algorithm applied train convolutional neural networks image classification cifar imagenet datasets experiments demonstrate new algorithm accelerates training deep architectures compared downpour common baseline approaches furthermore communication efficient
learning group invariant features kernel perspective analyze paper random feature map based theory invariance emph theory introduced cite anselmilrmtp13 specifically group invariant signal signature obtained cumulative distributions group transformed random projections analysis bridges invariant feature learning kernel methods show feature map defines expected haar integration kernel invariant specified group action show non linear random feature map approximates group invariant kernel uniformly set points moreover show defines function space dense equivalent invariant reproducing kernel hilbert space finally quantify error rates convergence empirical risk minimization well reduction sample complexity learning algorithm using invariant representation signal classification classical supervised learning setting
linear response methods accurate covariance estimates mean field variational bayes mean field variational bayes mfvb popular posterior approximation method due fast runtime large scale data sets however well known failing mfvb underestimates uncertainty model variables sometimes severely provides information model variable covariance generalize linear response methods statistical physics deliver accurate uncertainty estimates model variables individual variables coherently across variables call method linear response variational bayes lrvb mfvb posterior approximation exponential family lrvb simple analytic form even non conjugate models indeed make assumptions form true posterior demonstrate accuracy scalability method range models simulated real data
probabilistic line searches stochastic optimization deterministic optimization line searches standard tool ensuring stability efficiency stochastic gradients available direct equivalent far formulated uncertain gradients allow strict sequence decisions collapsing search space construct probabilistic line search combining structure existing deterministic methods notions bayesian optimization method retains gaussian process surrogate univariate optimization objective uses probabilistic belief wolfe conditions monitor descent algorithm low computational cost user controlled parameters experiments show effectively removes need define learning rate stochastic gradient descent
hybrid sampler poisson kingman mixture models paper concerns introduction new markov chain monte carlo scheme posterior sampling bayesian nonparametric mixture models priors belong general poisson kingman class present novel compact way representing infinite dimensional component model explicitly representing infinite component less memory storage requirements previous mcmc schemes describe comparative simulation results demonstrating efficacy proposed mcmc algorithm existing marginal conditional mcmc samplers
tree guided mcmc inference normalized random measure mixture models normalized random measures nrms provide broad class discrete random measures often used priors bayesian nonparametric models dirichlet process well known example nrms posterior inference methods nrm mixture models rely mcmc methods since easy implement convergence well studied however mcmc often suffers slow convergence acceptance rate low tree based inference alternative deterministic posterior inference method bayesian hierarchical clustering bhc incremental bayesian hierarchical clustering ibhc developed nrm mixture nrmm models respectively although ibhc promising method posterior inference nrmm models due efficiency applicability online inference convergence guaranteed since uses heuristics simply selects best solution multiple trials made paper present hybrid inference algorithm nrmm models combines merits mcmc ibhc trees built ibhc outlinespartitions data guides metropolis hastings procedure employ appropriate proposals inheriting nature mcmc tree guided mcmc tgmcmc guaranteed converge enjoys fast convergence thanks effective proposals guided trees experiments synthetic real world datasets demonstrate benefit method
reflection refraction hamiltonian monte carlo hamiltonian monte carlo hmc successful approach sampling continuous densities however difficulty simulating hamiltonian dynamics non smooth functions leading poor performance paper motivated behavior hamiltonian dynamics physical systems like optics introduce modification leapfrog discretization hamiltonian dynamics piecewise continuous energies intersections trajectory discontinuities detected momentum reflected refracted compensate change energy prove method preserves correct stationary distribution boundaries affine experiments show reducing number rejected samples method improves traditional hmc
planar ultrametrics image segmentation study problem hierarchical clustering planar graphs formulate terms finding closest ultrametric specified set distances solve using relaxation leverages minimum cost perfect matching subroutine efficiently explore space planar partitions apply algorithm problem hierarchical image segmentation
learning bayesian networks thousands variables present method learning bayesian networks data sets containingthousands variables without need structure constraints approachis made parts first novel algorithm effectively explores thespace possible parent sets node guides exploration towards themost promising parent sets basis approximated score function thatis computed constant time second part improvement existingordering based algorithm structure optimization new algorithm provablyachieves higher score compared original formulation large datasets containing thousand nodes novel approach consistently outper forms state art
parallel predictive entropy search batch global optimization expensive objective functions develop textit parallel predictive entropy search ppes novel algorithm bayesian optimization expensive black box objective functions iteration ppes aims select textit batch points maximize information gain global maximizer objective well known strategies exist suggesting single evaluation point based previous observations far fewer known selecting batches points evaluate parallel batch selection schemes studied resort greedy methods compute optimal batch best knowledge ppes first non greedy batch bayesian optimization strategy demonstrate benefit approach optimization performance synthetic real world applications including problems machine learning rocket science robotics
rapidly mixing gibbs sampling class factor graphs using hierarchy width gibbs sampling factor graphs widely used inference technique often produces good empirical results theoretical guarantees performance weak even tree structured graphs mixing time gibbs exponential number variables help understand behavior gibbs sampling introduce new hyper graph property called hierarchy width show suitable conditions weights bounded hierarchy width ensures polynomial mixing time study hierarchy width part motivated class factor graph templates hierarchical templates bounded hierarchy width regardless data used instantiate demonstrate rich application natural language processing gibbs sampling provably mixes rapidly achieves accuracy exceeds human volunteers
provably correct cases variational inference topic models variational inference efficient popular heuristic used context latent variable models provide first analysis instances variational inference algorithms converge global optimum setting topic models initializations natural used lda mostpopular implementation variational inference addition providing intuition heuristic might work practice multiplicative rather additive nature variational inference updates forces usenon standard proof arguments believe might general theoretical interest
large scale probabilistic predictors without guarantees validity paper studies theoretically empirically method turning machine learning algorithms probabilistic predictors automatically enjoys property validity perfect calibration computationally efficient price pay perfect calibration probabilistic predictors produce imprecise practice almost precise large data sets probabilities imprecise probabilities merged precise probabilities resulting predictors losing theoretical property perfect calibration consistently accurate existing methods empirical studies
accuracy self normalized log linear models calculation log normalizer major computational obstacle applications log linear models large output spaces problem fast normalizer computation therefore attracted significant attention theoretical applied machine learning literature paper analyze recently proposed technique known self normalization introduces regularization term training penalize log normalizers deviating makes possible use unnormalized model scores approximate probabilities empirical evidence suggests self normalization extremely effective theoretical understanding work generally applied largely lacking prove upper bounds loss accuracy due self normalization describe classes input distributionsthat self normalize easily construct explicit examples high variance input distributions theoretical results make predictions difficulty fitting self normalized models several classes distributions conclude empirical validation predictions real synthetic datasets
policy evaluation using return propose return alternative return currently used family algorithms benefit return accounts correlation different length returns difficult compute exactly suggest way approximating return provide empirical studies suggest superior return return variety problems
community detection via measure space embedding present new algorithm community detection algorithm uses random walks embed graph space measures modification means space applied algorithm therefore fast easily parallelizable evaluate algorithm standard random graph benchmarks including overlapping community benchmarks find performance better least good previously known algorithms also prove linear time number edges guarantee algorithm stochastic block model geq cdot half epsilon geq sqrt half epsilon log
consistency common neighbors link prediction stochastic blockmodels link prediction clustering key problems network structureddata spectral clustering strong theoretical guaranteesunder popular stochastic blockmodel formulation networks itcan expensive large graphs hand heuristic ofpredicting links nodes share common neighbors withthe query node much fast works well practice showtheoretically common neighbors heuristic extract clustersw graph dense enough even sparsergraphs addition cleaning step empirical results onsimulated real world data support conclusions
inference determinantal point processes without spectral knowledge determinantal point processes dpps point process models thatnaturally encode diversity points agiven realization positive definite kernel dpps possess desirable properties exactsampling analyticity moments learning parameters ofkernel likelihood based inference notstraightforward first kernel appears thelikelihood another kernel related throughan often intractable spectral decomposition issue typically bypassed machine learning bydirectly parametrizing kernel price someinterpretability model parameters follow approachhere second likelihood intractable normalizingconstant takes form large determinant case adpp finite set objects form fredholm determinant thecase dpp continuous domain main contribution derive bounds likelihood ofa dpp finite continuous domains unlike previous work bounds arecheap evaluate since rely approximating spectrumof large matrix operator usual arguments bounds thus yield cheap variationalinference moderately expensive exact markov chain monte carlo inference methods dpps
sample complexity learning mahalanobis distance metrics metric learning seeks transformation feature space enhances prediction quality given task work provide pac style sample complexity rates supervised metric learning give matching lower upper bounds showing sample complexity scales representation dimension assumptions made underlying data distribution addition leveraging structure data distribution provide rates fine tuned specific notion intrinsic complexity given dataset allowing relax dependence representation dimension show theoretically empirically augmenting metric learning optimization criterion simple norm based regularization important help adapt dataset intrinsic complexity yielding better generalization thus partly explaining empirical success similar regularizations reported previous works
matrix manifold optimization gaussian mixtures take new look parameter estimation gaussian mixture model gmms specifically advance riemannian manifold optimization manifold positive definite matrices potential replacement expectation maximization facto standard decades box invocation riemannian optimization however fails spectacularly obtains solution vastly slower building intuition geometric convexity propose simple reformulation remarkable consequences makes riemannian optimization match nontrivial result given poor record nonlinear programming also outperform many settings bring ideas fruition develop well tuned riemannian lbfgs method proves superior known competing methods riemannian conjugate gradient hope results encourage wider consideration manifold optimization machine learning statistics
frank wolfe bayesian quadrature probabilistic integration theoretical guarantees renewed interest formulating integration inference problem motivated obtaining full distribution numerical error propagated subsequent computation current methods bayesian quadrature demonstrate impressive empirical performance lack theoretical analysis important challenge reconcile probabilistic integrators rigorous convergence guarantees paper present first probabilistic integrator admits theoretical treatment called frank wolfe bayesian quadrature fwbq fwbq convergence true value integral shown exponential posterior contraction rates proven superexponential simulations fwbq competitive state art methods performs alternatives based frank wolfe optimisation approach applied successfully quantify numerical error solution challenging model choice problem cellular biology
scale nonlinear component analysis doubly stochastic gradients nonlinear component analysis kernel principle component analysis kpca kernel canonical correlation analysis kcca widely used machine learning statistics data analysis scale big datasets recent attempts employed random feature approximations convert problem primal form linear computational complexity however obtain high quality solutions number random features order magnitude number data points making approach directly applicable regime millions data points propose simple computationally efficient memory friendly algorithm based doubly stochastic gradients scale range kernel nonlinear component analysis kernel pca cca svd despite emph non convex nature problems method enjoys theoretical guarantees converges rate otil global optimum even top eigen subspace unlike many alternatives algorithm require explicit orthogonalization infeasible big datasets demonstrate effectiveness scalability algorithm large scale synthetic real world datasets
self normalized estimator counterfactual learning paper identifies severe problem counterfactual risk estimator typically used batch learning logged bandit feedback blbf proposes use alternative estimator avoids problem blbf setting learner receive full information feedback like supervised learning observes feedback actions taken historical policy makes blbf algorithms particularly attractive training online systems placement web search recommendation using historical logs counterfactual risk minimization crm principle offers general recipe designing blbf algorithms requires counterfactual risk estimator virtually existing works blbf focused particular unbiased estimator show conventional estimator suffers apropensity overfitting problem used learning complex hypothesis spaces propose replace risk estimator self normalized estimator showing neatly avoids problem naturally gives rise new learning algorithm normalized policy optimizer exponential models norm poem structured output prediction using linear rules evaluate empirical effectiveness norm poem severalmulti label classification problems finding consistently outperforms conventional estimator
distributionally robust logistic regression paper proposes distributionally robust approach logistic regression use wasserstein distance construct ball space probability distributions centered uniform distribution training samples radius wasserstein ball chosen judiciously guarantee contains unknown data generating distribution high confidence formulate distributionally robust logistic regression model minimizes worst case expected logloss function worst case taken distributions wasserstein ball prove optimization problem admits tractable reformulation encapsulates classical well popular regularized logistic regression problems special cases propose distributionally robust approach based wasserstein balls compute upper lower confidence bounds misclassification probability resulting classifier bounds given optimal values highly tractable linear programs validate theoretical sample guarantees simulated empirical experiments
top multiclass svm class ambiguity typical image classification problems large number classes classes difficult discriminate makes sense allow guesses evaluate classifiers based top error instead standard loss propose top multiclass svm direct method optimize top performance generalization well known multiclass svm based tight convex upper bound top error propose fast optimization scheme based efficient projection onto top simplex interest experiments datasets show consistent improvements top accuracy compared various baselines
measuring sample quality stein method improve efficiency monte carlo estimation practitioners turning biased markov chain monte carlo procedures trade asymptotic exactness computational speed reasoning sound reduction variance due rapid sampling outweigh bias introduced however inexactness creates new challenges sampler parameter selection since standard measures sample quality like effective sample size account asymptotic bias address challenges introduce new computable quality measure based stein method bounds discrepancy sample target expectations large class test functions use tool compare exact biased deterministic sample sequences illustrate applications hyperparameter selection convergence rate assessment quantifying bias variance tradeoffs posterior inference
asynchronous parallel stochastic gradient nonconvex optimization asynchronous parallel implementations stochastic gradient broadly used solving deep neural network received many successes practice recently however existing theories cannot explain convergence speedup properties mainly due nonconvexity deep learning formulations asynchronous parallel mechanism fill gaps theory provide theoretical supports paper studies asynchronous parallel implementations computer network shared memory system establish ergodic convergence rate sqrt algorithms prove linear speedup achievable number workers bounded sqrt total number iterations results generalize improve existing analysis convex minimization
distributed submodular cover succinctly summarizing massive data find subset ideally small possible well represents massive dataset corresponding utility measured according suitable utility function comparable whole dataset paper formalize challenge submodular cover problem utility assumed exhibit submodularity natural diminishing returns condition preva lent many data summarization applications classical greedy algorithm known provide solutions logarithmic approximation guarantees compared optimum solution however sequential centralized approach imprac tical truly large scale problems work develop first distributed algorithm discover submodular set cover easily implementable using mapreduce style computations theoretically analyze approach present approximation guarantees solutions returned discover also study natural trade communication cost num ber rounds required obtain solution extensive experiments demonstrate effectiveness approach several applications includ ing active set selection exemplar based clustering vertex cover tens millions data points using spark
parallel correlation clustering big graphs given similarity graph items correlation clustering groups similar items together dissimilar ones apart popular algorithms kwikcluster algorithm serially clusters neighborhoods vertices obtains approximation ratio unfortunately practice kwikcluster requires large number clustering rounds potential bottleneck large graphs present clusterwild algorithms parallel correlation clustering run polylogarithmic number rounds provably achieve nearly linear speedups uses concurrency control enforce serializability parallel clustering process guarantees approximation ratio clusterwild coordination free algorithm abandons consistency benefit better scaling leads provably small loss approximation ratio provide extensive experimental results algorithms outperform state art terms clustering accuracy running time show algorithms cluster billion edge graphs seconds cores achieving 15x speedup
fast bidirectional probability estimation markov models develop new bidirectional algorithm estimating markov chain multi step transition probabilities given markov chain want estimate probability hitting given target state ell steps starting given source distribution given target state use reverse local power iteration construct expanded target distribution mean quantity want estimate smaller variance sampled efficiently monte carlo algorithm method extends markov chain discrete finite countable state space extended compute functions multi step transition probabilities pagerank graph diffusions hitting return times etc main result sparse markov chains wherein number transitions states comparable number states running time algorithm uniform random target node order wise smaller monte carlo power iteration based algorithms particular method estimate probability using sqrt running time
evaluating statistical significance biclusters biclustering also known submatrix localization problem high practical relevance exploratory analysis high dimensional data develop framework performing statistical inference biclusters found score based algorithms since bicluster selected data dependent manner biclustering localization algorithm form selective inference framework gives exact non asymptotic confidence intervals values significance selected biclusters generalize approach obtain exact inference gaussian statistics
regularization path cross validation error lower bounds careful tuning regularization parameter indispensable many machine learning tasks significant impact generalization performances nevertheless current practice regularization parameter tuning art science hard tell many grid points would needed cross validation obtaining solution sufficiently small error paper propose novel framework computing lower bound errors function regularization parameter call regularization path error lower bounds proposed framework used providing theoretical approximation guarantee set solutions sense far error current best solution could away best possible error entire range regularization parameters demonstrate numerical experiments theoretically guaranteed choice regularization parameter sense possible reasonable computational costs
sampling probabilistic submodular models submodular supermodular functions found wide applicability machine learning capturing notions diversity regularity respectively notions deep consequences optimization problem approximately optimizing submodular functions received much attention however beyond optimization notions allow specifying expressive probabilistic models used quantify predictive uncertainty via marginal inference prominent well studied special cases include ising models determinantal point processes general class log submodular log supermodular models much richer little studied paper investigate use markov chain monte carlo sampling perform approximate inference general log submodular log supermodular models particular consider simple gibbs sampling procedure establish sufficient conditions first guaranteeing polynomial time second fast nlogn mixing also evaluate efficiency gibbs sampler examples models compare recently proposed variational approach
submodular hamming metrics show largely unexplored class functions positive polymatroids define proper discrete metrics pairs binary vectors fairly tractable optimize exploiting submodularity able give hardness results approximation algorithms optimizing metrics additionally demonstrate empirically effectiveness metrics associated algorithms metric minimization task form clustering also metric maximization task generating diverse best lists
extending gossip algorithms distributed estimation statistics efficient robust algorithms decentralized estimation networks essential many distributed systems whereas distributed estimation sample mean statistics subject good deal attention computation statistics relying expensive averaging pairs observations less investigated area yet data functionals essential describe global properties statistical population important examples including area curve empirical variance gini mean difference within cluster point scatter paper proposes new synchronous asynchronous randomized gossip algorithms simultaneously propagate data across network maintain local estimates statistic interest establish convergence rate bounds log synchronous asynchronous cases respectively number iterations explicit data network dependent terms beyond favorable comparisons terms rate analysis numerical experiments provide empirical evidence proposed algorithms surpasses previously introduced approach
newton stein method second order method glms via stein lemma consider problem efficiently computing maximum likelihood estimator generalized linear models glms number observations much larger number coefficients regime optimization algorithms immensely benefit fromapproximate second order information propose alternative way constructing curvature information formulatingit estimation problem applying stein type lemma allows improvements sub sampling andeigenvalue thresholding algorithm enjoys fast convergence rates resembling second order methods modest per iteration cost provide convergence analysis case rows design matrix samples bounded support show convergence phases aquadratic phase followed linear phase finally empirically demonstrate algorithm achieves highest performancecompared various algorithms several datasets
collaboratively learning preferences ordinal data personalized recommendation systems important predict preferences user items seen user yet similarly revenue management important predict outcomes comparisons among items never compared far multinomial logit model popular discrete choice model captures structure hidden preferences low rank matrix order predict preferences want learn underlying model noisy observations low rank matrix collected revealed preferences various forms ordinal data natural approach learn model solve convex relaxation nuclear norm minimization present convex relaxation approach contexts interest collaborative ranking bundled choice modeling cases show convex relaxation minimax optimal prove upper bound resulting error finite samples provide matching information theoretic lower bound
sgd algorithms based incomplete statistics large scale minimization empirical risk many learning problems ranging clustering ranking metric learning empirical estimates risk functional consist average tuples pairs triplets observations rather individual observations paper focus best implement stochastic approximation approach solve risk minimization problems argue large scale setting gradient estimates obtained sampling tuples data points replacement incomplete statistics instead sampling data points without replacement complete statistics based subsamples develop theoretical framework accounting substantial impact strategy generalization ability prediction model returned stochastic gradient descent sgd algorithm reveals method promote achieves much better trade statistical accuracy computational cost beyond rate bound analysis experiments auc maximization metric learning provide strong empirical evidence superiority proposed approach
alternating minimization regression problems vector valued outputs regression problems involving vector valued outputs equivalently multiple responses well known maximum likelihood estimator mle takes noise covariance structure account significantly accurate ordinary least squares ols estimator however existing literature compares ols mle terms asymptotic finite sample guarantees crucially computing mle general requires solving non convex optimization problem known efficiently solvable provide finite sample upper lower bounds estimation error ols mle popular models pooled model seemingly unrelated regression sur model provide precise instances mle significantly accurate ols furthermore models show output computationally efficient alternating minimization procedure enjoys performance guarantee mle universal constants finally show high dimensional settings well alternating minimization procedure leads significantly accurate solutions corresponding ols solutions error bound depends logarithmically data dimensionality
variance reduction stochastic gradient descent asynchronous variants study optimization algorithms based variance reduction stochastic gradientdescent sgd remarkable recent progress made directionthrough development algorithms like sag svrg saga algorithmshave shown outperform sgd theoretically empirically however asynchronous versions algorithms crucial requirement modernlarge scale applications studied bridge gap presentinga unifying framework captures many variance reduction techniques subsequently propose asynchronous algorithm grounded framework fast convergence rates important consequence general approachis yields asynchronous versions variance reduction algorithms assvrg saga byproduct method achieves near linear speedup sparsesettings common machine learning demonstrate empirical performanceof method concrete realization asynchronous svrg
subset selection pareto optimization selecting optimal subset large set variables fundamental problem various learning tasks feature selection sparse regression dictionary learning etc paper propose poss approach employs evolutionary pareto optimization find small sized subset good performance prove sparse regression poss able achieve best far theoretically guaranteed approximation performance efficiently particularly emph exponential decay subclass poss proven achieve optimal solution empirical study verifies theoretical results exhibits superior performance poss greedy convex relaxation methods
interpolating convex non convex tensor decompositions via subspace norm consider problem recovering low rank tensor noisy observation previous work shown recovery guarantee signal noise ratio ceil recovering order rank tensor size times cdots times recursive unfolding paper first improve bound much simpler approach careful analysis propose new norm called textit subspace norm based kronecker products factors obtained proposed simple estimator imposed kronecker structure allows show nearly ideal sqrt sqrt bound parameter controls blend non convex estimator mode wise nuclear norm minimization furthermore empirically demonstrate subspace norm achieves nearly ideal denoising performance even
minimum weight perfect matching via blossom belief propagation max product belief propagation popular message passing algorithm computing maximum posteriori map assignment distribution represented graphical model shown solve number combinatorial optimization problems including minimum weight matching shortest path network flow vertex cover following common assumption respective linear programming relaxation tight integrality gap present however shows integrality gap model known solved systematically via sequential applications paper develop first algorithm coined blossom solving minimum weight matching problem arbitrary graphs step sequential algorithm requires applying modified graph constructed contractions expansions blossoms odd sets vertices scheme guarantees termination runs number vertices original graph essence blossom offers distributed version celebrated edmonds blossom algorithm jumping many sub steps single moreover result provides interpretation edmonds algorithm sequence lps
bit marginal regression consider problem sparse signal recovery linear measurements quantized bits bit marginal regression proposed recovery algorithm study question choosing setting given budget bits cdot derive single easy compute expression characterizing trade choice turns optimal estimating unit vector corresponding signal level additive gaussian noise quantization well adversarial noise geq show lloyd max quantization constitutes optimal quantization scheme norm signal canbe estimated consistently maximum likelihood
lasso non linear measurements equivalent linear measurements consider estimating unknown structured sparse low rank etc signal x_0 vector measurements form y_i g_i a_i tx_0 a_i rows known measurement matrix potentially unknown nonlinear random link function measurement functions could arise applications measurement device nonlinearities uncertainties could also arise design g_i sign z_i corresponds noisy bit quantized measurements motivated classical work brillinger recent work plan vershynin estimate x_0 via solving generalized lasso hat arg min_ ax_0 lambda regularization parameter lambda typically non smooth convex regularizer promotes structure x_0 ell_1 norm nuclear norm approach seems naively ignore nonlinear function brillinger plan vershynin shown entries iid standard normal good estimator x_0 constant proportionality depends work considerably strengthen results obtaining explicit expressions hat x_0 regularized generalized lasso asymptotically precise grow large main result estimation performance generalized lasso non linear measurements asymptotically whose measurements linear y_i a_i tx_0 sigma z_i gamma gamma sigma gamma gamma gamma standard normal derived expressions estimation performance first known precise results context interesting consequence result optimal quantizer measurements minimizes estimation error lasso celebrated lloyd max quantizer
pseudo dimension nearly optimal auctions paper develops general approach rooted statistical learning theory learning approximately revenue maximizing auction data introduce level auctions interpolate simple auctions welfare maximization reserve prices optimal auctions thereby balancing competing demands expressivity simplicity prove auctions small representation error sense every product distribution bidders valuations exists level auction small expected revenue close optimal show set level auctions modest pseudo dimension polynomial therefore leads small learning error consequence results arbitrary single parameter settings learn mechanism expected revenue arbitrarily close optimal polynomial number samples
closed form estimators high dimensional generalized linear models propose class closed form estimators glms high dimensional sampling regimes class estimators based deriving closed form variants vanilla unregularized mle well defined even high dimensional settings available closed form perform thresholding operations mle variant obtain class estimators derive unified statistical analysis class estimators show enjoys strong statistical guarantees parameter error well variable selection surprisingly match complex regularized glm mles even closed form estimators computationally much simpler derive instantiations class closed form estimators well corollaries general theorem special cases logistic exponential poisson regression models corroborate surprising statistical computational performance class estimators via extensive simulations
fast provable algorithms isotonic regression l_p norms given directed acyclic graph set values vertices isotonic regression vector respects partial order described minimizes specified norm paper gives improved algorithms computing isotonic regression weighted ell_ norms rigorous performance guarantees algorithms quite practical variants implemented run fast practice
semi proximal mirror prox nonsmooth composite minimization propose new first order optimization algorithm solve high dimensional non smooth composite minimization problems typical examples problems objective decomposes non smooth empirical risk part non smooth regularization penalty proposed algorithm called semi proximal mirror prox leverages saddle point representation part objective handling part objective via linear minimization domain algorithm stands contrast classical proximal gradient algorithms smoothing require computation proximal operators iteration therefore impractical high dimensional problems establish theoretical convergence rate semi proximal mirror prox exhibits optimal complexity bounds number calls linear minimization oracle present promising experimental results showing interest approach comparison competing methods
competitive distribution estimation good turing good estimating distributions large alphabets fundamental machine learning tenet yet method known estimate distributions well example add constant estimators nearly min max optimal often perform poorly practice practical estimators absolute discounting jelinek mercer good turing known near optimal essentially distribution describe first universally near optimal probability estimators every discrete distribution provably nearly best following competitive ways first estimate every distribution nearly well best estimator designed prior knowledge distribution permutation second estimate every distribution nearly well best estimator designed prior knowledge exact distribution natural estimators restricted assign probability symbols appearing number times specifically distributions symbols samples show comparisons simple variant good turing estimator always within divergence best estimator involved estimator within tilde mathcal min sqrt conversely show estimator must divergence tilde omega min best estimator first comparison tilde omega min sqrt second
universal primal dual convex optimization framework propose new primal dual algorithmic framework prototypical constrained convex optimization template algorithmic instances framework universal since automatically adapt unknown holder continuity degree constant within dual formulation also guaranteed optimal convergence rates objective residual feasibility gap holder smoothness degree contrast existing primal dual algorithms framework avoids proximity operator objective function instead leverage computationally cheaper fenchel type operators main workhorses generalized conditional gradient gcg type methods contrast gcg type methods framework require objective function differentiable also process additional general linear inclusion constraints guarantees convergence rate primal problem
sample complexity episodic fixed horizon reinforcement learning recently significant progress understanding reinforcement learning discounted infinite horizon markov decision processes mdps deriving tight sample complexity bounds however many real world applications interactive learning agent operates fixed bounded period time example tutoring students exams handling customer service requests scenarios often better treated episodic fixed horizon mdps looser bounds sample complexity exist natural notion sample complexity setting number episodes required guarantee certain performance high probability pac guarantee paper derive upper pac bound order log lower pac bound log ignoring log terms match log terms additional linear dependency number states lower bound first kind setting upper bound leverages bernstein inequality improve previous bounds episodic finite horizon mdps time horizon dependency least
private graphon estimation sparse graphs design algorithms fitting high dimensional statistical model large sparse network without revealing sensitive information individual members given sparse input graph algorithms output node differentially private nonparametric block model approximation node differentially private mean output hides insertion removal vertex adjacent edges instance network obtained generative nonparametric model defined terms graphon model guarantees consistency number vertices tends infinity output algorithm converges appropriate version l_2 norm particular means estimate sizes multi way cuts results hold long bounded average degree grows least like log number vertices number blocks goes infinity appropriate rate give explicit error bounds terms parameters model several settings bounds improve match known nonprivate results
honor hybrid optimization non convex regularized problems recent years witnessed superiority non convex sparse learning formulations convex counterparts theory practice however due non convexity non smoothness regularizer efficiently solve non convex optimization problem large scale data still quite challenging paper propose efficient underline ybrid underline ptimization algorithm underline convex underline egularized problems honor specifically develop hybrid scheme effectively integrates quasi newton step gradient descent step contributions follows honor incorporates second order information greatly speed convergence avoids solving regularized quadratic programming involves matrix vector multiplications without explicitly forming inverse hessian matrix establish rigorous convergence analysis honor shows convergence guaranteed even non convex problems typically challenging analyze convergence non convex problems conduct empirical studies large scale data sets results demonstrate honor converges significantly faster state art algorithms
convergent gradient descent algorithm rank minimization semidefinite programming random linear measurements propose simple scalable fast gradient descent algorithm optimize nonconvex objective rank minimization problem closely related family semidefinite programs kappa log random measurements positive semidefinite times matrix rank condition number kappa method guaranteed converge linearly global optimum
super resolution grid super resolution problem recovering superposition point sources using bandlimited measurements corrupted noise signal processing problem arises numerous imaging problems ranging astronomy biology spectroscopy common take coarse fourier measurements object particular interest obtaining estimation procedures robust noise following desirable statistical computational properties seek use coarse fourier measurements bounded emph cutoff frequency hope take quantifiably small number measurements desire algorithm run quickly suppose point sources dimensions points separated least delta euclidean distance work provides algorithm following favorable guarantees algorithm uses fourier measurements whose frequencies bounded delta log factors previous algorithms require emph cutoff frequency large omega sqrt delta number measurements taken computational complexity algorithm bounded polynomial number points dimension emph dependence separation delta contrast previous algorithms depended inverse polynomially minimal separation exponentially dimension quantities estimation procedure simple take random bandlimited measurements opposed taking exponential number measurements hyper grid furthermore analysis algorithm elementary based concentration bounds sampling singular value decomposition
optimal rates random fourier features kernel methods represent powerful tools machine learning tackle problems expressed terms function values derivatives due capability represent model complex relations methods show good versatility computationally intensive poor scalability large data require operations gram matrices order mitigate serious computational limitation recently randomized constructions proposed literature allow application fast linear algorithms random fourier features rff among popular widely applied constructions provide easily computable low dimensional feature representation shift invariant kernels despite popularity rffs little understood theoretically approximation quality paper provide detailed finite sample theoretical analysis approximation quality rffs establishing optimal terms rff dimension growing set size performance guarantees uniform norm presenting guarantees norms also propose rff approximation derivatives kernel theoretical study approximation quality
combinatorial bandits revisited paper investigates stochastic adversarial combinatorial multi armed bandit problems stochastic setting semi bandit feedback derive problem specific regret lower bound discuss scaling dimension decision space propose escb algorithm efficiently exploits structure problem provide finite time analysis regret escb better performance guarantees existing algorithms significantly outperforms algorithms practice adversarial setting bandit feedback propose combexp algorithm regret scaling state art algorithms lower computational complexity combinatorial problems
fast convergence regularized learning games show natural classes regularized learning algorithms form recency bias achieve faster convergence rates approximate efficiency coarse correlated equilibria multiplayer normal form games player game uses algorithm class individual regret decays sum utilities converges approximate optimum improvement upon worst case rates show black box reduction algorithm class achieve tilde rates adversary maintaining faster rates algorithms class results extend rakhlin shridharan cite rakhlin2013 daskalakis cite daskalakis2014 analyzed player sum games specific algorithms
elicitation complexity elicitation study statistics properties computable via empirical risk minimization several recent papers approached general question properties elicitable suggest wrong question properties elicitable first eliciting entire distribution data set thus important question elicitable specifically minimum number regression parameters needed compute property building previous work introduce new notion elicitation complexity lay foundations calculus elicitation establish several general results techniques proving upper lower bounds elicitation complexity results provide tight bounds eliciting bayes risk loss large class properties includes spectral risk measures several new properties interest
online learning adversarial delays study performance standard online learning algorithms feedback delayed adversary show texttt online gradient descent texttt follow perturbed leader achieve regret sqrt delayed setting sum delays round feedback bound collapses optimal sqrt bound usual setting delays main contribution show standard algorithms online learning already simple regret bounds general setting delayed feedback making adjustments analysis algorithms results help affirm clarify success recent algorithms optimization machine learning operate delayed feedback model
structured estimation atomic norms general bounds applications structured estimation problems atomic norms recent advances literature express sample complexity estimation error bounds terms certain geometric measures particular gaussian width unit norm ball gaussian width spherical cap induced tangent cone restricted norm compatibility constant however given atomic norm bounding geometric measures difficult paper present general upper bounds geometric measures require simple information atomic norm consideration establish tightness bounds providing corresponding lower bounds show applications analysis certain atomic norms especially support norm existing result incomplete
subsampled power iteration unified algorithm block models planted csp present algorithm recovering planted solutions well known models stochastic block model planted constraint satisfaction problems csp via common generalization terms random bipartite graphs algorithm matches constant factor best known bounds number edges constraints needed perfect recovery running time linear number edges used time complexity significantly better spectral sdp based approaches main contribution algorithm case unequal sizes bipartition arises reduction planted csp algorithm succeeds significantly lower density spectral approaches surpassing barrier based spectral norm random matrix significant features algorithm analysis include critical use power iteration subsampling might independent interest analysis requires keeping track multiple norms evolving solution algorithm implemented statistically limited access input distribution iii algorithm extremely simple implement runs linear time thus practical even large instances
deep visual analogy making addition identifying content within single image relating images generating related images critical tasks image understanding recently deep convolutional networks yielded breakthroughs producing image labels annotations captions begun used producing high quality image outputs paper develop novel deep network trained end end perform visual analogy making task transforming query image according example pair related images solving problem requires accurately recognizing visual relationship generating transformed query image accordingly inspired recent advances language modeling propose solve visual analogies learning map images neural embedding analogical reasoning simple vector subtraction addition experiments model effectively models visual analogies several datasets shapes animated video game sprites car models
looking humans remarkable ability follow gaze people identify looking following eye gaze gaze following important ability allows understand people thinking actions performing even predict might next despite importance topic problem studied limited scenarios within computer vision community paper propose deep neural network based approach gaze following new benchmark dataset thorough evaluation given image location head approach follows gaze person identifies object looked training network able discover extract head pose gaze orientation select objects scene predicted line sight likely looked televisions balls food quantitative evaluation shows approach produces reliable results even viewing back head method outperforms several baseline approaches still far reaching human performance task overall believe challenging important task deserves attention community
spatial transformer networks convolutional neural networks define exceptionallypowerful class model still limited lack abilityto spatially invariant input data computationally parameterefficient manner work introduce new learnable module thespatial transformer explicitly allows spatial manipulation ofdata within network differentiable module insertedinto existing convolutional architectures giving neural networks ability toactively spatially transform feature maps conditional feature map without extra training supervision modification optimisation process show useof spatial transformers results models learn invariance translation scale rotation generic warping resulting state artperformance several benchmarks numberof classes transformations
training deep networks theoretical empirical evidence indicates depth neural networks crucial success however training becomes difficult depth increases training deep networks remains open problem introduce new architecture designed overcome called highway networks allow unimpeded information flow across many layers information highways inspired long short term memory recurrent networks use adaptive gating units regulate information flow even hundreds layers highway networks trained directly simple gradient descent enables study extremely deep efficient architectures
attention based models speech recognition recurrent sequence generators conditioned input data attention mechanism recently shown good performance range tasks including machine translation handwriting synthesis image caption generation extend attention mechanism features needed speech recognition show adaptation model used machine translation reaches competitive phoneme error rate per timit phoneme recognition task applied utterances roughly long ones trained offer qualitative explanation failure propose novel generic method adding location awareness attention mechanism alleviate issue new method yields model robust long inputs achieves per single utterances times longer repeated utterances finally propose change attention mechanism prevents concentrating much single frames reduces per level
deep convolutional inverse graphics network paper presents deep convolution inverse graphics network ign model aims learn interpretable representation images disentangled respect dimensional scene structure viewing transformations depth rotations lighting variations ign model composed multiple layers convolution convolution operators trained using stochastic gradient variational bayes sgvb algorithm propose training procedure encourage neurons graphics code layer represent specific transformation pose light given single input image model generate new images object variations pose lighting present qualitative quantitative tests model efficacy learning rendering engine varied object classes including faces chairs
end end memory networks introduce neural network recurrent attention model possibly large external memory architecture form memory network weston 2015 unlike model work trained end end hence requires significantly less supervision training making generally applicable realistic settings also seen extension rnnsearch case multiple computational steps hops performed per output symbol flexibility model allows apply tasks diverse synthetic question answering language modeling former approach competitive memory networks less supervision latter penn treebank text8 datasets approach demonstrates comparable performance rnns lstms cases show key concept multiple computational hops yields improved results
learning segment object candidates recent object detection systems rely critical steps set object proposals predicted efficiently possible set candidate proposals passed object classifier approaches shown fast achieving state art detection performance paper propose new way generate object proposals introducing approach based discriminative convolutional network model trained jointly objectives given image patch first part system outputs class agnostic segmentation mask second part system outputs likelihood patch centered full object test time model efficiently applied whole test image generates set segmentation masks assigned corresponding object likelihood score show model yields significant improvements state art object proposal algorithms particular compared previous approaches model obtains substantially higher object recall using fewer proposals also show model able generalize unseen categories seen training unlike previous approaches generating object masks rely edges superpixels form low level segmentation
inferring algorithmic patterns stack augmented recurrent nets despite recent achievements machine learning still far achieving real artificial intelligence paper discuss limitations standard deep learning approaches show limitations overcome learning grow complexity model structured way specifically study simplest sequence prediction problems beyond scope learnable standard recurrent networks algorithmically generated sequences learned models capacity count memorize sequences show basic algorithms learned sequential data using recurrent network associated trainable memory
attractor network dynamics enable preplay rapid path planning maze like environments rodents navigating well known environment rapidly learn revisit observed reward locations often single trial mechanism rapid path planning unknown ca3 region hippocampus plays important role emerging evidence suggests place cell activity hippocampal preplay periods trace future goal directed trajectories show particular mapping space allows immediate generation trajectories arbitrary start goal locations environment based mapped representation goal show representation implemented neural attractor network model resulting bump like activity profiles resembling ca3 region hippocampus neurons tend locally excite neurons similar place field centers inhibiting neurons distant place field centers stable bumps activity form arbitrary locations environment network initialized represent point environment weakly stimulated input corresponding arbitrary goal location show resulting activity interpreted gradient ascent value function induced reward goal location indeed networks large place fields show network properties cause bump move smoothly initial location goal around obstacles walls results illustrate attractor network hippocampal like attributes important rapid path planning
semi supervised convolutional neural networks text categorization via region embedding paper presents new semi supervised framework convolutional neural networks cnns text categorization unlike previous approaches rely word embeddings method learns embeddings small text regions unlabeled data integration supervised cnn proposed scheme embedding learning based idea view semi supervised learning intended useful task interest even though training done unlabeled data models achieve better results previous approaches sentiment classification topic classification tasks
return gating network combining generative models discriminative training natural image priors recent years approaches based machine learning achieved state art performance image restoration problems successful approaches include generative models natural images well discriminative training deep neural networks discriminative training feed forward architectures allows explicit control computational cost performing restoration therefore often leads better performance cost run time contrast generative models advantage trained adapted image restoration task simple use bayes rule paper show combine strengths approaches training discriminative feed forward architecture predict state latent variables generative model natural images apply idea successful gaussian mixture model gmm natural images show possible achieve comparable performance original gmm orders magnitude improvement run time maintaining advantage generative models
backpropagation energy efficient neuromorphic computing solving real world problems embedded neural networks requires training algorithms achieve high performance compatible hardware runs real time remaining energy efficient former deep learning using backpropagation recently achieved string successes across many domains datasets latter neuromorphic chips run spiking neural networks recently achieved unprecedented energy efficiency bring advances together must first resolve incompatibility backpropagation uses continuous output neurons synaptic weights neuromorphic designs employ spiking neurons discrete synapses approach treat spikes discrete synapses continuous probabilities allows training network using standard backpropagation trained network naturally maps neuromorphic hardware sampling probabilities create networks merged using ensemble averaging demonstrate trained sparsely connected network runs truenorth chip using mnist dataset high performance network ensemble achieve accuracy 121 per image high efficiency network ensemble achieve accuracy 408 per image
learning wake sleep recurrent attention models despite success convolutional neural networks computationally expensive must examine image locations stochastic attention based models shown improve computational efficiency test time remain difficult train intractable posterior inference high variance stochastic gradient estimates borrowing techniques literature training deep generative models present wake sleep recurrent attention model method training stochastic attention networks improves posterior inference reduces variability stochastic gradients show method greatly speed training time stochastic attention networks domains image classification caption generation
job learning bayesian decision theory goal deploy high accuracy system starting training examples consider job setting inputs arrive use real time crowdsourcing resolve uncertainty needed output prediction confident model improves time reliance crowdsourcing queries decreases cast setting stochastic game based bayesian decision theory allows balance latency cost accuracy objectives principled way computing optimal policy intractable develop approximation based monte carlo tree search tested approach datasets named entity recognition sentiment classification image classification ner task obtained order magnitude reduction cost compared full human annotation boosting performance relative expert provided labels also achieve improvement single human label whole set improvement online learning
color constancy learning predict chromaticity luminance color constancy recovery true surface color observed color requires estimating chromaticity scene illumination correct bias induces paper show per pixel color statistics natural scenes without spatial semantic context powerful cue color constancy specifically describe illuminant estimation method built around classifier identifying true chromaticity pixel given luminance absolute brightness across color channels inference pixel observed color restricts true chromaticity values explained candidate set illuminants applying classifier values yields distribution corresponding illuminants global estimate scene illuminant computed simple aggregation distributions across pixels begin simply defining luminance chromaticity classifier computing empirical histograms discretized chromaticity luminance values training set natural images histograms reflect preference hues corresponding smooth reflectance functions achromatic colors brighter pixels despite simplicity resulting estimation algorithm outperforms current state art color constancy methods next propose method learn luminance chromaticity classifier end end using stochastic gradient descent set chromaticity luminance likelihoods minimize errors final scene illuminant estimates training set leads improvements accuracy significantly tail error distribution
decoupled deep neural network semi supervised semantic segmentation propose novel deep neural network architecture semi supervised semantic segmentation using heterogeneous annotations contrary existing approaches posing semantic segmentation region based classification algorithm decouples classification segmentation learns separate network task architecture labels associated image identified classification network binary segmentation subsequently performed identified label segmentation network decoupled architecture enables learn classification segmentation networks separately based training data image level pixel wise class labels respectively facilitates reduce search space segmentation effectively exploiting class specific activation maps obtained bridging layers algorithm shows outstanding performance compared semi supervised approaches even much less training images strong annotations pascal voc dataset
action conditional video prediction using deep networks atari games motivated vision based reinforcement learning problems particular atari games recent benchmark aracade learning environment ale consider spatio temporal prediction problems future image frames dependent control variables actions well previous frames composed natural scenes frames atari games high dimensional size involve tens objects objects controlled actions directly many objects influenced indirectly involve entry departure objects involve deep partial observability propose evaluate deep neural network architectures consist encoding action conditional transformation decoding layers based convolutional neural networks recurrent neural networks experimental results show proposed architectures able generate visually realistic frames also useful control approximately 100 step action conditional futures games best knowledge paper first make evaluate long term predictions high dimensional video conditioned control inputs
bayesian active model selection application automated audiometry introduce novel information theoretic approach active model selection demonstrate effectiveness real world application although method work arbitrary models focus actively learning appropriate structure gaussian process models arbitrary observation likelihoods apply framework rapid screening noise induced hearing loss nihl widespread preventible disability diagnosed early construct model pure tone audiometric responses patients nihl using previously published model healthy responses proposed method shown capable diagnosing presence absence nihl drastically fewer samples existing approaches method extremely fast enables diagnosis performed real time
efficient robust automated machine learning success machine learning broad range applications led ever growing demand machine learning systems used shelf non experts effective practice systems need automatically choose good algorithm feature preprocessing steps new dataset hand also set respective hyperparameters recent work started tackle automated machine learning automl problem help efficient bayesian optimization methods work introduce robust new automl system based scikit learn using classifiers feature preprocessing methods data preprocessing methods giving rise structured hypothesis space 110 hyperparameters system dub auto sklearn improves existing automl methods automatically taking account past performance similar datasets constructing ensembles models evaluated optimization system first phase ongoing chalearn automl challenge comprehensive analysis 100 diverse datasets shows substantially outperforms previous state art automl also demonstrate performance gains due contributions derive insights effectiveness individual components auto sklearn
framework individualizing predictions disease trajectories exploiting multi resolution structure many complex diseases wide variety ways individual manifest disease challenge personalized medicine develop tools accurately predict trajectory individual disease turn enable clinicians optimize treatments represent individual disease trajectory continuous valued continuous time function describing severity disease time propose hierarchical latent variable model individualizes predictions disease trajectories model shares statistical strength across observations different resolutions population subpopulation individual level describe algorithm learning population subpopulation parameters offline online procedure dynamically learning individual specific parameters finally validate model task predicting course interstitial lung disease leading cause death among patients autoimmune disease scleroderma compare approach state art demonstrate significant improvements predictive accuracy
pointer networks introduce new neural architecture learn conditional probability output sequence elements arediscrete tokens corresponding positions input sequence problems cannot trivially addressed existent approaches sequence sequence neural turing machines number target classes eachstep output depends length input variable problems sorting variable sized sequences various combinatorialoptimization problems belong class model solvesthe problem variable size output dictionaries using recently proposedmechanism neural attention differs previous attentionattempts instead using attention blend hidden units anencoder context vector decoder step uses attention asa pointer select member input sequence output call architecture pointer net ptr net show ptr nets used learn approximate solutions threechallenging geometric problems finding planar convex hulls computingdelaunay triangulations planar travelling salesman problem using training examples alone ptr nets improve oversequence sequence input attention butalso allow generalize variable size output dictionaries show learnt models generalize beyond maximum lengthsthey trained hope results taskswill encourage broader exploration neural learning discreteproblems
reduced dimension fmri shared response model multi subject fmri data critical evaluating generality validity findings across subjects effective utilization helps improve analysis sensitivity develop shared response model aggregating multi subject fmri data accounts different functional topographies among anatomically aligned datasets model demonstrates improved sensitivity identifying shared response variety datasets anatomical brain regions interest furthermore removing identified shared response allows improved detection group differences ability identify shared shared opens model wide range multi subject fmri studies
efficient exact gradient update training deep networks large sparse targets important class problems involves training deep neural networks sparse prediction targets high dimension occur naturally neural language models learning word embeddings often posed predicting probability next words among vocabulary size 200 000 computing equally large typically non sparse dimensional output vector last hidden layer reasonable dimension 500 incurs prohibitive computational cost example updating times output weight matrix computing gradient needed backpropagation previous layers efficient handling large sparse network inputs trivial case large sparse targets thus far sidestepped approximate alternatives hierarchical softmax sampling based approximations training work develop original algorithmic approach family loss functions includes squared error spherical softmax compute exact loss gradient update output weights gradient backpropagation per example instead remarkably without ever computing dimensional output proposed algorithm yields speedup frac orders magnitude typical sizes critical part computations often dominates training time kind network architecture
precision recall gain curves analysis done right precision recall analysis abounds applications binary classification true negatives add value hence affect assessment classifier performance perhaps inspired many advantages receiver operating characteristic roc curves area curves accuracy based performance assessment many researchers taken report precision recall curves associated areas performance metric demonstrate paper practice fraught difficulties mainly incoherent scale assumptions area curve takes arithmetic mean precision values whereas beta score applies harmonic mean show fix plotting curves different coordinate system demonstrate new precision recall gain curves inherit key advantages roc curves particular area precision recall gain curves conveys expected f_1 score harmonic scale convex hull precision recall gain curve allows calibrate classifier scores determine operating point convex hull interval beta values point optimises beta demonstrate experimentally area traditional curves easily favour models lower expected f_1 score others use precision recall gain curves result better model selection
tractable approximation optimal point process filtering application neural encoding process dynamic state estimation filtering based point process observations general intractable numerical sampling techniques often practically useful lead limited conceptual insight optimal encoding decoding strategies significant relevance computational neuroscience develop analytically tractable bayesian approximation optimal filtering based point process observations allows introduce distributional assumptions sensory cell properties greatly facilitates analysis optimal encoding situations deviating common assumptions uniform coding analytic framework leads insights difficult obtain numerical algorithms consistent experiments distribution tuning curve centers interestingly find information gained absence spikes crucial performance
equilibrated adaptive learning rates non convex optimization parameter specific adaptive learning rate methods computationally efficient ways reduce ill conditioning problems encountered training large deep networks following recent work strongly suggests thecritical points encountered training networks saddle points find considering presence negative eigenvalues hessian could help design better suited adaptive learning rate schemes show popular jacobi preconditioner undesirable behavior presence positive negative curvature present theoretical empirical evidence called equilibration preconditioner comparatively better suited non convex problems introduce novel adaptive learning rate scheme called esgd based equilibration preconditioner experiments demonstrate schemes yield similar step directions esgd sometimes surpasses rmsprop terms convergence speed always clearly improving plain stochastic gradient descent
next system real world development evaluation application active learning active learning methods automatically adapt data collection selecting informative samples order accelerate machine learning real world testing comparing active learning algorithms requires collecting new datasets adaptively rather simply applying algorithms benchmark datasets norm passive machine learning research facilitate development testing deployment active learning real applications built open source software system large scale active learning research experimentation system called next provides unique platform real world reproducible active learning research paper details challenges building system demonstrates capabilities several experiments results show experimentation help expose strengths weaknesses active learning algorithms sometimes unexpected enlightening ways
gaussian process random fields gaussian processes successful supervised unsupervised machine learning tasks computational complexity constrained practical applications introduce new approximation large scale gaussian processes gaussian process random field gprf local gps coupled via pairwise potentials gprf likelihood simple tractable parallelizeable approximation full marginal likelihood enabling latent variable modeling hyperparameter selection large datasets demonstrate effectiveness synthetic spatial data well real world application seismic event location
mcmc variationally sparse gaussian processes gaussian process models form core part probabilistic machine learning considerable research effort made attacking issues models compute efficiently number data large approximate posterior likelihood gaussian estimate covariance function parameter posteriors paper simultaneously addresses using variational approximation posterior sparse sup port function otherwise free form result hybrid monte carlo sampling scheme allows non gaussian approximation function values covariance parameters simultaneously efficient computations based inducing point sparse gps
streaming distributed variational inference bayesian nonparametrics paper presents methodology creating streaming distributed inference algorithms bayesian nonparametric bnp models proposed framework processing nodes receive sequence data minibatches compute variational posterior make asynchronous streaming updates central model contrast previous algorithms proposed framework truly streaming distributed asynchronous learning rate free truncation free key challenge developing framework arising fact bnp models impose inherent ordering components finding correspondence minibatch central bnp posterior components performing update address paper develops combinatorial optimization problem component correspondences provides efficient solution technique paper concludes application methodology mixture model experimental results demonstrating practical scalability performance
fixed length poisson mrf adding dependencies multinomial propose novel distribution generalizes multinomial distribution enable dependencies dimensions novel distribution based parametric form poisson mrf model yang 2012 fundamentally different domain restriction fixed length vector like multinomial number trials fixed known thus propose fixed length poisson mrf lpmrf distribution develop methods estimate likelihood log partition function log normalizing constant possible poisson mrf model addition propose novel mixture topic models use lpmrf base distribution discuss similarities differences previous topic models recently proposed admixture poisson mrfs inouye 2014 show effectiveness lpmrf distribution multinomial models evaluating test set perplexity dataset abstracts wikipedia qualitatively show positive dependencies discovered lpmrf interesting intuitive finally show algorithms fast good scaling
human memory search initial visit emitting random walk imagine random walk outputs state visiting first time observed output therefore repeat censored version underlying walk consists permutation states prefix call model initial visit emitting random walk invite prior work shown random walks repeat censoring mechanism explain well human behavior memory search tasks great interest study human cognition various clinical applications however parameter estimation invite challenging naive likelihood computation marginalizing infinitely many hidden random walk trajectories intractable paper propose first efficient maximum likelihood estimate mle invite decomposing censored output series absorbing random walks also prove theoretical properties mle including identifiability consistency show invite outperforms several existing methods real world human response data memory search tasks
structured transforms small footprint deep learning consider task building compact deep learning pipelines suitable deploymenton storage power constrained mobile devices propose uni fied framework learn broad family structured parameter matrices arecharacterized notion low displacement rank structured transformsadmit fast function gradient evaluation span rich range parametersharing configurations whose statistical modeling capacity explicitly tunedalong continuum structured unstructured experimental results showthat transforms significantly accelerate inference forward backwardpasses training offer superior accuracy compactness speed tradeoffsin comparison number existing techniques keyword spotting applicationsin mobile speech recognition methods much effective thanstandard linear low rank bottleneck layers nearly retain performance ofstate art models providing fold compression
spectral learning large structured hmms comparative epigenomics develop latent variable model efficient spectral algorithm motivated recent emergence large data sets chromatin marks multiple human cell types natural model chromatin data cell type hidden markov model hmm model relationship multiple cell types connecting hidden states fixed tree known structure main challenge learning parameters models iterative methods slow naive spectral methods result time space complexity exponential number cell types exploit properties tree structure hidden states provide spectral algorithms computationally efficient current biological datasets provide sample complexity bounds algorithm evaluate experimentally biological data human cell types finally show beyond specific model algorithmic ideas applied graphical models
structural smoothing framework robust graph comparison paper propose general smoothing framework graph kernels taking textit structural similarity account apply derive smoothed variants popular graph kernels framework inspired state art smoothing techniques used natural language processing nlp however unlike nlp applications primarily deal strings show apply smoothing richer class inter dependent sub structures naturally arise graphs moreover discuss extensions pitman yor process adapted smooth structured objects thereby leading novel graph kernels kernels able tackle diagonal dominance problem respecting structural similarity sub structures especially presence edge label noise experimental evaluation shows kernels outperform unsmoothed variants also achieve statistically significant improvements classification accuracy several graph kernels recently proposed literature kernels competitive terms runtime offer viable option practitioners
optimization monte carlo efficient embarrassingly parallel likelihood free inference describe embarrassingly parallel anytime monte carlo method likelihood free models algorithm starts view stochasticity pseudo samples generated simulator controlled externally vector random numbers way outcome knowing deterministic instantiation run optimization procedure minimize distance summary statistics simulator data reweighing samples using prior jacobian accounting change volume transforming space summary statistics space parameters show weighted ensemble represents monte carlo estimate posterior distribution procedure run embarrassingly parallel node handling sample anytime allocating resources worst performing sample procedure validated experiments
inverse reinforcement learning locally consistent reward functions existing inverse reinforcement learning irl algorithms assumed expert demonstrated trajectory produced single reward function paper presents novel generalization irl problem allows trajectory generated multiple locally consistent reward functions hence catering realistic complex experts behaviors solving generalized irl problem thus involves learning reward functions also stochastic transitions state including unvisited states representing irl problem probabilistic graphical model expectation maximization algorithm devised iteratively learn different reward functions stochastic transitions order jointly improve likelihood expert demonstrated trajectories result likely partition trajectory segments generated different locally consistent reward functions selected derived empirical evaluation synthetic real world datasets shows irl algorithm outperforms state art clustering maximum likelihood irl interestingly reduced variant approach
consistent multilabel classification multilabel classification rapidly developing important aspect modern predictive modeling motivating study theoretical aspects end propose framework constructing analyzing multilabel classification metrics reveals novel results parametric form population optimal classifiers additional insight role label correlations particular show multilabel metrics constructed instance micro macro averages population optimal classifier decomposed binary classifiers based marginal instance conditional distribution label weak association labels via threshold thus analysis extends state art known multilabel classification metrics hamming loss general framework applicable many classification metrics common use based population optimal classifier propose computationally efficient general purpose plug classification algorithm prove consistency respect metric interest empirical results synthetic benchmark datasets supportive theoretical findings
approval voting optimal given approval votes crowdsourcing platforms ask workers express opinions approving set good alternatives seems reasonable way aggregate approval votes approval voting rule simply counts number times alternative approved challenge assertion proposing probabilistic framework noisy voting asking whether approval voting yields alternative likely best alternative given approval votes answer generally positive theoretical empirical results call attention situations approval voting suboptimal
normative theory adaptive dimensionality reduction neural networks make sense world brains must analyze high dimensional datasets streamed sensory organs analysis begins dimensionality reduction modelling early sensory processing requires biologically plausible online dimensionality reduction algorithms recently derived algorithm termed similarity matching multidimensional scaling mds objective function however existing algorithm number output dimensions set priori number output neurons cannot changed number informative dimensions sensory inputs variable need adaptive dimensionality reduction derive biologically plausible dimensionality reduction algorithms adapt number output dimensions eigenspectrum input covariance matrix formulate objective functions offline setting optimized projections input dataset onto principal subspace scaled eigenvalues output covariance matrix turn output eigenvalues computed soft thresholded hard thresholded iii equalized thresholded eigenvalues input covariance matrix online setting derive corresponding adaptive algorithms map onto dynamics neuronal activity networks biologically plausible local learning rules remarkably last networks neurons divided classes identify principal neurons interneurons biological circuits
efficient non greedy optimization decision trees decision trees randomized forests widely used computer vision machine learning standard algorithms decision tree induction optimize split functions node time according splitting criteria greedy procedure often leads suboptimal trees paper present algorithm optimizing split functions levels tree jointly leaf parameters based global objective show problem finding optimal linear combination oblique splits decision trees related structured prediction latent variables formulate convex concave upper bound tree empirical loss computing gradient proposed surrogate objective respect training exemplar tree depth thus training deep trees feasible use stochastic gradient descent optimization enables effective training large datasets experiments several classification benchmarks demonstrate resulting non greedy decision trees outperform greedy decision tree baselines
statistical topological data analysis kernel perspective consider problem statistical computations persistence diagrams summary representation topological features data diagrams encode persistent homology widely used invariant topological data analysis several avenues towards statistical treatment diagrams explored recently follow alternative route motivated success methods based embedding probability measures reproducing kernel hilbert spaces fact positive definite kernel persistence diagrams recently proposed connecting persistent homology popular kernel based learning techniques support vector machines however important properties kernel would enable principled use context probability measure embeddings remain explored contribution close gap proving universality variant original kernel demonstrate effective use sample hypothesis testing synthetic well real world data
variational consensus monte carlo practitioners bayesian statistics long depended markov chain monte carlo mcmc obtain samples intractable posterior distributions unfortunately mcmc algorithms typically serial scale large datasets typical modern machine learning recently proposed consensus monte carlo algorithm removes limitation partitioning data drawing samples conditional partition parallel scott 2013 fixed aggregation function combines samples yielding approximate posterior samples introduce variational consensus monte carlo vcmc variational bayes algorithm optimizes aggregation functions obtain samples distribution better approximates target resulting objective contains intractable entropy term therefore derive relaxation objective show relaxed problem blockwise concave mild conditions illustrate advantages algorithm inference tasks literature demonstrating superior quality posterior approximation moderate overhead optimization step algorithm achieves relative error reduction measured serial mcmc compared consensus monte carlo task estimating 300 dimensional probit regression parameter expectations similarly achieves error reduction task estimating cluster comembership probabilities gaussian mixture model components dimensions furthermore gains come moderate cost compared runtime serial mcmc achieving near ideal speedup instances
softstar heuristic guided probabilistic inference recent machine learning methods sequential behavior prediction estimate motives behavior rather behavior higher level abstraction improves generalization different prediction settings computing predictions often becomes intractable large decision spaces propose softstar algorithm softened heuristic guided search technique maximum entropy inverse optimal control model sequential behavior approach supports probabilistic search bounded approximation error significantly reduced computational cost compared sampling based methods present algorithm analyze approximation guarantees compare performance simulation based inference distinct complex decision tasks
gradient free hamiltonian monte carlo efficient kernel exponential families propose kernel hamiltonian monte carlo kmc gradient free adaptive mcmc algorithm based hamiltonian monte carlo hmc target densities classical hmc option due intractable gradients kmc adaptively learns target gradient structure fitting exponential family model reproducing kernel hilbert space computational costs reduced novel efficient approximations gradient asymptotically exact kmc mimics hmc terms sampling efficiency offers substantial mixing improvements state art gradient free samplers support claims experimental studies toy real world applications including approximate bayesian computation exact approximate mcmc
complete recipe stochastic gradient mcmc many recent markov chain monte carlo mcmc samplers leverage continuous dynamics define transition kernel efficiently explores target distribution tandem focus devising scalable variants subsample data use stochastic gradients place full data gradients dynamic simulations however stochastic gradient mcmc samplers lagged behind full data counterparts terms complexity dynamics considered since proving convergence presence stochastic gradient noise non trivial even simple dynamics significant physical intuition often required modify dynamical system account stochastic gradient noise paper provide general recipe constructing mcmc samplers including stochastic gradient versions based continuous markov processes specified via matrices constructively prove framework complete continuous markov process provides samples target distribution written framework show previous continuous dynamic samplers trivially reinvented framework avoiding complicated sampler specific proofs likewise use recipe straightforwardly propose new state adaptive sampler stochastic gradient riemann hamiltonian monte carlo sgrhmc experiments simulated data streaming wikipedia analysis demonstrate proposed sgrhmc sampler inherits benefits riemann hmc scalability stochastic gradient methods
barrier frank wolfe marginal inference introduce globally convergent algorithm optimizing tree reweighted trw variational objective marginal polytope algorithm based conditional gradient method frank wolfe moves pseudomarginals within marginal polytope repeated maximum posteriori map calls modular structure enables leverage black box map solvers exact approximate variational inference obtains accurate results tree reweighted algorithms optimize local consistency relaxation theoretically bound sub optimality proposed algorithm despite trw objective unbounded gradients boundary marginal polytope empirically demonstrate increased quality results found tightening relaxation marginal polytope well spanning tree polytope synthetic real world instances
practical optimal lsh angular distance show existence locality sensitive hashing lsh family angular distance yields approximate near neighbor search algorithm asymptotically optimal running time exponent unlike earlier algorithms property spherical lsh andoni indyk nguyen razenshteyn 2014 andoni razenshteyn 2015 algorithm also practical improving upon well studied hyperplane lsh charikar 2002 practice also introduce multiprobe version algorithm conduct experimental evaluation real synthetic data sets complement positive results fine grained lower bound quality lsh family angular distance lower bound implies lsh family exhibits trade evaluation time quality close optimal natural class lsh functions
principal differences analysis interpretable characterization differences distributions introduce principal differences analysis analyzing differences high dimensional distributions method operates finding projection maximizes wasserstein divergence resulting univariate populations relying cramer wold device requires assumptions form underlying distributions nature inter class differences sparse variant method introduced identify features responsible differences provide algorithms original minimax formulation well semidefinite relaxation addition deriving convergence results illustrate approach applied identify differences cell populations somatosensory cortex hippocampus manifested single cell rna seq broader framework extends beyond specific choice wasserstein divergence
kullback leibler proximal variational inference propose new variational inference method based kullback leibler proximal term make contributions towards improving efficiency variational inference firstly derive proximal point algorithm show equivalence gradient descent natural gradient stochastic variational inference secondly use proximal framework derive efficient variational algorithms non conjugate models propose splitting procedure separate non conjugate terms conjugate ones linearize non conjugate terms show resulting subproblem admits closed form solution overall approach converts non conjugate model subproblems involve inference well known conjugate models apply method many models derive generalizations non conjugate exponential family applications real world datasets show proposed algorithms easy implement fast converge perform well reduce computations
learning large scale poisson dag models based overdispersion scoring paper address question identifiability learning algorithms large scale poisson directed acyclic graphical dag models define general poisson dag models models node poisson random variable rate parameter depending values parents underlying dag first prove poisson dag models identifiable observational data present polynomial time algorithm learns poisson dag model suitable regularity conditions main idea behind algorithm based overdispersion variables conditionally poisson overdispersed relative variables marginally poisson algorithms exploits overdispersion along methods learning sparse poisson undirected graphical models faster computation provide theoretical guarantees simulation results small large scale dags
streaming min max hypergraph partitioning many applications data rich structure represented hypergraph data items represented vertices associations among items represented hyperedges equivalently given input bipartite graph types vertices items associations refer topics consider problem partitioning set items given number parts maximum number topics covered part partition minimized natural clustering problem various applications partitioning set information objects documents images videos load balancing context computation platforms paper focus streaming computation model problem items arrive online time item must assigned irrevocably part partition arrival time motivated scalability requirements focus class streaming computation algorithms memory limited linear number parts partition show greedy assignment strategy able recover hidden clustering items natural set recovery conditions also report results extensive empirical evaluation demonstrate greedy strategy yields superior performance compared alternative approaches
efficient output kernel learning multiple tasks paradigm multi task learning achieve better generalization learning tasks jointly thus exploiting similarity tasks rather learning independently previously relationship tasks user defined form output kernel recent approaches jointly learn tasks output kernel output kernel positive semidefinite matrix resulting optimization problems scalable number tasks eigendecomposition required step using theory positive semidefinite kernels show paper certain class regularizers output kernel constraint positive semidefinite dropped automatically satisfied relaxed problem leads unconstrained dual problem solved efficiently experiments several multi task multi class data sets illustrate efficacy approach terms computational efficiency well generalization performance
gradient estimation using stochastic computation graphs variety problems originating supervised unsupervised reinforcement learning loss function defined expectation collection random variables might part probabilistic model external world estimating gradient loss function using samples lies core gradient based learning algorithms problems introduce formalism stochastic computation graphs directed acyclic graphs include deterministic functions conditional probability distributions describe easily automatically derive unbiased estimator loss function gradient resulting algorithm computing gradient estimator simple modification standard backpropagation algorithm generic scheme propose unifies estimators derived variety prior work along variance reduction techniques therein could assist researchers developing intricate models involving combination stochastic deterministic operations enabling example attention memory control actions
lifted inference rules constraints lifted inference rules exploit symmetries fast reasoning statistical rela tional models computational complexity rules highly dependent onthe choice constraint language operate therefore coming upwith right kind representation critical success lifted inference paper propose new constraint language called setineq allowssubset equality inequality constraints represent substitutions vari ables theory constraint formulation strictly expressive thanexisting representations yet easy operate reformulate mainlifting rules decomposer generalized binomial recently proposed singleoccurrence map inference work constraint representation exper iments benchmark mlns exact sampling based inference demonstratethe effectiveness approach several existing techniques
sparse pca via bipartite matchings consider following multi component sparse pca problem given set data points seek extract small number sparse components emph disjoint supports jointly capture maximum possible variance components computed repeatedly solving single component problem deflating input data matrix greedy procedure suboptimal present novel algorithm sparse pca jointly optimizes multiple disjoint components extracted features capture variance lies within multiplicative factor arbitrarily close optimal algorithm combinatorial computes desired components solving multiple instances bipartite maximum weight matching problem complexity grows low order polynomial ambient dimension input data exponentially rank however effectively applied low dimensional sketch input data evaluate algorithm real datasets empirically demonstrate many cases outperforms existing deflation based approaches
empirical localization homogeneous divergences discrete sample spaces paper propose novel parameter estimator probabilistic models discrete space proposed estimator derived minimization homogeneous divergence constructed without calculation normalization constant frequently infeasible models discrete space investigate statistical properties proposed estimator consistency asymptotic normality reveal relationship alpha divergence small experiments show proposed estimator attains comparable performance mle drastically lower computational cost
weighted theta functions embeddings applications max cut clustering summarization introduce unifying generalization lov theta function associated geometric embedding graphs weights nodes edges show computed exactly semidefinite programming approximate using svm computations show theta function interpreted measure diversity graphs use idea graph embedding algorithms max cut correlation clustering document summarization well represented problems weighted graphs
online rank elicitation plackett luce dueling bandits approach study problem online rank elicitation assuming rankings set alternatives obey plackett luce distribution following setting dueling bandits problem learner allowed query pairwise comparisons alternatives sample pairwise marginals distribution online fashion using information learner seeks reliably predict probable ranking top alternative approach based constructing surrogate probability distribution rankings based sorting procedure pairwise marginals provably coincide marginals plackett luce distribution addition formal performance complexity analysis present first experimental studies
segregated graphs marginals chain graph models bayesian networks popular representation asymmetric example causal relationships random variables markov random fields mrfs complementary model symmetric relationships used computer vision spatial modeling social gene expression networks chain graph model lauritzen wermuth frydenberg interpretation hereafter chain graph model generalizes bayesian networks mrfs represent asymmetric symmetric relationships together graphical models set marginals distributions chain graph model induced presence hidden variables forms complex model recent approach study marginal graphical models consider well behaved supermodel supermodel marginals bayesian networks defined conditional independences termed ordinary markov model studied length evans richardson 2014 paper show special mixed graphs call segregated graphs associated via markov property supermodels marginal chain graphs defined conditional independences special features segregated graphs imply existence natural factorization supermodels imply many existing results chain graph model ordinary markov model carry results suggest segregated graphs define analogue ordinary markov model marginals chain graph models
approximating sparse pca incomplete data study well recover sparse principal componentsof data matrix using sketch formed elements show wide class optimization problems sketch close spectral norm original datamatrix recover near optimal solution optimizationproblem using sketch particular use approach toobtain sparse principal components show math data pointsin math dimensions math epsilon tilde max elements gives math epsilon additive approximation sparse pca problem math tilde stable rank data matrix demonstrate algorithms extensivelyon image text biological financial data results show able recover sparse pcas incomplete data using sparse sketch running timedrops factor
multi layer feature reduction tree structured group lasso via hierarchical projection tree structured group lasso tgl powerful technique uncovering tree structured sparsity features node encodes group features applied successfully many real world applications however extremely large feature dimensions solving tgl remains significant challenge due highly complicated regularizer paper propose novel multi layer feature reduction method mlfre quickly identify inactive nodes groups features coefficients solution hierarchically top fashion guaranteed irrelevant response thus remove detected nodes optimization without sacrificing accuracy major challenge developing testing rules due overlaps parents children nodes novel hierarchical projection algorithm mlfre able test nodes independently ancestor nodes moreover integrate mlfre low computational cost existing solvers experiments synthetic real data sets demonstrate speedup gained mlfre orders magnitude
recovering communities general stochastic block model without knowing parameters stochastic block model sbm recently gathered significant attention due new threshold phenomena however developments rely knowledge model parameters least number communities paper introduces efficient algorithms require knowledge yet achieve optimal information theoretic tradeoffs identified abbe sandon constant degree regime algorithm developed requires lower bound relative sizes communities achieves optimal accuracy scaling large degrees lower bound requirement removed regime diverging degrees logarithmic degree regime enhanced fully agnostic algorithm simultaneously learns model parameters achieves optimal limit exact recovery runs quasi linear time provide first algorithms affording efficiency universality information theoretic optimality strong weak consistency sbm
maximum likelihood learning arbitrary treewidth via fast mixing parameter sets inference typically intractable high treewidth undirected graphical models making maximum likelihood learning challenge way overcome restrict parameters tractable set typically set tree structured parameters paper explores alternative notion tractable set namely set fast mixing parameters markov chain monte carlo mcmc inference guaranteed quickly converge stationary distribution common practice approximate likelihood gradient using samples obtained mcmc procedures lack theoretical guarantees paper proves exponential family bounded sufficient statistics graphical models parameters constrained fast mixing set gradient descent gradients approximated sampling approximate maximum likelihood solution inside set high probability unregularized find solution epsilon accurate log likelihood requires total amount effort cubic epsilon disregarding logarithmic factors ridge regularized strong convexity allows solution epsilon accurate parameter distance effort quadratic epsilon provide fully polynomial time randomized approximation scheme
testing closeness unequal sized samples consider problem testing whether unequal sized samples drawn identical distributions versus distributions differ significantly specifically given target error parameter eps m_1 independent draws unknown distribution discrete support m_2 draws unknown distribution discrete support describe test distinguishing case case geq eps supported elements test successful high probability provided m_1 geq varepsilon m_2 omega left max frac sqrt m_1 varepsilon frac sqrt varepsilon right show tradeoff information theoretically optimal throughout range dependencies parameters m_1 eps constant factors consequence obtain algorithm estimating mixing time markov chain states log factor uses tilde tau_ mix queries next node oracle core testing algorithm relatively simple statistic seems perform well practice synthetic data natural language data believe statistic might prove useful primitive within larger machine learning natural language processing systems
learning causal graphs small interventions consider problem learning causal networks interventions intervention limited size pearl structural equation model independent errors sem objective minimize number experiments discover causal directions edges causal graph previous work focused use separating systems complete graphs task prove deterministic adaptive algorithm needs separating system order learn complete graphs worst case addition present novel separating system construction whose size close optimal arguably simpler previous work combinatorics also develop novel information theoretic lower bound number interventions applies full generality including randomized adaptive learning algorithms general chordal graphs derive worst case lower bounds number interventions building observations induced trees give new deterministic adaptive algorithm learn directions chordal skeleton completely worst case achievable scheme alpha approximation algorithm alpha independence number graph also show exist graph classes sufficient number experiments close lower bound extreme graph classes required number experiments multiplicatively alpha away lower bound simulations algorithm almost always performs close lower bound approach based separating systems complete graphs significantly worse random chordal graphs
regret based pruning extensive form games counterfactual regret minimization cfr leading algorithm finding nash equilibrium large sum imperfect information games cfr iterative algorithm repeatedly traverses game tree updating regrets information set introduce improvement cfr prunes path play tree descendants negative regret revisits sequence earliest subsequent cfr iteration regret could become positive path explored every iteration new algorithm maintains cfr convergence guarantees making iterations significantly faster even previously known pruning techniques used comparison improvement carries cfr recent variant cfr experiments show order magnitude speed improvement relative speed improvement increases size game
nonparametric von mises estimators entropies divergences mutual informations propose analyse estimators statistical functionals moredistributions nonparametric assumptions estimators derived von mises expansion andare based theory influence functions appearin semiparametric statistics literature show estimators based either data splitting leave techniqueenjoy fast rates convergence favorable theoretical properties apply framework derive estimators several popular informationtheoretic quantities via empirical evaluation show advantage thisapproach existing estimators
bounding errors expectation propagation expectation propagation popular algorithm variational inference comes theoretical guarantees article prove approximation errors made bounded bounds asymptotic interpretation number datapoints allows study convergence respect true posterior particular show converges rate mean order magnitude faster traditional gaussian approximation mode also give similar asymptotic expansions moments order well excess kullback leibler cost defined additional cost incurred using rather ideal gaussian approximation expansions highlight superior convergence properties approach deriving results likely applicable many similar approximate inference methods addition introduce bounds moments log concave distributions independent interest
market scoring rules act opinion pools risk averse agents market scoring rule msr popular tool designing algorithmic prediction markets incentive compatible mechanism aggregation probabilistic beliefs myopic risk neutral agents paper add growing body research aimed understanding precise manner price process induced msr incorporates private information agents deviate assumption risk neutrality first establish myopic trading agent risk averse utility function msr satisfying mild regularity conditions elicits agent risk neutral probability conditional latest market state rather true subjective probability hence show msr conditions effectively behaves like traditional method belief aggregation namely opinion pool agents true probabilities particular logarithmic market scoring rule acts logarithmic pool constant absolute risk aversion utility agents linear pool atypical budget constrained agent utility decreasing absolute risk aversion also point interpretation market maker conditions bayesian learner even agent beliefs static
local smoothness variance reduced optimization abstract propose family non uniform sampling strategies provably speed class stochastic optimization algorithms linear convergence including stochastic variance reduced gradient svrg stochastic dual coordinate ascent sdca large family penalized empirical risk minimization problems methods exploit data dependent local smoothness loss functions near optimum maintaining convergence guarantees bounds first quantify advantage gained local smoothness significant problems significantly better empirically provide thorough numerical results back theory additionally present algorithms exploiting local smoothness aggressive ways perform even better practice
high dimensional algorithm statistical optimization asymptotic normality provide general theory expectation maximization algorithm inferring high dimensional latent variable models particular make contributions parameter estimation propose novel high dimensional algorithm naturally incorporates sparsity structure parameter estimation appropriate initialization algorithm converges geometric rate attains estimator near optimal statistical rate convergence based obtained estimator propose new inferential procedure testing hypotheses low dimensional components high dimensional parameters broad family statistical models framework establishes first computationally feasible approach optimal estimation asymptotic inference high dimensions
associative memory via sparse recovery model associative memory structure learned dataset mathcal vectors signals way given noisy version vectors input nearest valid vector mathcal nearest neighbor provided output preferably via fast iterative algorithm traditionally binary ary hopfield neural networks used model structure paper first time propose model associative memory based sparse recovery signals basic premise simple dataset learn set linear constraints every vector dataset must satisfy provided linear constraints possess special properties possible cast task finding nearest neighbor sparse recovery problem assuming generic random models dataset show possible store super polynomial exponential number length vectors neural network size furthermore given noisy version stored vectors corrupted near linear number coordinates vector correctly recalled using neurally feasible algorithm
matrix completion monotonic single index models recent results matrix completion assume matrix consideration low rank columns union low rank subspaces real world settings however linear structure underlying models distorted typically unknown nonlinear transformation paper addresses challenge matrix completion face nonlinearities given observations matrix obtained applying lipschitz monotonic function low rank matrix task estimate remaining unobserved entries propose novel matrix completion method alternates low rank matrix estimation monotonic function estimation estimate missing matrix elements mean squared error bounds provide insight well matrix estimated based size rank matrix properties nonlinear transformation empirical results synthetic real world datasets demonstrate competitiveness proposed approach
sparse linear programming via primal dual augmented coordinate descent past decades linear programming widely used different areas considered mature technologies numerical optimization however complexity offered state art algorithms interior point method primal dual simplex methods still unsatisfactory problems machine learning huge number variables constraints paper investigate general algorithm based combination augmented lagrangian coordinate descent giving iteration complexity log epsilon nnz cost per iteration nnz number non zeros times constraint matrix practice reduce cost per iteration order non zeros columns rows corresponding active primal dual variables active set strategy algorithm thus yields tractable alternative standard methods large scale problems sparse solutions nnz conduct experiments large scale instances ell_1 regularized multi class svm sparse inverse covariance estimation nonnegative matrix factorization proposed approach finds solutions precision orders magnitude faster state art implementations interior point simplex methods
convergence rates sub sampled newton methods consider problem minimizing sum functions via projected iterations onto convex parameter set subset reals regime algorithms utilize sub sampling techniques known effective paper use sub sampling techniques together low rank approximation design new randomized batch algorithm possesses comparable convergence rate newton method yet much smaller per iteration cost proposed algorithm robust terms starting point step size enjoys composite convergence rate namely quadratic convergence start linear convergence iterate close minimizer develop theoretical analysis also allows select near optimal algorithm parameters theoretical results used obtain convergence rates previously proposed sub sampling based algorithms well demonstrate results apply well known machine learning problems lastly evaluate performance algorithm several datasets various scenarios
variance reduced stochastic gradient descent neighbors stochastic gradient descent sgd workhorse machine learning yet also known slow relative steepest descent recently variance reduction techniques svrg saga proposed overcome weakness asymptotically vanishing variance constant step size maintained resulting geometric convergence rates however methods either based occasional computations full gradients pivot points svrg keeping per data point corrections memory saga disadvantage cannot employ methods streaming setting speed ups relative sgd need certain number epochs order materialize paper investigates new class algorithms exploit neighborhood structure training data share use information past stochastic gradients across data points meant offering advantages asymptotic setting significant benefits transient optimization phase particular streaming single epoch setting investigate family algorithms thorough analysis show supporting experimental results side product provide simple unified proof technique broad class variance reduction algorithms
non convex statistical optimization sparse tensor graphical model consider estimation sparse graphical models characterize dependency structure high dimensional tensor valued data facilitate estimation precision matrix corresponding way tensor assume data follow tensor normal distribution whose covariance kronecker product structure penalized maximum likelihood estimation model involves minimizing non convex objective function spite non convexity estimation problem prove alternating minimization algorithm iteratively estimates sparse precision matrix fixing others attains estimator optimal statistical rate convergence well consistent graph recovery notably estimator achieves estimation consistency tensor sample unobserved previous work theoretical results backed thorough numerical studies
convergence rates active learning maximum likelihood estimation active learner given class models large set unlabeled examples ability interactively query labels subset examples goal learner learn model class fits data well previous theoretical work rigorously characterized label complexity active learning work focused pac agnostic pac model paper shift attention general setting maximum likelihood estimation provided certain conditions hold model class provide stage active learning algorithm problem conditions require fairly general cover widely popular class generalized linear models turn include models binary multi class classification regression conditional random fields provide upper bound label requirement algorithm lower bound matches lower order terms analysis shows unlike binary classification realizable case single extraround interaction sufficient achieve near optimal performance maximum likelihood estimation empirical side recent work 2012 2014 active linear logistic regression shows promise approach
kalman filter restless bandits indexable study restless bandit associated extremely simple scalar kalman filter model discrete time certain assumptions prove problem indexable sense whittle index non decreasing function relevant belief state spite long history problem appears first proof use results schur convexity mechanical words particularbinary strings intimately related palindromes
policy gradient coherent risk measures several authors recently developed risk sensitive policy gradient methods augment standard expected cost minimization problem measure variability cost studies focused specific risk measures variance conditional value risk cvar work extend policy gradient method whole class coherent risk measures widely accepted finance operations research among fields consider static time consistent dynamic risk measures static risk measures approach spirit policy gradient algorithms combines standard sampling approach convex programming dynamic risk measures approach actor critic style involves explicit approximation value function importantly contribution presents unified approach risk sensitive reinforcement learning generalizes extends previous results
dual augmented block minimization framework learning limited memory past years several techniques proposed training linear support vector machine svm limited memory setting dual block coordinate descent dual bcd method used balance cost spent computation paper consider general setting regularized emph empirical risk minimization erm data cannot fit memory particular generalize existing block minimization framework based strong duality emph augmented lagrangian technique achieve global convergence erm arbitrary convex loss function regularizer block minimization framework flexible sense given solver working sufficient memory integrate framework obtain solver globally convergent limited memory condition conduct experiments regularized classification regression problems corroborate convergence theory compare proposed framework algorithms adopted online distributed settings shows superiority proposed approach data size times larger memory capacity
quartz randomized dual coordinate ascent arbitrary sampling study problem minimizing average large number smooth convex functions penalized strongly convex regularizer propose analyze novel primal dual method quartz every iteration samples updates random subset dual variables chosen according arbitrary distribution contrast typical analysis directly bound decrease primal dual error expectation without need first analyze dual error depending choice sampling obtain efficient serial mini batch variants method serial case bounds match best known bounds sdca uniform importance sampling standard mini batching bounds predict initial data independent speedup well additional data driven speedup depends spectral sparsity properties data
generalization submodular cover via diminishing return property integer lattice consider generalization submodular cover problem based concept diminishing return property integer lattice motivated real scenarios machine learning cannot captured traditional submodular set functions show generalized submodular cover problem applied various problems devise bicriteria approximation algorithm algorithm guaranteed output log factor approximate solution satisfies constraints desired accuracy running time algorithm roughly log log size ground set maximum value coordinate dependency exponentially better naive reduction algorithms several experiments real artificial datasets demonstrate solution quality algorithm comparable naive algorithms running time several orders magnitude faster
universal catalyst first order optimization introduce generic scheme accelerating first order optimization methods sense nesterov builds upon new analysis accelerated proximal point algorithm approach consists minimizing convex objective approximately solving sequence well chosen auxiliary problems leading faster convergence strategy applies large class algorithms including gradient descent block coordinate descent sag saga sdca svrg finito miso proximal variants methods provide acceleration explicit support non strongly convex objectives addition theoretical speed also show acceleration useful practice especially ill conditioned problems measure significant improvements
fast memory optimal low rank matrix approximation paper revisit problem constructing near optimal rank approximation matrix times streaming data model columns revealed sequentially present sla streaming low rank approximation algorithm asymptotically accurate sqrt largest singular value means average mean square error converges grow large hat high probability hat denote output sla optimal rank approximation respectively algorithm makes pass data columns revealed random order passes columns arrive arbitrary order reduce memory footprint complexity sla uses random sparsification samples entry small probability delta turn sla memory optimal required memory space scales dimension output furthermore sla computationally efficient runs delta kmn time constant number operations made observed entry small log appropriate choice delta
stochastic online greedy learning semi bandit feedbacks greedy algorithm extensively studied field combinatorial optimization decades paper address online learning problem input greedy algorithm stochastic unknown parameters learned time first propose greedy regret epsilon quasi greedy regret learning metrics comparing performance offline greedy algorithm propose online greedy learning algorithms semi bandit feedbacks use multi armed bandit pure exploration bandit policies level greedy learning regret metrics respectively algorithms achieve log problem dependent regret bound time horizon general class combinatorial structures reward functions allow greedy solutions show bound tight problem instance parameters
linear multi resource allocation semi bandit feedback study idealised sequential resource allocation problem time step learner chooses allocation several resource types number tasks assigning resources task increases probability completed problem challenging alignment tasks resource types unknown feedback noisy main contribution new setting algorithm nearly optimal regret analysis along way draw connections problem minimising regret stochastic linear bandits heteroscedastic noise also present new results stochastic linear bandits hypercube significantly performs existing work especially sparse case
exactness approximate map inference continuous mrfs computing map assignment graphical models generally intractable result discrete graphical models map problem often approximated using linear programming relaxations much research focused characterizing relaxations tight relatively well understood discrete case results known continuous analog work use graph covers provide necessary sufficient conditions continuous map relaxations tight use characterization give simple proofs relaxation tight log concave decomposable log supermodular decomposable models conclude exploring relationship seemingly distinct classes functions providing specific conditions map relaxation cannot tight
consistency theory high dimensional variable screening variable screening fast dimension reduction technique assisting high dimensional feature selection preselection method selects moderate size subset candidate variables refining via feature selection produce final model performance variable screening depends computational efficiency ability dramatically reduce number variables without discarding important ones data dimension substantially larger sample size variable screening becomes crucial faster feature selection algorithms needed conditions guaranteeing selection consistency might fail hold article studies class linear screening methods establishes consistency theory special class particular prove restricted diagonally dominant rdd condition necessary sufficient condition strong screening consistency concrete examples show screening methods sis holp strong screening consistent subject additional constraints large probability rho sigma tau log random designs addition relate rdd condition irrepresentable condition highlight limitations sis
finite time analysis projected langevin monte carlo analyze projected langevin monte carlo lmc algorithm close cousin projected stochastic gradient descent sgd show lmc allows sample polynomial time posterior distribution restricted convex body concave log likelihood gives first markov chain sample log concave distribution first order oracle existing chains provable guarantees lattice walk ball walk hit run require zeroth order oracle proof uses elementary concepts stochastic calculus could useful generally understand sgd variants
optimal testing properties distributions given samples unknown distribution possible distinguish whether belongs class distributions versus far every distribution fundamental question receivedtremendous attention statistics albeit focusing onasymptotic analysis well computer science wherethe emphasis small sample size computationalcomplexity nevertheless even basic classes ofdistributions monotone log concave unimodal monotone hazard rate optimal sample complexity unknown provide general approach via obtain sample optimal computationally efficient testers distribution families core approach algorithm solves following problem given samplesfrom unknown distribution known distribution close chi distance far total variation distance optimality testers established providing matching lower bounds finally necessary building block tester important byproduct work first known computationally efficient proper learners discretelog concave monotone hazard rate distributions exhibit efficacy testers via experimental analysis
learning theory algorithms forecasting non stationary time series present data dependent learning bounds general scenario non stationary non mixing stochastic processes learning guarantees expressed terms data dependent measure sequential complexity discrepancy measure estimated data mild assumptions use learning bounds devise new algorithms non stationary time series forecasting report preliminary experimental results
accelerated mirror descent continuous discrete time study accelerated mirror descent dynamics continuous discrete time combining original continuous time motivation mirror descent recent ode interpretation nesterov accelerated method propose family continuous time descent dynamics convex functions lipschitz gradients solution trajectories guaranteed converge optimum rate show large family first order accelerated methods obtained discretization ode methods converge rate connection accelerated mirror descent ode provides intuitive approach design analysis accelerated first order algorithms
information theoretic lower bounds convex optimization erroneous oracles consider problem optimizing convex concave functions access erroneous zeroth order oracle particular given function consider optimization given access absolute error oracles return values epsilon epsilon relative error oracles return value epsilon epsilon epsilon larger show stark information theoretic impossibility results minimizing convex functions maximizing concave functions polytopes model
bandit smooth convex optimization improving bias variance tradeoff bandit convex optimization fundamental problems field online learning best algorithm general bandit convex optimization problem guarantees regret widetilde best known lower bound omega many attemptshave made bridge huge gap bounds particularly interesting special case problem assumes loss functions smooth case best known algorithm guarantees regret widetilde present efficient algorithm banditsmooth convex optimization problem guarantees regret widetilde result rules omega lower bound takes significant step towards resolution open problem
beyond sub gaussian measurements high dimensional structured estimation sub exponential designs consider problem high dimensional structured estimation norm regularized estimators lasso design matrix noise drawn sub exponential distributions existing results consider sub gaussian designs noise sample complexity non asymptotic estimation error shown depend gaussian width suitable sets contrast sub exponential setting show sample complexity estimation error depend exponential width corresponding sets analysis holds norm using generic chaining show exponential width set sqrt log times gaussian width set yielding gaussian width based results even sub exponential case certain popular estimators viz lasso group lasso using dimension based analysis show sample complexity fact order gaussian designs general analysis results first sub exponential setting readily applicable special sub exponential families log concave extreme value distributions
adaptive online learning propose general framework studying adaptive regret bounds online learning setting subsuming model selection data dependent bounds given data model dependent bound ask exist algorithm achieving bound show modifications recently introduced sequential complexity measures used answer question providing sufficient conditions adaptive rates achieved particular adaptive rate induces set called offset complexity measures obtaining small upper bounds quantities sufficient demonstrate achievability cornerstone analysis technique use sided tail inequalities bound suprema offset random processes framework recovers improves wide variety adaptive bounds including quantile bounds second order data dependent bounds small loss bounds addition derive new type adaptive bound online linear optimization based spectral norm well new online pac bayes theorem
teaching machines read comprehend teaching machines read natural language documents remains elusive challenge machine reading systems tested ability answer questions posed contents documents seen large scale training test datasets missing type evaluation work define new methodology resolves bottleneck provides large scale supervised reading comprehension data allows develop class attention based deep neural networks learn read real documents answer complex questions minimal prior knowledge language structure
saliency scale information towards unifying theory paper present definition visual saliency grounded information theory proposal shown relate variety classic research contributions scale space theory interest point detection bilateral filtering existing models visual saliency based proposed definition visual saliency demonstrate results competitive state art prediction human fixations segmentation salient objects also characterize different properties model including robustness image transformations extension wide range data types mesh models serving example finally relate proposal generally role saliency computation visual information processing draw connections putative mechanisms saliency computation human vision
semi supervised learning ladder networks combine supervised learning unsupervised learning deep neural networks proposed model trained simultaneously minimize sum supervised unsupervised cost functions backpropagation avoiding need layer wise pre training work builds top ladder network proposed valpola 2015 extend combining model supervision show resulting model reaches state art performance semi supervised mnist cifar classification addition permutation invariant mnist classification labels
enforcing balance allows local supervised learning spiking recurrent networks predict sensory inputs control motor trajectories brain must constantlylearn temporal dynamics based error feedback however remainsunclear supervised learning implemented biological neural networks learning recurrent spiking networks notoriously difficult localchanges connectivity unpredictable effect global dynamics commonly used learning rules temporal back propagation local thus biologically plausible furthermore reproducing thepoisson like statistics neural responses requires use networks balancedexcitation inhibition balance easily destroyed learning using top approach show networks integrate fire neuronscan learn arbitrary linear dynamical systems feeding back error asa feed forward input network uses types recurrent connections fastand slow fast connections learn balance excitation inhibition using avoltage based plasticity rule slow connections trained minimize theerror feedback using current based hebbian learning rule importantly balancemaintained fast connections crucial ensure global error signalsare available locally neuron turn resulting local learning rule forthe slow connections demonstrates spiking networks learn complexdynamics using purely local learning rules using balance key ratherthan additional constraint resulting network implements given functionwithin predictive coding scheme minimal dimensions activity
semi supervised sequence learning present approaches use unlabeled data improve sequence learningwith recurrent networks first approach predict comes next asequence language model nlp second approach use asequence autoencoder reads input sequence vector predictsthe input sequence algorithms used pretraining algorithm later supervised sequence learning algorithm words theparameters obtained pretraining step used starting pointfor supervised training models experiments find long shortterm memory recurrent networks pretrained approaches becomemore stable train generalize better pretraining able toachieve strong performance many classification tasks text classificationwith imdb dbpedia image recognition cifar
skip thought vectors describe approach unsupervised learning generic distributed sentence encoder using continuity text books train encoder decoder model tries reconstruct surrounding sentences encoded passage sentences share semantic syntactic properties thus mapped similar vector representations next introduce simple vocabulary expansion method encode words seen part training allowing expand vocabulary million words training model extract evaluate vectors linear models tasks semantic relatedness paraphrase detection image sentence ranking question type classification benchmark sentiment subjectivity datasets end result shelf encoder produce highly generic sentence representations robust perform well practice make encoder publicly available
learning linearize uncertainty training deep feature hierarchies solve supervised learning tasks achieving state art performance many problems computer vision however principled way train hierarchies unsupervised setting remained elusive work suggest new architecture loss training deep feature hierarchies linearize transformations observed unlabelednatural video sequences done training generative model predict video frames also address problem inherent uncertainty prediction introducing latent variables non deterministic functions input network architecture
synaptic sampling bayesian approach neural network plasticity rewiring reexamine article conceptual mathematical framework understanding organization plasticity spiking neural networks propose inherent stochasticity enables synaptic plasticity carry probabilistic inference sampling posterior distribution synaptic parameters view provides viable alternative existing models propose convergence synaptic weights maximum likelihood parameters explains priors weight distributions connection probabilities merged optimally learned experience simulations show model synaptic plasticity allows spiking neural networks compensate continuously unforeseen disturbances furthermore provides normative mathematical framework better understand permanent variability rewiring observed brain networks
natural neural networks introduce natural neural networks novel family algorithms speed convergence adapting internal representation training improve conditioning fisher matrix particular show specific example employs simple efficient reparametrization neural network weights implicitly whitening representation obtained layer preserving feed forward computation network networks trained efficiently via proposed projected natural gradient descent algorithm prong amortizes cost reparametrizations many parameter updates closely related mirror descent online learning algorithm highlight benefits method unsupervised supervised learning tasks showcase scalability training large scale imagenet challenge dataset
convolutional networks graphs learning molecular fingerprints introduce convolutional neural network operates directly graphs networks allow end end learning prediction pipelines whose inputs graphs arbitrary size shape architecture present generalizes standard molecular feature extraction methods based circular fingerprints show data driven features interpretable better predictive performance variety tasks
convolutional lstm network machine learning approach precipitation nowcasting goal precipitation nowcasting predict future rainfall intensity local region relatively short period time previous studies examined crucial challenging weather forecasting problem machine learning perspective paper formulate precipitation nowcasting spatiotemporal sequence forecasting problem input prediction target spatiotemporal sequences extending fully connected lstm lstm convolutional structures input state state state transitions propose convolutional lstm convlstm use build end end trainable model precipitation nowcasting problem experiments show convlstm network captures spatiotemporal correlations better consistently outperforms lstm state art operational rover algorithm precipitation nowcasting
scheduled sampling sequence prediction recurrent neural networks recurrent neural networks trained produce sequences tokens given input exemplified recent results machine translation image captioning current approach training consists maximizing likelihood token sequence given current recurrent state previous token inference unknown previous token replaced token generated model discrepancy training inference yield errors accumulate quickly along generated sequence propose curriculum learning strategy gently change training process fully guided scheme using true previous token towards less guided scheme mostly uses generated token instead experiments several sequence prediction tasks show approach yields significant improvements moreover used successfully winning bid mscoco image captioning challenge 2015
mind gap generative approach interpretable feature selection extraction present mind gap model mgm approach interpretable feature extraction selection placing interpretability criteria directly model allow model optimize parameters related interpretability directly report global set distinguishable dimensions assist data exploration hypothesis generation mgm extracts distinguishing features real world datasets animal features recipes ingredients disease occurrence also maintains improves performance compared related approaches perform user study domain experts show mgm ability help dataset exploration
max margin deep generative models deep generative models dgms effective learning multilayered representations complex data performing inference input data exploring generative ability however little work done examining empowering discriminative ability dgms making accurate predictions paper presents max margin deep generative models mmdgms explore strongly discriminative principle max margin learning improve discriminative power dgms retaining generative capability develop efficient doubly stochastic subgradient algorithm piecewise linear objective empirical results mnist svhn datasets demonstrate max margin learning significantly improve prediction performance dgms meanwhile retain generative ability mmdgms competitive state art fully discriminative networks employing deep convolutional neural networks cnns recognition generative models
cross domain matching bag words data via kernel embeddings latent distributions propose kernel based method finding matching instances across different domains multilingual documents images annotations instance assumed represented multiset features bag words representation documents major difficulty finding cross domain relationships similarity instances different domains cannot directly measured overcome difficulty proposed method embeds features different domains shared latent space regards instance distribution features shared latent space represent distributions efficiently nonparametrically employ framework kernel embeddings distributions embedding estimated minimize difference distributions paired instances keeping unpaired instances apart experiments show proposed method achieve high performance finding correspondence multi lingual wikipedia articles documents tags images tags
gaussian process model quasar spectral energy distributions propose method combining sources astronomical data spectroscopy photometry carry information sources light stars galaxies quasars extremely different spectral resolutions model treats spectral energy distribution sed radiation source latent variable jointly explains photometric spectroscopic observations place flexible nonparametric prior sed light source admits physically interpretable decomposition allows tractably perform inference use model predict distribution redshift quasar band low spectral resolution photometric data called photo problem method shows tools machine learning bayesian statistics allow leverage multiple resolutions information make accurate predictions well characterized uncertainties
neural adaptive sequential monte carlo sequential monte carlo smc particle filtering popular class methods sampling intractable target distribution using sequence simpler intermediate distributions like importance sampling based methods performance critically dependent proposal distribution bad proposal lead arbitrarily inaccurate estimates target distribution paper presents new method automatically adapting proposal using approximation kullback leibler divergence true posterior proposal distribution method flexible applicable parameterized proposal distribution supports online batch variants use new framework adapt powerful proposal distributions rich parameterizations based upon neural networks leading neural adaptive sequential monte carlo nasmc experiments indicate nasmc significantly improves inference non linear state space model outperforming adaptive proposal methods including extended kalman unscented particle filters experiments also indicate improved inference translates improved parameter learning nasmc used subroutine particle marginal metropolis hastings finally show nasmc able train latent variable recurrent neural network rnn achieving results compete state art polymorphic music modelling nasmc seen bridging gap adaptive smc methods recent work scalable black box variational inference
convolutional spike triggered covariance analysis neural subunit models subunit models provide powerful yet parsimonious description neural spike responses complex stimuli expressed cascade linear nonlinear stages first linear stage defined convolution filters recent interest models surged due biological plausibility accuracy characterizing early sensory responses however fitting subunit models poses difficult computational challenge due expense evaluating log likelihood ubiquity local optima address problem forging theoretical connection spike triggered covariance analysis nonlinear subunit models specifically show convolutional decomposition spike triggered average sta covariance stc provides asymptotically efficient estimator subunit model certain technical conditions also prove identifiability convolutional decomposition mild assumptions moment based methods outperform highly regularized versions gqm neural data macaque primary visual cortex achieves nearly prediction performance full maximum likelihood estimator yet substantially lower cost
rectified factor networks propose rectified factor networks rfns efficiently construct sparse non linear high dimensional representations input rfn models identify rare small events low interference code units small reconstruction error explain data covariance structure rfn learning generalized alternating minimization algorithm derived posterior regularization method enforces non negative normalized posterior means proof convergence correctness rfn learning algorithm benchmarks rfns compared unsupervised methods like autoencoders rbms factor analysis ica pca contrast previous sparse coding methods rfns yield sparser codes capture data covariance structure precisely significantly smaller reconstruction error test rfns pretraining technique deep networks different vision datasets rfns superior rbms autoencoders gene expression data pharmaceutical drug discovery studies rfns detected small rare gene modules revealed highly relevant new biological insights far missed unsupervised methods rfn package gpu cpu available http www bioinf jku software rfn
embed control locally linear latent dynamics model control raw images introduce embed control e2c method model learning control non linear dynamical systems raw pixel images e2c consists deep generative model belonging family variational autoencoders learns generate image trajectories latent space dynamics constrained locally linear model derived directly optimal control formulation latent space supports long term prediction image sequences exhibits strong performance variety complex control problems
bayesian dark knowledge consider problem bayesian parameter estimation deep neural networks important problem settings little data need accurate posterior predictive densities applications involving bandits active learning simple approach use online monte carlo methods sgld stochastic gradient langevin dynamics unfortunately method needs store many copies parameters wastes memory needs make predictions using many versions model wastes time describe method distilling monte carlo approximation posterior predictive density compact form namely single deep neural network compare recent approaches bayesian neural networks namely approach based expectation propagation hla15 approach based variational bayes bckw15 method performs better much simpler implement uses less computation test time
kernels cross spectrum analysis multi output gaussian processes provide convenient framework multi task problems illustrative motivating example multi task problem multi region electrophysiological time series data experimentalists interested power phase coherence channels recently wilson adams 2013 proposed spectral mixture kernel model spectral density single task gaussian process framework paper develop novel covariance kernel multiple outputs called cross spectral mixture csm kernel new flexible kernel represents power phase relationship multiple observation channels demonstrate expressive capabilities csm kernel implementation bayesian hidden markov model emission distribution multi output gaussian process csm covariance kernel results presented measured multi region electrophysiological data
end end learning lda mirror descent back propagation deep architecture develop fully discriminative learning approach supervised latent dirichlet allocation lda model using back propagation slda maximizes posterior probability prediction variable given input document different traditional variational learning gibbs sampling approaches proposed learning method applies mirror descent algorithm maximum posterior inference back propagation deep architecture together stochastic gradient mirror descent model parameter estimation leading scalable end end discriminative learning model byproduct also apply technique develop new learning method traditional unsupervised lda model lda experimental results real world regression classification tasks show proposed methods significantly outperform previous supervised topic models neural networks par deep neural networks
particle gibbs infinite hidden markov models infinite hidden markov models ihmm attractive nonparametric generalization classical hidden markov model automatically infer number hidden states system however due infinite dimensional nature transition dynamics performing inference ihmm difficult paper present infinite state particle gibbs algorithm resample state trajectories ihmm proposed algorithm uses efficient proposal optimized ihmms leverages ancestor sampling improve mixing standard algorithm algorithm demonstrates significant convergence improvements synthetic real world data sets
sparse local embeddings extreme multi label classification objective extreme multi label learning train classifier automatically tag novel data point relevant subset labels extremely large label set embedding based approaches make training prediction tractable assuming training label matrix low rank hence effective number labels reduced projecting high dimensional label vectors onto low dimensional linear subspace still leading embedding approaches unable deliver high prediction accuracies scale large problems low rank assumption violated real world applications paper develops sleec classifier address limitations main technical contribution sleec formulation learning small ensemble local distance preserving embeddings accurately predict infrequently occurring tail labels allows sleec break free traditional low rank assumption boost classification accuracy learning embeddings preserve pairwise distances nearest label vectors conducted extensive experiments several real world well benchmark data sets compare method state art methods extreme multi label classification experiments reveal sleec make significantly accurate predictions state art methods including embeddings much well trees much sleec also scale efficiently data sets million labels beyond pale leading embedding methods
robust spectral inference joint stochastic matrix factorization spectral inference provides fast algorithms provable optimality latent topic analysis real data algorithms require additional hoc heuristics even often produce unusable results explain poor performance casting problem topic inference framework joint stochastic matrix factorization jsmf showing previous methods violate theoretical conditions necessary good solution exist propose novel rectification method learns high quality topics interactions even small noisy data method achieves results comparable probabilistic techniques several domains maintaining scalability provable optimality
space time local embeddings space time profound concept physics concept shown useful dimensionality reduction present basic definitions interesting counter intuitions give theoretical propositions show space time powerful representation euclidean space apply concept manifold learning preserving local information empirical results non metric datasets show information preserved space time
fast universal algorithm learn parametric nonlinear embeddings nonlinear embedding algorithms stochastic neighbor embedding dimensionality reduction optimizing objective function involving similarities pairs input patterns result low dimensional projection input pattern common way define sample mapping optimize objective directly parametric mapping inputs neural net done using chain rule nonlinear optimizer slow objective involves quadratic number terms dependent entire mapping parameters using method auxiliary coordinates derive training algorithm works alternating steps train auxiliary embedding steps train mapping advantages algorithm universal specific learning algorithm choice embedding mapping constructed simply reusing existing algorithms embedding mapping user try possible mappings embeddings less effort algorithm fast reuse body methods developed nonlinear embeddings yielding linear time iterations
bayesian manifold learning locally linear latent variable model lvm introduce locally linear latent variable model lvm probabilistic model non linear manifold discovery describes joint distribution observations manifold coordinates locally linear maps conditioned set neighbourhood relationships model allows straightforward variational optimisation posterior distribution coordinates locally linear maps latent space observation space given data thus lvm encapsulates local geometry preserving intuitions underlie non probabilistic methods locally linear embedding lle probabilistic semantics make easy evaluate quality hypothesised neighbourhood relationships select intrinsic dimensionality manifold construct sample extensions combine manifold model additional probabilistic models capture structure coordinates within manifold
local causal discovery direct causes effects focus discovery identification direct causes effects target variable causal network state art algorithms generally need find global causal structures form complete partial directed acyclic graphs order identify direct causes effects target variable algorithms effective often unnecessary wasteful find global structures interested target variable class labels propose new local causal discovery algorithm called causal markov blanket cmb identify direct causes effects target variable based markov blanket discovery cmb designed conduct causal discovery among multiple variables focuses finding causal relationships specific target variable variables standard assumptions show theoretically experimentally proposed local causal discovery algorithm obtain comparable identification accuracy global methods significantly improve efficiency often order magnitude
discriminative robust transformation learning paper proposes framework learning features robust data variation particularly important limited number trainingsamples available framework makes possible tradeoff discriminative value learned features generalization error learning algorithm robustness achieved encouraging transform maps data features local isometry geometric property shown improve epsilon robustness thereby providing theoretical justification reductions generalization error observed experiments proposed optimization frameworkis used train standard learning algorithms deep neural networks experimental results obtained benchmark datasets labeled faces wild demonstrate value able balance discrimination robustness
max margin majority voting learning crowds learning crowds aims design proper aggregation strategies infer unknown true labels noisy labels provided ordinary web workers paper presents max margin majority voting improve discriminative ability majority voting presents bayesian generalization incorporate flexibility generative methods modeling noisy observations worker confusion matrices formulate joint learning regularized bayesian inference problem posterior regularization derived maximizing margin aggregated score potential true label alternative label bayesian model naturally covers dawid skene estimator empirical results demonstrate methods competitive often achieving better results state art estimators
best diverse labelings submodular energies beyond consider problem finding best diverse solutions energy minimization problems graphical models contrary sequential method batra greedily finds solution another infer solutions jointly shown recently jointly inferred labelings smaller total energy also qualitatively outperform sequentially obtained ones obstacle using new technique complexity corresponding inference problem since considerably slower algorithm method batra work show joint inference best diverse solutions formulated submodular energy minimization original map inference problem submodular hence fast inference techniques used addition theoretical results provide practical algorithms outperform current state art used submodular non submodular case
covariance controlled adaptive langevin thermostat large scale bayesian sampling monte carlo sampling bayesian posterior inference common approach used machine learning markov chain monte carlo procedures used often discrete time analogues associated stochastic differential equations sdes sdes guaranteed leave invariant required posterior distribution area current research addresses computational benefits stochastic gradient methods setting existing techniques rely estimating variance covariance subsampling error typically assume constant variance article propose covariance controlled adaptive langevin thermostat effectively dissipate parameter dependent noise maintaining desired target distribution proposed method achieves substantial speedup popular alternative schemes large scale machine learning applications
time sensitive recommendation recurrent user activities making personalized suggestions recommender system playing crucial role improving engagement users modern web services however recommendation algorithms explicitly take account temporal behavior recurrent activities users central less explored questions recommend desirable item emph right moment predict emph next returning time user service address questions propose novel framework connects self exciting point processes low rank models capture recurrent temporal patterns large collection user item consumption pairs show parameters model estimated via convex optimization furthermore develop efficient algorithm maintains epsilon convergence rate scales problems millions user item pairs thousands millions temporal events compared state arts synthetic real datasets model achieves superb predictive performance time sensitive recommendation questions finally point formulation incorporate extra context information users profile textual spatial features
parallel recursive best first search exact map inference graphical models paper presents evaluates power parallel search exact map inference graphical models introduce new parallel shared memory recursive best first search algorithm called sprbfaoo explores search space best first manner operating restricted memory experiments show sprbfaoo often superior current state art sequential search approaches leading considerable speed ups fold threads especially hard problem instances
logarithmic time online multiclass prediction study problem multiclass classification extremely large number classes goal obtaining train test time complexity logarithmic number classes develop top tree construction approaches constructing logarithmic depth trees theoretical front formulate new objective function optimized node tree creates dynamic partitions data pure terms class labels balanced demonstrate favorable conditions construct logarithmic depth trees leaves low label entropy however objective function nodes challenging optimize computationally address empirical problem new online decision tree construction procedure experiments demonstrate online algorithm quickly achieves improvement test error compared common logarithmic training time approaches makes plausible method computationally constrained large applications
scalable semi supervised aggregation classifiers present empirically evaluate efficient algorithm learns aggregate predictions ensemble binary classifiers algorithm uses structure ensemble predictions unlabeled data yield significant performance improvements without making assumptions structure origin ensemble without parameters scalably linear learning empirically demonstrate performance gains random forests
bounding cost search based lifted inference recently growing interest systematic search based importance sampling based lifted inference algorithms statistical relational models srms lifted algorithms achieve significant complexity reductions propositional counterparts using lifting rules leverage symmetries relational representation drawback algorithms use inference blind representation search space makes difficult efficiently pre compute tight upper bounds exact cost inference without running algorithm completion paper present principled approach address problem introduce lifted analogue propositional search space framework call lifted schematic given schematic based representation srm show efficiently compute tight upper bound time space cost exact inference current assignment remaining schematic show bounding method used within lifted importance sampling algorithm order perform effective rao blackwellisation demonstrate experimentally rao blackwellised version algorithm yields accurate estimates several real world datasets
efficient learning directed acyclic graph resource constrained prediction study problem reducing test time acquisition costs classification systems goal learn decision rules adaptively select sensors example necessary make confident prediction model system directed acyclic graph dag internal nodes correspond sensor subsets decision functions node choose whether acquire new sensor classify using available measurements problem naturally posed empirical risk minimization training data rather jointly optimizing highly coupled non convex problem decision nodes propose efficient algorithm motivated dynamic programming learn node policies dag reducing global objective series cost sensitive learning problems approach computationally efficient proven guarantees convergence optimal system fixed architecture addition present extension map budgeted learning problems large number sensors dag architecture demonstrate empirical performance exceeding state art algorithms data composed many sensors
estimating jaccard index missing observations matrix calibration approach jaccard index standard statistics comparing pairwise similarity data samples paper investigates problem estimating jaccard index matrix missing observations data samples starting jaccard index matrix approximated incomplete data method calibrates matrix meet requirement positive semi definiteness constraints simple alternating projection algorithm compared conventional approaches estimate similarity matrix based imputed data method strong advantage calibrated matrix guaranteed closer unknown ground truth frobenius norm calibrated matrix except special cases identical carried series empirical experiments results confirmed theoretical justification evaluation also reported significantly improved results real learning tasks benchmarked datasets
sample efficient path integral control uncertainty present data driven stochastic optimal control framework derived using path integral control approach find iterative control laws analytically without priori policy parameterization based probabilistic representation learned dynamics model proposed algorithm operates forward backward sweep manner differentiate related methods perform forward sampling find open loop optimal controls method uses significantly less sampled data find analytic control laws compared approaches within control family rely extensive sampling given dynamics models trials physical systems model free fashion addition learned controllers generalized new tasks without sampling based compositionality theory linearly solvable optimal control framework provide experimental results different systems comparisons state art model based methods demonstrate efficiency generalizability proposed framework
efficient thompson sampling online matrix factorization recommendation matrix factorization collaborative filtering effective widely used method recommendation systems however problem finding optimal trade exploration exploitation otherwise known bandit problem crucial problem collaborative filtering cold start previously addressed paper present novel algorithm online recommendation automatically combines finding relevantitems exploring new less recommended items approach called particle thompson sampling matrix factorization based general thompson sampling framework augmented novel efficient online bayesian probabilistic matrix factorization method based rao blackwellized particle filter extensive experiments collaborative filtering using several real world datasets demonstrate proposed algorithm significantly outperforms current state arts
parallelizing mcmc random partition trees modern scale data brought new challenges bayesian inference particular conventional mcmc algorithms computationally expensive large data sets promising approach solve problem embarrassingly parallel mcmc mcmc first partitions data multiple subsets runs independent sampling algorithms subset subset posterior draws aggregated via combining rules obtain final approximation existing mcmc algorithms limited approximation accuracy difficulty resampling article propose new mcmc algorithm part solves problems new algorithm applies random partition trees combine subset posterior draws distribution free easy resample adapt multiple scales provide theoretical justification extensive experiments illustrating empirical performance
fast lifted map inference via partitioning recently growing interest lifting map inference algorithms markov logic networks mlns key advantage lifted algorithms much smaller computational complexity propositional algorithms symmetries present mln symmetries detected using lifted inference rules unfortunately lifted inference rules sound complete often miss many symmetries problematic symmetries cannot exploited lifted inference algorithms ground mln search solutions much larger propositional space paper present novel approach cleverly introduces new symmetries time grounding main idea partition ground atoms force inference algorithm treat atoms part indistinguishable show systematically carefully refining growing partitions build advanced time space map inference algorithms experiments several real world datasets clearly show new algorithm superior previous approaches often finds useful symmetries search space existing lifted inference rules unable detect
active learning weak strong labelers active learner given hypothesis class large set unlabeled examples ability interactively query labels oracle subset examples goal learner learn hypothesis class fits data well making label queries possible work addresses active learning labels obtained strong weak labelers addition standard active learning setting extra weak labeler occasionally provide incorrect labels example learning classify medical images either expensive labels obtained physician oracle strong labeler cheaper occasionally incorrect labels obtained medical resident weak labeler goal learn classifier low error data labeled oracle using weak labeler reduce number label queries made labeler provide active learning algorithm setting establish statistical consistency analyze label complexity characterize provide label savings using strong labeler alone
fast guaranteed tensor decomposition via sketching tensor candecomp parafac decomposition wide applications statistical learning latent variable models data mining paper propose fast randomized tensor decomposition algorithms based sketching build idea count sketches introduce many novel ideas unique tensors develop novel methods randomized com putation tensor contractions via ffts without explicitly forming tensors tensor contractions encountered decomposition methods sor power iterations alternating least squares also design novel colliding hashes symmetric tensors save time computing sketches combine sketching ideas existing whitening tensor power iter ative techniques obtain fastest algorithm sparse dense tensors quality approximation method depend properties sparsity uniformity elements etc apply method topic mod eling obtain competitive results
spherical random features polynomial kernels compact explicit feature maps provide practical framework scale kernel methods large scale learning deriving maps many types kernels remains challenging open problem among commonly used kernels nonlinear classification polynomial kernels low approximation error thus far necessitated explicit feature maps large dimensionality especially higher order polynomials meanwhile polynomial kernels unbounded frequently applied data normalized unit norm question address work know priori data normalized devise compact map show putative affirmative answer question based random fourier features impossible setting introduce new approximation paradigm spherical random fourier srf features circumvents issues delivers compact approximation polynomial kernels data unit sphere compared prior work srf features less rank deficient compact achieve better kernel approximation especially higher order polynomials resulting predictions lower variance typically yield better classification accuracy
learnability influence networks establish pac learnability influence functions common influence models namely linear threshold independent cascade voter models present concrete sample complexity results case results model based interesting connections neural networks model based interpretation influence function expectation random draw subgraph use covering number arguments voter model based reduction linear regression show results case cascades partially observed see time steps node influenced also provide efficient polynomial time learning algorithms setting full observation cascades also contain time steps nodes influenced
pseudo euclidean iteration optimal recovery noisy ica independent component analysis ica popular model blind signal separation ica model assumes number independent source signals linearly mixed form observed signals propose new algorithm pegi pseudo euclidean gradient iteration provable model recovery ica gaussian noise main technical innovation algorithm use fixed point iteration pseudo euclidean indefinite inner product space use indefinite inner product resolves technical issues common several existing algorithms noisy ica leads algorithm conceptually simple efficient accurate testing second contribution combining pegi analysis objectives optimal recovery noisy ica model observed direct approach demixing inverse mixing matrix suboptimal signal recovery terms natural signal interference plus noise ratio sinr criterion several partial solutions proposed ica literature turns solution mixing matrix reconstruction problem used construct sinr optimal ica demixing despite fact sinr cannot computed data allows obtain practical provably sinr optimal recovery method ica arbitrary gaussian noise
compressive spectral embedding sidestepping svd spectral embedding based singular value decomposition svd widely used preprocessing step many learning tasks typically leading dimensionality reduction projecting onto number dominant singular vectors rescaling coordinate axes predefined function singular value however number vectors required capture problem structure grows problem size even partial svd computation becomes bottleneck paper propose low complexity compressive spectral embedding algorithm employs random projections finite order polynomial expansions compute approximations svd based embedding times matrix non zeros time complexity log embedding dimension log independent number singular vectors whose effect wish capture best knowledge first work circumvent dependence number singular vectors general svd based embeddings key sidestepping svd observation downstream inference tasks clustering classification interested using resulting embedding evaluate pairwise similarity metrics derived euclidean norm rather capturing effect underlying matrix arbitrary vectors partial svd tries numerical results network datasets demonstrate efficacy proposed method motivate exploration application large scale inference tasks
generalization adaptive data analysis holdout reuse overfitting bane data analysts even data plentiful formal approaches understanding problem focus statistical inference generalization individual analysis procedures yet practice data analysis inherently interactive adaptive process new analyses hypotheses proposed seeing results previous ones parameters tuned basis obtained results datasets shared reused investigation gap recently initiated authors dwork 2014 focused problem estimating expectations adaptively chosen functions paper give simple practical method reusing holdout testing set validate accuracy hypotheses produced learning algorithm operating training set reusing holdout set adaptively multiple times easily lead overfitting holdout set give algorithm enables validation large number adaptively chosen hypotheses provably avoiding overfitting illustrate advantages algorithm standard use holdout set via simple synthetic experiment also formalize address general problem data reuse adaptive data analysis show differential privacy based approach dwork 2014 applicable much broadly adaptive data analysis show simple approach based description length also used give guarantees statistical validity adaptive settings finally demonstrate incomparable approaches unified via notion approximate max information introduce particular allows preservation statistical validity guarantees even analyst adaptively composes algorithms guarantees based either approaches
online measure optimization measure important commonly used performance metric binary prediction tasks combining precision recall single score avoids disadvantages simple metrics like error rate especially cases imbalanced class distributions problem optimizing measure developing learning algorithms perform optimally sense measure recently tackled several authors paper study problem measure maximization setting online learning propose efficient online algorithm provide formal analysis convergence properties moreover first experimental results presented showing method performs well practice
matrix completion noisy side information study matrix completion problem side information side information considered several matrix completion applications generally shown useful empirically recently studied effect side information matrix completion theoretical viewpoint showing sample complexity significantly reduced given completely clean features however since reality given features noisy even weakly informative develop general model handle general feature set much noisy features help matrix recovery theory still important issue investigate paper propose novel model balances features observations simultaneously enabling leverage feature information yet robust feature noise moreover study effectof general features theory show using model sample complexity still lower matrix completion long features sufficiently informative result provides theoretical insight usefulness general side information finally consider synthetic data real applications relationship prediction semi supervised clustering showing model outperforms methods matrix completion features theory practice
market framework eliciting private data propose mechanism purchasing information sequence participants participants simply hold data points wish sell sophisticated information either way incentivized participate long believe data points representative information improve mechanism future prediction test set mechanism draws principles prediction markets bounded budget minimizes generalization error bregman divergence loss functions show modify mechanism preserve privacy participants information given time current prices predictions mechanism reveal almost information participant yet total participants information accurately aggregated
optimal ridge detection using coverage risk introduce concept coverage risk error measure density ridge estimation coverage risk generalizes mean integrated square error set estimation propose risk estimators coverage risk show select tuning parameters minimizing estimated risk study rate convergence coverage risk prove consistency risk estimators apply method simulated datasets cosmology data examples proposed method successfully recover underlying density structure
fast distributed center clustering outliers massive data clustering large data fundamental problem vast number applications due increasing size data practitioners interested clustering turned distributed computation methods work consider widely used center clustering problem variant used handle noisy data center outliers noise free setting demonstrate previously proposed distributed method actually approximation algorithm accurately explains strong empirical performance additionally noisy setting develop novel distributed algorithm also approximation algorithms highly parallel lend virtually distributed computing framework compare empirically best known noisy sequential clustering methods show distributed algorithms consistently close sequential versions algorithms hope distributed settings fast memory efficient match sequential counterparts
orthogonal nmf subspace exploration orthogonal nonnegative matrix factorization onmf aims approximate nonnegative matrix product dimensional nonnegative factors orthonormal columns yields potentially useful data representations superposition disjoint parts shown work well clustering tasks traditional methods underperform existing algorithms rely mostly heuristics despite good empirical performance lack provable performance guarantees present new onmf algorithm provable approximation guarantees constant dimension obtain additive eptas without assumptions input algorithm relies novel approximation related nonnegative principal component analysis nnpca problem given arbitrary data matrix nnpca seeks nonnegative components jointly capture variance nnpca algorithm independent interest generalizes previous work could obtain guarantees single component evaluate algorithms several real synthetic datasets show performance matches outperforms state art
fast classification rates high dimensional gaussian generative models consider problem binary classification covariates conditioned response values follow multivariate gaussian distributions focus setting covariance matrices conditional distributions corresponding generative model classifier derived via bayes rule also called linear discriminant analysis shown behave poorly high dimensional settings present novel analysis classification error linear discriminant approach given conditional gaussian models allows compare generative model classifier recently proposed discriminative approaches directly learn discriminant function finally logistic regression another classical discriminative model classifier show natural sparsity assumption letting denote sparsity bayes classifier number covariates number samples simple ell_1 regularized logistic regression classifier achieves fast misclassification error rates left frac log right much better approaches either inconsistent high dimensional settings achieve slower rate left sqrt frac log right
efficient parsimonious agnostic active learning develop new active learning algorithm streaming settingsatisfying important properties provably works anyclassifier representation classification problem including thosewith severe noise efficiently implementable ermoracle aggressive previous approachessatisfying create algorithm based newlydefined optimization problem analyze also conduct firstexperimental analysis efficient agnostic active learningalgorithms evaluating strengths weaknesses differentsettings
collaborative filtering graph information consistency scalable methods low rank matrix completion plays fundamental role collaborative filtering applications key idea variables lie smaller subspace ambient space often additional information variables known reasonable assume incorporating information lead better predictions tackle problem matrix completion pairwise relationships among variables known via graph formulate derive highly efficient conjugate gradient based alternating minimization scheme solves optimizations million observations orders magnitude faster state art stochastic gradient descent based methods theoretical front show methods generalize weighted nuclear norm formulations derive statistical consistency guarantees validate results real synthetic datasets
less nystr computational regularization study nystr type subsampling approaches large scale kernel methods prove learning bounds statistical learning setting random sampling high probability estimates considered particular prove approaches achieve optimal learning bounds provided subsampling level suitably chosen results suggest simple incremental variant nystr kernel ridge regression subsampling level controls time regularization computations extensive experimental analysis shows considered approach achieves state art performances benchmark large scale datasets
predtron family online algorithms general prediction problems modern prediction problems arising multilabel learning learning rank pose unique challenges classical theory supervised learning problems large prediction label spaces combinatorial nature involve sophisticated loss functions offer general framework derive mistake driven online algorithms associated loss bounds key ingredients framework general loss function general vector space representation predictions notion margin respect general norm general algorithm predtron yields perceptron algorithm variants instantiated classic problems binary classification multiclass classification ordinal regression multilabel classification multilabel ranking subset ranking derive novel algorithms notions margins loss bounds simulation study confirms behavior predicted bounds demonstrates flexibility design choices framework
optimality classifier chain multi label classification capture interdependencies labels multi label classification problems classifier chain tries take multiple labels instance account deterministic high order markov chain model since performance sensitive choice label order key issue determine optimal label order work first generalize model random label order present theoretical analysis generalization error proposed generalized model based results propose dynamic programming based classifier chain algorithm search globally optimal label order greedy classifier chain greedy algorithm find locally optimal comprehensive experiments number real world multi label data sets various domains demonstrate proposed algorithm outperforms state art approaches greedy algorithm achieves comparable prediction performance
smooth interactive submodular set cover interactive submodular set cover interactive variant submodular set cover hypothesis class submodular functions goal satisfy sufficiently plausible submodular functions target threshold using cost weighted actions possible models settings uncertainty regarding submodular function optimize paper propose new extension call smooth interactive submodular set cover allows target threshold vary depending plausibility hypothesis present first algorithm general setting theoretical guarantees optimality show extend approach deal real valued functions yields new theoretical results real valued submodular set cover interactive non interactive settings
tractable bayesian network structure learning bounded vertex cover number learning inference tasks bayesian networks hard general bounded tree width bayesian networks recently received lot attention way circumvent complexity issue however inference bounded tree width networks tractable learning problem remains hard even tree width paper propose bounded vertex cover number bayesian networks alternative bounded tree width networks particular show inference learning done polynomial time fixed vertex cover number bound contrast general bounded tree width cases hand also show learning problem hard parameter furthermore give alternative way learn bounded vertex cover number bayesian networks using integer linear programming ilp show feasible practice
secure multi party differential privacy study problem multi party interactive function computation differential privacy setting party interested computing function private bit parties bits function computed vary party moreover could central observer interested computing separate function parties bits differential privacy ensures remains uncertainty party bit even given transcript interactions parties bits performance party measured via accuracy function computed allow arbitrary cost metric measure distortion true computed function values main result optimality simple non interactive protocol party randomizes bit sufficiently shares privatized version parties optimality result general holds types functions heterogeneous privacy conditions parties types cost metrics average worst case inputs measures accuracy
adaptive stochastic optimization sets paths adaptive stochastic optimization optimizes objective function adaptively uncertainty adaptive stochastic optimization plays crucial role planning learning uncertainty unfortunately computationally intractable general paper introduces conditions objective function marginal likelihood rate bound marginal likelihood bound enable efficient approximate solution adaptive stochastic optimization several interesting classes functions satisfy conditions naturally version space reduction function hypothesis learning describe recursive adaptive coverage rac new adaptive stochastic optimization algorithm exploits conditions apply planning tasks uncertainty constrast earlier submodular optimization approach algorithm applies adaptive stochastic optimization algorithm sets paths
learning structured densities via infinite dimensional exponential families learning structure probabilistic graphical models well studied problem machine learning community due importance many applications current approaches mainly focused learning structure restrictive parametric assumptions limits applicability methods paper study problem estimating structure probabilistic graphical model without assuming particular parametric model consider probabilities members infinite dimensional exponential family parametrized reproducing kernel hilbert space rkhs kernel difficulty learning nonparametric densities evaluation normalizing constant order avoid issue procedure minimizes penalized score matching objective show efficiently minimize proposed objective using existing group lasso solvers furthermore prove procedure recovers graph structure high probability mild conditions simulation studies illustrate ability procedure recover true graph structure without knowledge data generating process
lifelong learning non tasks work aim extending theoretical foundations lifelong learning previous work analyzing scenario based assumption tasks sampled task environment limited strongly constrained data distributions instead study scenarios lifelong learning possible even though observed tasks form sample first sampled environment possibly dependencies second task environment allowed change time first case prove pac bayesian theorem seen direct generalization analogous previous result case second scenario propose learn inductive bias form transfer procedure present generalization bound show toy example used identify beneficial transfer algorithm
learning symmetric label noise importance unhinged convex potential minimisation facto approach binary classification however long servedio 2008 proved symmetric label noise sln minimisation convex potential linear function class result classification performance equivalent random guessing ostensibly shows convex losses sln robust paper propose convex classification calibrated loss prove sln robust loss avoids long servedio 2008 result virtue negatively unbounded loss modification hinge loss clamp hence call unhinged loss show optimal unhinged solution equivalent strongly regularised svm limiting solution convex potential implies strong regularisation makes standard learners sln robust experiments confirm unhinged loss sln robustness
algorithms logarithmic sublinear regret constrained contextual bandits study contextual bandits budget time constraints discrete contexts referred constrained contextual bandits time budget constraints significantly complicate exploration exploitation tradeoff introduce complex coupling among contexts time gain insight first study unit cost systems known context distribution expected rewards known develop approximation oracle referred adaptive linear programming alp achieves near optimality requires ordering expected rewards highly desirable features combine alp upper confidence bound ucb method general case expected rewards unknown priori show proposed ucb alp algorithm achieves logarithmic regret except certain boundary cases design algorithms obtain similar regret analysis results general systems unknown context distribution heterogeneous costs best knowledge first work shows achieve logarithmic regret constrained contextual bandits moreover work also sheds light study computationally efficient algorithms general constrained contextual bandits
random walks distances unweighted graphs large unweighted directed graphs commonly used capture relations entities fundamental problem analysis networks properly define similarity dissimilarity vertices despite significance problem statistical characterization proposed metrics limited introduce develop class techniques analyzing random walks graphs using stochastic calculus using techniques generalize results degeneracy hitting times analyze metric based laplace transformed hitting time ltht metric serves natural provably well behaved alternative expected hitting time establish general correspondence hitting times brownian motion analogous hitting times graph show ltht consistent respect underlying metric geometric graph preserves clustering tendency remains robust random addition non geometric edges tests simulated real world data show ltht matches theoretical predictions outperforms alternatives
column selection via adaptive sampling selecting good column row subset massive data matrices found many applications data analysis machine learning propose new adaptive sampling algorithm used improve relative error column selection algorithm algorithm delivers tighter theoretical bound approximation error also demonstrate empirically using well known relative error column subset selection algorithms experimental results synthetic real world data show algorithm outperforms non adaptive sampling well prior adaptive sampling approaches
multi class svms tighter data dependent generalization bounds novel algorithms paper studies generalization performance multi class classification algorithms obtain first time data dependent generalization error bound logarithmic dependence class size substantially improving state art linear dependence existing data dependent generalization analysis theoretical analysis motivates introduce new multi class classification machine based norm regularization parameter controls complexity corresponding bounds derive efficient optimization algorithm based fenchel duality theory benchmarks several real world datasets show proposed algorithm achieve significant accuracy gains state art
optimal linear estimation unknown nonlinear transform linear regression studies problem estimating model parameter beta observations y_i x_i linear model y_i langle x_i beta rangle epsilon_i consider significant generalization relationship langle x_i beta rangle y_i noisy quantized single bit potentially nonlinear noninvertible well unknown model known single index model statistics among things represents significant generalization bit compressed sensing propose novel spectral based estimation procedure show recover beta settings classes link function previous algorithms fail general algorithm requires mild restrictions unknown functional relationship y_i langle x_i beta rangle also consider high dimensional setting beta sparse introduce stage nonconvex framework addresses estimation challenges high dimensional regimes broad class link functions langle x_i beta rangle y_i establish minimax lower bounds demonstrate optimality estimators classical high dimensional regimes
risk sensitive robust decision making cvar optimization approach paper address problem decision making within markov decision process mdp framework risk modeling errors taken account approach minimize risk sensitive conditional value risk cvar objective opposed standard risk neutral expectation refer problem cvar mdp first contribution show cvar objective besides capturing risk sensitivity alternative interpretation expected cost worst case modeling errors given error budget result independent interest motivates cvar mdps unifying framework risk sensitive robust decision making second contribution present value iteration algorithm cvar mdps analyze convergence rate knowledge first solution algorithm cvar mdps enjoys error guarantees finally present results numerical experiments corroborate theoretical findings show practicality approach
learning incremental iterative regularization within statistical learning setting propose study iterative regularization algorithm least squares defined incremental gradient method particular show parameters fixed priori number passes data epochs acts regularization parameter prove strong universal consistency almost sure convergence risk well sharp finite sample bounds iterates results step towards understanding effect multiple epochs stochastic gradient techniques machine learning rely integrating statistical optimizationresults
regret learning bayesian games recent price anarchy analyses games complete information suggest coarse correlated equilibria characterize outcomes resulting regret learning dynamics near optimal welfare work provides main technical results lift conclusion games incomplete information bayesian games first near optimal welfare bayesian games follows directly smoothness based proof near optimal welfare game private information public second regret learning dynamics converge bayesian coarse correlated equilibrium incomplete information games results enabled interpretation bayesian game stochastic game complete information
sparse low rank tensor decomposition motivated problem robust factorization low rank tensor study question sparse low rank tensor decomposition present efficient computational algorithm modifies leurgans algoirthm tensor factorization method relies reduction problem sparse low rank matrix decomposition via notion tensor contraction use well understood convex techniques solving reduced matrix sub problem allows perform full decomposition tensor delineate situations problem recoverable provide theoretical guarantees algorithm validate algorithm numerical experiments
analysis robust pca via local incoherence investigate robust pca problem decomposing observed matrix sum low rank sparse error matrices via convex programming principal component pursuit pcp contrast previous studies assume support error matrix generated uniform bernoulli sampling allow non uniform sampling entries low rank matrix corrupted errors unequal probabilities characterize conditions error corruption individual entry based local incoherence low rank matrix correct matrix decomposition pcp guaranteed refined analysis robust pca captures robust entry low rank matrix combats error corruption order deal non uniform error corruption technical proof introduces new weighted norm develops exploits concentration properties norm satisfies
algorithmic stability uniform generalization central questions statistical learning theory determine conditions agents learn experience includes necessary sufficient conditions generalization given finite training set new observations paper prove algorithmic stability inference process equivalent uniform generalization across parametric loss functions provide various interpretations result instance relationship proved stability data processing reveals algorithmic stability improved post processing inferred hypothesis augmenting training examples artificial noise prior learning addition establish relationship algorithmic stability size observation space provides formal justification dimensionality reduction methods finally connect algorithmic stability size hypothesis space recovers classical pac result size complexity hypothesis space controlled order improve algorithmic stability improve generalization
mixing time estimation reversible markov chains single sample path article provides first procedure computing fully data dependent interval traps mixing time mix finite reversible ergodic markov chain prescribed confidence level interval computed single finite length sample path markov chain require knowledge parameters chain stands contrast previous approaches either provide point estimates require reset mechanism additional prior knowledge interval constructed around relaxation time relax strongly related mixing time width interval converges roughly sqrt rate length sample path upper lower bounds given number samples required achieve constant factor multiplicative accuracy lower bounds indicate unless restrictions placed chain procedure achieve accuracy level seeing state least omega relax times average finally future directions research identified
efficient compressive phase retrieval constrained sensing vectors propose robust efficient approach problem compressive phase retrieval goal reconstruct sparse vector magnitude number linear measurements proposed framework relies constrained sensing vectors stage reconstruction method consists standard convex programs solved sequentially recent years various methods proposed compressive phase retrieval suboptimal sample complexity lack robustness guarantees main obstacle straightforward convex relaxations type structure target given set underdetermined measurements standard framework recovering sparse matrix standard framework recovering low rank matrix however general efficient method recovering jointly sparse low rank matrix remained elusive deviating models generic measurements paper show sensing vectors chosen random incoherent subspace low rank sparse structures target signal effectively decoupled show recovery algorithm consists low rank recovery stage followed sparse recovery stage produce accurate estimate target number measurements mathsf log frac denote sparsity level dimension input signal also evaluate algorithm numerical simulation
unified view matrix completion general structural constraints matrix completion problems widely studied special low dimensional structures low rank structure induced decomposable norms paper present unified analysis matrix completion general low dimensional structural constraints induced norm regularization consider estimators general problem structured matrix completion provide unified upper bounds sample complexity estimation error analysis relies generic chaining establish intermediate results independent interest characterizing size complexity low dimensional subsets high dimensional ambient space certain textit modified complexity measure encountered analysis matrix completion problems characterized terms well understood complexity measure gaussian widths shown form restricted strong convexity holds matrix completion problems general norm regularization provide several non trivial examples structures included framework notably including recently proposed spectral support norm
copeland dueling bandits version dueling bandit problem addressed condorcet winner exist algorithms proposed instead seek minimize regret respect copeland winner unlike condorcet winner guaranteed exist first copeland confidence bound ccb designed small numbers arms second scalable copeland bandits scb works better large scale problems provide theoretical results bounding regret accumulated ccb scb substantially improving existing results existing results either offer bounds form log require restrictive assumptions offer bounds form log without requiring assumptions results offer best worlds log bounds without restrictive assumptions
regret lower bound optimal algorithm finite stochastic partial monitoring partial monitoring general model sequential learning limited feedback formalized game players game learner chooses action time opponent chooses outcome learner suffers loss receives feedback signal goal learner minimize total loss paper study partial monitoring finite actions stochastic outcomes derive logarithmic distribution dependent regret lower bound defines hardness problem inspired dmed algorithm honda takemura 2010 multi armed bandit problem propose dmed algorithm minimizes distribution dependent regret dmed significantly outperforms state art algorithms numerical experiments show optimality dmed respect regret bound slightly modify algorithm introducing hinge function dmed hinge derive asymptotical optimal regret upper bound dmed hinge matches lower bound
online learning adversaries memory price past mistakes framework online learning memory naturally captures learning problems temporal effects previously studied experts setting work extend notion learning memory general online convex optimization oco framework present algorithms attain low regret first algorithm applies lipschitz continuous loss functions obtaining optimal regret bounds convex strongly convex losses second algorithm attains optimal regret bounds applies broadly convex losses without requiring lipschitz continuity yet complicated implement complement theoretic results applications statistical arbitrage finance multi step ahead prediction statistics
revenue optimization strategic buyers present revenue optimization algorithm posted price auctions facing buyer random valuations seeks optimize gamma discounted surplus analyze problem introduce notion epsilon strategic buyer natural notion strategic behavior used past improve upon previous state art achieve optimal regret bound big log frac log gamma big seller offer prices finite set provide regret bound widetilde big sqrt frac log gamma big buyer offered prices interval
top selection multi armed bandits hidden bipartite graphs paper discusses efficiently choose unknowndistributions ones whose means greatest certainmetric small relative error study topic twostandard settings multi armed bandits hidden bipartitegraphs differ nature input distributions theformer setting distribution sampled manner arbitrary number times whereas latter eachdistribution defined population finite size andhence fully revealed samples settings weprove lower bounds total number samples needed proposeoptimal algorithms whose sample complexities match lower bounds
improved iteration complexity bounds cyclic block coordinate descent convex problems iteration complexity block coordinate descent bcd type algorithm extensive investigation recently shown convex problems classical cyclic bcgd block coordinate gradient descent achieves complexity number passes blocks however bounds least linearly depend number variable blocks least times worse gradient descent proximal gradient methods paper close theoretical performance gap cyclic bcd first show family quadratic nonsmooth problems complexity bounds cyclic block coordinate proximal gradient bcpg popular variant bcd match terms dependency log factor second establish improved complexity bound coordinate gradient descent cgd general convex problems match certain scenarios bounds sharper known bounds always least times worse analyses depend update order block variables inside cycle thus results also apply bcd methods random permutation random sampling without replacement another popular variant
cornering stationary restless mixing bandits remix ucb study restless bandit problem arms associated stationary varphi mixing processes rewards therefore dependent question arises setting carefully recovering independence ignoring values rewards shall see bandit problem tackle requires address exploration exploitation independence trade considering idea waiting arm new remix ucb algorithm generalization improved ucb problem hand introduce provide regret analysis bandit strategy noticeable features remix ucb reduces regular improved ucb varphi mixing coefficients scenario recovered varphi alpha able ensure controlled regret order left delta_ alpha alpha log alpha right delta_ encodes distance best arm best suboptimal arm even case alpha case varphi mixing coefficients summable
fighting bandits new kind smoothness focus adversarial multi armed bandit problem exp3 algorithm auer 2003 shown regret bound sqrt log time horizon number available actions arms recently audibert bubeck 2009 improved bound logarithmic factor via entirely different method present work provide new set analysis tools using notion convex smoothing provide several novel algorithms optimal guarantees first show regularization via tsallis entropy matches minimax rate audibert bubeck 2009 even tighter constant also fully generalizes exp3 second show wide class perturbation methods lead near optimal bandit algorithms long simple condition perturbation distribution mathcal met needs hazard function mathcal remain bounded gumbel weibull frechet pareto gamma distributions satisfy key property interestingly gaussian uniform distributions
asynchronous stochastic convex optimization noise noise sgd care show asymptotically completely asynchronous stochastic gradient procedures achieve optimal even constant factors convergence rates solution convex optimization problems nearly conditions required asymptotic optimality standard stochastic gradient procedures roughly noise inherent stochastic approximation scheme dominates noise asynchrony also give empirical evidence demonstrating strong performance asynchronous parallel stochastic optimization schemes demonstrating robustness inherent stochastic approximation problems allows substantially faster parallel asynchronous solution methods short show many stochastic approximation problems freddie mercury sings queen emph bohemian rhapsody nothing really matters
pareto regret frontier bandits given multi armed bandit problem desirable achieve smaller usual worst case regret special actions show price unbalanced worst case regret guarantees rather high specifically algorithm enjoys worst case regret respect action must exist another action worst case regret least horizon number actions also give upper bounds stochastic adversarial settings showing result cannot improved stochastic case pareto regret frontier characterised exactly constant factors
online learning gaussian payoffs side observations consider sequential learning problem gaussian payoffs side information selecting action learner receives information payoff every action form gaussian observations whose mean mean payoff variance depends pair infinite setup allows refined information transfer action another previous partial monitoring setups including recently introduced graph structured feedback case first time literature provide non asymptotic problem dependent lower bounds regret algorithm recover existing asymptotic problem dependent lower bounds finite time minimax lower bounds available literature also provide algorithms achieve problem dependent lower bound universal constant factor minimax lower bounds logarithmic factors
fast rates exp concave empirical risk minimization consider empirical risk minimization erm context stochastic optimization exp concave smooth losses general optimization framework captures several important learning problems including linear logistic regression learning svms squared hinge loss portfolio selection setting establish first evidence erm able attain fast generalization rates show expected loss erm solution dimensions converges optimal expected loss rate rate matches existing lower bounds constants improves log factor upon state art known attained online batch conversion computationally expensive online algorithms
adaptive low complexity sequential inference dirichlet process mixture models develop sequential low complexity inference procedure dirichlet process mixtures gaussians online clustering parameter estimation number clusters unknown priori present easily computable closed form parametric expression conditional likelihood hyperparameters recursively updated function streaming data assuming conjugate priors motivated large sample asymptotics propose noveladaptive low complexity design dirichlet process concentration parameter show number classes grow logarithmic rate prove large sample limit conditional likelihood datapredictive distribution become asymptotically gaussian demonstrate experiments synthetic real data sets approach superior otheronline state art methods
improved dropout shallow deep learning dropout witnessed great success training deep neural networks independently zeroing outputs neurons random also received surge interest shallow learning logistic regression however independent sampling dropout could suboptimal sake convergence paper propose use multinomial sampling dropout sampling features neurons according multinomial distribution different probabilities different features neurons exhibit optimal dropout probabilities analyze shallow learning multinomial dropout establish risk bound stochastic optimization minimizing sampling dependent factor risk bound obtain distribution dependent dropout sampling probabilities dependent second order statistics data distribution tackle issue evolving distribution neurons deep learning propose efficient adaptive dropout named textbf evolutional dropout computes sampling probabilities fly mini batch examples empirical studies several benchmark datasets demonstrate proposed dropouts achieve much faster convergence also smaller testing error standard dropout example cifar 100 data evolutional dropout achieves relative improvements prediction performance convergence speed compared standard dropout
communication optimal distributed clustering clustering large datasets fundamental problem number applications machine learning data often collected different sites clustering needs performed distributed manner low communication would like quality clustering distributed setting match centralized setting data resides single site work study graph geometric clustering problems distributed models point point model model broadcast channel give protocols models show nearly optimal proving almost matching communication lower bounds work highlights surprising power broadcast channel clustering problems roughly speaking cluster points vertices graph distributed across servers worst case partitioning communication complexity point point model broadcast model implement algorithms demonstrate phenomenon real life datasets showing algorithms also efficient practice
robustness kernel clustering clustering important unsupervised learning problem machine learning statistics among many existing algorithms kernel drawn much research attention due ability find non linear cluster boundaries inherent simplicity main approaches kernel means svd kernel matrix convex relaxations despite attention kernel clustering received theoretical applied quarters much known robustness methods paper first introduce semidefinite programming relaxation kernel clustering problem prove suitable model specification svd sdp approaches consistent limit albeit sdp strongly consistent achieves exact recovery whereas svd weakly consistent fraction misclassified nodes vanish also error bounds suggest sdp resilient towards outliers also demonstrate experiments
combinatorial semi bandit known covariance combinatorial stochastic semi bandit problem extension classical multi armed bandit problem algorithm pulls arm stage rewards pulled arms revealed difference single arm variant dependency structure arms crucial previous works setting either used worst case approach imposed independence arms introduce way quantify dependency structure problem design algorithm adapts algorithm based linear regression analysis uses techniques linear bandit literature comparing performance new lower bound prove optimal poly logarithmic factor number arms pulled
posteriori error bounds joint matrix decomposition problems joint matrix triangularization often used estimating joint eigenstructure set matrices applications signal processing machine learning consider problem approximate joint matrix triangularization matrices jointly diagonalizable real observe set noise perturbed versions matrices main result first order upper bound distance approximate joint triangularizer matrices exact joint triangularizer matrices bound depends observable matrices noise level particular depend optimization specific properties triangularizer proximity critical points typical existing bounds literature knowledge first posteriori bound joint matrix decomposition demonstrate bound synthetic data ground truth known
object based scene representations using fisher scores local subspace projections several works shown deep cnn classifiers easily transferred across datasets transfer cnn trained recognize objects imagenet object detector pascal voc less clear however ability cnns transfer knowledge across tasks common example transfer problem scene classification leverage localized object detections recognize holistic visual concepts problem currently addressed fisher vector representations shown ineffective high dimensional highly non linear features extracted modern cnns argued mostly due reliance model gaussian mixture diagonal covariances limited ability capture second order statistics cnn features problem addressed adoption better model mixture factor analyzers mfa approximates non linear data manifold collection local subspaces fisher score respect mfa mfa derived proposed image representation holistic image classifiers extensive experiments show mfa state art performance object scene transfer transfer actually outperforms training scene cnn large scene dataset representations also shown complementary sense combination outperforms representations combined produce state art scene classifier
mocap guided data augmentation pose estimation wild paper addresses problem human pose estimation wild significant challenge lack training data images humans annotated poses data necessary train state art cnn architectures propose solution generate large set photorealistic synthetic images humans pose annotations introduce image based synthesis engine artificially augments dataset real images human pose annotations using motion capture mocap data given candidate pose algorithm selects joint image whose pose locally matches projected pose selected images combined generate new synthetic image stitching local image patches kinematically constrained manner resulting images used train end end cnn full body pose estimation cluster training data large number pose classes tackle pose estimation way classification problem approach viable large training sets method outperforms state art terms pose estimation controlled environments human3 shows promising results wild images lsp demonstrates cnns trained artificial images generalize well real images
regret queueing bandits consider variant multiarmed bandit problem jobs queue service service rates different servers unknown study algorithms minimize queue regret expected difference queue lengths obtained algorithm obtained genie aided matching algorithm knows exact service rates naive view problem would suggest queue regret grow logarithmically since queue regret cannot larger classical regret results standard mab problem give algorithms ensure queue regret increases logarithmically time paper shows surprisingly complex behavior particular naive intuition correct long bandit algorithm queues relatively long regenerative cycles case queue regret similar cumulative regret scales essentially logarithmically however show early stage queueing bandit eventually gives way late stage optimal queue regret scaling demonstrate algorithm order wise achieves asymptotic queue regret also exhibits close optimal switching time early stage late stage
efficient nonparametric smoothness estimation sobolev quantities norms inner products distances probability density functions important theory nonparametric statistics rarely used practice partly due lack practical estimators also include special cases quantities used many applications propose analyze family estimators sobolev quantities unknown probability density functions bound finite sample bias variance estimators finding generally minimax rate optimal estimators significantly computationally tractable previous estimators exhibit statistical computational trade allowing adapt computational constraints also draw theoretical connections recent work fast sample testing empirically validate estimators synthetic data
completely random measures modelling block structured sparse networks statistical methods network data often parameterize edge probability attributing latent traits block structure vertices assume exchangeability sense aldous hoover representation theorem assumptions however incompatible traits found real world networks power law degree distribution recently caron fox 2014 proposed use different notion exchangeability kallenberg 2005 obtained network model permits edge inhomogeneity power law degree distribution whilst retaining desirable statistical properties however model capture latent vertex traits block structure work introduce use block structure network models obeying kallenberg notion exchangeability thereby obtain collapsed model admits inference block structure edge inhomogeneity derive simple expression likelihood efficient sampling method obtained model significantly difficult implement existing approaches block modelling performs well real network datasets
disco nets dissimilarity coefficients networks present new type probabilistic model call dissimilarity coefficient networks disco nets disco nets allow efficiently sample posterior distribution parametrised neural network training disco nets learned minimising dissimilarity coefficient true distribution estimated distribution allows tailor training loss related task hand empirically show modeling uncertainty output value disco nets outperform equivalent non probabilistic predictive networks disco nets accurately model uncertainty output outperforming existing probabilistic models based deep neural networks
architecture deep hierarchical generative models present architecture lets train deep directed generative models many layers latent variables include deterministic paths latent variables generated output provide richer set connections computations inference generation enables effective communication information throughout model training improve performance natural images incorporate lightweight autoregressive model reconstruction distribution techniques permit end end training models layers latent variables experiments show approach achieves state art performance standard image modelling benchmarks expose latent class structure absence label information provide convincing imputations occluded regions natural images
multi batch bfgs method machine learning question parallelize stochastic gradient descent sgd method received much attention literature paper focus instead batch methods use sizeable fraction training set iteration facilitate parallelism employ second order information order improve learning process follow multi batch approach batch changes iteration cause difficulties bfgs employs gradient differences update hessian approximations gradients computed using different data points process unstable paper shows perform stable quasi newton updating multi batch setting illustrates behavior algorithm distributed computing platform studies convergence properties convex nonconvex cases
higher order factorization machines factorization machines fms supervised learning approach use second order feature combinations even data high dimensional unfortunately despite increasing interest fms exists date efficient training algorithm higher order fms hofms paper present first generic yet efficient algorithms training arbitrary order hofms also present new variants hofms shared parameters greatly reduce model size prediction times maintaining similar accuracy demonstrate proposed approaches different link prediction tasks
bio inspired redundant sensing architecture sensing process deriving signals environment allows artificial systems interact physical world shannon theorem specifies maximum rate information acquired however upper bound hard achieve many man made systems biological visual systems hand highly efficient signal representation processing mechanisms allow precise sensing work argue redundancy critical characteristics superior performance show architectural advantages utilizing redundant sensing including correction mismatch error significant precision enhancement proof concept demonstration designed heuristic based analog digital converter dimensional quantizer monte carlo simulation error probabilistic distribution priori performance approaching shannon limit feasible actual measurements without knowing error distribution observe least bit extra precision results also help explain biological processes including dominance binocular vision functional roles fixational eye movements structural mechanisms allowing hyperacuity
learning supervised pagerank gradient based gradient free optimization methods paper consider non convex loss minimization problem learning supervised pagerank models account features nodes edges propose gradient based random gradient free methods solve problem algorithms based concept inexact oracle unlike state art gradient based method manage provide theoretically convergence rate guarantees finally compare performance proposed optimization methods state art applied ranking task
linear relaxations finding diverse elements metric spaces choosing diverse subset large collection points metric space fundamental problem applications feature selection recommender systems web search data summarization etc various notions diversity proposed tailored different applications general algorithmic goal find subset points maximize diversity obeying cardinality generally matroid constraint goal paper develop novel linear programming framework allows design approximation algorithms problems study objective known sum min diversity known effective many applications give first constant factor approximation algorithm framework allows easily incorporate additional constraints well secondary objectives also prove hardness result natural diversity objectives called planted clique assumption finally study empirical performance algorithm several standard datasets first study approximation quality algorithm comparing objective compare quality solutions produced method popular diversity maximization algorithms
stochastic optimization large scale optimal transport optimal transport defines powerful framework compare probability distributions geometrically faithful way however practical impact still limited computational burden propose new class stochastic optimization algorithms cope large scale problems routinely encountered machine learning applications methods able manipulate arbitrary distributions either discrete continuous simply requiring able draw samples typical setup high dimensional learning problems alleviates need discretize densities giving access provably convergent methods output correct distance without discretization error algorithms rely main ideas dual problem cast maximization expectation entropic regularization primal problem results smooth dual optimization optimization addressed algorithms provably faster convergence instantiate ideas different computational setups comparing discrete distribution another show incremental stochastic optimization schemes beat current state art finite dimensional solver sinkhorn algorithm comparing discrete distribution continuous density formulation semi discrete dual program amenable averaged stochastic gradient descent leading better performance approximately solving problem discretization iii dealing continuous densities propose stochastic gradient descent reproducing kernel hilbert space rkhs currently known method solve problem efficient discretizing beforehand densities backup claims set discrete semi discrete continuous benchmark problems
threshold bandits without censored feedback consider emph threshold bandit setting variant classical multi armed bandit problem reward round depends piece side information known emph threshold value learner selects actions arms action generates random sample fixed distribution action receives unit payoff event sample exceeds threshold value consider versions problem emph uncensored emph censored case determine whether sample always observed threshold met using new tools understand popular ucb algorithm show uncensored case essentially difficult classical multi armed bandit setting finally show censored case exhibits challenges give guarantees event sequence threshold values generated optimistically
mistake bounds binary matrix completion study problem completing binary matrix online learning setting trial predict matrix entry receive true entry propose matrix exponentiated gradient algorithm solve problem provide mistake bound algorithm scales margin complexity underlying matrix bound suggests interpretation row matrix prediction task finite set objects columns using show algorithm makes number mistakes comparable logarithmic factor number mistakes made kernel perceptron optimal kernel hindsight discuss applications algorithm predicting well best biclustering problem predicting labeling graph without knowing graph advance
soundnet learning sound representations unlabeled video learn rich natural sound representations capitalizing large amounts unlabeled sound data collected wild leverage natural synchronization vision sound learn acoustic representation using million unlabeled videos unlabeled video advantage economically acquired massive scales yet contains useful signals natural sound propose student teacher training procedure transfers discriminative visual knowledge well established visual recognition models sound modality using unlabeled video bridge sound representation yields significant performance improvements state art results standard benchmarks acoustic scene object classification visualizations suggest high level semantics automatically emerge sound network even though trained without ground truth labels
doubly convolutional neural networks building large models parameter sharing accounts success deep convolutional neural networks cnns paper propose doubly convolutional neural networks dcnns significantly improve performance cnns exploring idea stead allocating set convolutional filters independently learned dcnn maintains groups filters filters within group translated versions practically dcnn easily implemented step convolution procedure supported modern deep learning libraries perform extensive experiments image classification benchmarks cifar cifar 100 imagenet show dcnns consistently outperform competing architectures also verified replacing convolutional layer doubly convolutional layer depth cnn improve performance moreover various design choices dcnns demonstrated shows dcnn serve dual purpose building accurate models reducing memory footprint without sacrificing accuracy
maximizing influence ising network mean field optimal solution influence maximization social networks typically studied context contagion models irreversible processes paper consider alternate model treats individual opinions spins ising system dynamic equilibrium formalize textit ising influence maximization problem natural physical interpretation maximizing magnetization given budget external magnetic field mean field approximation present gradient ascent algorithm uses susceptibility efficiently calculate local maxima magnetization develop number sufficient conditions magnetization concave algorithm converges global optimum apply algorithm random real world networks demonstrating remarkably optimal external fields external fields maximize magnetization exhibit phase transition focusing high degree individuals high temperatures focusing low degree individuals low temperatures also establish number novel results structure steady states ferromagnetic ising model general graphs independent interest
learning rational behavior predicting solutions unknown linear programs define study problem predicting solution linear program given partial information objective constraints generalizes problem learning predict purchasing behavior rational agent unknown objective function studied name learning revealed preferences give mistake bound learning algorithms settings first objective known learner arbitrary fixed set constraints unknown example defined additional known constraint goal learner predict optimal solution given union known unknown constraints models problem predicting behavior rational agent whose goals known whose resources unknown second setting objective unknown changing controlled way constraints also change every day known example given set constraints partial information objective task learner predict optimal solution partially known
fairness learning classic contextual bandits introduce study fairness multi armed bandit problems fairness definition demands given pool applicants worse applicant never favored better despite learning algorithm uncertainty true payoffs classic stochastic bandits problem provide provably fair algorithm based chained confidence intervals prove cumulative regret bound cubic dependence number arms show fair algorithm must dependence providing strong separation fair unfair learning extends general contextual case general contextual case prove tight connection fairness kwik knows knows learning model kwik algorithm class functions transformed provably fair contextual bandit algorithm vice versa tight connection allows provide provably fair algorithm linear contextual bandit problem polynomial dependence dimension show different class functions worst case exponential gap regret fair non fair learning algorithms
powerful generative model using random weights deep image representation extent success deep visualization due training could deep visualization using untrained random weight networks address issue explore new powerful generative models popular deep visualization tasks using untrained random weight convolutional neural networks first invert representations feature spaces reconstruct images white noise inputs reconstruction quality statistically higher method applied well trained networks architecture next synthesize textures using scaled correlations representations multiple layers results almost indistinguishable original natural texture synthesized textures based trained network third recasting content image style various artworks create artistic images high perceptual quality highly competitive prior work gatys pretrained networks knowledge first demonstration image representations using untrained deep neural networks work provides new fascinating tool study representation deep network architecture sheds light new understandings deep visualization possibly lead way compare network architectures without training
improved error bounds tree representations metric spaces estimating optimal phylogenetic trees hierarchical clustering trees metric data important problem evolutionary biology data analysis intuitively goodness fit metric space tree depends inherent treeness well metric properties intrinsic dimension existing algorithms embedding metric spaces tree metrics provide distortion bounds depending cardinality cardinality simple property set argue bounds fully capture rich structure endowed metric consider embedding metric space tree proposed gromov proving stability result obtain improved additive distortion bound depending hyperbolicity doubling dimension metric observe gromov method dual well known single linkage hierarchical clustering slhc method means duality able transport results setting slhc additive distortion bounds previously unknown
adaptive optimal training animal behavior neuroscience experiments often require training animals perform tasks designed elicit various sensory cognitive motor behaviors training typically involves series gradual adjustments stimulus conditions rewards order bring learning however training protocols usually hand designed relying combination intuition guesswork trial error often require weeks months achieve desired level task performance combine ideas reinforcement learning adaptive optimal experimental design formulate methods adaptive optimal training animal behavior work addresses intriguing problems first seeks infer learning rules underlying animal behavioral changes training second seeks exploit rules select stimuli maximize rate learning toward desired objective develop test methods using data collected rats training interval sensory discrimination task show accurately infer parameters policy gradient based learning algorithm describes animal internal model task evolves course training formulate theory optimal training involves selecting sequences stimuli drive animal internal policy toward desired location parameter space simulations show method theory provide substantial speedup standard training methods feel results hold considerable theoretical practical implications researchers reinforcement learning experimentalists seeking train animals
pac bayesian theory meets bayesian inference exhibit strong link frequentist pac bayesian bounds bayesian marginal likelihood negative log likelihood loss function show minimization pac bayesian generalization bounds maximizes bayesian marginal likelihood provides alternative explanation bayesian occam razor criteria assumption data generated distribution moreover negative log likelihood unbounded loss function motivate propose pac bayesian theorem tailored sub gamma loss family show approach sound classical bayesian linear regression tasks
nearly isometric embedding relaxation many manifold learning algorithms aim create embeddings low distortion isometric data intrinsic dimension often impossible obtain isometric embedding dimensions possible dimensions yet geometry preserving algorithms cannot latter paper proposes embedding algorithm overcomes problem algorithm directly computes data embedding distortion loss iteratively updates order decrease distortion measure propose based push forward riemannian metric associated coordinates experiments confirm superiority algorithm obtaining low distortion embeddings
graph clustering block models model free results clustering graphs stochastic block model sbm extensions well studied guarantees correctness exist assumption data sampled model paper propose framework obtain correctness guarantees without assuming data comes model guarantees obtain depend instead statistics data checked also show framework ties existing model based framework exploit results model based recovery well strengthen results existing area research
learning transferrable representations unsupervised domain adaptation supervised learning large scale labelled datasets deep layered models caused paradigm shift diverse areas learning recognition however approach still suffers generalization issues presence domain shift training test data distribution since unsupervised domain adaptation algorithms directly address domain shift problem labelled source dataset unlabelled target dataset recent papers shown promising results fine tuning networks domain adaptation loss functions try align mismatch training testing data distributions nevertheless recent deep learning based domain adaptation approaches still suffer issues high sensitivity gradient reversal hyperparameters overfitting fine tuning stage paper propose unified deep learning framework representation cross domain transformation target label inference jointly optimized end end fashion unsupervised domain adaptation experiments show proposed method significantly outperforms state art algorithms object recognition digit classification experiments large margin make learned models well source code available immediately upon acceptance
measuring neural net robustness constraints despite high accuracy neural nets shown susceptible adversarial examples small perturbation input cause become mislabeled propose metrics measuring robustness neural net devise novel algorithm approximating metrics based encoding robustness linear program show metrics used evaluate robustness deep neural nets experiments mnist cifar datasets algorithm generates informative estimates robustness metrics compared estimates based existing algorithms furthermore show existing approaches improving robustness overfit adversarial examples generated using specific algorithm finally show techniques used additionally improve neural net robustness according metrics propose also according previously proposed metrics
forward model purkinje cell synapses facilitates cerebellar anticipatory control motor system solve problem anticipatory control spite wide spectrum response dynamics different musculo skeletal systems transport delays well response latencies throughout central nervous system great extent highly skilled motor responses result reactive feedback system originating brain stem spinal cord combined feed forward anticipatory system adaptively fine tuned sensory experience originates cerebellum based interaction design counterfactual predictive control cfpc architecture anticipatory adaptive motor control scheme feed forward module based cerebellum steers error feedback controller counterfactual error signals signals trigger reactions actual errors would code current forthcoming errors order determine optimal learning strategy derive novel learning rule feed forward module involves eligibility trace operates synaptic level particular eligibility trace provides mechanism beyond incidence detection convolves history prior synaptic inputs error signals context cerebellar physiology solution implies purkinje cell synapses generate eligibility traces using forward model system controlled engineering perspective cfpc provides general purpose anticipatory control architecture equipped learning rule exploits full dynamics closed loop system
estimating nonlinear neural response functions using priors kronecker methods jointly characterizing neural responses terms several external variables promises novel insights circuit function remains computationally prohibitive practice use gaussian process priors exploit recent advances fast inference learning based kronecker methods efficiently estimate multidimensional nonlinear tuning functions estimator require considerably less data traditional methods provides principled uncertainty estimates apply tools hippocampal recordings open field exploration use characterize joint dependence ca1 responses position animal several variables including animal speed direction motion network oscillations results provide unprecedentedly detailed quantification tuning hippocampal neurons model generality suggests approach used estimate neural response properties brain regions
bayesian method reducing bias neural representational similarity analysis neuroscience similarity matrix neural activity patterns response different sensory stimuli different cognitive states reflects structure neural representational space existing methods derive point estimations neural activity patterns noisy neural imaging data similarity calculated point estimations show approach translates structured noise estimated patterns spurious bias structure resulting similarity matrix especially severe signal noise ratio low experimental conditions cannot fully randomized cognitive task propose alternative bayesian framework computing representational similarity treat covariance structure neural activity patterns hyper parameter generative model neural data directly estimate covariance structure imaging data marginalizing unknown activity patterns converting estimated covariance structure correlation matrix offers much less biased estimate neural representational similarity method also simultaneously estimate signal noise map informs learned representational structure supported strongly learned covariance matrix used structured prior constrain bayesian estimation neural activity patterns code freely available brainiak https github com intelpni brainiak python toolkit brain imaging analysis
learning communicate deep multi agent reinforcement learning consider problem multiple agents sensing acting environments goal maximising shared utility environments agents must learn communication protocols order share information needed solve tasks embracing deep neural networks able demonstrate end end learning protocols complex environments inspired communication riddles multi agent computer vision problems partial observability propose approaches learning domains reinforced inter agent learning rial differentiable inter agent learning dial former uses deep learning latter exploits fact learning agents backpropagate error derivatives noisy communication channels hence approach uses centralised learning decentralised execution experiments introduce new environments studying learning communication protocols present set engineering innovations essential success domains
total variation classes beyond minimax rates limitations linear smoothers consider problem estimating function defined locations dimensional grid side lengths equal function constrained discrete total variation bounded derive minimax optimal squared l_2 estimation error rate parametrized total variation denoising also known fused lasso seen rate optimal several simpler estimators exist laplacian smoothing laplacian eigenmaps natural question simpler estimators perform well prove estimators broadly estimators given linear transformations input data suboptimal class functions bounded variation extends fundamental findings donoho johnstone 1998 dimensional total variation spaces higher dimensions implication computationally simpler methods cannot used sophisticated denoising tasks without sacrificing statistical accuracy also derive minimax rates discrete sobolev spaces dimensional grids sense smaller total variation function spaces indeed small enough spaces linear estimators optimal well known ones laplacian smoothing laplacian eigenmaps show lastly investigate adaptivity total variation denoiser smaller sobolev function spaces
exponential family embeddings word embeddings powerful approach capturing semantic similarity among terms vocabulary paper develop exponential family embeddings extends idea word embeddings types high dimensional data examples studied several types data neural data real valued observations count data market basket analysis ratings data movie recommendation system main idea observation modeled conditioned set latent embeddings observations called context way context defined depends problem language context surrounding words neuroscience context close neurons market basket data context items shopping cart instance embedding defines context exponential family conditional distributions embedding vectors shared across data infer embeddings stochastic gradient descent algorithm connects closely generalized linear models applications neural activity zebrafish users shopping behavior movie ratings found exponential family embedding models effective dimension reduction methods better reconstruct held data find interesting qualitative structure
nearest neighbors global local weighted nearest neighbors algorithm fundamental non parametric methods pattern recognition machine learning question setting optimal number neighbors well optimal weights received much attention throughout years nevertheless problem seems remained unsettled paper offer simple approach locally weighted regression classification make bias variance tradeoff explicit formulation enables phrase notion optimal weights efficiently find weights well optimal number neighbors efficiently adaptively data point whose value wish estimate applicability approach demonstrated several datasets showing superior performance standard locally weighted methods
reward augmented maximum likelihood neural structured prediction key problem structured output prediction enabling direct optimization task reward function matters test evaluation paper presents simple computationally efficient method incorporates task reward maximum likelihood training establish connection maximum likelihood regularized expected reward showing approximately equivalent vicinity optimal solution show maximum likelihood generalized optimizing conditional probability auxiliary outputs sampled proportional exponentiated scaled rewards apply framework optimize edit distance output space sampling edited targets experiments speech recognition machine translation neural sequence sequence models show notable improvements maximum likelihood baseline simply sampling target output augmentations
probabilistic model social decision making based reward maximization fundamental problem cognitive neuroscience humans make decisions act behave relation humans adopt hypothesis interactive social setting brains perform bayesian inference intentions cooperativeness others using probabilistic representations employ framework partially observable markov decision processes pomdps model human decision making social context focusing specifically volunteer dilemma version classic public goods game show pomdp model explains behavior subjects well neural activity recorded using fmri game decisions subjects modeled across trials using interpretable parameters furthermore expected reward predicted model subject correlated activation brain areas related reward expectation social interactions results suggest probabilistic basis human social decision making within framework expected reward maximization
active learning oracle epiphany present theoretical analysis active learning realistic interactions human oracles previous empirical studies shown oracles abstaining difficult queries accumulating enough information make label decisions formalize phenomenon oracle epiphany model analyze active learning query complexity oracles realizable agnos tic cases analysis shows active learning possible oracle epiphany incurs additional cost depending epiphany happens results suggest new principled active learning approaches realistic oracles
regularizing rademacher observation losses recently shown supervised learning linear classifiers popular losses logistic square loss equivalent optimizing equivalent loss sufficient statistics class rademacher observations rados also shown learning rados brings solutions prominent problems state art learning examples comparatively inferior fact less convenient protecting learning private examples learning distributed datasets without entity resolution bis repetita placent proofs equivalence different rely specific properties corresponding losses whether unified generalized inevitably comes mind first contribution show fit theory equivalence example rado losses second contribution show generalization unveils surprising new connection regularized learning particular sufficient condition regularizing loss examples equivalent regularizing rados data equivalent rado loss way efficient algorithm regularized rado loss efficient changing regularizer third contribution give formal boosting algorithm regularized exponential rado loss boost ridge lasso slope infty elastic nets using master routine regularized exponential rado loss equivalent regularized logistic loss examples obtain first efficient proxy minimisation regularized logistic loss examples using wide spectrum regularizers experiments readily available code display regularization significantly improves rado based learning compares favourably example based learning
non generative framework convex relaxations unsupervised learning give novel formal theoretical framework unsupervised learning distinctive characteristics first assume generative model based worst case performance metric second comparative namely performance measured respect given hypothesis class allows avoid known computational hardness results improper algorithms based convex relaxations show several families unsupervised learning models previously analyzed probabilistic assumptions otherwise provably intractable efficiently learned framework convex optimization
learning tree structured potential games many real phenomena including behaviors involve strategic interactions learned data focus learning tree structured potential games equilibria represented local maxima underlying potential function cast learning problem within max margin setting show problem hard even strategic interactions form tree develop variant dual decomposition estimate underlying game demonstrate synthetic real decision voting data game theoretic perspective carving local maxima enables meaningful recovery
equality opportunity supervised learning propose criterion discrimination specified sensitive attribute supervised learning goal predict target based available features assuming data predictor target membership protected group available show optimally adjust learned predictor remove discrimination according definition framework also improves incentives shifting cost poor classification disadvantaged groups decision maker respond improving classification accuracy
interaction networks learning objects relations physics reasoning objects relations physics central human intelligence key goal artificial intelligence introduce interaction network model reason objects complex systems interact supporting dynamical predictions well inferences abstract properties system model takes graphs input performs object relation centric reasoning way analogous simulation implemented using deep neural networks evaluate ability reason several challenging physical domains body problems rigid body collision non rigid dynamics results show trained accurately simulate physical trajectories dozens objects thousands time steps estimate abstract quantities energy generalize automatically systems different numbers configurations objects relations interaction network implementation first general purpose learnable physics engine powerful general framework reasoning object relations wide variety complex real world domains
beta risk new surrogate risk learning weakly labeled data past years machine learning community paid attention developping new methods learning weakly labeled data field covers different settings like semi supervised learning learning label proportions multi instance learning noise tolerant learning etc paper presents generic framework deal weakly labeled scenarios introduce beta risk generalized formulation standard empirical risk based surrogate margin based loss functions risk allows express reliability labels derive different kinds learning algorithms specifically focus svms propose soft margin beta svm algorithm behaves better state art
binarized neural networks introduce method train binarized neural networks bnns neural networks binary weights activations run time train time binary weights activations used computing parameter gradients forward pass bnns drastically reduce memory size accesses replace arithmetic operations bit wise operations expected substantially improve power efficiency validate effectiveness bnns conducted sets experiments torch7 theano frameworks bnns achieved nearly state art results mnist cifar svhn datasets also report preliminary results challenging imagenet dataset last least wrote binary matrix multiplication gpu kernel possible run mnist bnn times faster unoptimized gpu kernel without suffering loss classification accuracy code training running bnns available line
regularization stochastic transformations perturbations deep semi supervised learning effective convolutional neural networks trained large sets labeled data however creating large labeled datasets costly time consuming task semi supervised learning uses unlabeled data train model higher accuracy limited set labeled data available paper consider problem semi supervised learning convolutional neural networks techniques randomized data augmentation dropout random max pooling provide better generalization stability classifiers trained using gradient descent multiple passes individual sample network might lead different predictions due non deterministic behavior techniques propose unsupervised loss function takes advantage stochastic nature methods minimizes difference predictions multiple passes training sample network evaluate proposed method several benchmark datasets
generating images perceptual similarity metrics based deep networks propose class loss functions call deep perceptual similarity metrics deepsim allowing generate sharp high resolution images compressed abstract representations instead computing distances image space compute distances image features extracted deep neural networks metric reflects perceptual similarity images much better thus leads better results demonstrate examples use cases proposed loss networks invert alexnet convolutional network modified version variational autoencoder generates realistic high resolution random images
exploiting tradeoffs exact recovery heterogeneous stochastic block models stochastic block model sbm widely used random graph model networks communities despite recent burst interest community detection sbm statistical computational points view still gaps understanding fundamental limits recovery paper consider sbm full generality restriction number sizes communities grow number nodes well connectivity probabilities inside across communities stochastic block models provide guarantees exact recovery via semidefinite program well upper lower bounds sbm parameters exact recoverability results exploit tradeoffs among various parameters heterogenous sbm provide recovery guarantees many new interesting sbm configurations
tensor switching networks present novel neural network algorithm tensor switching network generalizes rectified linear unit relu nonlinearity tensor valued hidden units network copies entire input vector different locations expanded representation location determined hidden unit activity way even simple linear readout representation implement highly expressive deep network like function network hence avoids vanishing gradient problem construction cost larger representation size develop several methods train network including equivalent kernels infinitely wide deep networks pass linear learning algorithm backpropagation inspired representation learning algorithms experimental results demonstrate network indeed expressive consistently learns faster standard relu networks
finite dimensional bfry priors variational bayesian inference power law models bayesian nonparametric methods based dirichlet process gamma process beta process proven effective capturing aspects various datasets arising machine learning however recognized processes limitations terms ability capture power law behavior considerable interest models based stable processs generalized gamma process ggp stable beta process sbp models present new challenges terms practical statistical implementation analogy tractable processes finite dimensional dirichlet process describe class random processes call iid finite dimensional bfry processes enables begin develop efficient posterior inference algorithms variational bayes readily scale massive datasets illustrative purposes describe simple variational bayes algorithm normalized mixture models demonstrate usefulness experiments synthetic real world datasets
temporal regularized matrix factorization high dimensional time series prediction time series prediction problems becoming increasingly high dimensional modern applications climatology demand forecasting example latter problem number items demand needs forecast might large 000 addition data generally noisy full missing values thus modern applications require methods highly scalable deal noisy data terms corruptions missing values however classical time series methods usually fall short handling issues paper present temporal regularized matrix factorization trmf framework supports data driven temporal learning forecasting develop novel regularization schemes use scalable matrix factorization methods eminently suited high dimensional time series data many missing values proposed trmf highly general subsumes many existing approaches time series analysis make interesting connections graph regularization methods context learning dependencies autoregressive framework experimental results show superiority trmf terms scalability prediction quality particular trmf orders magnitude faster methods problem dimension 000 generates better forecasts real world datasets wal mart commerce datasets
composing graphical models neural networks structured representations fast inference propose general modeling inference framework combines complementary strengths probabilistic graphical models deep learning methods model family composes latent graphical models neural network observation likelihoods inference use recognition networks produce local evidence potentials combine model distribution using efficient message passing algorithms components trained simultaneously single stochastic variational inference objective illustrate framework automatically segmenting categorizing mouse behavior raw depth video demonstrate several example models
pac reinforcement learning rich observations propose study new model reinforcement learning rich observations generalizing contextual bandits sequential decision making models require agent take actions based observations features goal achieving long term performance competitive large set policies avoid barriers sample efficient learning associated large observation spaces general pomdps focus problems summarized small number hidden states long term rewards predictable reactive function class setting design analyze new reinforcement learning algorithm least squares value elimination exploration prove algorithm learns near optimal behavior number episodes polynomial relevant parameters logarithmic number policies independent size observation space result provides theoretical justification reinforcement learning function approximation
algorithms matching lower bounds approximately convex optimization recent years rapidly increasing number applications practice requires solving non convex objectives like training neural networks learning graphical models maximum likelihood estimation etc though simple heuristics gradient descent modifications tend work well theoretical understanding weak consider possibly natural class non convex functions could hope obtain provable guarantees functions approximately convex functions real real exists emph convex function errnoise fixed value errnoise want minimize output point min_ err quite natural conjecture fixed err problem gets harder larger errnoise however exact dependency err errnoise known paper strengthen known emph information theoretic lower bounds trade err errnoise substantially exhibit algorithm matches lower bounds large class convex bodies
proximal stochastic methods nonsmooth nonconvex finite sum optimization analyze stochastic algorithms optimizing nonconvex nonsmooth finite sum problems nonsmooth part convex surprisingly unlike smooth case knowledge fundamental problem limited example known whether proximal stochastic gradient method constant minibatch converges stationary point tackle issue develop fast stochastic algorithms provably converge stationary point constant minibatches furthermore using variant algorithms obtain provably faster convergence batch proximal gradient descent results based recent variance reduction techniques convex optimization novel analysis handling nonconvex nonsmooth functions also prove global linear convergence rate interesting subclass nonsmooth nonconvex functions subsumes several recent works
simple practical accelerated method finite sums abstract describe novel optimization method finite sums empirical risk minimization problems building recently introduced saga method method achieves accelerated convergence rate strongly convex smooth problems method parameter step size radically simpler accelerated methods finite sums additionally applied terms non smooth yielding method applicable many areas operator splitting methods would traditionally applied
unsupervised learning physical interaction video prediction core challenge agent learning interact world predict actions affect objects environment many existing methods learning dynamics physical interactions require labeled object information however scale real world interaction learning variety scenes objects acquiring labeled data becomes increasingly impractical learn physical object motion without labels develop action conditioned video prediction model explicitly models pixel motion predicting distribution pixel motion previous frames model explicitly predicts motion partially invariant object appearance enabling generalize previously unseen objects explore video prediction real world interactive agents also introduce dataset 000 robot interactions involving pushing motions including test set novel objects dataset accurate prediction videos conditioned robot future actions amounts learning visual imagination different futures based different courses action experiments show proposed method produces accurate video predictions quantitatively qualitatively compared prior methods
threshold learning optimal decision making decision making uncertainty commonly modelled process competitive stochastic evidence accumulation threshold drift diffusion model however unknown animals learn decision thresholds examine threshold learning constructing reward function averages many trials wald cost function defines decision optimality rewards highly stochastic hence challenging optimize address ways first simple factor reward modulated learning rule derived williams reinforce method neural networks second bayesian optimization reward function gaussian process bayesian optimization converges fewer trials reinforce slower computationally greater variance reinforce method also better model acquisition behaviour animals similar learning rule proposed modelling basal ganglia function
collaborative recurrent autoencoder recommend learning fill blanks hybrid methods utilize content rating information commonly used many recommender systems however use either handcrafted features bag words representation surrogate content information neither effective natural enough address problem develop collaborative recurrent autoencoder crae denoising recurrent autoencoder drae models generation content sequences collaborative filtering setting model generalizes recent advances recurrent deep learning input non based input provides new denoising scheme along novel learnable pooling scheme recurrent autoencoder first develop hierarchical bayesian model drae generalize setting synergy denoising enables crae make accurate recommendations learning fill blanks sequences experiments real world datasets different domains citeulike netflix show jointly modeling order aware generation sequences content information performing ratings crae able significantly outperform state art recommendation task based ratings sequence generation task based content information
finding significant combinations features presence categorical covariates high dimensional settings number features typically much larger number samples methods systematically examine arbitrary combinations features huge dimensional space recently begun explored however none current methods able assess association feature combinations target variable conditioning categorical covariate order correct potential confounding effects propose fast automatic conditional search facs algorithm significant discriminative itemset mining method conditions categorical covariates scales log number states categorical covariate based cochran mantel haenszel test facs demonstrates superior speed statistical power simulated real world datasets compared state art opening door numerous applications biomedicine
synthesizing preferred inputs neurons neural networks via deep generator networks deep neural networks dnns demonstrated state art results many pattern recognition tasks especially vision classification problems understanding inner workings computational brains fascinating basic science interesting right similar study human brain enable researchers improve dnns path understanding neural network functions internally study neurons learned detect method called activation maximization synthesizes input image highly activates neuron dramatically improve qualitative state art activation maximization harnessing powerful learned prior deep generator network algorithm generates qualitatively state art synthetic images look almost real reveals features learned neuron interpretable way generalizes well new datasets somewhat well different network architectures without requiring prior relearned considered high quality generative method case generating novel creative interesting recognizable images
learning infinite rbms frank wolfe work propose infinite restricted boltzmann machine rbm whose maximum likelihood estimation mle corresponds constrained convex optimization consider frank wolfe algorithm solve program provides sparse solution interpreted inserting hidden unit iteration optimization process takes form sequence finite models increasing complexity side benefit used easily efficiently identify appropriate number hidden units optimization resulting model also used initialization typical state art rbm training algorithms contrastive divergence leading models consistently higher test likelihood random initialization
sorting typicality inverse moment matrix sos polynomial study surprising phenomenon related representation cloud data points using polynomials start previously unnoticed empirical observation given collection cloud data points sublevel sets certain distinguished polynomial capture shape cloud accurately distinguished polynomial sum squares sos derived simple manner inverse empirical moment matrix fact sos polynomial directly related orthogonal polynomials christoffel function allows generalize interpret extremality properties orthogonal polynomials provide mathematical rationale observed phenomenon among diverse potential applications illustrate relevance results network intrusion detection task obtain performances similar existing dedicated methods reported literature
improving pac exploration using median means present first application median means pac exploration algorithm mdps using median means allows significantly reduce dependence bounds range values value function take introducing dependence potentially much smaller variance bellman operator additionally algorithm first algorithm pac bounds applied mdps unbounded rewards
reconstructing parameters spreading models partial observations spreading processes often modelled stochastic dynamics occurring top given network edge weights corresponding transmission probabilities knowledge veracious transmission probabilities essential prediction optimization control diffusion dynamics unfortunately cases transmission rates unknown need reconstructed spreading data moreover realistic settings impossible monitor state node every time thus data highly incomplete introduce efficient dynamic message passing algorithm able reconstruct parameters spreading model given partial information activation times nodes network method generalizable large class dynamic models well case temporal graphs
dynamic filter networks traditional convolutional layer learned filters stay fixed training contrast introduce new framework dynamic filter network filters generated dynamically conditioned input show architecture powerful increased flexibility thanks adaptive nature yet without excessive increase number model parameters wide variety filtering operation learned way including local spatial transformations also others like selective blurring adaptive feature extraction moreover multiple layers combined recurrent architecture demonstrate effectiveness dynamic filter network tasks video stereo prediction reach state art performance moving mnist dataset much smaller model visualizing learned filters illustrate network picked flow information looking unlabelled training data suggests network used pretrain networks various supervised tasks unsupervised way like optical flow depth estimation
generating long term trajectories using deep hierarchical networks study problem modeling spatiotemporal trajectories long time horizons using expert demonstrations instance sports agents often choose action sequences long term goals mind achieving certain strategic position conventional policy learning approaches based markov decision processes generally fail learning cohesive long term behavior high dimensional state spaces effective fairly myopic decision making yields desired behavior key difficulty conventional models single scale learn single state action policy instead propose hierarchical policy class automatically reasons long term short term goals instantiate hierarchical neural network showcase approach case study learning imitate demonstrated basketball trajectories show generates significantly realistic trajectories compared non hierarchical baselines judged professional sports analysts
cooperative inverse reinforcement learning autonomous system helpful humans pose unwarranted risks needs align values humans environment way actions contribute maximization value humans propose formal definition value alignment problem cooperative inverse reinforcement learning cirl cirl problem cooperative partial information game agents human robot rewarded according human reward function robot initially know contrast classical irl human assumed act optimally isolation optimal cirl solutions produce behaviors active teaching active learning communicative actions effective achieving value alignment show computing optimal joint policies cirl games reduced solving pomdp prove optimality isolation suboptimal cirl derive approximate cirl algorithm
review networks caption generation propose novel extension encoder decoder framework called review network review network generic enhance existing encoder decoder model paper consider rnn decoders cnn rnn encoders review network performs number review steps attention mechanism encoder hidden states outputs thought vector review step thought vectors used input attention mechanism decoder show conventional encoder decoders special case framework empirically show framework improves state art encoder decoder systems tasks image captioning source code captioning
gradient based sampling adaptive importance sampling least squares modern data analysis random sampling efficient widely used strategy overcome computational difficulties brought large sample size previous studies researchers conducted random sampling according input data independent response variable however response variable also informative sampling paper propose adaptive sampling called gradient based sampling dependent input data output fast solving least square problems draw data points random sampling full data according gradient values sampling computationally saving since running time computing sampling probabilities reduced full sample size dimension input theoretically establish error bound analysis general importance sampling respect solution full data result establishes improved performance use gradient based sampling synthetic real data sets used empirically argue gradient based sampling obvious advantage existing sampling methods aspects statistical efficiency computational saving
robust means theoretical revisit last years many variations quadratic means clustering procedure proposed aiming robustify performance algorithm presence outliers general terms main approaches developed based penalized regularization methods based trimming functions work present theoretical analysis robustness consistency properties variant classical quadratic means algorithm robust means borrows ideas outlier detection regression show outliers dataset enough breakdown clustering procedure however focus well structured datasets robust means recover underlying cluster structure spite outliers finally show slight modifications general non asymptotic results consistency quadratic means remain valid robust variant
boosting abstention present new boosting algorithm key scenario binary classification abstention algorithm abstain predicting label point price fixed cost round algorithm selects pair functions base predictor base abstention function define convex upper bounds natural loss function associated problem prove calibrated respect bayes solution algorithm benefits general margin based learning guarantees derive ensembles pairs base predictor abstention functions terms rademacher complexities corresponding function classes give convergence guarantees algorithm along linear time weak learning algorithm abstention stumps also report results several experiments suggesting algorithm provides significant improvement practice confidence based algorithms
estimating class prior posterior noisy positives unlabeled data develop classification algorithm estimating posterior distributions positive unlabeled data robust noise positive labels effective high dimensional data recent years several algorithms proposed learn positive unlabeled data however many contributions remain theoretical performing poorly real high dimensional data typically contaminated noise build previous work develop practical classification algorithms explicitly model noise positive labels utilize univariate transforms built discriminative classifiers prove univariate transforms preserve class prior enabling estimation univariate space avoiding kernel density estimation high dimensional data theoretical development parametric nonparametric algorithms proposed constitute important step towards wide spread use robust classification algorithms positive unlabeled data
bootstrap model aggregation distributed statistical learning distributed privacy preserving learning often given set probabilistic models estimated different local repositories asked combine single model gives efficient statistical estimation simple method linearly average parameters local models however tends degenerate applicable non convex models models different parameter dimensions practical strategy generate bootstrap samples local models learn joint model based combined bootstrap set unfortunately bootstrap procedure introduces additional noise significantly deteriorate performance work propose variance reduction methods correct bootstrap noise including weighted estimator statistically efficient practically powerful theoretical empirical analysis provided demonstrate methods
noise tolerant life long matrix completion via adaptive sampling study problem recovering incomplete times matrix rank columns arriving online time known problem life long matrix completion widely applied recommendation system computer vision system identification etc challenge design provable algorithms tolerant large amount noises small sample complexity work give algorithms achieving strong guarantee realistic noise models bounded deterministic noise adversary add bounded yet unstructured noise column problem present algorithm returns matrix small error sample complexity almost small best prior results noiseless case sparse random noise corrupted columns sparse drawn randomly give algorithm exactly recovers mu_0 incoherent matrix probability least delta sample complexity small mu_0rn log delta result advances state art work matches lower bound worst case also study scenario hidden matrix lies mixture subspaces show sample complexity even smaller proposed algorithms perform well experimentally synthetic real world datasets
fpnn field probing neural networks data building discriminative representations data important task computer graphics computer vision research convolutional neural networks cnns shown operate images great success variety tasks lifting convolution operators 3dcnns seems like plausible promising next step unfortunately computational complexity cnns grows cubically respect voxel resolution moreover since geometry representations boundary based occupied regions increase proportionately size discretization resulting wasted computation work represent spaces volumetric fields propose novel design employs field probing filters efficiently extract features field probing filter set probing points sensors perceive space learning algorithm optimizes weights associated probing points also locations deforms shape probing filters adaptively distributes space optimized probing points sense space intelligently rather operating blindly entire domain show field probing significantly efficient 3dcnns providing state art performance classification tasks object recognition benchmark datasets
causal meets submodular subset selection directed information study causal subset selection directed information measure prediction causality typical tasks causal sensor placement covariate selection correspondingly formulated cardinality constrained directed information maximizations attack hard problems show first problem submodular necessarily monotonic second nearly submodular substantiate idea approximate submodularity introduce novel quantity namely submodularity index smi general set functions moreover show based smi greedy algorithm performance guarantee maximization possibly non monotonic non submodular functions justifying usage much broader class problems evaluate theoretical results several case studies also illustrate application subset selection causal structure learning
improving variational autoencoders inverse autoregressive flow propose simple scalable method improving flexibility variational inference transformation autoregressive neural networks autoregressive neural networks rnns pixelcnn powerful models potentially interesting use variational posterior approximation however ancestral sampling networks long sequential operation therefore typically slow modern parallel hardware gpus show inverting autoregressive neural networks obtain equally powerful posterior models sample efficiently modern hardware show data transformations inverse autoregressive flows iaf used transform simple distribution latent variables much flexible distribution still allowing compute resulting variables probability density function method simple implement made arbitrarily flexible contrast previous work well applicable models high dimensional latent spaces convolutional generative models method applied novel deep architecture variational auto encoders experiments natural images demonstrate autoregressive flow leads significant performance gains
adaptive smoothed online multi task learning paper addresses challenge jointly learning per task model parameters inter task relationships multi task online learning setting proposed algorithm features probabilistic interpretation efficient updating rules flexible modulation whether learners focus specific task jointly address tasks paper also proves sub linear regret bound compared best linear predictor hindsight experiments multi task learning benchmark datasets show advantageous performance proposed approach several state art online multi task learning baselines
limits learning missing data study regression classification setting learning algorithm allowed access limited number attributes per example known limited attribute observation model well studied model provide first lower bounds giving limit precision attainable algorithm several variants regression notably linear regression absolute loss squared loss well classification hinge loss complement lower bounds general purpose algorithm gives upper bound achievable precision limit setting learning missing data
safe exploration finite markov decision processes gaussian processes classical reinforcement learning agents accept arbitrary short term loss long term gain exploring environment infeasible safety critical applications robotics even single unsafe action cause system failure harm environment paper address problem safely exploring finite markov decision processes mdp define safety terms priori unknown safety constraint depends states actions satisfies certain regularity conditions expressed via gaussian process prior develop novel algorithm safemdp task prove completely explores safely reachable part mdp without violating safety constraint achieve cautiously explores safe states actions order gain statistical confidence safety unvisited state action pairs noisy observations collected navigating environment moreover algorithm explicitly considers reachability exploring mdp ensuring get stuck state safe way demonstrate method digital terrain models task exploring unknown map rover
sparse support recovery non smooth loss functions paper study support recovery guarantees underdetermined sparse regression using ell_1 norm regularizer non smooth loss function data fidelity precisely focus detail cases ell_1 ell_ infty losses contrast usual ell_2 loss losses routinely used account either sparse ell_1 loss uniform ell_ infty loss noise models theoretical analysis performance still lacking article extend existing theory smooth ell_2 case non smooth cases derive sharp condition ensures support vector recover stable small additive noise observations long loss constraint size tuned proportionally noise level distinctive feature theory also explains happens support unstable support stable anymore identify extended support show extended support stable small additive noise exemplify usefulness theory give detailed numerical analysis support stability instability compressed sensing recovery different losses highlights different parameter regimes ranging total support stability progressively increasing support instability
crowdsourced clustering querying edges triangles consider task clustering items using answers non expert crowd workers cases workers often able label items directly however reasonable assume compare items judge whether similar important question queries make compare types random edge queries pair items revealed random triangles triple since far expensive query possible edges triangles need work partial observations subject fixed query budget constraint generative model data available consider determine cost query entropy models exist use average response time per query workers surrogate cost addition theoretical justification several simulations experiments real data sets amazon mechanical turk empirically demonstrate fixed budget triangle queries uniformly outperform edge queries even though contrast edge queries triangle queries reveal dependent edges provide reliable edges fixed budget many also provide sufficient condition number observations edge densities inside outside clusters minimum cluster size required exact recovery true adjacency matrix via triangle queries using convex optimization based clustering algorithm
dual decomposed learning factorwise oracle structural svm large output domain many applications machine learning involve structured output large domain learning structured predictor prohibitive due repetitive calls expensive inference oracle work show decomposing training structural support vector machine svm series multiclass svm problems connected messages replace expensive structured oracle factorwise maximization oracle fmo allows efficient implementation complexity sublinear factor domain greedy direction method multiplier gdmm algorithm proposed exploit sparsity messages guarantees epsilon sub optimality log epsilon passes fmo calls conduct experiments chain structured problems fully connected problems large output domains proposed approach orders magnitude faster state art training algorithms structural svm
sampling bayesian program learning towards learning programs data introduce problem sampling programs posterior distributions conditioned data within setting propose algorithm uses symbolic solver efficiently sample programs proposal combines constraint based program synthesis sampling via random parity constraints give theoretical guarantees well samples approximate true posterior empirical results showing algorithm efficient practice evaluating approach program learning problems domains text editing computer aided programming
multiple play bandits position based model sequentially learning place items multi position displays lists task cast multiple play semi bandit setting however major concern context system cannot decide whether user feedback item actually exploitable indeed much content simply ignored user present work proposes exploit available information regarding display position bias called position based click model pbm first discuss model differs cascade model variants considered several recent works multiple play bandits provide novel regret lower bound model well computationally efficient algorithms display good empirical theoretical performance
image restoration using deep convolutional encoder decoder networks symmetric skip connections paper propose deep fully convolutional encoding decoding framework image restoration denoising super resolution network composed multiple layers convolution deconvolution operators learning end end mappings corrupted images original ones convolutional layers act feature extractor capture abstraction image contents eliminating noises corruptions deconvolutional layers used recover image details propose symmetrically link convolutional deconvolutional layers skip layer connections training converges much faster attains higher quality local optimum first skip connections allow signal back propagated bottom layers directly thus tackles problem gradient vanishing making training deep networks easier achieving restoration performance gains consequently second skip connections pass image details convolutional layers deconvolutional layers beneficial recovering original image significantly large capacity handle different levels noises using single model experimental results show network achieves better performance recent state art methods
optimistic bandit convex optimization introduce general powerful scheme predicting information use optimization algorithms allows devise computationally efficient algorithm bandit convex optimization new state art guarantees lipschitz loss functions loss functions lipschitz gradients first algorithm admitting polynomial time complexity regret polynomial dimension action space improves upon original regret bound lipschitz loss functions achieving regret widetilde algorithm improves upon best existing polynomial dimension bound computationally terms regret loss functions lipschitz gradients achieving regret widetilde
computing maximizing influence linear threshold triggering models establish upper lower bounds influence set nodes certain types contagion models derive sets bounds first designed linear threshold models second broadly applicable general class triggering models subsumes popular independent cascade models well quantify gap upper lower bounds case linear threshold model illustrate gains upper bounds independent cascade models relation existing results importantly lower bounds monotonic submodular implying greedy algorithm influence maximization guaranteed produce maximizer within factor truth although problem exact influence computation hard general bounds evaluated efficiently leads attractive highly scalable algorithm influence maximization rigorous theoretical guarantees
clustering bregman divergences asymptotic analysis clustering particular means clustering central topic data analysis clustering bregman divergences recently proposed generalization means clustering already widely used applications paper analyze theoretical properties bregman clustering number clusters large establish quantization rates describe limiting distribution centers infty extending well known results means clustering
community detection evolving graphs clustering fundamental step many information retrieval data mining applications detecting clusters graphs also key tool finding community structure social behavioral networks many applications input graph evolves time continual decentralized manner maintain good clustering clustering algorithm needs repeatedly probe graph furthermore often limitations frequency probes either imposed explicitly online platform case crawling proprietary social networks like twitter implicitly resource limitations case crawling web paper study model clustering evolving graphs captures aspect problem model based classical stochastic block model used assess rigorously quality various static clustering methods model algorithm supposed reconstruct planted clustering given ability query small pieces local information graph limited rate design analyze clustering algorithms work model show asymptotically tight upper lower bounds accuracy finally perform simulations demonstrate main asymptotic results hold true also practice
dueling bandits beyond condorcet winners general tournament solutions recent work deriving log anytime regret bounds stochastic dueling bandit problems considered mostly condorcet winners always exist recently winners defined copeland set always exist work consider broad notion winners defined tournament solutions social choice theory include copeland set special case also include several notions winners top cycle uncovered set banks set like copeland set always exist develop family ucb style dueling bandit algorithms general tournament solutions show log anytime regret bounds experiments confirm ability algorithms achieve low regret relative target winning set interest
learning metric embedding face recognition using multibatch method work motivated engineering task achieving near state art face recognition minimal computing budget running embedded system main technical contribution centers around novel training method called multibatch similarity learning task generating invariant face signature training pairs face images multibatch method first generates signatures mini batch face images constructs unbiased estimate full gradient relying pairs mini batch prove variance multibatch estimator bounded mild conditions contrast standard gradient estimator relies random pairs variance order smaller variance multibatch estimator significantly speeds convergence rate stochastic gradient descent using multibatch method train deep convolutional neural network achieves accuracy lfw benchmark prediction runtime takes msec single arm cortex core furthermore entire training process took hours single titan gpu
convergence guarantees kernel based quadrature rules misspecified settings kernel based quadrature rules becoming important machine learning statistics achieve super sqrt convergence rates numerical integration thus provide alternatives monte carlo integration challenging settings integrands expensive evaluate integrands high dimensional rules based assumption integrand certain degree smoothness expressed integrand belongs certain reproducing kernel hilbert space rkhs however assumption violated practice integrand black box function general theory established convergence kernel quadratures misspecified settings contribution proving kernel quadratures consistent even integrand belong assumed rkhs integrand less smooth assumed specifically derive convergence rates depend unknown lesser smoothness integrand degree smoothness expressed via powers rkhss via sobolev spaces
stochastic variational deep kernel learning deep kernel learning combines non parametric flexibility kernel methods inductive biases deep learning architectures propose novel deep kernel learning model stochastic variational inference procedure generalizes deep kernel learning approaches enable classification multi task learning additive covariance structures stochastic gradient training specifically apply additive base kernels subsets output features deep neural architectures jointly learn parameters base kernels deep network gaussian process marginal likelihood objective within framework derive efficient form stochastic variational inference leverages local kernel interpolation inducing points structure exploiting algebra show improved performance stand alone deep networks svms state art scalable gaussian processes several classification benchmarks including airline delay dataset containing million training points cifar imagenet
deep submodular functions definitions learning propose study new class submodular functions called deep submodular functions dsfs define dsfs situate within broader context classes submodular functions relationship various matroid ranks sums concave composed modular functions scms notably find dsfs constitute strictly broader class scms thus motivating use comprise submodular functions interestingly dsfs seen special cases certain deep neural networks dnns hence name finally provide method learn dsfs max margin framework offer preliminary results applying synthetic real world data instances
scaled least squares estimator glms large scale problems study problem efficiently estimating coefficients generalized linear models glms large scale setting number observations much larger number predictors show glms random necessarily gaussian design glm coefficients approximately proportional corresponding ordinary least squares ols coefficients using relation design algorithm achieves accuracy maximum likelihood estimator mle iterations attain cubic convergence rate cheaper batch optimization algorithm least factor mathcal provide theoretical guarantees algorithm analyze convergence behavior terms data dimensions finally demonstrate performance algorithm extensive numerical studies large scale real synthetic datasets show achieves highest performance compared several widely used optimization algorithms
high rank matrix completion clustering self expressive models propose efficient algorithms simultaneous clustering completion incomplete high dimensional data lie union low dimensional subspaces cast problem finding completion data matrix point reconstructed linear affine combination data points since problem hard propose lifting framework reformulate problem group sparse recovery incomplete data point dictionary built using incomplete data subject rank constraints solve problem efficiently propose rank pursuit algorithm convex relaxation solution algorithms recover missing entries provides similarity matrix clustering algorithms deal low rank high rank matrices suffer initialization need know dimensions subspaces work small number data points extensive experiments synthetic data real problems video motion segmentation completion motion capture data show data matrix low rank algorithm performs par better low rank matrix completion methods high rank data matrices method significantly outperforms existing algorithms
stochastic composite convex minimization propose stochastic optimization method minimization sum convex functions lipschitz continuous gradient well restricted strong convexity approach suitable setting computationally advantageous process smooth term decomposition stochastic gradient estimate functions separately proximal operators doubly regularized empirical risk minimization problems prove convergence characterization proposed algorithm expectation standard assumptions stochastic gradient estimate smooth term method operates primal space considered stochastic extension operator splitting method finally numerical evidence supports effectiveness method real world problems
tree structured reinforcement learning sequential object localization existing object proposal algorithms usually search possible object regions multiple locations scales emph separately ignore interdependency among different objects deviate human perception procedure incorporate global interdependency objects object localization propose effective tree structured reinforcement learning tree approach sequentially search objects fully exploiting current observation historical search paths tree approach learns multiple searching policies maximizing long term reward reflects localization accuracies objects starting taking entire image proposal tree approach allows agent sequentially discover multiple objects via tree structured traversing scheme allowing multiple near optimal policies tree offers diversity search paths able find multiple objects single feed forward pass therefore tree better cover different objects various scales quite appealing context object proposal experiments pascal voc 2007 2012 validate effectiveness tree achieve comparable recalls current object proposal algorithms via much fewer candidate windows
non convex burer monteiro approach works smooth semidefinite programs semidefinite programs sdp solved polynomial time interior point methods scalability issue address shortcoming decade ago burer monteiro proposed solve sdp equality constraints via rank restricted non convex surrogates remarkably applications local optimization methods seem converge global optima non convex surrogates reliably although theory supports empirical success complete explanation remains open question paper consider class sdp includes applications max cut community detection stochastic block model robust pca phase retrieval synchronization rotations show low rank burer monteiro formulation sdp class almost never spurious local optima
neurons equipped intrinsic plasticity learn stimulus intensity statistics experience constantly shapes neural circuits variety plasticity mechanisms functional roles plasticity mechanisms well understood remains unclear changes neural excitability contribute learning develop normative interpretation intrinsic plasticity key component unsupervised learning introduce novel generative mixture model accounts class specific statistics stimulus intensities derive neural circuit learns input classes intensities analytically show inference learning generative model achieved neural circuit intensity sensitive neurons equipped specific form numerical experiments verify analytical derivations show robust behavior artificial natural stimuli results link non trivial input statistics particular statistics stimulus intensities classes neuron sensitive generally work paves way toward new classification algorithms robust intensity variations
greedy feature construction present effective method supervised feature construction main goal approach construct feature representation set linear hypotheses sufficient capacity large enough contain satisfactory solution considered problem small enough allow good generalization small number training examples achieve goal greedy procedure constructs features empirically fitting squared error residuals proposed constructive procedure consistent output rich set features effectiveness approach evaluated empirically fitting linear ridge regression model constructed feature space empirical results indicate superior performance approach competing methods
dynamic mode decomposition reproducing kernels koopman spectral analysis spectral analysis koopman operator infinite dimensional linear operator observable gives modal description global behavior nonlinear dynamical system without explicit prior knowledge governing equations paper consider spectral analysis koopman operator reproducing kernel hilbert space rkhs propose modal decomposition algorithm perform analysis using finite length data sequences generated nonlinear system algorithm essence reduced calculation set orthogonal bases krylov matrix rkhs eigendecomposition projection koopman operator onto subspace spanned bases algorithm returns decomposition dynamics finite number modes thus thought feature extraction procedure nonlinear dynamical system therefore consider applications machine learning using extracted features presented analysis illustrate method applications using synthetic real world data
learning number neurons deep networks nowadays number layers neurons layer deep network typically set manually deep wide networks proven effective general come high memory computation cost thus making impractical constrained platforms networks however known many redundant parameters could thus principle replaced compact architectures paper introduce approach automatically determining number neurons layer deep network learning end propose make use group sparsity regularizer parameters network group defined act single neuron starting overcomplete network show approach reduce number parameters retaining even improving network accuracy
strategic attentive writer learning macro actions present novel deep recurrent neural network architecture learns build implicit plans end end manner purely interacting environment reinforcement learning setting network builds internal plan continuously updated upon observation next input environment also partition internal representation contiguous sub sequences learning long plan committed followed without replaning combining properties proposed model dubbed strategic attentive writer straw learn high level temporally abstracted macro actions varying lengths solely learnt data without prior information macro actions enable structured exploration economic computation experimentally demonstrate straw delivers strong improvements several atari games employing temporally extended planning strategies pacman frostbite time general algorithm applied sequence data end also show trained text prediction task straw naturally predicts frequent grams instead macro actions demonstrating generality approach
active learning imperfect labelers study active learning labeler return incorrect labels also abstain labeling consider different noise abstention conditions labeler propose algorithm utilizes abstention responses analyze statistical consistency query complexity fairly natural assumptions noise abstention rate labeler algorithm adaptive sense automatically request less queries informed less noisy labeler couple algorithm lower bounds show technical conditions achieves nearly optimal query complexity
probabilistic linear multistep methods present derivation theoretical investigation adams bashforth adams moulton family linear multistep methods solving ordinary differential equations starting gaussian process framework limit formulation coincides classical deterministic methods used higher order initial value problem solvers century furthermore natural probabilistic framework provided formulation allows derive probabilistic versions methods spirit number probabilistic ode solvers presented recent literature contrast higher order runge kutta methods require multiple intermediate function evaluations per step adams family methods make use previous function evaluations increased accuracy arising higher order multistep approach comes little additional computational cost show careful choice covariance function posterior mean standard deviation numerical solution made exactly coincide value given deterministic method local truncation error respectively provide rigorous proof convergence new methods well empirical investigation fifth order demonstrating convergence rates practice
supervision less computation statistical computational tradeoffs weakly supervised learning consider weakly supervised binary classification problem labels randomly flipped probability alpha although exist numerous algorithms problem remains theoretically unexplored statistical accuracies computational efficiency algorithms depend degree supervision quantified alpha paper characterize effect alpha establishing information theoretic computational boundaries namely minimax optimal statistical accuracy achieved algorithms polynomial time algorithms oracle computational model small alpha result shows gap boundaries represents computational price achieving information theoretic boundary due lack supervision interestingly also show gap narrows alpha increases words supervision correct labels improves optimal statistical accuracy expected also enhances computational efficiency achieving accuracy
mutual information symmetric rank matrix estimation proof replica formula factorizing low rank matrices many applications machine learning statistics probabilistic models bayes optimal setting general expression mutual information proposed using heuristic statistical physics computations proven specific cases show rigorously prove conjectured formula symmetric rank case allows express minimal mean square error characterize detectability phase transitions large set estimation problems ranging community detection sparse pca also show large set parameters iterative algorithm called approximate message passing bayes optimal exists however gap currently known polynomial algorithms expected information theoretically additionally proof technique interest exploits essential ingredients interpolation method introduced statistical physics guerra analysis approximate message passing algorithm theory spatial coupling threshold saturation coding approach generic applicable open problems statistical estimation heuristic statistical physics predictions available
coin betting parameter free online learning recent years number parameter free algorithms developed online linear optimization hilbert spaces learning expert advice algorithms achieve optimal regret bounds depend unknown competitors without tune learning rates oracle choices present new intuitive framework design parameter free algorithms online linear optimization hilbert spaces learning expert advice based reductions betting outcomes adversarial coins instantiate using betting algorithm based krichevsky trofimov estimator resulting algorithms simple parameters tuned improve match previous results terms regret guarantee per round complexity
normalized spectral map synchronization algorithmic advancement synchronizing maps important order solve wide range practice problems possible large scale dataset paper provide theoretical justifications spectral techniques map synchronization problem takes input collection objects noisy maps estimated pairs objects outputs clean maps pairs objects show simple normalized spectral method projects blocks top eigenvectors data matrix map space leads surprisingly good results noise modelled naturally random permutation matrix algorithm normspecsync leads competing theoretical guarantees state art convex optimization techniques yet much efficient demonstrate usefulness algorithm couple applications optimal complexity exactness among existing methods
explore commit strategies study problem minimising regret armed bandit problems gaussian rewards objective use simple setting illustrate strategies based exploration phase stopping time followed exploitation necessarily suboptimal results hold regardless whether difference means arms known besides main message also refine existing deviation inequalities allow design fully sequential strategies finite time regret guarantees asymptotically optimal horizon grows order optimal minimax sense furthermore provide empirical evidence theory also holds practice discuss extensions non gaussian multiple armed case
learning kernels random features randomized features provide computationally efficient way approximate kernel machines machine learning tasks however methods require user defined kernel input extend randomized feature approach task learning kernel via associated random features specifically present efficient optimization problem learns kernel supervised manner prove consistency estimated kernel well generalization bounds class estimators induced optimized kernel experimentally evaluate technique several datasets approach efficient highly scalable attain competitive results fraction training cost techniques
robustness classifiers adversarial random noise several recent works shown state art classifiers vulnerable worst case adversarial perturbations datapoints hand empirically observed classifiers relatively robust random noise paper propose study semi random noise regime generalizes random worst case noise regimes propose first quantitative analysis robustness nonlinear classifiers general noise regime establish precise theoretical bounds robustness classifiers general regime depend curvature classifier decision boundary bounds confirm quantify empirical observations classifiers satisfying curvature constraints robust random noise moreover quantify robustness classifiers terms subspace dimension semi random noise regime show bounds remarkably interpolate worst case random noise regimes perform experiments show derived bounds provide accurate estimates applied various state art deep neural networks datasets result suggests bounds curvature classifiers decision boundaries support experimentally generally offers important insights onto geometry high dimensional classification problems
adaptive skills adaptive partitions asap introduce adaptive skills adaptive partitions asap framework learns skills temporally extended actions options well apply believe necessary truly general skill learning framework key building block needed scale lifelong learning agents asap framework also able solve related new tasks simply adapting applies existing learned skills prove asap converges local optimum natural conditions finally experimental results include robocup domain demonstrate ability asap learn reuse skills well solve multiple tasks considerably less experience solving task scratch
gaussian process bandit optimisation multi fidelity evaluations many scientific engineering applications tasked optimisation expensive evaluate black box function func traditional methods problem assume availability single function however many cases cheap approximations func obtainable example expensive real world behaviour robot approximated cheap computer simulation use approximations eliminate low function value regions cheaply use expensive evaluations func small promising region speedily identify optimum formalise task emph multi fidelity bandit problem target function approximations sampled gaussian process develop mfgpucb novel method based upper confidence bound techniques theoretical analysis demonstrate exhibits precisely behaviour achieves better regret strategies ignore multi fidelity information mfgpucbs outperforms naive strategies multi fidelity methods several synthetic real experiments
flexible models microclustering application entity resolution generative models clustering implicitly assume number data points cluster grows linearly total number data points finite mixture models dirichlet process mixture models pitman yor process mixture models make assumption infinitely exchangeable clustering models however applications assumption inappropriate example performing entity resolution size cluster unrelated size data set cluster contain negligible fraction total number data points applications require models yield clusters whose sizes grow sublinearly size data set address requirement defining microclustering property introducing new class models exhibit property compare models within class commonly used clustering models using entity resolution data sets
stochastic gradient richardson romberg markov chain monte carlo stochastic gradient markov chain monte carlo mcmc algorithms become increasingly popular bayesian inference large scale applications even though methods proved useful several scenarios performance often limited bias study propose novel sampling algorithm aims reduce bias mcmc keeping variance reasonable level approach based numerical sequence acceleration method namely richardson romberg extrapolation simply boils running almost mcmc algorithm twice parallel different step sizes illustrate framework popular stochastic gradient langevin dynamics sgld algorithm propose novel mcmc algorithm referred stochastic gradient richardson romberg langevin dynamics sgrrld provide formal theoretical analysis show sgrrld asymptotically consistent satisfies central limit theorem non asymptotic bias mean squared error bounded results show sgrrld attains higher rates convergence sgld finite time asymptotically achieves theoretical accuracy methods based higher order integrators support findings using synthetic real data experiments
online differentially private tensor decomposition tensor decomposition positioned pervasive tool era big data paper resolve many key algorithmic questions regarding robustness memory efficiency differential privacy tensor decomposition propose simple variants tensor power method enjoy strong properties propose first streaming method linear memory requirement moreover present noise calibrated tensor power method efficient privacy guarantees heart guarantees lies careful perturbation analysis derived paper improves existing results significantly
maximal sparsity deep networks iterations many sparse estimation algorithms comprised fixed linear filter cascaded thresholding nonlinearity collectively resemble typical neural network layer consequently lengthy sequence algorithm iterations viewed deep network shared hand crafted layer weights therefore quite natural examine degree learned network model might act viable surrogate traditional sparse estimation domains ample training data available possibility reduced computational budget readily apparent ceiling imposed number layers work primarily focuses estimation accuracy particular well known signal dictionary coherent columns quantified large rip constant tractable iterative algorithms unable find maximally sparse representations contrast demonstrate theoretically empirically potential trained deep network recover minimal ell_0 norm representations regimes existing methods fail resulting system effectively learn novel iterative sparse estimation algorithms deployed practical photometric stereo estimation problem goal remove sparse outliers disrupt estimation surface normals scene
efficient high order interaction aware feature selection based conditional mutual information study introduces novel feature selection approach cmicot evolution filter methods sequential forward selection sfs whose scoring functions based conditional mutual information state study novel saddle point max min optimization problem build scoring function able identify joint interactions several features method fills gap based sfs techniques high order dependencies high dimensional case estimation prohibitively high sample complexity mitigate cost using greedy approximation binary representatives makes technique able effectively used superiority approach demonstrated comparison recently proposed interaction aware filters several interaction agnostic state art ones publicly available benchmark datasets
geometric dirichlet means algorithm topic inference propose geometric algorithm topic learning inference built convex geometry topics arising latent dirichlet allocation lda model nonparametric extensions end study optimization geometric loss function surrogate lda likelihood method involves fast optimization based weighted clustering procedure augmented geometric corrections overcomes computational statistical inefficiencies encountered techniques based gibbs sampling variational inference achieving accuracy comparable gibbs sampler topic estimates produced method shown statistically consistent conditions algorithm evaluated extensive experiments simulated real data
interaction screening efficient sample optimal learning ising models consider problem learning underlying graph unknown ising model spins collection samples generated model suggest new estimator computationally efficient requires number samples near optimal respect previously established information theoretic lower bound statistical estimator physical interpretation terms interaction screening estimator consistent efficiently implemented using convex optimization prove appropriate regularization estimator recovers underlying graph using number samples logarithmic system size exponential maximum coupling intensity maximum node degree
multi armed bandits competing optimal sequences consider sequential decision making problem adversarial setting regret measured respect optimal sequence actions feedback adheres bandit setting well known obtaining sublinear regret setting impossible general arises question better linear regret previous works show environment guaranteed vary slowly furthermore given prior knowledge regarding variation limit amount changes suffered environment task feasible caveat however prior knowledge likely available practice causes obtained regret bounds somewhat irrelevant main result regret guarantee scales variation parameter environment without requiring prior knowledge whatsoever also resolve open problem posted gur zeevi besbes nips important key component result statistical test identifying non stationarity sequence independent random variables test either identifies non stationarity upper bounds absolute deviation corresponding sequence mean values terms total variation test interesting right potential found useful additional settings
catching heuristics optimal control policies seemingly contradictory theories attempt explain humans move intercept airborne ball theory posits humans predict ball trajectory optimally plan future actions claims instead performing complicated computations humans employ heuristics reactively choose appropriate actions based immediate visual feedback paper show interception strategies appearing heuristics understood computational solutions optimal control problem faced ball catching agent acting uncertainty modeling catching continuous partially observable markov decision process employing stochastic optimal control theory discover main heuristics described literature optimal solutions catcher sufficient time continuously visually track ball specifically varying model parameters noise time ground contact perceptual latency show different strategies arise different circumstances catcher policy switches generating reactive predictive behavior based ratio system observation noise ratio reaction time task duration thus provide rational account human ball catching behavior unifying explanation seemingly contradictory theories target interception basis stochastic optimal control
riemannian svrg fast stochastic optimization riemannian manifolds study optimization finite sums emph geodesically smooth functions riemannian manifolds although variance reduction techniques optimizing finite sums witnessed tremendous attention recent years existing work limited vector space problems introduce emph riemannian svrg rsvrg new variance reduced riemannian optimization method analyze rsvrg geodesically emph convex emph nonconvex smooth functions analysis reveals rsvrg inherits advantages usual svrg method factors depending curvature manifold influence convergence knowledge rsvrg first emph provably fast stochastic riemannian method moreover paper presents first non asymptotic complexity analysis novel even batch setting nonconvex riemannian optimization results several implications instance offer riemannian perspective variance reduced pca promises short transparent convergence analysis
comprehensive linear speedup analysis asynchronous stochastic parallel optimization zeroth order first order asynchronous parallel optimization received substantial successes extensive attention recently core theoretical questions much speedup benefit asynchronous parallelization bring paper provides comprehensive generic analysis study speedup property broad range asynchronous parallel stochastic algorithms zeroth order first order methods result recovers improves existing analysis special cases provides insights understanding asynchronous parallel behaviors suggests novel asynchronous parallel zeroth order method first time experiments provide novel applications proposed asynchronous parallel zeroth order method hyper parameter tuning model blending problems
stochastic gradient mcmc stale gradients stochastic gradient mcmc mcmc played important role large scale bayesian learning well developed theoretical convergence properties applications mcmc becoming increasingly popular employ distributed systems stochastic gradients computed based outdated parameters yielding termed stale gradients stale gradients could directly used mcmc impact convergence properties well studied paper develop theory show bias mse mcmc algorithm depend staleness stochastic gradients estimation variance relative expected estimate based prescribed number samples independent simple bayesian distributed system mcmc stale gradients computed asynchronously set workers theory indicates linear speedup decrease estimation variance number workers experiments synthetic data deep neural networks validate theory demonstrating effectiveness scalability mcmc stale gradients
disentangling factors variation deep representation using adversarial training propose deep generative model learning distill hidden factors variation within set labeled observations complementary codes code describes factors variation relevant solving specified task code describes remaining factors variation irrelevant solving task available source supervision training process comes ability distinguish among different observations belonging category concrete examples include multiple images object different viewpoints multiple speech samples speaker instances factors variation irrelevant classification implicitly expressed intra class variabilities relative position object image linguistic content utterance existing approaches solving problem rely heavily access pairs observations sharing single factor variation different objects observed exact conditions assumption often encountered realistic settings data acquisition controlled labels uninformative components available work propose overcome limitation augmenting deep convolutional autoencoders form adversarial training factors variation implicitly captured organization learned embedding space used solving single image analogies experimental results synthetic real datasets show proposed method capable disentangling influences style content factors using flexible representation well generalizing unseen styles content classes
consistent kernel mean estimation functions random variables provide theoretical foundation non parametric estimation functions random variables using kernel mean embeddings show continuous function consistent estimators mean embedding random variable lead consistent estimators mean embedding matern kernels sufficiently smooth functions also provide rates convergence results extend functions multiple random variables variables dependent require estimator mean embedding joint distribution starting point independent sufficient separate estimators mean embeddings marginal distributions either case results cover mean embeddings based samples well reduced set expansions terms dependent expansion points latter serves justification using expansions limit memory resources applying approach basis probabilistic programming
decorrelated feature space partitioning distributed sparse regression fitting statistical models computationally challenging sample size dimension dataset huge attractive approach scaling problem size first partition dataset subsets fit using distributed algorithms dataset partitioned either horizontally sample space vertically feature space majority literature focuses sample space partitioning feature space partitioning effective existing methods partitioning features however either vulnerable high correlations inefficient reducing model dimension paper solve problems new embarrassingly parallel framework named deco distributed variable selection parameter estimation deco variables first partitioned allocated distributed workers decorrelated subset data within worker fitted via algorithm designed high dimensional problems show incorporating decorrelation step deco achieve consistent variable selection parameter estimation subset almost assumptions addition convergence rate nearly minimax optimal sparse weakly sparse models depend partition number extensive numerical experiments provided illustrate performance new framework
coupled generative adversarial networks propose coupled generative adversarial nets cogan framework generating pairs corresponding images different domains framework consists pair generative adversarial nets responsible generating images domain show enforcing simple weight sharing constraint cogan learns generate pairs corresponding images without existence pairs corresponding images domains training set words cogan learns joint distribution images domains images drawn separately marginal distributions individual domains contrast existing multi modal generative models require corresponding images training apply cogan several pair image generation tasks task cogan learns generate convincing pairs corresponding images demonstrate applications cogan framework domain adaptation cross domain image generation tasks
matching networks shot learning learning examples remains key challenge machine learning despite recent advances important domains vision language standard supervised deep learning paradigm offer satisfactory solution learning new concepts rapidly little data work employ ideas metric learning based deep neural features recent advances augment neural networks external memories framework learns network maps small labelled support set unlabelled example label obviating need fine tuning adapt new class types define shot learning problems vision using omniglot imagenet language tasks algorithm improves shot accuracy imagenet accuracy accuracy omniglot compared competing approaches also demonstrate usefulness model language modeling introducing shot task penn treebank
distributed flexible nonlinear tensor factorization tensor factorization powerful tool analyse multi way data recently proposed nonlinear factorization methods although capable capturing complex relationships computationally quite expensive suffer severe learning bias case extreme data sparsity therefore propose distributed flexible nonlinear tensor factorization model avoids expensive computations structural restrictions kronecker product existing tgp formulations allowing arbitrary subset tensor entries selected training meanwhile derive tractable tight variational evidence lower bound elbo enables highly decoupled parallel computations high quality inference based new bound develop distributed key value free inference algorithm mapreduce framework fully exploit memory cache mechanism fast mapreduce systems spark experiments demonstrate advantages method several state art approaches terms predictive performance computational efficiency
tracking best expert non stationary stochastic environments study dynamic regret multi armed bandit experts problem non stationary stochastic environments introduce new parameter measures total statistical variance loss distributions rounds process study amount affects regret investigate interaction gamma counts number times distributions change well measures far distributions deviates time striking result find even gamma lambda restricted constant regret lower bound bandit setting still grows highlight full information setting constant regret becomes achievable constant gamma lambda made independent constant lambda regret still dependency propose algorithms upper bound guarantee prove matching lower bounds well
deep alternative neural network exploring contexts early possible action recognition contexts crucial action recognition video current methods often mine contexts extracting hierarchical local features focus high order encodings paper instead explores contexts early possible leverages evolutions action recognition particular introduce novel architecture called deep alternative neural network dann stacking alternative layers alternative layer consists volumetric convolutional layer followed recurrent layer former acts local feature learner latter used collect contexts compared feed forward neural networks dann learns contexts local features beginning setting helps preserve hierarchical context evolutions show essential recognize similar actions besides present adaptive method determine temporal size network input based optical flow energy develop volumetric pyramid pooling layer deal input clips arbitrary sizes demonstrate advantages dann benchmarks hmdb51 ucf101 report competitive superior results state art
learning parametric sparse models image super resolution learning accurate prior knowledge natural images great importance single image super resolution existing methods either learn prior low high resolution patch pairs estimate prior models input low resolution image specifically high frequency details learned former methods though effective heuristic limitations dealing blurred images latter suffers limitations frequency aliasing paper propose combine lines ideas image super resolution specifically parametric sparse prior desirable high resolution image patches learned input low resolution image training image dataset learned sparse priors sparse codes thus image patches accurately recovered solving sparse coding problem experimental results show proposed method outperforms existing state art methods terms subjective objective image qualities
kernel observers systems theoretic modeling inference spatiotemporally evolving processes consider problem estimating latent state spatiotemporally evolving continuous function using sensor measurements show layering dynamical systems prior temporal evolution weights kernel model valid approach spatiotemporal modeling necessarily require design complex nonstationary kernels furthermore show predictive model utilized determine sensing locations guarantee hidden state phenomena recovered measurements provide sufficient conditions number spatial location samples required guarantee state recovery provide lower bound minimum number samples required robustly infer hidden states approach outperforms existing methods numerical experiments
learning brain regions via large scale online structured sparse dictionary learning propose multivariate online dictionary learning method obtaining decompositions brain images structured sparse components aka atoms sparsity understood usual sense dictionary atoms constrained contain mostly zeros imposed via ell_1 norm constraint structured mean atoms piece wise smooth compact thus making blobs opposed scattered patterns activation propose use sobolev laplacian penalty impose type structure combining penalties obtain decompositions properly delineate brain structures functional images non trivially extends online dictionary learning work mairal 2010 price factor overall running time like mairal 2010 reference method online nature proposed algorithm allows scale arbitrarily sized datasets experiments brain data show proposed method extracts structured denoised dictionaries intepretable better capture inter subject variability small medium large scale regimes alike compared state art models
scaling factorial hidden markov models stochastic variational inference without messages factorial hidden markov models fhmms powerful models sequential data scale well long sequences propose scalable inference learning algorithm fhmms draws ideas stochastic variational inference neural network copula literatures unlike existing approaches proposed algorithm requires message passing procedure among latent variables distributed network computers speed learning experiments corroborate proposed algorithm introduce approximation bias compared proven structured mean field algorithm achieves better performance long sequences large fhmms
bandit framework strategic regression consider learner problem acquiring data dynamically training regression model training data collected strategic data sources fundamental challenge incentivize data holders exert effort improve quality reported data despite quality directly verifiable learner work study dynamic data acquisition process data holders contribute multiple times using bandit framework leverage long term incentive future job opportunities incentivize high quality contributions propose strategic regression upper confidence bound ucb framework ucb style index combined simple payment rule index worker approximates quality past contributions used learner determine whether worker receives future work linear regression certain family non linear regression problems show ucb enables sqrt log bayesian nash equilibrium bne worker exerting target effort level learner chosen number data acquisition stages ucb framework also desirable properties indexes updated online fashion hence computationally light slight variant namely private ucb psr ucb able preserve log log differential privacy workers data small compromise incentives achieving log sqrt bne
convolutional neural networks graphs fast localized spectral filtering work interested generalizing convolutional neural networks cnns low dimensional regular grids image video speech represented high dimensional irregular domains social networks brain connectomes words embedding represented graphs present formulation cnns context spectral graph theory provides necessary mathematical background efficient numerical schemes design fast localized convolutional filters graphs importantly proposed technique offers linear computational complexity constant learning complexity classical cnns universal graph structure experiments mnist 20news demonstrate ability novel deep learning system learn local stationary compositional features graphs
stein variational gradient descent general purpose bayesian inference algorithm propose general purpose variational inference algorithm forms natural counterpart gradient descent optimization method iteratively transports set particles match target distribution applying form functional gradient descent minimizes divergence empirical studies performed various real world models datasets method competitive existing state art methods derivation method based new theoretical result connects derivative divergence smooth transforms stein identity recently proposed kernelized stein discrepancy independent interest
deep learning models retinal response natural scenes central challenge sensory neuroscience understand neural computations circuit mechanisms underlie encoding ethologically relevant natural stimuli multilayered neural circuits nonlinear processes synaptic transmission spiking dynamics present significant obstacle creation accurate computational models responses natural stimuli demonstrate deep convolutional neural networks cnns capture retinal responses natural scenes nearly within variability cell response markedly accurate linear nonlinear models generalized linear models glms moreover find additional surprising properties cnns less susceptible overfitting counterparts trained small amounts data generalize better tested stimuli drawn different distribution natural scenes white noise examination learned cnns reveals several properties first richer set feature maps necessary predicting responses natural scenes compared white noise second temporally precise responses slowly varying inputs originate feedforward inhibition similar known retinal mechanisms third injection latent noise sources intermediate layers enables model capture sub poisson spiking variability observed retinal ganglion cells fourth augmenting cnns recurrent lateral connections enables capture contrast adaptation emergent property accurately describing retinal responses natural scenes methods readily generalized sensory modalities stimulus ensembles overall work demonstrates cnns accurately capture sensory circuit responses natural scenes also yield information circuit internal structure function
safe efficient policy reinforcement learning work take fresh look old new algorithms policy return based reinforcement learning expressing common form derive novel algorithm retrace lambda desired properties low variance safely uses samples collected behaviour policy whatever degree policyness efficient makes best use samples collected near policy behaviour policies analyse contractive nature related operator policy policy evaluation control settings derive online sample based algorithms believe first return based policy control algorithm converging without glie assumption greedy limit infinite exploration corollary prove convergence watkins lambda open problem since 1989 illustrate benefits retrace lambda standard suite atari 2600 games
yggdrasil optimized system training deep decision trees scale deep distributed decision trees tree ensembles grown importance due need model increasingly large datasets however planet standard distributed tree learning algorithm implemented systems xgboost spark mllib scales poorly data dimensionality tree depths grow present yggdrasil new distributed tree learning method outperforms existing methods 24x unlike planet yggdrasil based vertical partitioning data partitioning feature along set optimized data structures reduce cpu communication costs training yggdrasil trains directly compressed data compressible features labels introduces efficient data structures training uncompressed data minimizes communication nodes using sparse bitvectors moreover planet approximates split points feature binning yggdrasil require binning analytically characterize impact approximation evaluate yggdrasil mnist dataset high dimensional dataset yahoo yggdrasil faster order magnitude
sample complexity automated mechanism design design revenue maximizing combinatorial auctions multi item auctions bundles goods fundamental problems computational economics unsolved even bidders items sale traditional economic models assumed bidders valuations drawn underlying distribution auction designer perfect knowledge distribution despite strong oftentimes unrealistic assumption remarkable revenue maximizing combinatorial auction remains unknown recent years automated mechanism design emerged practical promising approaches designing high revenue combinatorial auctions scalable automated mechanism design algorithms take input samples bidders valuation distribution search high revenue auction rich auction class work provide first sample complexity analysis standard hierarchy deterministic combinatorial auction classes used automated mechanism design particular provide tight sample complexity bounds number samples needed guarantee empirical revenue designed mechanism samples close expected revenue underlying unknown distribution bidder valuations auction classes hierarchy addition helping set automated mechanism design firm foundations results also push boundaries learning theory particular hypothesis functions used contexts defined multi stage combinatorial optimization procedures rather simple decision boundaries common machine learning
deep exploration via bootstrapped dqn efficient exploration remains major challenge reinforcement learning common dithering strategies exploration epsilon greedy carry temporally extended deep exploration lead exponentially larger data requirements however algorithms statistically efficient computationally tractable complex environments randomized value functions offer promising approach efficient exploration generalization existing algorithms compatible nonlinearly parameterized value functions first step towards addressing contexts develop bootstrapped dqn demonstrate bootstrapped dqn combine deep exploration deep neural networks exponentially faster learning dithering strategy arcade learning environment bootstrapped dqn substantially improves learning speed cumulative performance across games
search improves label active learning investigate active learning access distinct oracles label standard search search oracle models situation human searches database seed counterexample existing solution search stronger label natural implement many situations show algorithm using oracles provide exponentially large problem dependent improvements label alone
efficient robust spiking neural circuit navigation inspired echolocating bats demonstrate spiking neural circuit azimuth angle detection inspired echolocation circuits horseshoe bat rhinolophus ferrumequinum utilize devise model navigation target tracking capturing several key aspects information transmission biology network using simple local information based sensor implementing cardioid angular gain function operates biological spike rate network tracks large angular targets degrees within sec rms error study navigational ability model foraging target localization tasks forest obstacles show network requires less 200x spike triggered decisions suffering loss performance compared proportional integral derivative controller presence additive noise superior performance obtained higher average spike rate 100 1000 even accelerated networks requires 20x 10x lesser decisions respectively demonstrating superior computational efficiency bio inspired information processing systems
theoretical comparisons positive unlabeled learning positive negative learning learning binary classifier trained positive unlabeled data without negative data although data missing sometimes outperforms learning ordinary supervised learning hitherto neither theoretical experimental analysis given explain phenomenon paper theoretically compare learning learning based upper bounds estimation errors find simple conditions learning likely outperform learning prove terms upper bounds either learning depending class prior probability sizes data given infinite data improve learning theoretical findings well agree experimental results artificial benchmark data even experimental setup match theoretical assumptions exactly
quantized random projections non linear estimation cosine similarity random projections constitute simple yet effective technique dimensionality reduction applications learning search problems present paper consider problem estimating cosine similarities projected data undergo scalar quantization bits argue maximum likelihood estimator mle principled approach deal non linearity resulting quantization subsequently study computational statistical properties specific focus trade bit depth number projections given fixed budget bits storage transmission along way also touch upon existence qualitative counterpart johnson lindenstrauss lemma presence quantization
cnnpack packing convolutional neural networks frequency domain deep convolutional neural networks cnns successfully used number applications however storage computational requirements largely prevented widespread use mobile devices present effective cnn compression approach frequency domain focuses smaller weights weights underlying connections treating convolutional filters images decompose representations frequency domain common parts cluster centers shared similar filters individual private parts individual residuals large number low energy frequency coefficients parts discarded produce high compression without significantly compromising accuracy relax computational burden convolution operations cnns linearly combining convolution responses discrete cosine transform dct bases compression speed ratios proposed algorithm thoroughly analyzed evaluated benchmark image datasets demonstrate superiority state art methods
verification based solution structured mab problems consider problem finding best arm stochastic mutli armed bandit mab game propose general framework based verification applies multiple well motivated generalizations classic mab problem generalizations additional structure known advance causing task verifying optimality candidate easier discovering best arm results focused scenario failure probability delta must low essentially show high confidence regime identifying best arm easy task verification demonstrate effectiveness framework applying improving state art results problems linear bandits dueling bandits condorcet assumption copeland dueling bandits unimodal bandits graphical bandits
neurally guided procedural models amortized inference procedural graphics programs using neural networks probabilistic inference algorithms sequential monte carlo smc provide powerful tools constraining procedural models computer graphics require many samples produce desirable results paper show create procedural models learn satisfy constraints augment procedural models neural networks control model makes random choices based output generated thus far call models neurally guided procedural models pre computation train models maximize likelihood example outputs generated via smc used efficient smc importance samplers generating high quality results samples evaluate method system like models image based constraints given desired quality threshold neurally guided models generate satisfactory results 10x faster unguided models
edge exchangeable graphs sparsity many popular network models rely assumption vertex exchangeability distribution graph invariant relabelings vertices however aldous hoover theorem guarantees graphs dense empty probability whereas many real world graphs sparse present alternative notion exchangeability random graphs call edge exchangeability distribution graph sequence invariant order edges demonstrate edge exchangeable models unlike models traditionally vertex exchangeable exhibit sparsity outline general framework graph generative models contrast pioneering work caron fox 2015 models within framework stationary across steps graph sequence particular model grows graph instantiating latent atoms single random measure dataset size increases rather adding new atoms measure
learning forecasting opinion dynamics social networks social media social networking sites become global pinboard exposition discussion news topics ideas social media users often update opinions particular topic learning opinions shared friends context learn data driven model opinion dynamics able accurately forecast users opinions paper introduce slant probabilistic modeling framework opinion dynamics represents users opinions time means marked jump diffusion stochastic differential equations allows efficient model simulation parameter estimation historical fine grained event data leverage framework derive set efficient predictive formulas opinion forecasting identify conditions opinions converge steady state experiments data gathered twitter show model provides good fit data formulas achieve accurate forecasting alternatives
probing compositionality intuitive functions people learn complex functional structure taking inspiration areas cognitive science propose accomplished harnessing compositionality complex structure decomposed simpler building blocks formalize idea within framework bayesian regression using grammar gaussian process kernels show participants prefer compositional non compositional function extrapolations samples human prior functions best described compositional model people perceive compositional functions predictable non compositional otherwise similar counterparts argue compositional nature intuitive functions consistent broad principles human cognition
learning shape correspondence anisotropic convolutional neural networks convolutional neural networks achieved extraordinary results many computer vision pattern recognition applications however adoption computer graphics geometry processing communities limited due non euclidean structure data paper propose anisotropic convolutional neural network acnn generalization classical cnns non euclidean domains classical convolutions replaced projections set oriented anisotropic diffusion kernels use acnns effectively learn intrinsic dense correspondences deformable shapes fundamental problem geometry processing arising wide variety applications tested acnns performance challenging settings achieving state art results difficult recent correspondence benchmarks
improved techniques training gans present variety new architectural features training procedures apply generative adversarial networks gans framework using new techniques achieve state art results semi supervised classification mnist cifar svhn generated images high quality confirmed visual turing test model generates mnist samples humans cannot distinguish real data cifar samples yield human error rate also present imagenet samples unprecedented resolution show methods enable model learn recognizable features imagenet classes
automated scalable segmentation neurons multispectral images reconstruction neuroanatomy fundamental problem neuroscience stochastic expression colors individual cells promising tool although use nervous system limited due various sources variability expression moreover intermingled anatomy neuronal trees challenging existing segmentation algorithms propose method automate segmentation neurons potentially pseudo colored images method uses spatio color relations voxels generates supervoxels reduce problem size orders magnitude final segmentation parallelizable supervoxels quantify performance gain insight generate simulated images noise level characteristics density expression number fluorophore types variable also present segmentations real brainbow images mouse hippocampus reveal many dendritic segments
optimal cluster recovery labeled stochastic block model consider problem community detection clustering labeled stochastic block model lsbm finite number clusters sizes linearly growing global population items every pair items labeled independently random label ell appears probability ell items clusters indexed respectively objective reconstruct clusters observation random labels clustering sbm extensions attracted much attention recently existing work aimed characterizing set parameters possible infer clusters either positively correlated true clusters vanishing proportion misclassified items exactly matching true clusters find set parameters exists clustering algorithm misclassified items average general lsbm solves open problem raised cite abbe2015community develop algorithm based simple spectral methods achieves fundamental performance limit within mbox polylog computations without priori knowledge model parameters
phased exploration greedy exploitation stochastic combinatorial partial monitoring games partial monitoring games repeated games learner receives feedback might different adversary move even reward gained learner recently general model combinatorial partial monitoring cpm games proposed cite lincombinatorial2014 learner action space exponentially large adversary samples moves bounded continuous space according fixed distribution paper gave confidence bound based algorithm gcb achieves log distribution independent log distribution dependent regret bounds implementation algorithm depends separate offline oracles distribution dependent regret additionally requires existence unique optimal action learner adopting cpm model first contribution phased exploration greedy exploitation pege algorithmic framework problem different algorithms within framework achieve sqrt log distribution independent log distribution dependent regret respectively crucially framework needs simpler argmax oracle gcb distribution dependent regret require existence unique optimal action second contribution another algorithm pege2 combines gap estimation pege algorithm achieve log regret bound matching gcb guarantee removing dependence size learner action space however like gcb pege2 requires access offline oracles existence unique optimal action finally discuss algorithm efficiently applied cpm problem practical interest namely online ranking feedback top
dual space gradient descent online learning crucial goal kernel online learning bound model size common approaches employ budget maintenance procedures restrict model sizes using removal projection merging strategies although projection merging literature known effective strategies demand extensive computation whilst removal strategy fails retain information removed vectors alternative way address model size problem apply random features approximate kernel function allows model maintained directly random feature space hence effectively resolve curse kernelization however approach still suffers serious shortcoming needs use high dimensional random feature space achieve sufficiently accurate kernel approximation consequently leads significant increase computational cost address aforementioned challenges present paper dual space gradient descent dualsgd novel framework utilizes random features auxiliary space maintain information data points removed budget maintenance consequently approach permits budget maintained simple direct elegant way simultaneously mitigating impact dimensionality issue learning performance provide convergence analysis extensively conduct experiments real world datasets demonstrate predictive performance scalability proposed method comparison state art baselines
data programming creating large training sets quickly large labeled training sets critical building blocks supervised learning methods key enablers deep learning techniques applications creating labeled training sets time consuming expensive part applying machine learning therefore propose paradigm programmatic creation training sets called data programming users provide set labeling functions programs heuristically label subsets data noisy conflict viewing labeling functions implicitly describing generative model noise show recover parameters model denoise generated training set establish theoretically recover parameters generative models handful settings show modify discriminative loss function make noise aware demonstrate method range discriminative models including logistic regression lstms experimentally 2014 tac kbp slot filling challenge show data programming would led new winning score also show applying data programming lstm model leads tac kbp score almost points state art lstm baseline second place competition additionally initial user studies observed data programming easier way non experts create machine learning models training data limited unavailable
near optimal smoothing structured conditional probability matrices utilizing structure probabilistic model significantly increase learning speed motivated several recent applications particular bigram models language processing consider learning low rank conditional probability matrices expected risk choice makes smoothing careful handling low probability elements paramount derive iterative algorithm extends classical non negative matrix factorization naturally incorporate additive smoothing prove converges stationary points penalized empirical risk derive sample complexity bounds global minimizer penalized risk show within small factor optimal sample complexity framework generalizes sophisticated smoothing techniques including absolute discounting
urn model majority voting classification ensembles work analyze class prediction parallel randomized ensembles majority voting urn model given test instance ensemble viewed urn marbles different colors marble represents individual classifier color represents class label prediction corresponding classifier sequential querying classifiers ensemble seen draws without replacement urn analysis classical urn model based hypergeometric distribution makes possible estimate confidence outcome majority voting fraction individual predictions known estimates used speed prediction ensemble specifically aggregation votes halted confidence final prediction sufficiently high assumes uniform prior distribution possible votes analysis shown equivalent previous based dirichlet distributions advantage current approach prior knowledge possible vote outcomes readily incorporated bayesian framework show incorporating type problem specific knowledge statistical analysis majority voting leads faster classification ensemble allows estimate expected average speed beforehand
multi fidelity multi armed bandit study variant classical stochastic armed bandit observing outcome arm expensive cheap approximations outcome available example online advertising performance approximated displaying shorter time periods narrower audiences formalise task emph multi fidelity bandit time step forecaster choose play arm fidelities highest fidelity desired outcome expends cost costm ssth fidelity approximation expends costm costm returns biased estimate highest fidelity develop mfucb novel upper confidence bound procedure setting prove naturally adapts sequence available approximations costs thus attaining better regret naive strategies ignore approximations instance online advertising example mfucbs would use lower fidelities quickly eliminate suboptimal ads reserve larger expensive experiments small set promising candidates complement result lower bound show mfucbs nearly optimal certain conditions
probabilistic inference generating functions poisson latent variable models graphical models latent count variables arise number fields standard exact inference techniques variable elimination belief propagation apply models latent variables countably infinite support result approximations truncation mcmc employed present first exact inference algorithms class models latent count variables developing novel representation countably infinite factors probability generating functions performing variable elimination generating functions approach exact runs pseudo polynomial time much faster existing approximate techniques leads better parameter estimates problems population ecology avoiding error introduced approximate likelihood computations
adaptive maximization pointwise submodular functions budget constraint study worst case adaptive optimization problem budget constraint useful modeling various practical applications artificial intelligence machine learning investigate near optimality greedy algorithms problem modular non modular cost functions cases prove simple greedy algorithms near optimal best near optimal utility function satisfies pointwise submodularity pointwise cost sensitive submodularity respectively implies combined algorithm near optimal respect optimal algorithm uses half budget discuss applications theoretical results also report experiments comparing greedy algorithms active learning problem
dual learning machine translation neural machine translation nmt making good progress past years tens millions bilingual sentence pairs needed training however human labeling costly tackle training data bottleneck develop dual learning mechanism enable nmt system automatically learn unlabeled data dual learning game mechanism inspired following observation machine translation task dual task english french translation primal versus french english translation dual primal dual tasks form closed loop generate informative feedback signals train translation models even without involvement human labeler dual learning mechanism use agent represent model primal task agent represent model dual task ask teach reinforcement learning process based feedback signals generated process language model likelihood output model reconstruction error original sentence primal dual translations iteratively update models convergence using policy gradient methods call corresponding approach neural machine translation emph dual nmt experiments show dual nmt works well english leftrightarrow french translation especially learning monolingual data bilingual data warm start achieves comparable accuracy nmt trained full bilingual data french english translation task
iterative refinement approximate posterior directed belief networks variational methods rely recognition network approximate posterior directed graphical models offer better inference learning previous methods recent advances exploit capacity flexibility approach expanded kinds models trained however proposal posterior capacity recognition network limited constrain representational power generative model increase variance monte carlo estimates address issues introduce iterative refinement procedure improving approximate posterior recognition network show training refined posterior competitive state art methods advantages refinement evident increased effective sample size implies lower variance gradient estimates
unsupervised risk estimation using conditional independence structure show estimate model test error unlabeled data distributions different training distribution assuming certain conditional independencies preserved train test need assume optimal predictor train test true distribution lies parametric family also efficiently compute gradients estimated error hence perform unsupervised discriminative learning technical tool method moments allows exploit conditional independencies absence fully specified model framework encompasses large family losses including log exponential loss extends structured output settings conditional random fields
hierarchical question image attention visual question answering number recent works proposed attention models visual question answering vqa generate spatial maps highlighting image regions relevant answering question paper argue addition modeling look visual attention equally important model words listen question attention present novel attention model vqa jointly reasons image question attention addition model reasons question consequently image via attention mechanism hierarchical fashion via novel dimensional convolution neural networks cnn model improves state art vqa dataset coco dataset using resnet performance improved vqa coco
bayesian optimization finite budget approximate dynamic programming approach consider problem optimizing expensive objective function finite budget total evaluations prescribed context optimal solution strategy bayesian optimization formulated dynamic programming instance results complex problem uncountable dimension increasing state space uncountable control space show approximate solution dynamic programming problem using rollout propose rollout heuristics specifically designed bayesian optimization setting present numerical experiments showing resulting algorithm optimization finite budget outperforms several popular bayesian optimization algorithms
learning learn gradient descent gradient descent move hand designed features learned features machine learning wildly successful spite optimization algorithms still designed hand paper show design optimization algorithm cast learning problem allowing algorithm learn exploit structure problems interest automatic way learned algorithms implemented lstms outperform generic hand designed competitors tasks trained also generalize well new tasks similar structure demonstrate number tasks including simple convex problems training neural networks styling images neural art
computational statistical tradeoffs learning rank massive heterogeneous modern data sets fundamental interest provide guarantees accuracy estimation computational resources limited application learning rank provide hierarchy rank breaking mechanisms ordered complexity thus generated sketch data allows number data points collected gracefully traded computational resources available guaranteeing desired level accuracy theoretical guarantees proposed generalized rank breaking implicitly provide trade offs explicitly characterized certain canonical scenarios structure data
pairwise choice markov chains datasets capturing human choices grow richness scale particularly online domains increasing need choice models flexible enough handle data violate traditional choice theoretic axioms regularity stochastic transitivity luce choice axiom work introduce pairwise choice markov chain pcmc model discrete choice inferentially tractable model assume traditional axioms still satisfying foundational axiom uniform expansion viewed weaker version luce axiom show pcmc model significantly outperforms multinomial logit mnl model prediction tasks empirical data sets known exhibit violations luce axiom analysis also synthesizes several recent observations connecting multinomial logit model markov chains pcmc model retains multinomial logit model special case
incremental variational sparse gaussian process regression recent work scaling gaussian process regression gpr large datasets primarily focused sparse gpr leverages small set basis functions approximate full gaussian process inference however majority approaches batch methods operate entire training dataset precluding use datasets streaming large fit memory although previous work considered incrementally solving variational sparse gpr algorithms fail update basis functions therefore perform suboptimally propose novel incremental learning algorithm variational sparse gpr based stochastic mirror ascent probability densities reproducing kernel hilbert space new formulation allows algorithm update basis functions online accordance manifold structure probability densities fast convergence conduct several experiments show proposed approach achieves better empirical performance terms prediction error recent state art incremental solutions variational sparse gpr
combinatorial multi armed bandit general reward functions paper study stochastic combinatorial multi armed bandit cmab framework allows general nonlinear reward function whose expected value depend means input random variables possibly entire distributions variables framework enables much larger class reward functions max function nonlinear utility functions existing techniques relying accurate estimations means random variables upper confidence bound ucb technique work directly functions propose new algorithm called stochastically dominant confidence bound sdcb estimates distributions underlying random variables stochastically dominant confidence bounds prove sdcb achieve log distribution dependent regret tilde sqrt distribution independent regret time horizon apply results max problem expected utility maximization problems particular max provide first polynomial time approximation scheme ptas offline problem give first tilde sqrt bound epsilon approximation regret online problem epsilon
observational interventional priors dose response learning controlled interventions provide direct source information learning causal effects particular dose response curve learned varying treatment level observing corresponding outcomes however interventions expensive time consuming observational data treatment controlled known mechanism sometimes available strong assumptions observational data allows estimation dose response curves estimating curves nonparametrically hard sample sizes controlled interventions small observational case large number measured confounders need marginalized paper introduce hierarchical gaussian process prior constructs distribution dose response curve learning observational data reshapes distribution nonparametric affine transform learned controlled interventions function composition different sources shown speed learning demonstrate thorough sensitivity analysis application modeling effect therapy cognitive skills premature infants
graph reconstruction via empirical risk minimization fast learning rates scalability problem predicting connections set data points finds many applications systems biology social network analysis among others paper focuses textit graph reconstruction problem prediction rule obtained minimizing average error possible pairs nodes training graph first contribution derive learning rates order log problem significantly improving upon slow rates order established seminal work biau bleakley 2006 strikingly fast rates universal contrast similar results known statistical learning problems classification density level set estimation ranking clustering require strong assumptions distribution data motivated applications large graphs second contribution deals computational complexity graph reconstruction specifically investigate extent learning rates preserved replacing empirical reconstruction risk computationally cheaper monte carlo version obtained sampling replacement pairs nodes finally illustrate theoretical results numerical experiments synthetic real graphs
deepmath deep sequence models premise selection study effectiveness neural sequence models premise selection automated theorem proving key bottleneck progress formalized mathematics propose stage approach task yields good results premise selection task mizar corpus avoiding hand engineered features existing state art models knowledge first time deep learning applied theorem proving large scale
efficient second order online learning sketching propose sketched online newton son online second order learning algorithm enjoys substantially improved regret guarantees ill conditioned data son enhanced version online newton step via sketching techniques enjoys running time linear dimension sketch size develop sparse forms sketching methods oja rule making computation linear sparsity features together algorithm eliminates computational obstacles previous second order online learning approaches
gaussian processes survival analysis introduce semi parametric bayesian model survival analysis model centred parametric baseline hazard uses gaussian process model variations away nonparametrically well dependence covariates opposed many methods survival analysis framework impose unnecessary constraints hazard rate survival function furthermore model handles left right interval censoring mechanisms common survival analysis propose mcmc algorithm perform inference approximation scheme based random fourier features make computations faster report experimental results synthetic real data showing model performs better competing models cox proportional hazards anova ddp random survival forests
power optimization samples consider problem optimization samples monotone submodular functions bounded curvature numerous applications function optimized known priori instead learned data guarantees optimizing functions sampled data paper show monotone submodular function curvature approximation algorithm maximization cardinality constraints polynomially many samples drawn uniform distribution feasible sets moreover show algorithm optimal exists submodular function curvature algorithm achieve better approximation curvature assumption crucial general monotone submodular functions algorithm obtain constant factor approximation maximization cardinality constraint observing polynomially many samples drawn distribution feasible sets even function statistically learnable
global optimality local search low rank matrix recovery show spurious local minima non convex factorized parametrization low rank matrix recovery incoherent linear measurements noisy measurements show local minima close global optimum together curvature bound saddle points yields polynomial time global convergence guarantee stochastic gradient descent random initialization
state space model cross region dynamic connectivity meg eeg cross region dynamic connectivity describes spatio temporal dependence neural activity among multiple brain regions interest rois provide important information understanding cognition estimating connectivity magnetoencephalography meg electroencephalography eeg well suited tools millisecond temporal resolution however localizing source activity brain requires solving determined linear problem typical step approaches researchers first solve linear problem general priors assuming independence across rois secondly quantify cross region connectivity work propose step state space model improve estimation dynamic connectivity model treats mean activity individual rois state variable describes non stationary dynamic dependence across rois using time varying auto regression compared step method first obtains commonly used minimum norm estimates source activity fits auto regressive model state space model yielded smaller estimation errors simulated data model assumptions held applied empirical meg data participant scene processing experiment state space model also demonstrated intriguing preliminary results indicating leading lagged linear dependence early visual cortex higher level scene sensitive region could reflect feed forward feedback information flow within visual cortex scene processing
hypothesis testing unsupervised domain adaptation applications alzheimer disease consider samples different data sources mathbf x_s sim source mathbf x_t sim target observe transformed versions mathbf x_s mathbf x_t known function class cdot cdot goal perform statistical test checking source target removing distortions induced transformations problem closely related concepts underlying numerous domain adaptation algorithms case motivated need combine clinical imaging based biomarkers multiple sites batches problem fairly common impediment conduct analyses much larger sample sizes develop framework addresses problem using ideas hypothesis testing transformed measurements distortions need estimated tandem testing derive simple algorithm study convergence consistency properties detail also provide lower bound strategies based recent work continuous optimization dataset individuals risk neurological disease results competitive alternative procedures twice expensive cases operationally infeasible implement
objective online matching submodular allocations online allocation problems widely studied due numerous practical applications particularly internet advertising well considerable theoretical interest main challenge problems making assignment decisions face uncertainty future input effective algorithms need predict constraints likely bind learn balance short term gain value long term resource availability many important applications algorithm designer faced multiple objectives optimize particular online advertising fairly common optimize multiple metrics clicks conversions impressions well metrics largely uncorrelated share voice buyer surplus considerable work multi objective offline optimization entire input known advance little known online case particularly case adversarial input paper give first results objective online submodular optimization providing almost matching upper lower bounds allocating items agents submodular value functions also study practically relevant special cases problem related internet advertising obtain improved results algorithms nearly best possible well efficient easy implement practice
constant factor criteria approximation guarantee means paper studies means algorithm clustering well class ell sampling algorithms means belongs shown constant factor beta selecting beta cluster centers ell sampling yields constant factor approximation optimal clustering centers expectation without conditions dataset result extends previously known log guarantee case beta constant factor criteria regime also improves upon existing constant factor criteria result holds constant probability
causal bandits learning good interventions via causal inference study problem using causal models improve rate good interventions learned online stochastic environment formalism combines multi arm bandits causal inference model novel type bandit feedback exploited existing approaches propose new algorithm exploits causal feedback prove bound simple regret strictly better quantities algorithms use additional causal information
unsupervised domain adaptation residual transfer networks recent success deep neural networks relies massive amounts labeled data target task labeled data unavailable domain adaptation transfer learner different source domain paper propose new approach domain adaptation deep networks jointly learn adaptive classifiers transferable features labeled data source domain unlabeled data target domain relax shared classifier assumption made previous methods assume source classifier target classifier differ residual function enable classifier adaptation plugging several layers deep network explicitly learn residual function reference target classifier fuse features multiple layers tensor product embed reproducing kernel hilbert spaces match distributions feature adaptation adaptation achieved feed forward models extending new residual layers loss functions trained efficiently via back propagation empirical evidence shows new approach outperforms state art methods standard domain adaptation benchmarks
data driven estimation laplace beltrami operator approximations laplace beltrami operators manifolds graph laplacians become popular tools data analysis machine learning discretized operators usually depend bandwidth parameters whose tuning remains theoretical practical problem paper address problem unormalized graph laplacian establishing oracle inequality opens door well founded data driven procedure bandwidth selection approach relies recent results lacour massart 2015 called lepski method
fast algorithms robust pca via gradient descent consider problem robust pca fully partially observed settings without corruptions well known matrix completion problem statistical standpoint problem recently well studied conditions recovery possible many observations need many corruptions tolerate via polynomial time algorithms understood paper presents analyzes non convex optimization approach greatly reduces computational complexity problems compared best available algorithms particular fully observed case denoting rank dimension reduce complexity log epsilon log epsilon big savings rank big partially observed case show complexity algorithm log log epsilon best known run time provable algorithm partial observation setting small compared also allows near linear run time exploited fully observed case well simply running algorithm subset observations
nestt nonconvex primal dual splitting method distributed stochastic optimization study stochastic distributed algorithm nonconvex problems whose objective consists sum nonconvex l_i smooth functions plus nonsmooth regularizer proposed nonconvex primal dual splitting nestt algorithm splits problem subproblems utilizes augmented lagrangian based primal dual scheme solve distributed stochastic manner special non uniform sampling version nestt achieves epsilon stationary solution using mathcal sum_ sqrt l_i epsilon gradient evaluations mathcal times better proximal gradient descent methods also achieves linear convergence rate nonconvex ell_1 penalized quadratic problems polyhedral constraints reveal fundamental connection primal dual based methods primal methods iag sag saga
fundamental limits budget fidelity trade label crowdsourcing digital crowdsourcing modern approach perform certain large projects using small contributions large crowd taskmaster typically breaks project small batches tasks assigns called workers imperfect skill levels crowdsourcer collects analyzes results inference serving purpose project work problem human loop computation problem modeled analyzed information theoretic rate distortion framework purpose identify ultimate fidelity achieve form query crowd decoding inference algorithm given budget results established joint source channel coding scheme represent query scheme inference parallel noisy channels model workers imperfect skill levels also present analyze query scheme dubbed ary incidence coding study optimized query pricing setting
supervised learning tensor networks tensor networks approximations high order tensors efficient work successful physics mathematics applications demonstrate algorithms optimizing tensor networks adapted supervised learning tasks using matrix product states tensor trains parameterize non linear kernel learning models mnist data set obtain less test set classification error discuss interpretation additional structure imparted tensor network learned model
understanding probabilistic sparse gaussian process approximations good sparse approximations essential practical inference gaussian processes computational cost exact methods prohibitive large datasets fully independent training conditional fitc variational free energy vfe approximations recent popular methods despite superficial similarities approximations surprisingly different theoretical properties behave differently practice thoroughly investigate methods regression analytically illustrative examples draw conclusions guide practical application
locally adaptive normal distribution multivariate normal density monotonic function distance mean ellipsoidal shape due underlying euclidean metric suggest replace metric locally adaptive smoothly changing riemannian metric favors regions high local density resulting locally adaptive normal distribution land generalization normal distribution manifold setting data assumed lie near potentially low dimensional manifold embedded land parametric depending mean covariance maximum entropy distribution given metric underlying metric however non parametric develop maximum likelihood algorithm infer distribution parameters relies combination gradient descent monte carlo integration extend land mixture models provide corresponding algorithm demonstrate efficiency land fit non trivial probability distributions synthetic data eeg measurements human sleep
anchor free correlated topic modeling identifiability algorithm topic modeling many algorithms guarantee identifiability topics developed premise exist anchor words words appear positive probability topic follow work resorted higher order statistics data corpus relax anchor word assumption reliable estimates higher order statistics hard obtain however identification topics models hinges uncorrelatedness topics unrealistic paper revisits topic modeling based second order moments proposes anchor free topic mining framework proposed approach guarantees identification topics much milder condition compared anchor word assumption thereby exhibiting much better robustness practice associated algorithm involves eigen decomposition small linear programs makes easy implement scale large problem instances experiments using tdt2 reuters 21578 corpus demonstrate proposed anchor free approach exhibits favorable performance measured using coherence similarity count clustering accuracy metrics compared prior art
optimal learning multi pass stochastic gradient methods analyze learning properties stochastic gradient method multiple passes data mini batches allowed particular consider square loss show universal step size choice number passes acts regularization parameter optimal finite sample bounds achieved early stopping moreover show larger step sizes allowed considering mini batches analysis based unifying approach encompassing batch stochastic gradient methods special cases
contextual semibandits via supervised learning oracles study online decision making problem round learner chooses list items based side information receives scalar feedback value individual item reward linearly related feedback problems known contextual semibandits arise crowdsourcing recommendation many domains paper reduces contextual semibandits supervised learning allowing leverage powerful supervised learning methods partial feedback setting first reduction applies mapping feedback reward known leads computationally efficient algorithm near optimal regret show algorithm outperforms state art approaches real world learning rank datasets demonstrating advantage oracle based algorithms second reduction applies previously unstudied setting linear mapping feedback reward unknown regret guarantees superior prior techniques ignore feedback
approximation softmax scalable estimation probabilities softmax representation probabilities categorical variables plays prominent role modern machine learning numerous applications areas large scale classification neural language modeling recommendation systems however softmax estimation expensive large scale inference high cost associated computing normalizing constant introduce efficient approximation softmax probabilities takes form rigorous lower bound exact probability bound expressed product pairwise probabilities leads scalable estimation based stochastic optimization allows perform doubly stochastic estimation subsampling training instances class labels show new bound interesting theoretical properties demonstrate use classification problems
satisfying real world goals dataset constraints goal minimizing misclassification error training set often several real world goals might defined different datasets example require classifier also make positive predictions specified rate subpopulation fairness achieve specified empirical recall real world goals include reducing churn respect previously deployed model stabilizing online training paper propose handling multiple goals multiple datasets training dataset constraints using ramp penalty accurately quantify costs present efficient algorithm approximately optimize resulting non convex constrained optimization problem experiments benchmark real world industry datasets demonstrate effectiveness approach
blind regression nonparametric regression latent variable models via collaborative filtering introduce framework blind regression motivated matrix completion recommendation systems given users movies subset user movie ratings goal predict unobserved user movie ratings given data complete partially observed matrix following framework non parametric statistics posit user movie features respectively corresponding rating noisy measurement unknown function contrast classical regression features observed making challenging apply standard regression methods predict unobserved ratings
inspired classical taylor expansion differentiable functions provide prediction algorithm consistent lipschitz functions fact analysis framework naturally leads variant collaborative filtering shedding insight widespread success collaborative filtering practice assuming entry sampled independently probability least max delta delta delta prove expected fraction estimates error greater epsilon less gamma epsilon plus polynomially decaying term gamma variance additive entry wise noise term experiments movielens netflix datasets suggest algorithm provides principled improvements basic collaborative filtering competitive matrix factorization methods
generative adversarial imitation learning consider learning policy example expert behavior without interaction expert access reinforcement signal approach recover expert cost function inverse reinforcement learning extract policy cost function reinforcement learning approach indirect slow propose new general framework directly extracting policy data obtained reinforcement learning following inverse reinforcement learning show certain instantiation framework draws analogy imitation learning generative adversarial networks derive model free imitation learning algorithm obtains significant performance gains existing model free methods imitating complex behaviors large high dimensional environments
fast active set methods online spike inference calcium imaging fluorescent calcium indicators popular means observing spiking activity large neuronal populations unfortunately extracting spike train neuron raw fluorescence calcium imaging data nontrivial problem present fast online active set method solve sparse nonnegative deconvolution problem importantly algorithm progresses time series sequentially beginning end thus enabling real time online spike inference imaging session algorithm generalization pool adjacent violators algorithm pava isotonic regression inherits linear time computational complexity gain remarkable increases processing speed order magnitude compared currently employed state art convex solvers relying interior point methods method exploit warm starts therefore optimizing model hyperparameters requires handful passes data algorithm enables real time simultaneous deconvolution traces whole brain zebrafish imaging data laptop
path normalized optimization recurrent neural networks relu activations investigate parameter space geometry recurrent neural networks rnns develop adaptation path sgd optimization method attuned geometry learn plain rnns relu activations several datasets require capturing long term dependency structure show path sgd significantly improve trainability relu rnns compared rnns trained sgd even various recently suggested initialization schemes
improved regret bounds oracle based adversarial contextual bandits propose new oracle based algorithm bistro adversarial contextual bandit problem either contexts drawn sequence contexts known priori losses picked adversarially algorithm computationally efficient assuming access offline optimization oracle enjoys regret order frac log frac number actions number iterations number baseline policies result first break frac barrier achieved recent algorithms left major open problem analysis employs recent relaxation framework rakhlin sridharan icml
diffusion convolutional neural networks present diffusion convolutional neural networks dcnns new model graph structured data introduction diffusion convolution operation show diffusion based representations learned graph structured data used effective basis node classification dcnns several attractive qualities including latent representation graphical data invariant isomorphism well polynomial time prediction learning represented tensor operations efficiently implemented gpu several experiments real structured datasets demonstrate dcnns able outperform probabilistic relational models kernel graph methods relational node classification tasks
faster projection free convex optimization spectrahedron minimizing convex function spectrahedron set times positive semidefinite matrices unit trace important optimization task many applications optimization machine learning signal processing also notoriously difficult solve large scale since standard techniques require compute expensive matrix decompositions alternative conditional gradient method aka frank wolfe algorithm regained much interest recent years mostly due application specific setting key benefit method avoids expensive matrix decompositions together simply requires single eigenvector computation per iteration much efficient downside method general converges inferior rate error minimizing beta smooth function iterations scales like beta rate improve even function also strongly convex work present modification method tailored spectrahedron per iteration complexity method essentially identical standard method single eigenvecor computation required minimizing alpha strongly convex beta smooth function textit expected error method iterations left min frac beta left frac beta sqrt rank alpha right left frac beta sqrt alpha lambda_ min right right beyond significant improvement convergence rate also follows optimum low rank method provides better accuracy rank tradeoff standard method best knowledge first result attains provably faster convergence rates variant optimization spectrahedron also present encouraging preliminary empirical results
structured matrix recovery via generalized dantzig selector recent years structured matrix recovery problems gained considerable attention real world applications recommender systems computer vision much existing work focused matrices low rank structure limited progress made matrices types structure paper present non asymptotic analysis estimation generally structured matrices via generalized dantzig selector based sub gaussian measurements show estimation error always succinctly expressed terms geometric measures gaussian widths suitable sets associated structure underlying true matrix derive general bounds geometric measures structures characterized unitarily invariant norms large family covering matrix norms practical interest examples provided illustrate utility theoretical development
convex layer modeling latent structure unsupervised learning structured predictors long standing pursuit machine learning recently conditional random field auto encoder proposed layer setting allowing latent structured representation automatically inferred aside nonconvex also requires demanding inference normalization paper develop convex relaxation layer conditional model captures latent structure estimates model parameters jointly optimally expand applicability resorting weaker form inference maximum posteriori flexibility model demonstrated structures based total unimodularity graph matching linear chain experimental results confirm promise method
finite sample analysis fixed nearest neighbor density functional estimators provide finite sample analysis general framework using nearest neighbor statistics estimate functionals nonparametric continuous probability density including entropies divergences rather plugging consistent density estimate requires sample size functional interest estimators consider fix perform bias correction efficient computationally show statistically leading faster convergence rates framework unifies several previous estimators first finite sample guarantees
deep learning games investigate reduction supervised learning game playing reveals new connections learning methods convex layer problems demonstrate equivalence global minimizers training problem nash equilibria simple game show game extended general acyclic neural networks differentiable convex gates establishing bijection nash equilibria critical kkt points deep learning problem based connections investigate alternative learning methods find regret matching achieve competitive training performance producing sparser models current deep learning approaches
congruent opposite neurons sisters multisensory integration segregation experiments reveal dorsal medial superior temporal mstd ventral intraparietal vip areas visual vestibular cues integrated infer heading direction types neurons roughly number congruent cells whose preferred heading directions similar response visual vestibular cues opposite cells whose preferred heading directions nearly opposite offset 180 degree response visual vestibular cues congruent neurons known responsible cue integration computational role opposite neurons remains largely unknown propose opposite neurons serve encode disparity information cues necessary multisensory segregation build computational model composed reciprocally coupled modules mstd vip module consists groups congruent opposite neurons model congruent neurons modules reciprocally connected congruent manner whereas opposite neurons reciprocally connected opposite manner mimicking experimental protocol model reproduces characteristics congruent opposite neurons demonstrates module sisters congruent opposite neurons jointly achieve optimal multisensory information integration segregation study sheds light understanding brain implements optimal multisensory integration segregation concurrently distributed manner
statistical inference cluster trees cluster tree provides intuitive summary density function reveals essential structure high density clusters true cluster tree estimated finite sample unknown true density paper addresses basic question quantifying uncertainty assessing statistical significance different features empirical cluster tree first study variety metrics used compare different trees analyzing properties assessing suitability inference task propose methods construct summarize confidence sets unknown true cluster tree introduce partial ordering cluster trees use prune statistically insignificant features empirical tree yielding interpretable parsimonious cluster trees finally provide variety simulations illustrate proposed methods furthermore demonstrate utility analysis graft versus host disease gvhd data set
minimizing regret reflexive banach spaces nash equilibria continuous sum games study general adversarial online learning problem given decision set reflexive banach space sequence reward vectors dual space iteration choose action based observed sequence previous rewards goal minimize regret defined gap realized reward reward best fixed action hindsight using results infinite dimensional convex analysis generalize method dual averaging follow regularized leader setting obtain upper bounds worst case regret generalize many previous results assumption uniformly continuous rewards obtain explicit regret bounds setting decision set set probability distributions compact metric space importantly make convexity assumptions either set reward functions also prove general lower bound worst case regret online algorithm apply results problem learning repeated player sum games compact metric spaces first prove players play hannan consistent strategy probability empirical distributions play weakly converge set nash equilibria game show mild assumptions dual averaging infinite dimensional space probability distributions indeed achieves hannan consistency
online sequence sequence model using partial conditioning sequence sequence models achieved impressive results various tasks however unsuitable tasks require incremental predictions made data arrives tasks long input sequences output sequences generate output sequence conditioned entire input sequence paper present neural transducer make incremental predictions input arrives without redoing entire computation unlike sequence sequence models neural transducer computes next step distribution conditioned partially observed input sequence partially generated sequence time step transducer decide emit many output symbols data processed using encoder presented input transducer discrete decision emit symbol every time step makes difficult learn conventional backpropagation however possible train transducer using dynamic programming algorithm generate target discrete decisions experiments show neural transducer works well settings required produce output predictions data come also find neural transducer performs well long sequences even attention mechanisms used
feature selection functional data classification recursive maxima hunting dimensionality reduction key issues design effective machine learning methods automatic induction work introduce recursive maxima hunting rmh variable selection classification problems functional data context variable selection techniques especially attractive reduce dimensionality facilitate interpretation improve accuracy predictive models method recursive extension maxima hunting performs variable selection identifying maxima relevance function measures strength correlation predictor functional variable class label stage information associated selected variable removed subtracting conditional expectation process results extensive empirical evaluation used illustrate problems investigated rmh comparable higher predictive accuracy standard simensionality reduction techniques pca pls state art feature selection methods functional data maxima hunting
homotopy smoothing non smooth problems lower complexity epsilon paper develop novel moto moothing hops algorithm solving family non smooth problems composed non smooth term explicit max structure smooth term simple non smooth term whose proximal mapping easy compute best known iteration complexity solving non smooth optimization problems epsilon without assumption strong convexity work show proposed hops achieved lower iteration complexity tilde epsilon theta theta capturing local sharpness objective function around optimal solutions best knowledge lowest iteration complexity achieved far considered non smooth optimization problems without strong convexity assumption hops algorithm employs nesterov smoothing technique nesterov accelerated gradient method runs stages gradually decreases smoothing parameter stage wise manner yields sufficiently good approximation original function show hops enjoys linear convergence many well known non smooth problems empirical risk minimization piece wise linear loss function ell_1 norm regularizer finding point polyhedron cone programming etc experimental results verify effectiveness hops comparison nesterov smoothing algorithm primal dual style first order methods
nested mini batch means new algorithm proposed accelerates mini batch means algorithm sculley 2010 using distance bounding approach elkan 2003 argue incorporating distance bounds mini batch algorithm already used data preferentially reused end propose using nested mini batches whereby data mini batch iteration automatically reused iteration using nested mini batches presents difficulties first unbalanced use data bias estimates resolve ensuring data sample contributes exactly centroids second choosing mini batch sizes address balancing premature fine tuning centroids redundancy induced slow experiments show resulting nmbatch algorithm effective often arriving within empirical minimum 100 times earlier standard mini batch algorithm
density estimation via discrepancy based adaptive sequential partition given iid observations unknown continuous distribution defined domain omega propose nonparametric method learn piecewise constant function approximate underlying probability density function density estimate piecewise constant function defined binary partition omega key ingredient algorithm use discrepancy concept originates quasi monte carlo analysis control partition process resulting algorithm simple efficient provable convergence rate demonstrate empirically efficiency density estimation method also show utilized find good initializations means
budgeted stream based active learning via adaptive submodular maximization active learning enables reduce annotation cost adaptively selecting unlabeled instances labeled pool based active learning several effective methods theoretical guarantees developed maximizing utility function satisfying adaptive submodularity contrast methods stream based active learning based adaptive submodularity paper propose new class utility functions policy adaptive submodular functions prove class includes many existing adaptive submodular functions appearing real world problems provide general framework based policy adaptive submodularity makes possible convert existing pool based methods stream based methods give theoretical guarantees performance addition empirically demonstrate effectiveness comparing existing heuristics common benchmark datasets
lifelong learning weighted majority votes better understanding potential benefits information transfer representation learning important step towards goal building intelligent systems able persist world learn time work consider setting learner encounters stream tasks able retain limited information encountered task learned predictor contrast previous works analyzing scenario make distributional assumptions task generating process instead formulate complexity measure captures diversity observed tasks provide lifelong learning algorithm error guarantees every observed task rather average show sample complexity reductions comparison solving every task isolation terms task complexity measure algorithmic framework naturally viewed learning representation encountered tasks neural network
deep feature analysis underlying rapid visual categorization rapid categorization paradigms long history experimental psychology characterized short presentation times speeded behavioral responses tasks highlight efficiency visual system processes natural object categories previous studies shown feed forward hierarchical models visual cortex provide good fit human visual decisions time recent work computer vision demonstrated significant gains object recognition accuracy increasingly deep hierarchical architectures unclear well models account human visual decisions reveal underlying brain processes conducted large scale psychophysics study assess correlation computational models human behavioral responses rapid animal non animal categorization task considered visual representations varying complexity analyzing output different stages processing state art deep networks found recognition accuracy increases higher stages visual processing higher level stages indeed outperforming human participants task human decisions agree best predictions intermediate stages overall results suggest human participants rely visual features intermediate complexity complexity visual representations afforded modern deep network models exceed complexity used human participants rapid categorization
incremental boosting convolutional neural network facial action unit recognition recognizing facial action units aus spontaneous facial expressions still challenging problem recently cnns shown promise facial recognition however learned cnns often overfitted generalize well unseen subjects due limited coded training images proposed novel incremental boosting cnn cnn integrate boosting cnn via incremental boosting layer selects discriminative neurons lower layer incrementally updated successive mini batches addition novel loss function accounts errors incremental boosted classifier individual weak classifiers proposed fine tune cnn experimental results benchmark databases demonstrated cnn yields significant improvement traditional cnn boosting cnn without incremental learning well outperforming state art cnn based methods recognition improvement impressive aus lowest frequencies databases
multivariate tests association based univariate tests testing vector random variables independence propose testing whether distance vector arbitrary center point independent distance vector another arbitrary center point univariate test prove minimal assumptions enough consistent univariate independence test distances guarantee power detect dependence random vectors increases sample size univariate test distribution free multivariate test also distribution free consider multiple center points aggregate center specific univariate tests power improved resulting multivariate test distribution free specific aggregation methods univariate test distribution free show certain multivariate tests recently proposed literature viewed instances general approach moreover show experiments novel tests constructed using approach better power computational time competing approaches
surge surface regularized geometry estimation single image paper introduces approach regularize surface normal depth predictions pixel given single input image approach infers reasons underlying planar surfaces depicted image snap predicted normals depths inferred planar surfaces maintaining fine detail within objects approach comprises components fourstream convolutional neural network cnn depths surface normals likelihoods planar region planar boundary predicted pixel followed dense conditional random field dcrf integrates predictions normals depths compatible regularized planar region planar boundary information dcrf formulated gradients passed surface normal depth cnns via backpropagation addition propose new planar wise metrics evaluate geometry consistency within planar surfaces tightly related dependent editing applications show regularization yields relative improvement planar consistency nyu dataset
memory efficient backpropagation time propose novel approach reduce memory consumption backpropagation time bptt algorithm training recurrent neural networks rnns approach uses dynamic programming balance trade caching intermediate results recomputation algorithm capable tightly fitting within almost user set memory budget finding optimal execution policy minimizing computational cost computational devices limited memory capacity maximizing computational performance given fixed memory budget practical use case provide asymptotic computational upper bounds various regimes algorithm particularly effective long sequences sequences length 1000 algorithm saves memory usage using third time per iteration standard bptt
scan order gibbs sampling models matters bounds much gibbs sampling markov chain monte carlo sampling technique iteratively samples variables conditional distributions common scan orders variables random scan systematic scan due benefits locality hardware systematic scan commonly used even though statistical guarantees random scan conjectured mixing times random scan systematic scan differ logarithmic factor show counterexample case prove mixing times differ polynomial factor mild conditions prove relative bounds introduce method augmenting state space study systematic scan using conductance
lightrnn memory computation efficient recurrent neural networks recurrent neural networks rnns achieved state art performances many natural language processing tasks language modeling machine translation however vocabulary large rnn model become big possibly beyond memory capacity gpu device training become inefficient work propose novel technique tackle challenge key idea use component shared embedding word representations allocate every word vocabulary table row associated vector column associated another vector depending position table word jointly represented components row vector column vector since words row share row vector words column share column vector need sqrt vectors represent vocabulary unique words far less vectors required existing approaches based component shared embedding design new rnn algorithm evaluate using language modeling task several benchmark datasets results show algorithm significantly reduces model size speeds training process without sacrifice accuracy achieves similar better perplexity compared state art language models remarkably billion word benchmark dataset algorithm achieves comparable perplexity previous language models whilst reducing model size factor 100 speeding training process factor name proposed algorithm emph lightrnn reflect small model size high training speed
direct feedback alignment provides learning deep neural networks artificial neural networks commonly trained back propagation algorithm gradient learning provided back propagating error layer layer output layer hidden layers recently discovered method called feedback alignment shows weights used propagating error backward symmetric weights used propagation activation forward fact random feedback weights work evenly well network learns make feedback useful work feedback alignment principle used training hidden layers independently rest network initial condition error propagated fixed random feedback connections directly output layer hidden layer simple method able achieve training error even convolutional networks deep networks completely without error back propagation method step towards biologically plausible machine learning error signal almost local symmetric reciprocal weights required experiments show test performance mnist cifar almost good obtained back propagation fully connected networks combined dropout method achieves error permutation invariant mnist task
variational bayes monte carlo steroids variational approaches often used approximate intractable posteriors normalization constants hierarchical latent variable models often effective practice known approximation error arbitrarily large propose new class bounds marginal log likelihood directed latent variable models approach relies random projections simplify posterior contrast standard variational methods bounds guaranteed tight high probability provide new approach learning latent variable models based optimizing new bounds log likelihood demonstrate empirical improvements benchmark datasets vision language sigmoid belief networks neural network used approximate posterior
agnostic estimation misspecified phase retrieval models goal noisy high dimensional phase retrieval estimate sparse parameter boldsymbol beta mathbb realizations model boldsymbol top boldsymbol beta varepsilon based model propose significant semi parametric generalization called misspecified phase retrieval mpr boldsymbol top boldsymbol beta varepsilon unknown operatorname cov boldsymbol top boldsymbol beta example mpr encompasses boldsymbol top boldsymbol beta varepsilon increasing special case despite generality mpr model eludes reach existing semi parametric estimators paper propose estimation procedure consists solving cascade convex programs provably recovers direction boldsymbol beta theory backed thorough numerical results
following leader fast rates linear prediction curved constraint sets regularities follow leader ftl algorithm perhaps simplest online learning algorithms known perform well loss functions used positively curved paper ask whether lucky settings ftl achieves sublinear small regret particular study fundamental problem linear prediction non empty convex compact domain amongst results prove curvature boundary domain act losses curved case prove long mean loss vectors positive lengths bounded away ftl enjoys logarithmic growth rate regret polyhedral domains stochastic data enjoys finite expected regret building previously known meta algorithm also get algorithm simultaneously enjoys worst case guarantees bound available ftl
combining fully convolutional recurrent neural networks biomedical image segmentation segmentation images fundamental problem biomedical image analysis deep learning approaches achieved state art segmentation performance exploit contexts using neural networks known segmentation methods including convolution convolution planes orthogonal slices lstm multiple directions suffer incompatibility highly anisotropic dimensions common biomedical images paper propose new framework image segmentation based combination fully convolutional network fcn recurrent neural network rnn responsible exploiting intra slice inter slice contexts respectively best knowledge first framework image segmentation explicitly leverages image anisotropism evaluating using dataset isbi neuronal structure segmentation challenge house image stacks fungus segmentation approach achieves promising results comparing known based segmentation approaches
product cut introduce theoretical algorithmic framework multi way graph partitioning relies multiplicative cut based objective refer objective product cut provide detailed investigation mathematical properties objective effective algorithm optimization proposed model strong mathematical underpinnings corresponding algorithm achieves state art performance benchmark data sets
stochastic gradient methods distributionally robust optimization divergences develop efficient solution methods robust empirical risk minimization problem designed give calibrated confidence intervals performance provide optimal tradeoffs bias variance methods apply distributionally robust optimization problems proposed ben tal put weight observations inducing high loss via worst case approach non parametric uncertainty set underlying data distribution algorithm solves resulting minimax problems nearly computational cost stochastic gradient descent use several carefully designed data structures sample size per iteration cost method scales log allows give optimality certificates distributionally robust optimization provides little extra cost compared empirical risk minimization stochastic gradient methods
man computer programmer woman homemaker debiasing word embeddings blind application machine learning runs risk amplifying biases present data danger facing word embedding popular framework represent text data vectors used many machine learning natural language processing tasks show even word embeddings trained google news articles exhibit female male gender stereotypes disturbing extent raises concerns widespread use describe often tends amplify biases geometrically gender bias first shown captured direction word embedding second gender neutral words shown linearly separable gender definition words word embedding using properties provide methodology modifying embedding remove gender stereotypes association words receptionist female maintaining desired associations words queen female using crowd worker evaluation well standard benchmarks empirically demonstrate algorithms significantly reduce gender bias embeddings preserving useful properties ability cluster related concepts solve analogy tasks resulting embeddings used applications without amplifying gender bias
optimal spectral transportation application music transcription many spectral unmixing methods rely non negative decomposition spectral data onto dictionary spectral templates particular state art music transcription systems decompose spectrogram input signal onto dictionary representative note spectra typical measures fit used quantify adequacy decomposition compare data template entries frequency wise small displacements energy frequency bin another well variations timber disproportionally harm fit address issues means optimal transportation propose new measure fit treats frequency distributions energy holistically opposed frequency wise building harmonic nature sound new measure invariant shifts energy harmonically related frequencies well small local displacements energy equipped new measure fit dictionary note templates considerably simplified set dirac vectors located target fundamental frequencies musical pitch values turns gives ground fast simple decomposition algorithm achieves state art performance real musical data
combining adversarial guarantees stochastic fast rates online learning consider online learning algorithms guarantee worst case regret rates adversarial environments deployed safely perform robustly yet adapt optimally favorable stochastic environments perform well variety settings practical importance quantify friendliness stochastic environments means well known bernstein generalized tsybakov margin condition recent algorithms squint hedge setting metagrad online convex optimization show particular form data dependent individual sequence regret guarantees implies adapt automatically bernstein parameters stochastic environment prove algorithms attain fast rates respective settings expectation high probability
towards conceptual compression introduce convolutional draw homogeneous deep generative model achieving state art performance latent variable image modeling algorithm naturally stratifies information higher lower level details creating abstract features addressing fundamentally desired properties representation learning furthermore hierarchical ordering latents creates opportunity selectively store global information image yielding high quality conceptual compression framework
peripheral representations improve clutter metrics complex scenes previous studies proposed image based clutter measures correlate human search times eye movements however models take account fact effects clutter interact foveated nature human visual system visual clutter fovea increasing detrimental influence perception introduce new foveated clutter model predict detrimental effects target search utilizing forced fixation search task use feature congestion rosenholtz non foveated clutter model stack peripheral architecture top feature congestion foveated model introduce peripheral integration feature congestion pifc coefficient fundamental ingredient model modulates clutter non linear gain contingent eccentricity finally show foveated feature congestion ffc clutter scores 0001 correlate better target detection hit rate regular feature congestion 0774 forced fixation search extend foveation clutter models showing stronger correlations cases thus model allows enrich clutter perception research computing fixation specific clutter maps code building peripheral representations available
gap safe screening rules sparse group lasso statistical learning high dimension sparse regularizations proven useful boost computational statistical efficiency contexts natural handle refined structures pure sparsity instance group sparsity sparse group lasso recently introduced context linear regression enforce sparsity feature group level propose first provably safe screening rules sparse group lasso rules allow discard early solver features groups inactive optimal solution thanks efficient dual gap computations relying geometric properties epsilon norm safe screening rules sparse group lasso lead significant gains term computing time coordinate descent implementation
learning treewidth bounded bayesian networks thousands variables present method learning treewidth bounded bayesian networks data sets containing thousands variables bounding treewidth bayesian network greatly reduces complexity inferences yet global property graph considerably increases difficulty learning process novel algorithm accomplishes task scaling large domains large treewidths novel approach consistently outperforms state art experiments thousands variables
ancestral causal inference constraint based causal discovery limited data notoriously difficult challenge due many borderline independence test decisions several approaches improve reliability predictions exploiting redundancy independence information proposed recently though promising existing approaches still greatly improved terms accuracy scalability present novel method reduces combinatorial explosion search space using coarse grained representation causal information drastically reducing computation time additionally propose method score causal predictions based confidence crucially implementation also allows easily combine observational interventional data incorporate various types available background knowledge prove soundness asymptotic consistency method demonstrate outperform state art synthetic data achieving speedup several orders magnitude illustrate practical feasibility applying challenging protein data set
visual question answering question representation update qru method aims reasoning natural language questions visual images given natural language question image model updates question representation iteratively selecting image regions relevant query learns give correct answer model contains several reasoning layers exploiting complex visual relations visual question answering vqa task proposed network end end trainable back propagation weights initialized using pre trained convolutional neural network cnn gated recurrent unit gru method evaluated challenging datasets coco vqa yields state art performance
identification overidentification linear structural equation models paper address problems identifying linear structural equation models discovering constraints imply first extend half trek criterion cover broader class models apply extension finding testable constraints implied model show semi markovian linear model recursively decomposed simpler sub models resulting improved identification constraint discovery power finally show unlike existing methods developed linear models resulting method subsumes identification constraint discovery algorithms non parametric models
valid optimal assignment kernels applications graph classification success kernel methods initiated design novel positive semidefinite functions particular structured data leading design paradigm convolution kernel decomposes structured objects parts sums pairs parts assignment kernels contrast obtained optimal bijection parts provide valid notion similarity general however optimal assignments yield indefinite functions complicates use kernel methods characterize class base kernels used compare parts guarantees positive semidefinite optimal assignment kernels base kernels give rise hierarchies optimal assignment kernels computed linear time histogram intersection apply results developing weisfeiler lehman optimal assignment kernel graphs provides high classification accuracy widely used benchmark data sets improving original weisfeiler lehman kernel
constraints based convex belief propagation inference markov random fields subject consistency structure fundamental problem arises many real life applications order enforce consistency classical approaches utilize consistency potentials encode constraints feasible instances unfortunately comes price serious computational bottleneck paper suggest tackle consistency incorporating constraints beliefs permits derivation closed form message passing algorithm refer constraints based convex belief propagation cbcbp experiments show cbcbp outperforms standard approach least order magnitude faster
combinatorial energy learning image segmentation introduce new machine learning approach image segmentation uses neural network model conditional energy segmentation given image approach combinatorial energy learning image segmentation celis places particular emphasis modeling inherent combinatorial nature dense image segmentation problems propose efficient algorithms learning deep neural networks model energy function local optimization energy space supervoxel agglomerations extensively evaluate method publicly available microscopy dataset billion voxels ground truth data billion voxel test set find method improves volumetric reconstruction accuracy compared state art baseline methods graph based segmentation output convolutional neural network trained predict boundaries well random forest classifier trained agglomerate supervoxels generated convolutional neural network
scalable end end gaussian process adapter irregularly sampled time series classification present general framework classification sparse irregularly sampled time series properties time series result substantial uncertainty values underlying temporal processes making data difficult deal using standard classification methods assume fixed dimensional feature spaces address challenges propose uncertainty aware classification framework based special computational layer refer gaussian process adapter connect irregularly sampled time series data black box classifier learnable using gradient descent show scale required computations based combining structured kernel interpolation framework lanczos approximation method discriminatively train gaussian process adapter combination number classifiers end end using backpropagation
stochastic variance reduction methods saddle point problems consider convex concave saddle point problems objective functions split many components extend recent stochastic variance reduction methods svrg saga provide first large scale linearly convergent algorithms class problems common machine learning algorithmic extension straightforward comes challenges opportunities convex minimization analysis apply use notion monotone operators prove convergence showing particular algorithm applies larger class problems variational inequalities notions splits terms functions terms partial derivatives split need done convex concave terms non uniform sampling key efficient algorithm theory practice incremental algorithms easily accelerated using simple extension catalyst framework leading algorithm always superior accelerated batch algorithms
dimensionality reduction massive sparse datasets using coresets paper present practical solution performance guarantees problem dimensionality reduction large scale sparse matrices show applications approach computing principle component analysis pca times matrix using pass stream rows solution uses coresets scaled subset rows approximates sum squared distances emph every dimensional emph affine subspace open theoretical problem compute coreset independent open practical problem compute non trivial approximation pca large sparse databases wikipedia document term matrix reasonable time answer questions affirmatively main technical result new framework deterministic coreset constructions based reduction problem counting items stream
efficient state space modularization planning theory behavioral neural signatures even state spaces modest size planning plagued curse dimensionality problem particularly acute human animal cognition given limited capacity working memory time pressures planning often occurs natural environment hierarchically organized modular representations long suggested underlie capacity biological systems efficiently flexibly plan complex environments however principles underlying efficient modularization remain obscure making difficult identify behavioral neural signatures develop normative theory efficient state space representations partitions environment distinct modules minimizing average information theoretic description length planning within environment thereby optimally trading complexity planning across within modules show optimal representations provide unifying account diverse range hitherto unrelated phenomena multiple levels behavior neural representation
adaptive newton method empirical risk minimization statistical accuracy consider empirical risk minimization large scale datasets introduce ada newton adaptive algorithm uses newton method adaptive sample sizes main idea ada newton increase size training set factor larger way minimization variable current training set local neighborhood optimal argument next training set allows exploit quadratic convergence property newton method reach statistical accuracy training set iteration newton method show theoretically iteratively increase sample size applying single newton iterations without line search staying within statistical accuracy regularized empirical risk particular double size training set iteration number samples sufficiently large numerical experiments various datasets confirm possibility increasing sample size factor iteration implies ada newton achieves statistical accuracy full training set passes dataset
retain interpretable predictive model healthcare using reverse time attention mechanism accuracy interpretability dominant features successful predictive models typically choice must made favor complex black box models recurrent neural networks rnn accuracy versus less accurate interpretable traditional models logistic regression tradeoff poses challenges medicine accuracy interpretability important addressed challenge developing reverse time attention model retain application electronic health records ehr data retain achieves high accuracy remaining clinically interpretable based level neural attention model detects influential past visits significant clinical variables within visits key diagnoses retain mimics physician practice attending ehr data reverse time order recent clinical visits likely receive higher attention retain tested large health system ehr dataset million visits completed 263k patients year period demonstrated predictive accuracy computational scalability comparable state art methods rnn ease interpretability comparable traditional models
joint quantile regression vector valued rkhss addressing give complete picture average relationship provided standard regression novel framework estimating predicting simultaneously several conditional quantiles introduced proposed methodology leverages kernel based multi task learning curb embarrassing phenomenon quantile crossing step estimation procedure post processing moreover framework comes along theoretical guarantees efficient coordinate descent learning algorithm numerical experiments benchmark real datasets highlight enhancements approach regarding prediction error crossing occurrences training time
learnable visual markers propose new approach designing visual markers analogous codes markers augmented reality robotic fiducial tags based advances deep generative networks approach markers obtained color images synthesized deep network input bit strings whereas another deep network trained recover bit strings back photos markers networks trained simultaneously joint backpropagation process takes characteristic photometric geometric distortions associated marker fabrication capture account additionally stylization loss based statistics activations pretrained classification network inserted learning order shift marker appearance towards texture prototype experiments demonstrate markers obtained using approach capable retaining bit strings long enough practical ability automatically adapt markers according usage scenario desired capacity well ability combine information encoding artistic stylization unique properties approach byproduct approach provides insight structure patterns suitable recognition convnets ability distinguish composite patterns
exponential expressivity deep neural networks transient chaos combine riemannian geometry mean field theory high dimensional chaos study nature signal propagation deep neural networks random weights results reveal phase transition expressivity random deep networks networks chaotic phase computing nonlinear functions whose global curvature grows exponentially depth width prove generic class random functions cannot efficiently computed shallow network going beyond prior work restricts analysis single functions moreover formally quantify demonstrate long conjectured idea deep networks disentangle exponentially curved manifolds input space flat manifolds hidden space theoretical framework analyzing expressive power deep networks broadly applicable provides basis quantifying previously abstract notions geometry deep functions
multiplicative integration recurrent neural networks introduce general simple structural design called multiplicative integration improve recurrent neural networks rnns changes way information flow gets integrated computational building block rnn introducing almost extra parameters new structure easily embedded many popular rnn models including lstms grus empirically analyze learning behaviour conduct evaluations several tasks using different rnn models experimental results demonstrate multiplicative integration provide substantial performance boost many existing rnn models
interpretable nonlinear dynamic modeling neural trajectories central challenge neuroscience understanding neural system implements computation dynamics propose nonlinear time series model aimed characterizing interpretable dynamics neural trajectories model assumes low dimensional continuous dynamics finite volume incorporates prior assumption globally contractional dynamics avoid overly enthusiastic extrapolation outside support observed trajectories show model recover qualitative features phase portrait attractors slow points bifurcations also producing reliable long term future predictions variety dynamical models real neural data
globally optimal training generalized polynomial neural networks nonlinear spectral methods optimization problem behind neural networks highly non convex training stochastic gradient descent variants requires careful parameter tuning provides guarantee achieve global optimum contrast show quite weak assumptions data particular class feedforward neural networks trained globally optimal linear convergence rate knowledge first practically feasible method achieves guarantee method principle applied deep networks restrict simplicity paper hidden layer networks experiments confirms models already rich enough achieve good performance series real world datasets
linear feature encoding reinforcement learning feature construction vital importance reinforcement learning quality value function policy largely determined corresponding features recent successes deep reinforcement learning increase importance understanding feature construction typical deep approaches use linear output layer means deep interpreted feature construction encoding network followed linear value function approximation paper develops evaluates theory linear feature encoding extend theoretical results feature quality linear value function approximation uncontrolled case controlled case develop supervised linear feature encoding method motivated insights linear value function approximation theory well empirical successes deep resulting encoder surprisingly effective method linear value function approximation using raw images inputs
graphical time warping joint alignment multiple curves dynamic time warping dtw fundamental technique time series analysis comparing curve another using flexible time warping function however designed compare single pair curves many applications metabolomics image series analysis alignment simultaneously needed multiple pairs underlying warping functions often related independent application dtw pair sub optimal solution yet largely unknown efficiently conduct joint alignment warping functions simultaneously considered since given warping function constrained others dynamic programming cannot applied paper show joint alignment problem transformed network flow problem thus exactly efficiently solved max flow algorithm guarantee global optimality name proposed approach graphical time warping gtw emphasizing graphical nature solution dependency structure warping functions represented graph modifications dtw windowing weighting readily derivable within gtw also discuss optimal tuning parameters hyperparameters gtw illustrate power gtw using synthetic data real case study astrocyte calcium movie
mixed linear regression multiple components paper study mixed linear regression mlr problem goal recover multiple underlying linear models unlabeled linear measurements propose non convex objective function show locally strongly convex neighborhood ground truth use tensor method initialization initial models local strong convexity region employ general convex optimization algorithms minimize objective function best knowledge approach provides first exact recovery guarantees mlr problem geq components moreover method near optimal computational complexity tilde well near optimal sample complexity tilde constant furthermore show non convex formulation extended solving subspace clustering problem well particular initialized within small constant distance true subspaces method converges global optima recovers true subspaces time linear number points furthermore empirical results indicate even random initialization approach converges global optima linear time providing speed orders magnitude
statistical inference pairwise graphical models using score matching probabilistic graphical models widely used model complex systems aid scientific discoveries result large body literature focused consistent model selection however scientists often interested understanding uncertainty associated estimated parameters current literature addressed thoroughly paper propose novel estimator edge parameters pairwise graphical models based hyv arinen scoring rule hyv arinen scoring rule especially useful cases normalizing constant cannot obtained efficiently closed form prove estimator sqrt consistent asymptotically normal result allows construct confidence intervals edge parameters well hypothesis tests establish results conditions typically assumed literature consistent estimation however require estimator consistently recovers graph structure particular prove asymptotic distribution estimator robust model selection mistakes uniformly valid large number data generating processes illustrate validity estimator extensive simulation studies
hardness online sleeping combinatorial optimization problems show several online combinatorial optimization problems admit efficient regret algorithms become computationally hard sleeping setting subset actions becomes unavailable round specifically show sleeping versions problems least hard pac learning dnf expressions long standing open problem show hardness sleeping versions online shortest paths online minimum spanning tree online subsets online truncated permutations online minimum cut online bipartite matching hardness result sleeping version online shortest paths problem resolves open problem presented colt 2015 koolen 2015
algorithm nearest neighbor search via monotonic embedding fast algorithms nearest neighbor search large part focused distance develop approach distance begins explicit exact embedding points show embedding efficiently combined random projection methods search locality sensitive hashing random projection trees rigorously establish correctness methodology show experimentation competitive practice available alternatives
local maxima likelihood gaussian mixture models structural results algorithmic consequences provide fundamental results population infinite sample likelihood function gaussian mixture models geq components first main result shows population likelihood function bad local maxima even special case equally weighted mixtures well separated spherical gaussians prove log likelihood value bad local maxima arbitrarily worse global optimum thereby resolving open question srebro 2007 second main result shows algorithm first order variant random initialization converge bad critical points probability least omega establish first order variant converge strict saddle points almost surely indicating poor performance first order method attributed existence bad local maxima rather bad saddle points overall results highlight necessity careful initialization using algorithm practice even applied highly favorable settings
learning user perceived clusters feature level supervision semi supervised clustering algorithms proposed identify data clusters align user perceived ones via aid side information seeds pairwise constrains however traditional side information mostly instance level subject sampling bias non randomly sampled instances supervision mislead algorithms wrong clusters paper propose learning feature level supervision show kind supervision easily obtained form perception vectors many applications present novel algorithms called perception embedded clustering exploit perception vectors well traditional side information find clusters perceived user extensive experiments conducted real datasets results demonstrate effectiveness empirically
infogan interpretable representation learning information maximizing generative adversarial nets paper describes infogan information theoretic extension generative adversarial network able learn disentangled representations completely unsupervised manner infogan generative adversarial network also maximizes mutual information small subset latent variables observation derive lower bound mutual information objective optimized efficiently show training procedure interpreted variation wake sleep algorithm specifically infogan successfully disentangles writing styles digit shapes mnist dataset pose lighting rendered images background digits central digit svhn dataset also discovers visual concepts include hair styles presence absence eyeglasses emotions celeba face dataset experiments show infogan learns interpretable representations competitive representations learned existing fully supervised methods
neural universal discrete denoiser present new framework applying deep neural networks dnn devise universal discrete denoiser unlike approaches utilize supervised learning denoising require additional training data setting ground truth label clean data available devise pseudo labels novel objective function dnn trained way supervised learning become discrete denoiser experimentally show resulting algorithm dubbed neural dude significantly outperforms previous state art several applications systematic rule choosing hyperparameter attractive feature practice
primal dual method conic constrained distributed optimization problems consider cooperative multi agent consensus optimization problems undirected network agents agents connected edge directly communicate objective minimize sum agent specific composite convex functions agent specific private conic constraint sets hence optimal consensus decision lie intersection private sets provide convergence rates sub optimality infeasibility consensus violation examine effect underlying network topology convergence rates proposed decentralized algorithms show extend methods handle time varying communication networks
eliciting categorical data optimal aggregation models collecting aggregating categorical data crowdsourcing platforms typically fall broad categories assuming agents honest consistent heterogeneous error rates assuming agents strategic seek maximize expected reward former often leads tractable aggregation elicited data latter usually focuses optimal elicitation consider aggregation paper develop bayesian model wherein agents differing quality information also respond incentives model generalizes categories enables joint exploration optimal elicitation aggregation model enables exploration analytically experimentally optimal aggregation categorical data optimal multiple choice interface design
depth single image harmonizing overcomplete local network predictions single color image contain many cues informative towards different aspects local geometric structure approach problem monocular depth estimation using neural network produce mid level representation summarizes cues network trained characterize local scene geometry predicting every image location depth derivatives different orders orientations scales however instead single estimate derivative network outputs probability distributions allow express confidence coefficients ambiguity others scene depth estimated harmonizing overcomplete set network predictions using globalization procedure finds single consistent depth map best matches local derivative distributions demonstrate efficacy approach evaluation nyu depth data set
seboost boosting stochastic learning using subspace optimization techniques present seboost technique boosting performance existing stochastic optimization methods seboost applies secondary optimization process subspace spanned last steps descent directions method inspired sesop optimization method large scale problems adapted stochastic learning framework applied top existing optimization method need tweak internal algorithm show method able boost performance different algorithms make robust changes hyper parameters boosting steps seboost applied large sets descent steps additional subspace optimization hardly increases overall computational burden introduce hyper parameters control balance baseline method secondary optimization process method evaluated several deep learning tasks demonstrating promising results
reshaped wirtinger flow solving quadratic system equations study problem recovering vector bbr magnitude measurements y_i langle ba_i rangle work along line wirtinger flow approach citet candes2015phase solves problem minimizing nonconvex loss function via gradient algorithm shown converge global optimal point good initialization contrast smooth loss function used adopt nonsmooth lower order loss function design gradient like algorithm referred reshaped show random gaussian measurements reshaped enjoys geometric convergence global optimal point long number measurements order dimension unknown improves sample complexity achieves sample complexity truncated citet chen2015solving without truncation gradient step furthermore reshaped costs less computationally runs faster numerically truncated bypassing higher order variables loss function truncations gradient loop analysis reshaped simplified
training evaluating multimodal word embeddings large scale web annotated images paper focus training evaluating effective word embeddings text visual information specifically introduce large scale dataset 300 million sentences describing million images crawled downloaded publicly available pins image sentence descriptions uploaded users pinterest dataset 200 times larger coco standard large scale image dataset sentence descriptions addition construct evaluation dataset directly assess effectiveness word embeddings terms finding semantically similar related words phrases word phrase pairs evaluation dataset collected click data millions users image search system thus contain rich semantic relationships based datasets propose compare several recurrent neural networks rnns based multimodal text image models experiments show model benefits incorporating visual information word embeddings weight sharing strategy crucial learning multimodal embeddings project page http www stat ucla edu junhua mao multimodal_embedding html datasets introduced work gradually released project page
online ica understanding global dynamics nonconvex optimization via diffusion processes solving statistical learning problems often involves nonconvex optimization despite empirical success nonconvex statistical optimization methods global dynamics especially convergence desirable local minima remain less well understood theory paper propose new analytic paradigm based diffusion processes characterize global dynamics nonconvex statistical optimization concrete example study stochastic gradient descent sgd tensor decomposition formulation independent component analysis particular cast different phases sgd diffusion processes solutions stochastic differential equations initialized unstable equilibrium global dynamics sgd transit consecutive phases unstable ornstein uhlenbeck process slowly departing initialization solution ordinary differential equation quickly evolves towards desirable local minimum iii stable ornstein uhlenbeck process oscillating around desirable local minimum proof techniques based upon stroock varadhan weak convergence markov chains diffusion processes independent interest
vime variational information maximizing exploration scalable effective exploration remains key challenge reinforcement learning methods optimality guarantees setting discrete state action spaces methods cannot applied high dimensional deep scenarios contemporary relies simple heuristics epsilon greedy exploration adding gaussian noise controls paper introduces variational information maximizing exploration vime exploration strategy based maximization information gain agent belief environment dynamics propose practical implementation using variational inference bayesian neural networks efficiently handles continuous state action spaces vime modifies mdp reward function applied several different underlying algorithms demonstrate vime achieves significantly better performance compared heuristic exploration methods across variety continuous control tasks algorithms including tasks sparse rewards
deconvolving feedback loops recommender systems collaborative filtering popular technique infer users preferences new content based collective information users preferences recommender systems use information make personalized suggestions users users accept recommendations creates feedback loop recommender system loops iteratively influence collaborative filtering algorithm predictions time investigate whether possible identify items affected feedback loops state sufficient assumptions deconvolve feedback loops keeping inverse solution tractable furthermore develop metric unravel recommender system influence entire user item rating matrix use metric synthetic real world datasets identify extent recommender system affects final rating matrix rank frequently recommended items distinguish whether user rated item recommended intrinsic preference results indicate possible recover ratings matrix intrinsic user preferences using single snapshot ratings matrix without temporal information
non parametric learning method confidently estimating patient clinical state dynamics estimating patient clinical state multiple concurrent physiological streams plays important role determining therapeutic intervention necessary triaging patients hospital paper construct non parametric learning algorithm estimate clinical state patient algorithm addresses several known challenges clinical state estimation eliminating bias introduced therapeutic intervention censoring increasing timeliness state estimation ensuring sufficient accuracy ability detect anomalous clinical states benefits obtained combining tools non parametric bayesian inference permutation testing generalizations empirical bernstein inequality algorithm validated using real world data cancer ward large academic hospital
semiparametric differential graph models many cases network analysis attractive study network varies different conditions individual static network propose novel graphical model namely latent differential graph model networks different conditions represented semiparametric elliptical distributions respectively variation networks differential graph characterized difference latent precision matrices propose estimator differential graph based quasi likelihood maximization nonconvex regularization show estimator attains faster statistical rate parameter estimation state art methods enjoys oracle property mild conditions thorough experiments synthetic real world data support theory
non convex pass framework generalized factorization machine rank matrix sensing develop efficient alternating framework learning generalized version factorization machine gfm steaming data provable guarantees instances sampled dimensional random gaussian vectors target second order coefficient matrix gfm rank algorithm converges linearly achieves epsilon recovery error retrieving log epsilon training instances consumes memory pass dataset requires matrix vector product operations iteration key ingredient framework construction estimation sequence endowed called conditionally independent rip condition rip special cases gfm framework applied symmetric asymmetric rank matrix sensing problems inductive matrix completion phase retrieval
sublinear time orthogonal tensor decomposition recent work wang nips 2015 gives fastest known algorithms orthogonal tensor decomposition provable guarantees algorithm based computing sketches input tensor requires reading entire input show number cases achieve theoretical guarantees sublinear time even without reading input tensor instead using sketches estimate inner products tensor decomposition algorithms use importance sampling achieve sublinear time need know norms tensor slices show number important cases symmetric tensors sum_ lambda_i u_i otimes lambda_i estimate norms sublinear time whenever even important case small values also estimate norms asymmetric tensors sublinear time possible general show tensor slice norms slightly sublinear time possible main strengths work empirical number cases algorithm orders magnitude faster existing methods accuracy
achieving budget optimality adaptive schemes crowdsourcing adaptive schemes tasks assigned based data collected thus far widely used practical crowdsourcing systems efficiently allocate budget however existing theoretical analyses crowdsourcing systems suggest gain adaptive task assignments minimal bridge gap investigate question strictly general probabilistic model recently introduced model practical crowdsourcing data sets generalized dawid skene model characterize fundamental trade budget accuracy introduce novel adaptive scheme matches fundamental limit quantify gain adaptivity comparing trade non adaptive schemes confirm gain significant made arbitrarily large depending distribution difficulty level tasks hand
joint line segmentation transcription end end handwritten paragraph recognition offline handwriting recognition systems require cropped text line images training recognition hand annotation position transcript line level costly obtain hand automatic line segmentation algorithms prone errors compromising subsequent recognition paper propose modification popular efficient multi dimensional long short term memory recurrent neural networks mdlstm rnns enable end end processing handwritten paragraphs particularly replace collapse layer transforming dimensional representation sequence predictions recurrent version select line time proposed model neural network performs kind implicit line segmentation computing attention weights image representation experiments paragraphs rimes iam databases yield results competitive networks trained line level constitute significant step towards end end transcription full documents
human decision making limited time abstract subjective expected utility theory assumes decision makers possess unlimited computational resources reason choices however virtually decisions everyday life made resource constraints decision makers bounded rationality experimentally tested predictions made formalization bounded rationality based ideas statistical mechanics information theory systematically tested human subjects ability solve combinatorial puzzles different time limitations found bounded rational model accounts well data decomposition fitted model parameter subjects expected utility function resource parameter provide interesting insight subjects information capacity limits results confirm humans gradually fall back learned prior choice patterns confronted increasing resource limitations
joint best diverse labelings parametric submodular minimization consider problem jointly inferring best diverse labelings binary high order submodular energy graphical model recently shown problem solved global optimum many practically interesting diversity measures noted labelings called nested nestedness property also holds labelings class parametric submodular minimization problems different values global parameter gamma give rise different solutions popular example parametric submodular minimization monotonic parametric max flow problem also widely used computing multiple labelings main contribution work establish close relationship diversity submodular energies parametric submodular minimization particular joint best diverse labelings obtained running non parametric submodular minimization special case max flow solver different values gamma parallel certain diversity measures importantly values gamma computed closed form advance prior optimization theoretical results suggest simple yet efficient algorithms joint best diverse problem outperform competitors terms runtime quality results particular show paper new methods compute exact best diverse labelings faster popular method batra sense obtains approximate solutions
even faster svd decomposition yet without agonizing pain study svd obtain first singular vectors matrix approximately recently breakthroughs discovered svd musco musco provided first gap free theorem block krylov method shamir discovered first variance reduction stochastic method bhojanapalli provided fastest nnz poly eps type algorithm using alternating minimization paper improve breakthroughs providing new framework solving svd particular obtain faster gap free convergence speed outperforming obtain first accelerated stochastic method outperforming nnz running time regime outperform without even using alternating minimization certain parameter regimes
fast accurate spike sorting high channel count probes kilosort new silicon technology enabling large scale electrophysiological recordings vivo hundreds thousands channels interpreting recordings requires scalable accurate automated methods spike sorting minimize time required manual curation results introduce kilosort new integrated spike sorting framework uses template matching spike detection spike clustering kilosort models electrical voltage sum template waveforms triggered spike times allows overlapping spikes identified resolved unlike previous algorithms compress data pca kilosort operates raw data allows construct accurate model waveforms processing times faster previous algorithms thanks batch based optimization gpus compare kilosort established algorithm show favorable performance much reduced processing times novel post clustering merging step based continuity templates reduced substantially number manual operations required data neurons near error rates paving way fully automated spike sorting multichannel electrode recordings
batched gaussian process bandit optimization via determinantal point processes gaussian process bandit optimization emerged powerful tool optimizing noisy black box functions example machine learning hyper parameter optimization evaluation target function require training model involve days even weeks computation methods called bayesian optimization allow sequential exploration parameter space however often desirable propose batches sets parameter values explore simultaneously especially large parallel processing facilities disposal batch methods require modeling interaction different evaluations batch expensive complex scenarios paper propose new approach parallelizing bayesian optimization modeling diversity batch via determinantal point processes dpps whose kernels learned automatically allows generalize previous result well prove better regret bounds based dpp sampling experiments variety synthetic real world robotics hyper parameter optimization tasks indicate dpp based methods especially based dpp sampling outperform state art methods
stochastic multiple choice learning training diverse deep ensembles many practical perception systems exist within larger processes often include interactions users additional components capable evaluating quality predicted solutions contexts beneficial provide oracle mechanisms multiple highly likely hypotheses rather single prediction work pose task producing multiple outputs learning problem ensemble deep networks introducing novel stochastic gradient descent based approach minimize loss respect oracle method simple implement agnostic architecture loss function parameter free approach achieves lower oracle error compared existing methods wide range tasks deep architectures also show qualitatively solutions produced approach often provide interpretable representations task ambiguity
optimal sparse linear encoders sparse pca principal components analysis pca optimal linear encoder data sparse linear encoders sparse pca produce interpretable features promote better generalization given level sparsity best approximation pca efficient algorithms achieve optimal combinatorial tradeoff answer questions providing first polynomial time algorithms construct emph optimal sparse linear auto encoders additionally demonstrate performance algorithms real data
using social dynamics make individual predictions variational inference stochastic kinetic model social dynamics concerned primarily interactions among individuals resulting group behaviors modeling temporal evolution social systems via interactions individuals within systems particular availability large scale data social networks sensor networks offers unprecedented opportunity predict state changing events individual level examples events include disease transmission opinion transition elections rumor propagation unlike previous research focusing collective effects social systems study makes efficient inferences individual level order cope dynamic interactions among large number individuals introduce stochastic kinetic model capture adaptive transition probabilities propose efficient variational inference algorithm complexity grows linearly rather exponentially number individuals validate method performed epidemic dynamics experiments wireless sensor network data collected thousand people years proposed algorithm used track disease transmission predict probability infection individual results demonstrate method efficient sampling nonetheless achieving high accuracy
learning additive exponential family graphical models via ell_ norm regularized estimation investigate subclass exponential family graphical models sufficient statistics defined arbitrary additive forms propose ell_ norm regularized maximum likelihood estimators learn model parameters samples first joint mle estimator estimates parameters simultaneously second node wise conditional mle estimator estimates parameters node individually estimators statistical analysis shows mild conditions extra flexibility gained additive exponential family models comes almost cost statistical efficiency monte carlo approximation method developed efficiently optimize proposed estimators advantages estimators gaussian graphical models nonparanormal estimators demonstrated synthetic real data sets
residual networks behave like ensembles relatively shallow networks work propose novel interpretation residual networks showing seen collection many paths differing length moreover residual networks seem enable deep networks leveraging short paths training support observation rewrite residual networks explicit collection paths unlike traditional models paths residual networks vary length lesion study reveals paths show ensemble like behavior sense strongly depend finally surprising paths shorter might expect short paths needed training longer paths contribute gradient example gradient residual network 110 layers comes paths layers deep results reveal key characteristics seem enable training deep networks residual networks avoid vanishing gradient problem introducing short paths carry gradient throughout extent deep networks
full capacity unitary recurrent neural networks recurrent neural networks powerful models processing sequential data generally plagued vanishing exploding gradient problems unitary recurrent neural networks urnns use unitary recurrence matrices recently proposed means avoid issues however previous experiments recurrence matrices restricted product parameterized unitary matrices open question remains parameterization fail represent unitary matrices restricted representational capacity limit learned address question propose full capacity urnns optimize recurrence matrix unitary matrices leading significantly improved performance urnns use restricted capacity recurrence matrix contribution consists main components first provide theoretical argument determine unitary parameterization restricted capacity using argument show recently proposed unitary parameterization restricted capacity hidden state dimension greater second show complete full capacity unitary recurrence matrix optimized differentiable manifold unitary matrices resulting multiplicative gradient step simple require gradient clipping learning rate adaptation confirm utility claims empirically evaluating new full capacity urnns synthetic natural data achieving superior performance compared lstms original restricted capacity urnns
quantum perceptron models demonstrate quantum computation provide non trivial improvements computational statistical complexity perceptron model develop quantum algorithms perceptron learning first algorithm exploits quantum information processing determine separating hyperplane using number steps sublinear number data points namely sqrt second algorithm illustrates classical mistake bound frac gamma improved frac sqrt gamma quantum means gamma denotes margin improvements achieved application quantum amplitude amplification version space interpretation perceptron model
mapping estimation discrete optimal transport interested computation transport map optimal transport problem computational approaches optimal transport use kantorovich relaxation problem learn probabilistic coupling mgamma address problem learning underlying transport map funct linked original monge problem consequently lowers potential usage methods contexts samples computations mandatory paper propose new way jointly learn coupling approximation transport map use jointly convex formulation efficiently optimized additionally jointly learning coupling transport map allows smooth result optimal transport generalize samples examples empirically show interest relevance method tasks domain adaptation image editing
stochastic gradient geodesic mcmc methods propose stochastic gradient mcmc methods sampling bayesian posterior distributions defined riemann manifolds known geodesic flow hyperspheres methods first scalable sampling methods manifolds aid stochastic gradients novel dynamics conceived 2nd order integrators developed adopting embedding techniques geodesic integrator methods require global coordinate system manifold involve inner iterations synthetic experiments show validity method application challenging inference spherical topic models indicate practical usability efficiency
variational information maximization feature selection feature selection fundamental problems machine learning extensive body work information theoretic feature selection exists based maximizing mutual information subsets features class labels practical methods forced rely approximations due difficulty estimating mutual information demonstrate approximations made existing methods based unrealistic assumptions formulate flexible general class assumptions based variational distributions use tractably generate lower bounds mutual information bounds define novel information theoretic framework feature selection prove optimal tree graphical models proper choice variational distributions experiments demonstrate proposed method strongly outperforms existing information theoretic feature selection approaches
minimax approach supervised learning given task predicting loss function set probability distributions gamma optimal decision rule minimizing worst case expected loss gamma paper address question introducing generalization maximum entropy principle applying principle sets distributions marginal constrained empirical marginal provide minimax interpretation maximum likelihood problem generalized linear models connects minimax problem loss function generalized linear model cases quadratic logarithmic loss functions revisit well known linear logistic regression models approach reveals novel models loss functions particular loss derive classification approach call minimax svm minimax svm minimizes worst case expected loss proposed gamma solving tractable optimization problem perform several numerical experiments minimax svm outperforms svm
fast distributed submodular cover public private data summarization paper introduce public private framework data summarization motivated privacy concerns personalized recommender systems online social services systems usually access massive data generated large pool users major fraction data public visible used users however user also contribute private data shared users ensure privacy goal provide succinct summary massive dataset ideally small possible customized summaries built user contain elements public data diversity users private data personalization formalize challenge assume scoring function according user evaluates utility summary satisfies submodularity widely used notion data summarization applications thus model data summarization targeted user instance submodular cover problem however data massive infeasible use centralized greedy algorithm find customized summary even single user moreover large pool users time consuming find summaries separately instead develop fast distributed algorithm submodular cover fastcover provides succinct summary shot users show solution provided fastcover competitive centralized algorithm number rounds exponentially smaller state art results moreover implemented fastcover spark demonstrate practical performance number concrete applications including personalized location recommendation personalized movie recommendation dominating set tens millions data points varying number users
domain separation networks cost large scale data collection annotation often makes application machine learning algorithms new tasks datasets prohibitively expensive approach circumventing cost training models synthetic data annotations provided automatically despite appeal models often fail generalize synthetic real images necessitating domain adaptation algorithms manipulate models successfully applied existing approaches focus either mapping representations domain learning extract features invariant domain extracted however focusing creating mapping shared representation domains ignore individual characteristics domain hypothesize explicitly modeling unique domain improve model ability extract domain invariant features inspired work private shared component analysis explicitly learn extract image representations partitioned subspaces component private domain shared across domains model trained perform task care source domain also use partitioned representation reconstruct images domains novel architecture results model outperforms state art range unsupervised domain adaptation scenarios additionally produces visualizations private shared representations enabling interpretation domain adaptation process
multimodal residual learning visual deep neural networks continue advance state art image recognition tasks various methods however applications methods multimodality remain limited present multimodal residual networks mrn multimodal residual learning visual question answering extends idea deep residual learning unlike deep residual learning mrn effectively learns joint representation visual language information main idea use element wise multiplication joint residual mappings exploiting residual learning attentional models recent studies various alternative models introduced multimodality explored based study achieve state art results visual dataset open ended multiple choice tasks moreover introduce novel method visualize attention effect joint representations learning block using back propagation algorithm even though visual features collapsed without spatial information
optimizing affinity based binary hashing using auxiliary coordinates supervised binary hashing wants learn function maps high dimensional feature vector vector binary codes application fast image retrieval typically results difficult optimization problem nonconvex nonsmooth discrete variables involved much work simply relaxed problem training solving continuous optimization truncating codes posteriori gives reasonable results quite suboptimal recent work tried optimize objective directly binary codes achieved better results hash function still learned posteriori remains suboptimal propose general framework learning hash functions using affinity based loss functions uses auxiliary coordinates closes loop optimizes jointly hash functions binary codes gradually match resulting algorithm seen iterated version procedure optimizing first codes learning hash function compared optimization guaranteed obtain better hash functions much slower demonstrated experimentally various supervised datasets addition framework facilitates design optimization algorithms arbitrary types loss hash functions
coresets scalable bayesian logistic regression use bayesian methods large scale data settings attractive rich hierarchical models uncertainty quantification prior specification provide standard bayesian inference algorithms computationally expensive however making direct application large datasets difficult infeasible recent work scaling bayesian inference focused modifying underlying algorithms example use random data subsample iteration leverage insight data often redundant instead obtain weighted subset data called coreset much smaller original dataset use small coreset number existing posterior inference algorithms without modification paper develop efficient coreset construction algorithm bayesian logistic regression models provide theoretical guarantees size approximation quality coreset fixed known datasets expectation wide class data generative models crucially proposed approach also permits efficient construction coreset streaming parallel settings minimal additional effort demonstrate efficacy approach number synthetic real world datasets find practice size coreset independent original dataset size furthermore constructing coreset takes negligible amount time compared required run mcmc
parallel knowledge gradient method batch bayesian optimization many applications black box optimization evaluate multiple points simultaneously evaluating performances several different neural network architectures parallel computing environment paper develop novel batch bayesian optimization algorithm parallel knowledge gradient method construction method provides step bayes optimal batch points sample provide efficient strategy computing bayes optimal batch points demonstrate parallel knowledge gradient method finds global optima significantly faster previous batch bayesian optimization algorithms synthetic test functions tuning hyperparameters practical machine learning algorithms especially function evaluations noisy
learning multiagent communication backpropagation many tasks require collaboration multiple agents typically communication protocol agents manually specified altered training paper explore simple neural model called commnet uses continuous communication fully cooperative tasks model consists multiple agents communication learned alongside policy apply model diverse set tasks demonstrating ability agents learn communicate amongst yielding improved performance non communicative agents baselines cases possible interpret language devised agents revealing simple effective strategies solving task hand
optimal binary classifier aggregation general losses address problem aggregating ensemble predictors known loss bounds semi supervised binary classification setting minimize prediction loss incurred unlabeled data find minimax optimal predictions general class loss functions including convex many non convex losses extending recent analysis problem misclassification error result family semi supervised ensemble aggregation algorithms efficient linear learning convex optimization minimax optimal without relaxations decision rules take form familiar decision theory applying sigmoid functions notion ensemble margin without assumptions typically made margin based learning
generalized reparameterization gradient reparameterization gradient become widely used method obtain monte carlo gradients optimize variational objective however technique easily apply commonly used distributions beta gamma without approximations practical applications reparameterization gradient fit gaussian distributions paper introduce generalized reparameterization gradient method extends reparameterization gradient wider class variational distributions generalized reparameterizations use invertible transformations latent variables lead transformed distributions weakly depend variational parameters results new monte carlo gradients combine reparameterization gradients score function gradients demonstrate approach variational inference complex probabilistic models generalized reparameterization effective even single sample variational distribution enough obtain low variance gradient
conditional generative moment matching networks maximum mean discrepancy mmd successfully applied learn deep generative models characterizing joint distribution variables via kernel mean embedding paper present conditional generative moment matching networks cgmmn learn conditional distribution given input variables based conditional maximum mean discrepancy cmmd criterion learning performed stochastic gradient descent gradient calculated back propagation evaluate cgmmn wide range tasks including predictive modeling contextual generation bayesian dark knowledge distills knowledge bayesian model learning relatively small cgmmn student network results demonstrate competitive performance tasks
credit assignment compiler joint prediction many machine learning applications involve jointly predicting multiple mutually dependent output variables learning search family methods complex decision problem cast sequence decisions via search space although methods shown promise theory practice implementing burdensomely awkward paper show search space defined arbitrary imperative program turning learning search credit assignment compiler altogether algorithmic improvements compiler radically reduce complexity programming running time demonstrate feasibility approach multiple joint prediction tasks cases obtain accuracies high alternative approaches drastically reduced execution programming time
short dot computing large linear transforms distributedly using coded short dot products faced saturation moore law increasing size dimension data system designers increasingly resorted parallel distributed computing reduce computation time machine learning algorithms however distributed computing often bottle necked small fraction slow processors called stragglers reduce speed computation fusion node wait processors complete processing combat effect stragglers recent literature proposes introducing redundancy computations across processors using repetition based strategies erasure codes fusion node exploit redundancy completing computation using outputs subset processors ignoring stragglers paper propose novel technique call short dot introduce redundant computations coding theory inspired fashion computing linear transforms long vectors instead computing long dot products required original linear transform construct larger number redundant short dot products computed efficiently individual processors subset short dot products required fusion node finish computation successfully demonstrate probabilistic analysis well experiments computing clusters short dot offers significant speed compared existing techniques also derive trade offs length dot products resilience stragglers number processors required finish strategy compare achieved strategy
spatio temporal hilbert maps continuous occupancy representation dynamic environments consider problem building continuous occupancy representations dynamic environments robotics applications problem hardly discussed previously due complexity patterns urban environments spatial temporal dependencies address problem learning kernel classifier efficient feature space key novelty approach incorporation variations time domain spatial domain propose method propagate motion uncertainty kernel using hierarchical model main benefit approach directly predict occupancy state map future past observations valuable tool robot trajectory planning uncertainty approach preserves main computational benefits static hilbert maps using stochastic gradient descent fast optimization model parameters incremental updates new data captured experiments conducted road intersections urban environment demonstrated spatio temporal hilbert maps accurately model changes map outperforming techniques various aspects
learning hmms nonparametric emissions via spectral decompositions continuous matrices recently surge interest using spectral methods estimating latent variable models however usually assumed distribution observations conditioned latent variables either discrete belongs parametric family paper study estimation state hidden markov model hmm smoothness assumptions olderian conditions emission densities leveraging recent advances continuous linear algebra numerical analysis develop computationally efficient spectral algorithm learning nonparametric hmms technique based computing svd nonparametric estimates density functions viewing emph continuous matrices derive sample complexity bounds via concentration results nonparametric density estimation novel perturbation theory results continuous matrices implement method using chebyshev polynomial approximations method competitive baselines synthetic real problems also computationally efficient
integrated perception recurrent multi task neural networks modern discriminative predictors shown match natural intelligences specific perceptual tasks image classification object part detection boundary extraction etc however major advantage natural intelligences still work well perceptual problems together solving efficiently coherently integrated manner order capture advantages machine perception ask questions whether deep neural networks learn universal image representations useful single task solutions different tasks integrated framework answer proposing new architecture call multinet deep image features shared tasks tasks interact recurrent manner encoding results analysis common shared representation data manner show performance individual tasks standard benchmarks improved first sharing features significantly integrating solutions common representation
blind attacks machine learners importance studying robustness learners malicious data well established much work done establishing robust estimators effective data injection attacks attacker omniscient ability attacker provably harm learning access little information largely unstudied study potential blind attacker provably limit learner performance data injection attack without observing learner training set parameter distribution drawn provide examples simple yet effective attacks settings firstly informed learner knows strategy chosen attacker secondly blind learner knows proportion malicious data family malicious distribution chosen attacker belongs attack analyze minimax rates convergence establish lower bounds learner minimax risk exhibiting limits learner ability learn data injection attack even attacker blind
optimistic gittins indices starting thomspon sampling algorithm recent years seen resurgence interest bayesian algorithms multi armed bandit mab problem algorithms seek exploit prior information arm biases several shown regret optimal design emerged principled approach contrast cared bayesian regret discounted infinite horizon fixed pre specified rate celebrated gittins index theorem offers optimal algorithm unfortunately gittins analysis appear carry minimizing bayesian regret sufficiently large horizons computing gittins index onerous relative essentially incumbent index scheme bayesian mab problem present paper proposes sequence optimistic approximations gittins index show use approximations concert use increasing discount factor appears offer compelling alternative variety index schemes proposed bayesian mab problem recent years addition show simplest approximations yields regret matches lai robbins lower bound including achieving matching constants
sub sampled newton methods non uniform sampling consider problem finding minimizer convex function mathbb rightarrow mathbb form defeq sum_ f_i low rank factorization nabla f_i readily available consider regime propose randomized newton type algorithms exploit textit non uniform sub sampling nabla f_i well inexact updates means reduce computational complexity applicable wide range problems machine learning non uniform sampling distributions based block norm squares block partial leverage scores considered certain assumptions show algorithms inherit linear quadratic convergence rate achieve lower computational complexity compared similar existing methods addition show algorithms exhibit robustness better dependence problem specific quantities condition number numerically demonstrate advantages algorithms several real datasets
learned region sparsity diversity also predicts visual attention learned region sparsity achieved state art performance classification tasks exploiting integrating sparse set local information global decisions underlying mechanism resembles people sample information image eye movements making similar decisions paper incorporate biologically plausible mechanism inhibition return learned region sparsity model thereby imposing diversity selected regions investigate mechanisms sparsity diversity relate visual attention testing model different types visual search tasks report state art results predicting locations human gaze fixations even though model trained image level labels without object location annotations notably classification performance extended model remains original work suggests new computational perspective visual attention mechanisms shows inclusion attention based mechanisms improve computer vision techniques
adaptive concentration inequalities sequential decision problems key challenge sequential decision problems determine many samples needed agent make reliable decisions good probabilistic guarantees introduce hoeffding like concentration inequalities hold random adaptively chosen number samples inequalities tight natural assumptions greatly simplify analysis common sequential decision problems particular apply sequential hypothesis testing best arm identification sorting resulting algorithms rival exceed state art theoretically empirically
cooperative graphical models study rich family distributions capture variable interactions significantly expressive representable low treewidth pairwise graphical models log supermodular models call cooperative graphical models yet family retains structure carefully exploit efficient inference techniques algorithms combine polyhedral structure submodular functions new ways variational inference methods obtain lower upper bounds partition function fully convex upper bound minimized sdp via tree reweighted belief propagation lower bound tightened via belief propagation mean field algorithms resulting algorithms easy implement experiments show effectively obtain good bounds marginals synthetic real world examples
correlated pca principal components analysis data noise correlated given matrix observed data principal components analysis pca computes small number orthogonal directions contain variability provably accurate solutions pca use long time however best knowledge existing theoretical guarantees assume data corrupting noise mutually independent least uncorrelated valid practice often always paper study pca problem setting data noise correlated noise often also referred data dependent noise obtain correctness result standard eigenvalue decomposition evd based solution pca simple assumptions data noise correlation also develop analyze generalization evd cluster evd improves upon evd certain regimes
hierarchical object representation open ended object category learning recognition robots lack ability learn new objects past experiences migrate robot new environment must often completely generate knowledge base running since open ended domains set categories learned predefined feasible assume pre program object categories required robots therefore autonomous robots must ability continuously execute learning recognition concurrent interleaved fashion paper proposes open ended object recognition system concurrently learns object categories statistical features encoding objects particular propose extension latent dirichlet allocation learn structural semantic features topics low level feature occurrences category independently moreover topics category discovered unsupervised fashion updated incrementally using new object views approach contains similarities organization visual cortex builds hierarchy increasingly sophisticated representations results show fulfilling performance approach different types objects moreover system demonstrates capability learning training examples competes state art systems
optimal tagging markov chain optimization many information systems use tags keywords describe annotate content allow efficient organization categorization items well facilitate relevant search queries selected set tags item considerable effect volume traffic eventually reaches item tagging systems tags exclusively chosen item owner turn interested maximizing traffic principled approach assigning tags prove valuable paper introduce problem optimal tagging task choose subset tags new item probability browsing users reaching item maximized formulate problem modeling traffic using markov chain asking transitions chain modified maximize traffic certain state interest resulting optimization problem involves maximizing certain function subsets cardinality constraint show optimization problem hard approximation via simple greedy algorithm due monotonicity submodularity furthermore structure problem allows efficient computation greedy step demonstrate effectiveness method perform experiments tagging datasets show greedy algorithm outperforms baselines
bayesian optimization automated model selection despite success kernel based nonparametric methods kernel selection still requires considerable expertise often described black art present sophisticated method automatically searching appropriate kernel infinite space potential choices previous efforts direction focused traversing kernel grammar examining data via computation marginal likelihood proposed search method based bayesian optimization model space reason model evidence function maximized explicitly reason data distribution induces similarity potential model choices terms explanations offer observed data light construct novel kernel models explain given dataset method capable finding model explains given dataset well without human assistance often fewer computations model evidence previous approaches claim demonstrate empirically
multi view anomaly detection via robust probabilistic latent variable models propose probabilistic latent variable models multi view anomaly detection task finding instances inconsistent views given multi view data proposed model views non anomalous instance assumed generated single latent vector hand anomalous instance assumed multiple latent vectors different views generated different latent vectors inferring number latent vectors used instance dirichlet process priors obtain multi view anomaly scores proposed model seen robust extension probabilistic canonical correlation analysis noisy multi view data present bayesian inference procedures proposed model based stochastic algorithm effectiveness proposed model demonstrated terms performance detecting multi view anomalies
inference reparameterization neural population codes behavioral experiments humans animals suggest brain performs probabilistic inference interpret environment present new general purpose biologically plausible neural implementation approximate inference neural network represents uncertainty using probabilistic population codes ppcs distributed neural representations naturally encode probability distributions support marginalization evidence integration biologically plausible manner connecting multiple ppcs together probabilistic graphical model represent multivariate probability distributions approximate inference graphical models accomplished message passing algorithms disseminate local information throughout graph attractive often accurate example algorithm loopy belief propagation lbp uses local marginalization evidence integration operations perform approximate inference efficiently even complex models unfortunately subtle feature lbp renders neurally implausible however lbp elegantly reformulated sequence tree based reparameterizations trp graphical model express trp updates nonlinear dynamical system fast slow timescales show produces neurally plausible solution combining ideas show network ppcs represent multivariate probability distributions implement trp updates perform probabilistic inference simulations gaussian graphical models demonstrate neural network inference quality comparable direct evaluation lbp robust noise thus provides promising mechanism general probabilistic inference population codes brain
efficient neural codes metabolic constraints neural codes inevitably shaped various kinds biological constraints emph noise metabolic cost formulate coding framework explicitly deals noise metabolic costs associated neural representation information analytically derive optimal neural code monotonic response functions arbitrary stimulus distributions single neuron theory predicts family optimal response functions depending metabolic budget noise characteristics interestingly well known histogram equalization solution viewed special case metabolic resources unlimited pair neurons theory suggests severe metabolic constraints coding increasingly efficient coding scheme compared advantage could large fold substantially larger previous estimation predictions could generalized case large neural populations particular analytical results provide theoretical basis predominant segregation cells early visual processing areas overall provide unified framework optimal neural codes monotonic tuning curves brain makes predictions directly tested physiology experiments
learning deep parsimonious representations paper aim facilitating generalization deep networks supporting interpretability learned representations towards goal propose clustering based regularization encourages parsimonious representations means style objective easy optimize flexible supporting various forms clustering including sample spatial clustering well clustering demonstrate effectiveness approach tasks unsupervised learning classification fine grained categorization shot learning
equivalence high dimensional bayes optimal inference estimation due computational difficulty performing mmse minimum mean squared error inference maximum posteriori map often used surrogate however accuracy map suboptimal high dimensional inference number model parameters order number samples work demonstrate mmse performance asymptotically achievable via optimization appropriately selected convex penalty regularization function smoothed version widely applied map algorithm findings provide new derivation interpretation recent optimal estimators discovered karoui pnas 2013 well extending non additive noise models demonstrate performance optimal estimators numerical simulations overall heart work revelation remarkable equivalence seemingly different computational problems namely high dimensional bayesian integration high dimensional convex optimization essence show former computationally difficult integral computed solving latter simpler optimization problem
minimizing quadratic functions constant time sampling based optimization method quadratic functions proposed method approximately solves following dimensional quadratic minimization problem constant time independent min_ bbr bracket bracket diag bracket bbr times matrix bbr vectors theoretical analysis specifies number samples delta epsilon approximated solution satisfies epsilon probability delta empirical performance accuracy runtime positively confirmed numerical experiments
learning structured sparsity deep neural networks high demand computation resources severely hinders deployment large scale deep neural networks dnn resource constrained devices work propose structured sparsity learning ssl method regularize structures filters channels filter shapes layer depth dnns ssl learn compact structure bigger dnn reduce computation cost obtain hardware friendly structured sparsity dnn efficiently accelerate dnn evaluation experimental results show ssl achieves average speedups convolutional layer computation alexnet cpu gpu respectively shelf libraries speedups twice speedups non structured sparsity regularize dnn structure improve classification accuracy results show cifar regularization layer depth reduces layer deep residual network resnet layers improves accuracy still higher original resnet layers alexnet ssl reduces error
adversarial multiclass classification risk minimization perspective recently proposed adversarial classification methods shown promising results cost sensitive multivariate losses contrast empirical risk minimization erm methods use convex surrogate losses approximate desired non convex target loss function adversarial methods minimize non convex losses treating properties training data uncertain worst case within minimax game despite difference formulation recast adversarial classification loss erm method novel prescribed loss function demonstrate number theoretical practical advantages closely related hinge loss erm methods establishes adversarial classification loss method fills long standing gap multiclass hinge loss classification simultaneously guaranteeing fisher consistency universal consistency also providing dual parameter sparsity high accuracy predictions practice
unified methods exploiting piecewise linear structure convex optimization develop methods rapidly identifying important components convex optimization problem purpose achieving fast convergence times considering novel problem formulation minimization sum piecewise functions describe principled general mechanism exploiting piecewise linear structure convex optimization result leads theoretically justified working set algorithm novel screening test generalize improve upon many prior results exploiting structure convex optimization empirical comparisons study scalability methods find screening scales surprisingly poorly size problem working set algorithm convincingly outperforms alternative approaches
testing differences gaussian graphical models applications brain connectivity functional brain networks well described estimated data gaussian graphical models ggms using sparse inverse covariance estimators comparing functional connectivity subjects populations calls comparing estimated ggms goal identify differences ggms known similar structure characterize uncertainty differences confidence intervals obtained using parametric distribution parameters sparse estimator sparse penalties enable statistical guarantees interpretable models even high dimensional low sample settings characterizing distributions sparse models inherently challenging penalties produce biased estimator recent work invokes sparsity assumptions effectively remove bias sparse estimator lasso distributions used give confidence intervals edges ggms extension differences however case comparing ggms estimators make use assumed joint structure among ggms inspired priors brain functional connectivity derive distribution parameter differences joint penalty parameters known sparse difference leads introduce debiased multi task fused lasso whose distribution characterized efficient manner show debiased lasso multi task fused lasso used obtain confidence intervals edge differences ggms validate techniques proposed set synthetic examples well neuro imaging dataset created study autism
synthesis mcmc belief propagation markov chain monte carlo mcmc belief propagation popular algorithms computational inference graphical models principle mcmc exact probabilistic method however often suffers exponentially slow mixing contrast deterministic method typically fast empirically successful however general lacking control accuracy loopy graphs paper introduce mcmc algorithms correcting approximation error provide way compensate errors via consecutive aware mcmc framework based loop calculus approach allows express error sum weighted generalized loops although full series computationally intractable known truncated series summing regular loops computable polynomial time planar pair wise binary gms also provides highly accurate approximation empirically motivated first propose polynomial time approximation mcmc scheme truncated series general non planar pair wise binary models main idea use worm algorithm known provide fast mixing related problems design appropriate rejection scheme sample regular loops furthermore also design efficient rejection free mcmc scheme approximating full series main novelty underlying design utilizing concept cycle basis provides efficient decomposition generalized loops essence proposed mcmc schemes run transformed built upon non trivial solution experiments show synthesis mcmc outperforms direct mcmc bare schemes
value iteration networks introduce value iteration network vin fully differentiable neural network planning module embedded within vins learn plan suitable predicting outcomes involve planning based reasoning policies reinforcement learning key approach novel differentiable approximation value iteration algorithm represented convolutional neural network trained end end using standard backpropagation evaluate vin based policies discrete continuous path planning domains natural language based search task show learning explicit planning computation vin policies generalize better new unseen domains
sequential neural models stochastic layers efficiently propagate uncertainty latent state representation recurrent neural networks paper introduces stochastic recurrent neural networks glue deterministic recurrent neural network state space model together form stochastic sequential neural generative model clear separation deterministic stochastic layers allows structured variational inference network track factorization model posterior distribution retaining nonlinear recursive structure recurrent neural network averaging uncertainty latent path like state space model improve state art results blizzard timit speech modeling data sets large margin achieving comparable performances competing methods polyphonic music modeling
graphons mergeons work develop theory hierarchical clustering graphs modelling assumption graphs sampled graphon powerful general model generating graphs analyzing large networks graphons far richer class graph models stochastic blockmodels primary setting recent progress statistical theory graph clustering define means algorithm produce correct clustering give sufficient conditions method statistically consistent provide explicit algorithm satisfying properties
deep learning predicting human strategic behavior predicting behavior human participants strategic settings important problem many domains existing work either assumes participants perfectly rational attempts directly model participant cognitive processes based insights cognitive psychology experimental economics work present alternative deep learning approach automatically performs cognitive modeling without relying expert knowledge introduce novel architecture allows single network generalize across different input output dimensions using matrix units rather scalar units show performance significantly outperforms previous state art relies expert constructed features
global analysis expectation maximization mixtures gaussians expectation maximization among popular algorithms estimating parameters statistical models however iterative algorithm based maximum likelihood principle generally guaranteed find stationary points likelihood objective points far maximizer article addresses disconnect statistical principles behind algorithmic properties specifically provides global analysis specific models observations comprise sample mixture gaussians achieved studying sequence parameters idealized execution infinite sample limit fully characterizing limit points sequence terms initial parameters based convergence analysis establishing statistical consistency lack thereof actual sequence parameters produced
supervised learning lens compression work continues study relationship sample compression schemes statistical learning mostly investigated within framework binary classification first extend investigation multiclass categorization prove case learnability equivalent compression logarithmic sample size uniform convergence property implies compression constant size use compressibility learnability equivalence show multiclass categorization pac agnostic pac learnability equivalent derive compactness theorem learnability consider supervised learning general loss functions show case order maintain compressibility learnability equivalence necessary consider approximate variant compression use show pac agnostic pac equivalent even loss function values
matrix completion spurious local minimum matrix completion basic machine learning problem wide applications especially collaborative filtering recommender systems simple non convex optimization algorithms popular effective practice despite recent progress proving various non convex algorithms converge good initial point remains unclear random arbitrary initialization suffices practice prove commonly used non convex objective function matrix completion spurious local minima local minima must also global therefore many popular optimization algorithms stochastic gradient descent provably solve matrix completion textit arbitrary initialization polynomial time
clustering cluster queries propose framework semi supervised active clustering framework ssac learner allowed interact domain expert asking whether given instances belong cluster study query computational complexity clustering framework consider setting expert conforms center based clustering notion margin show trade computational complexity query complexity prove case means clustering expert conforms solution means access relatively queries allows efficient solutions otherwise hard problems particular provide probabilistic polynomial time bpp algorithm clustering setting asks big log log cluster queries runs time complexity big log number clusters number instances success algorithm guaranteed data satisfying margin condition without queries show problem hard also prove lower bound number queries needed computationally efficient clustering algorithm setting
metagrad multiple learning rates online learning online convex optimization well known certain subclasses objective functions much easier arbitrary convex functions interested designing adaptive methods automatically get fast rates many subclasses possible without manual tuning previous adaptive methods able interpolate strongly convex general convex functions present new method metagrad adapts much broader class functions including exp concave strongly convex functions also various types stochastic non stochastic functions without curvature instance metagrad achieve logarithmic regret unregularized hinge loss even though curvature data come favourable probability distribution metagrad main feature simultaneously considers multiple learning rates unlike previous methods provable regret guarantees however learning rates monotonically decreasing time tuned based theoretically derived bound regret instead weighted directly proportional empirical performance data using tilted exponential weights master algorithm
unsupervised feature extraction time contrastive learning nonlinear ica nonlinear independent component analysis ica provides appealing framework unsupervised feature learning models proposed far identifiable first propose new intuitive principle unsupervised deep learning time series uses nonstationary structure data learning principle time contrastive learning tcl finds representation allows optimal discrimination time segments windows surprisingly show tcl related nonlinear ica model ica redefined include temporal nonstationarities particular show tcl combined linear ica estimates nonlinear ica model point wise transformations sources solution unique thus providing first identifiability result nonlinear ica rigorous constructive well general
phased lstm accelerating recurrent network training long event based sequences recurrent neural networks rnns become state art choice extracting patterns temporal sequences current rnn models ill suited process irregularly sampled data triggered events generated continuous time sensors neurons data occur example input comes novel event driven artificial sensors generate sparse asynchronous streams events multiple conventional sensors different update intervals work introduce phased lstm model extends lstm unit adding new time gate gate controlled parametrized oscillation frequency range require updates memory cell small percentage cycle even sparse updates imposed oscillation phased lstm network achieves faster convergence regular lstms tasks require learning long sequences model naturally integrates inputs sensors arbitrary sampling rates thereby opening new areas investigation processing asynchronous sensory events carry timing information also greatly improves performance lstms standard rnn applications order magnitude fewer computes
tractable operations arithmetic circuits probabilistic models consider tractable representations probability distributions polytime operations support particular consider recently proposed arithmetic circuit representation probabilistic sentential decision diagram psdd show psdd supports polytime multiplication operator support polytime operator summing variables polytime multiplication operator make psdds suitable broader class applications compared arithmetic circuits general support multiplication example show psdd multiplication leads simple effective compilation algorithm probabilistic graphical models represent model factor psdd multiply
using fast weights attend recent past recently research artificial neural networks largely restricted systems types variable neural activities represent current recent input weights learn capture regularities among inputs outputs payoffs good reason restriction synapses dynamics many different time scales suggests artificial neural networks might benefit variables change slower activities much faster standard weights fast weights used store temporary memories recent past provide neurally plausible way implementing type attention past recently proven helpful sequence sequence models using fast weights avoid need store copies neural activity patterns
bayesian intermittent demand forecasting large inventories present scalable robust bayesian method demand forecasting context large commerce platform paying special attention intermittent bursty target statistics inference approximated newton raphson algorithm reduced linear time kalman smoothing allows operate several orders magnitude larger problems previous related work study large real world sales datasets method outperforms competing approaches fast medium moving items
blazing trails beating path sample efficient monte carlo planning study sampling based planning problem markov decision processes mdps access generative model usually referred monte carlo planning objective return good estimate optimal value function state minimizing number calls generative model sample complexity propose new algorithm trailblazer able handle mdps finite infinite number transitions state action next states trailblazer adaptive algorithm exploits possible structures mdp exploring subset states reachable following near optimal policies provide bounds sample complexity depend measure quantity near optimal states algorithm behavior considered extension monte carlo sampling estimating expectation problems alternate maximization actions expectation next states finally another appealing feature trailblazer simple implement computationally efficient
sdp relaxation randomized rounding energy disaggregation develop scalable computationally efficient method task energy disaggregation home appliance monitoring problem goal estimate energy consumption appliance based total energy consumption signal household current state art models problem inference factorial hmms finds approximate solution resulting quadratic integer program via quadratic programming take principled approach better suited integer programming problems find approximate optimum combining convex semidefinite relaxations randomized rounding well scalable admm method exploits special structure resulting semidefinite program simulation results demonstrate superiority methods synthetic real world datasets
fast mixing markov chains strongly rayleigh measures dpps constrained sampling study probability measures induced set functions constraints measures arise variety real world settings prior knowledge resource limitations pragmatic considerations impose constraints consider task rapidly sampling constrained measures develop fast markov chain samplers first main result mcmc sampling strongly rayleigh measures present sharp polynomial bounds mixing time corollary result yields fast mixing sampler determinantal point processes dpps yielding knowledge first provably fast mcmc sampler dpps since inception decades ago beyond measures develop mcmc samplers probabilistic models hard constraints identify sufficient conditions chains mix rapidly illustrate claims empirically verifying dependence mixing times key factors governing theoretical bounds
unsupervised learning structure images key goal computer vision recover underlying structure gives rise observations world endowed understanding agents abstract away complexity rendering process form stable disentangled representations scene elements paper learn strong deep generative models structures recover structures images via probabilistic inference demonstrate high quality samples report log likelihoods several datasets including shapenet establish first benchmarks literature also show models inference networks trained jointly end end directly images without use ground truth labels demonstrates first time feasibility learning infer representations world purely unsupervised manner
multiple quantile graphical model introduce multiple quantile graphical model mqgm extends neighborhood selection approach meinshausen buhlmann learning sparse graphical models latter defined basic subproblem modeling conditional mean variable sparse function others approach models set conditional quantiles variable sparse function others hence offers much richer expressive class conditional distribution estimates establish suitable regularity conditions mqgm identifies exact conditional independencies probability tending problem size grows even outside usual homoskedastic gaussian data model develop efficient algorithm fitting mqgm using alternating direction method multipliers also describe strategy sampling joint distribution underlies mqgm estimate lastly present detailed experiments demonstrate flexibility effectiveness mqgm modeling hetereoskedastic non gaussian data
unsupervised learning noisy networks applications data complex networks play important role plethora disciplines natural sciences cleaning noisy observed networks poses important challenge network analysis existing methods utilize labeled data alleviate noise effect network however labeled data usually expensive collect unlabeled data gathered cheaply paper propose optimization framework mine useful structures noisy networks unsupervised manner key feature optimization framework ability utilize local structures well global patterns network extend method incorporate multi resolution networks order add resistance high levels noise also generalize framework utilize partial labels enhance performance specifically focus method multi resolution data recovering clusters genomic regions localize space additionally use capture generated partial labels denoise network empirically demonstrate effectiveness framework denoising network improving community detection results
towards unifying hamiltonian monte carlo slice sampling unify slice sampling hamiltonian monte carlo hmc sampling demonstrating connection via hamiltonian jacobi equation hamiltonian mechanics insight enables extension hmc slice sampling broader family samplers called monomial gamma samplers mgs provide theoretical analysis mixing performance samplers proving limit single parameter mgs draws decorrelated samples desired target distribution show parameter tends toward limit performance gains achieved cost increasing numerical difficulty practical convergence issues theoretical results validated synthetic data real world applications
differential privacy without sensitivity exponential mechanism general method construct randomized estimator satisfies varepsilon differential privacy recently wang showed gibbs posterior data dependent probability distribution contains bayesian posterior essentially equivalent exponential mechanism certain boundedness conditions loss function exponential mechanism provides way build varepsilon differential private algorithm requires boundedness loss function quite stringent learning problems paper focus varepsilon delta differential privacy gibbs posteriors convex lipschitz loss functions result extends classical exponential mechanism allowing loss functions unbounded sensitivity
generalized correspondence lda models lda identifying functional regions brain paper presents generalized correspondence lda lda generalization correspondence lda model allows variable spatial representations associated topics increased flexibility terms strength correspondence data types induced model present variants lda associates topics different spatial representation apply corpus neuroimaging data context dataset topic corresponds functional brain region region spatial extent captured probability distribution neural activity region cognitive function captured probability distribution linguistic terms illustrate qualitative improvements offered lda terms types topics extracted alternative spatial representations well model ability incorporate priori knowledge neuroimaging literature furthermore demonstrate novel features lda improve predictions missing data
kronecker determinantal point processes determinantal point processes dpps probabilistic models subsets ground set items recently gained prominence several applications rely diverse subsets however applicability large problems still limited due complexity core tasks sampling learning enable efficient sampling learning dpps introducing krondpp dpp model whose kernel matrix decomposes tensor product multiple smaller kernel matrices decomposition immediately enables fast exact sampling contrary expect leveraging kronecker product structure speeding dpp learning turns difficult overcome challenge derive batch stochastic optimization algorithms efficiently learning parameters krondpp
variance reduction stochastic gradient langevin dynamics stochastic gradient based monte carlo methods stochastic gradient langevin dynamics useful tools posterior inference large scale datasets many machine learning applications methods scale large datasets using noisy gradients calculated using mini batch subset dataset however high variance inherent noisy gradients degrades performance leads slower mixing paper present techniques reducing variance stochastic gradient langevin dynamics yielding novel stochastic monte carlo methods improve performance reducing variance stochastic gradient show proposed method better theoretical guarantees convergence rate stochastic langevin dynamics complemented impressive empirical results obtained variety real world datasets different machine learning tasks regression classification independent component analysis mixture modeling theoretical empirical contributions combine make compelling case using variance reduction stochastic monte carlo methods
online pricing strategic patient buyers consider seller unlimited supply single good faced stream buyers buyer window time would like purchase would buy lowest price window provided price lower private value otherwise would buy setting give algorithm attains regret sequence buyers respect best fixed price hindsight prove algorithm perform better worst case
exploiting structure stochastic gradient methods using raw clusters amount data available world growing faster ability deal however take advantage internal structure data become much smaller machine learning purposes paper focus fundamental machine learning tasks empirical risk minimization erm provide faster algorithms help clustering structure data introduce simple notion raw clustering efficiently computed data propose algorithms based clustering information accelerated algorithm clusteracdm built novel haar transformation applied dual space erm problem variance reduction based algorithm clustersvrg introduces new gradient estimator using clustering algorithms outperform classical counterparts acdm svrg respectively
clustering signed networks geometric mean laplacians signed networks allow model positive negative relationships analyze existing extensions spectral clustering signed networks turns existing approaches recover ground truth clustering several situations either positive negative network structures contain noise analysis shows problems arise existing approaches take form arithmetic mean laplacians positive negative part solution propose use geometric mean laplacians positive negative part show outperforms existing approaches geometric mean matrices computationally expensive show eigenvectors geometric mean computed efficiently leading numerical scheme sparse matrices independent interest
robust spectral detection global structures data learning regularization spectral methods popular detecting global structures given data represented matrix however data matrix sparse noisy classic spectral methods usually fail work due localization eigenvectors singular vectors induced sparsity noise work propose general method solve localization problem learning regularization matrix localized eigenvectors using matrix perturbation analysis demonstrate learned regularizations suppress eigenvalues associated localized eigenvectors enable recover informative eigenvectors representing global structure show applications method several inference problems community detection networks clustering pairwise similarities rank estimation matrix completion problems using extensive experiments illustrate method solves localization problem works theoretical detectability limits different kinds synthetic data contrast existing spectral algorithms based data matrix non backtracking matrix laplacians rank regularizations perform poorly sparse case noise
perspective transformer nets learning single view object reconstruction without supervision understanding world fundamental problem computer vision however learning good representation objects still open problem due high dimensionality data many factors variation involved work investigate task single view object reconstruction learning agent perspective formulate learning process interaction representations propose encoder decoder network novel projection loss defined projective transformation importantly projection loss enables unsupervised learning using observation without explicit supervision demonstrate ability model generating volume single image sets experiments learning single class objects learning multi class objects testing novel object classes results show superior performance better generalization ability object reconstruction projection loss involved
launch iterate reducing prediction churn practical applications machine learning often involve successive training iterations changes features training examples ideally changes output new model improvements wins previous iteration practice predictions change neutrally many examples resulting extra net wins losses referred unnecessary churn changes predictions problematic usability applications make harder expensive measure change statistically significant positive paper formulate problem present stabilization operator regularize classifier towards previous classifier use markov chain monte carlo stabilization operator produce model consistent predictions without adversely affecting accuracy investigate properties proposal theoretical analysis experiments benchmark datasets different classification algorithms demonstrate method resulting reduction churn
data poisoning attacks factorization based collaborative filtering recommendation collaborative filtering systems important modern information commerce applications systems becoming increasingly popular industry outputs could affect business decision making introducing incentives adversarial party compromise availability integrity systems introduce data poisoning attack collaborative filtering systems demonstrate powerful attacker full knowledge learner generate malicious data maximize malicious objectives time mimicking normal user behaviors avoid detected complete knowledge assumption seems extreme enables robust assessment vulnerability collaborative filtering schemes highly motivated attacks present efficient solutions popular factorization based collaborative filtering algorithms alternative minimization formulation nuclear norm minimization method finally test effectiveness proposed algorithms real world data discuss potential defensive strategies
scaling memory augmented neural networks sparse reads writes neural networks augmented external memory ability learn algorithmic solutions complex tasks models appear promising applications language modeling machine translation however scale poorly space time amount memory grows limiting applicability real world domains present end end differentiable memory access scheme call sparse access memory sam retains representational power original approaches whilst training efficiently large memories show sam achieves asymptotic lower bounds space time complexity find implementation runs 000 times faster 000 times less physical memory non sparse models sam learns comparable data efficiency existing models range synthetic tasks shot omniglot character recognition scale tasks requiring 100 000 time steps memories well show approach adapted models maintain temporal associations memories recently introduced differentiable neural computer
optimal architectures solvable model deep networks deep neural networks received considerable attention due success training real world machine learning applications also great interest understanding sensory processing cortical sensory hierarchies purpose work advance theoretical understanding computational benefits architectures using simple model clustered noisy inputs simple learning rule provide analytically derived recursion relations describing propagation signals along deep network analysis equations defining performance measures show model networks optimal depths explore dependence optimal architecture system parameters
scalable adaptive stochastic optimization using random projections adaptive stochastic gradient methods adagrad gained popularity particular training deep neural networks commonly used studied variant maintains diagonal matrix approximation second order information accumulating past gradients used tune step size adaptively certain situations full matrix variant adagrad expected attain better performance however high dimensions computationally impractical present ada radagrad computationally efficient approximations full matrix adagrad based randomized dimensionality reduction able capture dependencies features achieve similar performance full matrix adagrad much smaller computational cost show regret ada close regret full matrix adagrad exponentially smaller dependence dimension diagonal variant empirically show ada radagrad perform similarly full matrix adagrad task training convolutional neural networks well recurrent neural networks radagrad achieves faster convergence diagonal adagrad
spectral learning dynamic systems nonequilibrium data observable operator models ooms related models important powerful tools modeling analyzing stochastic systems exactly describe dynamics finite rank systems efficiently consistently estimated spectral learning assumption identically distributed data paper investigate properties spectral learning without assumption due requirements analyzing large time scale systems show equilibrium dynamics system extracted nonequilibrium observation data imposing equilibrium constraint addition propose binless extension spectral learning continuous data comparison continuous valued spectral algorithms binless algorithm achieve consistent estimation equilibrium dynamics linear complexity
local minimax complexity stochastic convex optimization extend traditional worst case minimax analysis stochastic convex optimization introducing localized form minimax complexity individual functions main result gives function specific lower upper bounds number stochastic subgradient evaluations needed optimize either function hardest local alternative given numerical precision bounds expressed terms localized computational analogue modulus continuity central statistical minimax analysis show computational modulus continuity explicitly calculated concrete cases relates curvature function optimum also prove superefficiency result demonstrates meaningful benchmark acting computational analogue fisher information statistical estimation nature practical implications results demonstrated simulations
theoretically grounded application dropout recurrent neural networks recurrent neural networks rnns stand forefront many recent developments deep learning yet major difficulty models tendency overfit dropout shown fail applied recurrent layers recent results intersection bayesian modelling deep learning offer bayesian interpretation common deep learning techniques dropout grounding dropout approximate bayesian inference suggests extension theoretical results offering insights use dropout rnn models apply new variational inference based dropout technique lstm gru models assessing language modelling sentiment analysis tasks new approach outperforms existing techniques best knowledge improves single model state art language modelling penn treebank test perplexity extends arsenal variational tools deep learning
brains beats developed task optimized deep neural networks dnns achieved state art performance different evaluation scenarios automatic music tagging dnns subsequently used probe neural representations music representational similarity analysis revealed existence representational gradient across superior temporal gyrus stg anterior stg shown sensitive low level stimulus features encoded shallow dnn layers whereas posterior stg shown sensitive high level stimulus features encoded deep dnn layers
communication efficient parallel algorithm decision tree decision tree extensions gradient boosting decision trees random forest widely used machine learning algorithm due practical effectiveness model interpretability emergence big data increasing need parallelize training process decision tree however existing attempts along line suffer high communication costs paper propose new algorithm called emph parallel voting decision tree tree tackle challenge partitioning training data onto number machines algorithm performs local voting global voting iteration local voting top attributes selected machine according local data indices top attributes aggregated server globally top attributes determined majority voting among local candidates finally full grained histograms globally top attributes collected local machines order identify best informative attribute split point tree achieve low communication cost independent total number attributes thus scale well furthermore theoretical analysis shows algorithm learn near optimal decision tree since find best attribute large probability experiments real world datasets show tree significantly outperforms existing parallel decision tree algorithms tradeoff accuracy efficiency
leveraging sparsity efficient submodular data summarization facility location problem widely used summarizing large datasets additional applications sensor placement image retrieval clustering difficulty problem submodular optimization algorithms require calculation pairwise benefits items dataset infeasible large problems recent work proposed calculate nearest neighbor benefits limitation several strong assumptions invoked obtain provable approximation guarantees paper establish extra assumptions necessary solving sparsified problem almost optimal standard assumptions problem analyze different method sparsification better model methods locality sensitive hashing accelerate nearest neighbor computations extend use problem broader family similarities validate approach demonstrating rapidly generates interpretable summaries
avoiding imposters delinquents adversarial crowdsourcing peer prediction consider crowdsourcing model workers asked rate quality items previously generated workers unknown set alpha workers generate reliable ratings remaining workers behave arbitrarily possibly adversarially manager experiment also manually evaluate quality small number items wishes curate together almost high quality items fraction low quality items perhaps surprisingly show possible amount work required manager worker scale dataset curated tilde beta alpha epsilon ratings per worker tilde beta epsilon ratings manager beta fraction high quality items results extend general setting peer prediction including peer grading online classrooms
designing smoothing functions improved worst case competitive ratio online optimization online optimization covers problems online resource allocation online bipartite matching adwords central problem commerce advertising adwords separable concave returns analyze worst case competitive ratio primal dual algorithms class online convex conic optimization problems contains previous examples special cases defined positive orthant derive sufficient condition objective function guarantees constant worst case competitive ratio greater equal frac monotone objective functions provide new examples online problems positive orthant positive semidefinite cone satisfy sufficient condition show smoothing improve competitive ratio algorithms particular separable functions show optimal smoothing derived solving convex optimization problem result allows directly optimize competitive ratio bound class smoothing functions hence design effective smoothing customized given cost function
forget process introduce forget process efficient non parametric meta algorithm online probabilistic sequence prediction piecewise stationary repeating sources method works taking bayesian approach partition stream data postulated task specific segments simultaneously building model task provide regret guarantees respect piecewise stationary data sources logarithmic loss validate method empirically across range sequence prediction task identification problems
generating videos scene dynamics capitalize large amounts unlabeled video order learn model scene dynamics video recognition tasks action classification video generation tasks future prediction propose generative adversarial network video spatio temporal convolutional architecture untangles scene foreground background experiments suggest model generate tiny videos second full frame rate better simple baselines show utility predicting plausible futures static images moreover experiments visualizations show model internally learns useful features recognizing actions minimal supervision suggesting scene dynamics promising signal representation learning believe generative video models impact many applications video understanding simulation
robustness estimator composition formalize notions robustness composite estimators via notion breakdown point composite estimator successively applies estimators data decomposed disjoint parts applies first estimator part second estimator outputs first estimator composition estimators informally breakdown point minimum fraction data points significantly modified also significantly modify output estimator typically desirable large breakdown point main result shows mild conditions individual estimators breakdown point composite estimator product breakdown points individual estimators also demonstrate several scenarios ranging regression statistical testing analysis easy apply useful understanding worst case robustness sheds powerful insights onto associated data analysis
improved deep metric learning multi class pair loss objective deep metric learning gained much popularity recent years following success deep learning however existing frameworks deep metric learning based contrastive loss triplet loss often suffer slow convergence partially employ negative example interacting negative classes update paper propose address problem new metric learning objective called multi class pair loss proposed objective function firstly generalizes triplet loss allowing joint comparison among negative examples specifically negative examples secondly reduces computational burden evaluating deep embedding vectors via efficient batch construction strategy using pairs examples instead demonstrate superiority proposed loss triplet loss well competing loss functions variety tasks several visual recognition benchmark including fine grained object recognition verification image clustering retrieval face verification identification
preference completion partial rankings propose novel efficient algorithm collaborative preference completion problem involves jointly estimating individualized rankings set entities shared set items based limited number observed affinity values approach exploits observation preferences often recorded numerical scores predictive quantity interest underlying rankings thus attempts closely match recorded scores lead overfitting impair generalization performance instead propose estimator directly fits underlying preference order combined nuclear norm constraints encourage low rank parameters besides approximate correctness ranking order proposed estimator makes generative assumption numerical scores observations consequence proposed estimator fit consistent partial ranking subset items represented directed acyclic graph dag generalizing standard techniques fit preference scores despite generality supervision representing total blockwise total orders computational complexity algorithm within log factor standard algorithms nuclear norm regularization based estimates matrix completion show promising empirical results novel challenging application collaboratively ranking associations brain regions cognitive neuroscience terms
bayesian optimization mixed constraints slack variable augmented lagrangian augmented lagrangian convert constrained optimization problem sequence simpler unconstrained problems usually solved local solvers recently surrogate based bayesian optimization sub solvers successfully deployed framework global search presence inequality constraints however drawback expected improvement evaluations relied monte carlo introduce alternative slack variable show formulation evaluated library routines slack variables furthermore facilitate equality well inequality constraints mixtures thereof show new slack albo compares favorably original superiority conventional alternatives reinforced several new mixed constraint examples
privacy odometers filters pay composition paper initiate study adaptive composition differential privacy length composition privacy parameters chosen adaptively function outcome previously run analyses case much delicate setting covered existing composition theorems algorithms chosen adaptively privacy parameters must fixed front indeed even clear define differential privacy adaptive parameter setting proceed defining objects cover main use cases composition theorems privacy filter stopping time rule allows analyst halt computation pre specified privacy budget exceeded privacy odometer allows analyst track realized privacy loss goes without needing pre specify privacy budget show unlike case privacy parameters fixed adaptive parameter setting use cases distinct show exist privacy filters bounds comparable constants existing privacy composition theorems also give privacy odometer nearly matches non adaptive private composition theorems sometimes worse small asymptotic factor moreover show inherent valid privacy odometer adaptive parameter setting must lose factor shows formal separation filter odometer use cases
large margin discriminant dimensionality reduction prediction space paper establish duality boosting svm use derive novel discriminant dimensionality reduction algorithm particular using multiclass formulation boosting svm note use combination mapping linear classification maximize multiclass margin svm implemented using pre defined mapping induced kernel optimizing linear classifiers boosting linear classifiers pre defined mapping predictor learned combination weak learners argue intermediate mapping boosting predictor preserving discriminant aspects data controlling dimension mapping possible achieve discriminant low dimensional representations data use aforementioned duality propose new method large margin discriminant dimensionality reduction ladder jointly learns mapping linear classifiers efficient manner leads data driven mapping embed data number dimensions experimental results show embedding significantly improve performance tasks hashing image scene classification
tight complexity bounds optimizing composite objectives provide tight upper lower bounds complexity minimizing average convex functions using gradient prox oracles component functions show significant gap complexity deterministic randomized optimization smooth functions show accelerated gradient descent agd accelerated variant svrg optimal deterministic randomized settings respectively gradient oracle sufficient optimal rate non smooth functions access prox oracles reduces complexity present optimal methods based smoothing improve methods using gradient accesses
automatic neuron detection calcium imaging data using convolutional networks calcium imaging important technique monitoring activity thousands neurons simultaneously calcium imaging datasets grow size automated detection individual neurons becoming important apply supervised learning approach problem show convolutional networks achieve near human accuracy superhuman speed accuracy superior popular pca ica method based precision recall relative ground truth annotation human expert results suggest convolutional networks efficient flexible tool analysis large scale calcium imaging data
hierarchical deep reinforcement learning integrating temporal abstraction intrinsic motivation learning goal directed behavior environments sparse feedback major challenge reinforcement learning algorithms key difficulties insufficient exploration resulting agent unable learn robust policies intrinsically motivated agents explore new behavior sake rather directly solve external goals intrinsic behaviors could eventually help agent solve tasks posed environment present hierarchical dqn dqn framework integrate hierarchical action value functions operating different temporal scales goal driven intrinsically motivated deep reinforcement learning top level value function learns policy intrinsic goals lower level function learns policy atomic actions satisfy given goals dqn allows flexible goal specifications functions entities relations provides efficient space exploration complicated environments demonstrate strength approach problems sparse delayed feedback complex discrete stochastic decision process stochastic transitions classic atari game montezuma revenge
conditional image generation pixelcnn decoders work explores conditional image generation new image density model based pixelcnn architecture model conditioned vector including descriptive labels tags latent embeddings created networks conditioned class labels imagenet database model able generate diverse realistic scenes representing distinct animals objects landscapes structures conditioned embedding produced convolutional network given single image unseen face generates variety new portraits person different facial expressions poses lighting conditions also show conditional pixelcnn serve powerful decoder image autoencoder additionally gated convolutional layers proposed model improve log likelihood pixelcnn match state art performance pixelrnn imagenet greatly reduced computational cost
natural parameter networks class probabilistic neural networks neural networks achieved state art performance various applications unfortunately applications training data insufficient often prone overfitting effective way alleviate problem exploit bayesian approach using bayesian neural networks bnn another shortcoming lack flexibility customize different distributions weights neurons according data often done probabilistic graphical models address problems propose class probabilistic neural networks dubbed natural parameter networks npn novel lightweight bayesian treatment npn allows usage arbitrary exponential family distributions model weights neurons different traditional bnn npn takes distributions input goes layers transformation producing distributions match target output distributions bayesian treatment efficient backpropagation performed learn natural parameters distributions weights neurons output distributions layer byproducts used second order representations associated tasks link prediction experiments real world datasets show npn achieve state art performance
long term causal effects via behavioral game theory planned experiments gold standard reliably comparing causal effect switching baseline policy new policy critical shortcoming classical experimental methods however typically take account dynamic nature response policy changes instance experiment seek understand effects new pricing policy auction revenue agents adapt bidding response experimental pricing changes thus causal effects new pricing policy adaptation period long term causal effects captured classical methodology even though clearly indicative value new policy formalize framework define estimate long term causal effects policy changes multiagent economies central approach behavioral game theory leverage formulate ignorability assumptions necessary causal inference assumptions estimate long term causal effects latent space approach behavioral model agents act conditional latent behaviors combined temporal model behaviors evolve time
perforatedcnns acceleration elimination redundant convolutions propose novel approach reduce computational cost evaluation convolutional neural networks factor hindered deployment low power devices mobile phones inspired loop perforation technique source code optimization speed bottleneck convolutional layers skipping evaluation spatial positions propose analyze several strategies choosing positions demonstrate perforation accelerate modern convolutional networks alexnet vgg factor additionally show perforation complementary recently proposed acceleration method zhang
probabilistic programming approach probabilistic data analysis probabilistic techniques central data analysis different approaches challenging apply combine compare paper introduces composable generative population models cgpms computational abstraction extends directed graphical models used describe compose broad class probabilistic data analysis techniques examples include discriminative machine learning hierarchical bayesian models multivariate kernel methods clustering algorithms arbitrary probabilistic programs demonstrate integration cgpms bayesdb probabilistic programming platform express data analysis tasks using modeling definition language structured query language practical value illustrated ways first paper describes analysis database earth satellites identifies records probably violate kepler third law composing causal probabilistic programs non parametric bayes lines probabilistic code second reports lines code accuracy cgpms compared baseline solutions standard machine learning libraries
learning bayesian networks ancestral constraints consider problem learning bayesian networks optimally subject background knowledge form ancestral constraints approach based recently proposed framework optimal structure learning based non decomposable scores general enough accommodate ancestral constraints proposed framework exploits oracles learning structures using decomposable scores cannot accommodate ancestral constraints since non decomposable show empower oracles passing decomposable constraints handle inferred ancestral constraints cannot handle empirically demonstrate approach orders magnitude efficient alternative frameworks based integer linear programming
solving random systems quadratic equations via truncated generalized gradient flow paper puts forth novel algorithm termed emph truncated generalized gradient flow tggf solve mathbb mathbb system quadratic equations y_i langle rangle ldots even left mathbb mathbb right random known emph hard general prove soon number equations order number unknowns tggf recovers solution exactly global unimodular constant high probability complexity growing linearly time required read data left left y_i right right specifically tggf proceeds stages novel emph orthogonality promoting initialization obtained simple power iterations refinement initial estimate successive updates scalable emph truncated generalized gradient iterations former sharp contrast existing spectral initializations latter handles rather challenging nonconvex nonsmooth emph amplitude based cost function numerical tests demonstrate novel orthogonality promoting initialization method returns accurate robust estimates relative spectral counterparts even initialization refinement truncation outperforms wirtinger based alternatives corroborating superior performance tggf state art algorithms
balancing suspense surprise timely decision making endogenous information acquisition develop bayesian model decision making time pressure endogenous information acquisition model decision maker decides observe costly information sampling underlying continuous time stochastic process time series conveys information potential occurrence non occurrence adverse event terminate decision making process attempt predict occurrence adverse event decision maker follows policy determines acquire information time series continuation stop acquiring information make final prediction stopping show optimal policy rendezvous structure structure whenever new information sample gathered time series optimal date acquiring next sample becomes computable optimal interval information samples balances trade decision maker surprise drift posterior belief observing new information suspense probability adverse event occurs time interval information samples moreover characterize continuation stopping regions decision maker state space show depend decision maker beliefs also context current realization time series
structure blind signal recovery consider problem recovering signal observed gaussian noise set signals convex compact specified beforehand use classical linear estimators achieve risk within constant factor minimax risk however set unspecified designing estimator blind hidden structure signal remains challenging problem propose new family estimators recover signals observed gaussian noise instead specifying set signal lives assume existence well performing linear estimator proposed estimators enjoy exact oracle inequalities efficiently computed convex optimization present several numerical illustrations show potential approach
spatiotemporal residual networks video action recognition stream convolutional networks convnets shown strong performance human action recognition videos recently residual networks resnets arisen new technique train extremely deep architectures paper introduce spatiotemporal resnets combination approaches novel architecture generalizes resnets spatiotemporal domain introducing residual connections ways first inject residual connections appearance motion pathways stream architecture allow spatiotemporal interaction streams second transform pretrained image convnets spatiotemporal networks equipping learnable convolutional filters initialized temporal residual connections operate adjacent feature maps time approach slowly increases spatiotemporal receptive field depth model increases naturally integrates image convnet design principles whole model trained end end allow hierarchical learning complex spatiotemporal features evaluate novel spatiotemporal resnet using widely used action recognition benchmarks exceeds previous state art
cma optimal covariance update storage complexity covariance matrix adaptation evolution strategy cma arguably powerful real valued derivative free optimization algorithms finding many applications machine learning cma monte carlo method sampling sequence multi variate gaussian distributions given function values sampled points updating storing covariance matrix dominates time space complexity iteration algorithm propose numerically stable quadratic time covariance matrix update scheme minimal memory requirements based maintaining triangular cholesky factors requires modification cumulative step size adaption csa mechanism cma replace inverse square root covariance matrix inverse triangular cholesky factor triangular cholesky factor changes smoothly matrix square root modification change behavior cma terms required objective function evaluations verified empirically thus described algorithm replace standard cma updating storing covariance matrix matters
latent attention program synthesis automatic translation natural language descriptions programs long standing challenging problem work consider simple yet important sub problem translation textual descriptions programs devise novel neural network architecture task train end end specifically introduce latent attention computes multiplicative weights words description stage process goal better leveraging natural language structures indicate relevant parts predicting program elements architecture reduces error rate compared prior art also propose shot learning scenario program synthesis simulate existing dataset demonstrate variation training procedure scenario outperforms original procedure significantly closing gap model trained data
sound apalm clapping faster nonsmooth nonconvex optimization stochastic asynchronous palm introduce stochastic asynchronous proximal alternating linearized minimization sapalm method block coordinate stochastic proximal gradient method solving nonconvex nonsmooth optimization problems sapalm first asynchronous parallel optimization method provably converges large class nonconvex nonsmooth problems prove sapalm matches best known rates convergence among synchronous asynchronous methods problem class provide upper bounds number workers expect see linear speedup match best bounds known less complex problems show practice sapalm achieves linear speedup demonstrate state art performance several matrix factorization problems
efficient streaming algorithm submodular cover problem initiate study classical submodular cover problem data streaming model refer streaming submodular cover ssc show single pass streaming algorithm using sublinear memory size stream fail provide non trivial approximation guarantees ssc hence consider relaxed version ssc seek find partial cover design first efficient bicriteria submodular cover streaming esc streaming algorithm problem provide theoretical guarantees performance supported numerical evidence algorithm finds solutions competitive near optimal offline greedy algorithm despite requiring single pass data stream numerical experiments evaluate performance esc streaming active set selection large scale graph cover problems
attend infer repeat fast scene understanding generative models present framework efficient inference structured image models explicitly reason objects achieve performing probabilistic inference using recurrent neural network attends scene elements processes time crucially model learns choose appropriate number inference steps use scheme learn perform inference partially specified models variable sized variational auto encoders fully specified models probabilistic renderers show models learn identify multiple objects counting locating classifying elements scene without supervision decomposing images various numbers objects single forward pass neural network unprecedented speed show networks produce accurate inferences compared supervised counterparts structure leads improved generalization
ensemble diversity approach supervised binary hashing binary hashing well known approach fast approximate nearest neighbor search information retrieval much work focused affinity based objective functions involving hash functions binary codes objective functions encode neighborhood information data points often inspired manifold learning algorithms ensure hash functions differ constraints penalty terms encourage codes orthogonal dissimilar across bits couples binary variables complicates already difficult optimization propose much simpler approach train hash function bit independently introduce diversity among using techniques classifier ensembles surprisingly find faster trivially parallelizable also improves complex coupled objective function achieves state art precision recall experiments image retrieval
end end goal driven web navigation propose goal driven web navigation benchmark task evaluating agent abilities understand natural language plan partially observed environments challenging task agent navigates website represented graph consisting web pages nodes hyperlinks directed edges find web page query appears agent required sophisticated high level reasoning based natural languages efficient sequential decision making capability succeed release software tool called webnav automatically transforms website goal driven web navigation task example make wikinav dataset constructed english wikipedia extensively evaluate different variants neural net based artificial agents wikinav observe proposed goal driven web navigation well reflects advances models making suitable benchmark evaluating future progress furthermore extend wikinav question answer pairs jeopardy test proposed agent based recurrent neural networks strong inverted index based search engines artificial agents trained wikinav outperforms engined based approaches demonstrating capability proposed goal driven navigation good proxy measuring progress real world tasks focused crawling question answering
power adaptivity identifying statistical alternatives paper studies trade different kinds pure exploration breadth versus depth focus biased coin problem asking many total coin flips required identify heavy coin infinite bag containing heavy coins mean theta_1 light coins mean theta_0 theta_1 heavy coins drawn bag proportion alpha alpha theta_0 theta_1 unknown key difficulty problem lies distinguishing whether kinds coins similar means whether heavy coins extremely rare existing solutions problem require prior knowledge parameters theta_0 theta_1 alpha propose adaptive algorithm requires knowledge yet still obtains near optimal sample complexity guarantees contrast provide lower bound showing non adaptive strategies require least quadratically samples characterizing gap adaptive nonadaptive strategies make connections anomaly detection prove lower bounds sample complexity differentiating single parametric distribution mixture distributions
probabilistic framework deep learning develop probabilistic framework deep learning based deep rendering mixture model drmm new generative probabilistic model explicitly capture variations data due latent task nuisance variables demonstrate max sum inference drmm yields algorithm exactly reproduces operations deep convolutional neural networks dcns providing first principles derivation framework provides new insights successes shortcomings dcns well principled route improvement drmm training via expectation maximization algorithm powerful alternative dcn back propagation initial training results promising classification based drmm variants outperforms dcns supervised digit classification training faster achieving similar accuracy moreover drmm applicable semi supervised unsupervised learning tasks achieving results state art several categories mnist benchmark comparable state art cifar10 benchmark
minimax estimation maximum mean discrepancy radial kernels maximum mean discrepancy mmd distance space probability measures found numerous applications machine learning nonparametric testing distance based notion embedding probabilities reproducing kernel hilbert space paper present first known lower bounds estimation mmd based finite samples lower bounds hold radial universal kernel match existing upper bounds constants depend properties kernel using lower bounds establish minimax rate optimality empirical estimator statistic variant usually employed applications
adaptive neural compilation paper proposes adaptive neural compilation framework address problem learning efficient program traditional code optimisation strategies used compilers based applying pre specified set transformations make code faster execute without changing semantics contrast work involves adapting programs make efficient considering correctness target input distribution approach inspired recent works differentiable representations programs show possible compile programs written low level language differentiable representation also show programs representation optimised make efficient target distribution inputs experimental results demonstrate approach enables learning specifically tuned algorithms given data distributions high success rate
tagger deep unsupervised perceptual grouping present framework efficient perceptual inference explicitly reasons segmentation inputs features rather trained specific segmentation framework learns grouping process unsupervised manner alongside supervised task enable neural network group representations different objects iterative manner differentiable mechanism achieve fast convergence allowing system amortize joint iterative inference groupings representations contrast many recently proposed methods addressing multi object scenes system assume inputs images therefore directly handle modalities evaluate method multi digit classification cluttered images require texture segmentation remarkably method achieves improved classification performance convolutional networks despite fully connected making use grouping mechanism furthermore observe system greatly improves upon semi supervised result baseline ladder network dataset results evidence grouping powerful tool help improve sample efficiency
scaled bregman theorem applications bregman divergences play central role design analysis range machine learning algorithms handful popular theorems present new theorem shows bregman distortions employing potentially non convex generator exactly written scaled bregman divergence computed transformed data property viewed standpoints geometry scaled isometry adaptive metrics convex optimization relating generalized perspective transforms admissible distortions include geodesic distances curved manifolds projections gauge normalisation theorem allows leverage wealth convenience bregman divergences analysing algorithms relying aforementioned bregman distortions illustrate novel applications theorem reduction multi class density ratio class probability estimation new adaptive projection free yet norm enforcing dual norm mirror descent algorithm reduction clustering flat manifolds clustering curved manifolds experiments domains validate analyses suggest scaled bregman theorem might worthy addition popular handful bregman divergence properties pervasive machine learning
learning feed forward shot learners shot learning usually tackled using generative models discriminative embeddings discriminative methods based deep learning effective learning scenarios ill suited shot learning need large amounts training data paper propose method learn parameters deep model shot construct learner second deep network called learnet predicts parameters pupil network single exemplar manner obtain efficient feed forward shot learner trained end end minimizing shot classification objective learning learn formulation order make construction feasible propose number factorizations parameters pupil network demonstrate encouraging results learning characters single exemplars omniglot tracking visual objects single initial exemplar visual object tracking benchmark
error analysis generalized nystr kernel regression nystr method used successfully improve computational efficiency kernel ridge regression krr recently theoretical analysis nystr krr including generalization bound convergence rate established based reproducing kernel hilbert space rkhs associated symmetric positive semi definite kernel however real world applications rkhs always optimal kernel function necessary symmetric positive semi definite paper consider generalized nystr kernel regression gnkr ell_2 coefficient regularization kernel requires continuity boundedness error analysis provided characterize generalization performance column norm sampling introduced construct refined hypothesis space particular fast learning rate polynomial decay reached gnkr experimental analysis demonstrates satisfactory performance gnkr column norm sampling
breaking bandwidth barrier geometrical adaptive entropy estimation estimators information theoretic measures entropy mutual information samples basic workhorse many downstream applications modern data science state art approaches either geometric nearest neighbor based kernel based bandwidth chosen data independent vanishing sub linearly sample size paper combine approaches design new estimators entropy mutual information strongly outperform state art methods estimator uses bandwidth choice fixed distances choice data dependent linearly vanishing sample size necessitates bias cancellation term universal independent underlying distribution byproduct obtain unified way obtaining kernel estimators corresponding theoretical contribution relating geometry distances asymptotic order statistics independent mathematical interest
asynchronous parallel greedy coordinate descent paper propose study asynchronous parallel greedy coordinate descent asy gcd algorithm minimizing smooth function bounded constraints iteration workers asynchronously conduct greedy coordinate descent updates block variables first part paper analyze theoretical behavior asy gcd prove linear convergence rate second part develop efficient kernel svm solver based asy gcd shared memory multi core setting since algorithm fully asynchronous core need idle wait cores resulting algorithm enjoys good speedup outperforms existing multi core kernel svm solvers including asynchronous stochastic coordinate descent multi core libsvm
structured prediction theory based factor graph complexity present general theoretical analysis structured prediction series new results give new data dependent margin guarantees structured prediction wide family loss functions general family hypotheses arbitrary factor graph decomposition tightest margin bounds known standard multi class general structured prediction problems guarantees expressed terms data dependent complexity measure emph factor graph complexity show estimated data bounded terms familiar quantities several commonly used hypothesis sets sparsity measure features graphs proof techniques include generalizations talagrand contraction lemma independent interest extend theory leveraging principle voted risk minimization vrm show learning possible even complex factor graphs present new learning bounds advanced setting use devise new algorithms emph voted conditional random field vcrf emph voted structured boosting structboost algorithms make use complex features factor graphs yet benefit favorable learning guarantees also report results experiments vcrf several datasets validate theory
parameter learning log supermodular distributions consider log supermodular models binary variables probabilistic models negative log densities submodular models provide probabilistic interpretations common combinatorial optimization tasks image segmentation paper focus primarily parameter estimation models known upper bounds intractable log partition function show bound based separable optimization base polytope submodular function always inferior bound based perturb map ideas learn parameters given approximation log partition function expectation randomization use stochastic subgradient technique maximize lower bound log likelihood also extended conditional maximum likelihood illustrate new results set experiments binary image denoising highlight flexibility probabilistic model learn missing data
exact recovery hard thresholding pursuit hard thresholding pursuit htp class truncated gradient descent methods finding sparse solutions ell_0 constrained loss minimization problems htp style methods shown strong approximation guarantee impressive numerical performance high dimensional statistical learning applications however current theoretical treatment methods traditionally restricted analysis parameter estimation consistency remains open problem analyze support recovery performance sparsistency type methods recovering global minimizer original hard problem paper bridge gap showing first time exact recovery global sparse minimizer possible htp style methods restricted strong condition number bounding conditions show htp style methods able recover support certain relaxed sparse solutions without assuming bounded restricted strong condition number numerical results simulated data confirms theoretical predictions
new liftable classes first order probabilistic inference statistical relational models provide compact encodings probabilistic dependencies relational domains result highly intractable graphical models goal lifted inference carry probabilistic inference without needing reason individual separately instead treating exchangeable undistinguished objects whole paper study domain recursion inference rule despite central role early theoretical results domain lifted inference later believed redundant show rule powerful expected fact significantly extends range models lifted inference runs time polynomial number individuals domain includes open problem called symmetric transitivity model first order logic encoding birthday paradox identify new classes s2fo2 s2ru domain liftable theories respectively subsume fo2 recursively unary theories largest classes domain liftable theories known far show using domain recursion achieve exponential speedup even theories cannot fully lifted existing set inference rules
variational inference mixed probabilistic submodular models consider problem variational inference probabilistic models log submodular log supermodular higher order potentials models represent arbitrary distributions binary variables thus generalize commonly used pairwise markov random fields models log supermodular potentials efficient approximate inference algorithms known inference considered models hard general present efficient approximate algorithms exploiting recent advances field discrete optimization demonstrate effectiveness approach large set experiments model allows reasoning preferences sets items complements substitutes
unifying count based exploration intrinsic motivation consider agent uncertainty environment problem generalizing uncertainty across states specifically focus problem exploration non tabular reinforcement learning drawing inspiration intrinsic motivation literature use density models measure uncertainty propose novel algorithm deriving pseudo count arbitrary density model technique enables generalize count based exploration algorithms non tabular case apply ideas atari 2600 games providing sensible pseudo counts raw pixels transform pseudo counts exploration bonuses obtain significantly improved exploration number hard games including infamously difficult montezuma revenge
approximate maximum entropy principles via goemans williamson applications provable variational methods well known maximum entropy principle due jaynes states given mean parameters maximum entropy distribution matching exponential family popular machine learning due occam razor interpretation unfortunately calculating potentials maximum entropy distribution intractable bgs14 provide computationally efficient versions principle mean parameters pairwise moments design distributions approximately match given pairwise moments entropy comparable maximum entropy distribution matching moments additionally provide surprising applications approximate maximum entropy principle designing provable variational methods partition function calculations ising models without assumptions potentials model precisely show get approximation guarantees log partition function comparable low temperature limit setting optimization quadratic forms hypercube an06
multi step inertial forward backward splitting method non convex optimization paper propose multi step inertial forward backward splitting algorithm minimizing sum non necessarily convex functions proper lower semi continuous differentiable lipschitz continuous gradient first prove global convergence scheme help kurdyka ojasiewicz property non smooth part also partly smooth relative smooth submanifold establish finite identification latter provide sharp local linear convergence analysis proposed method illustrated problems arising statistics machine learning
fast flexible monotonic functions ensembles lattices many machine learning problems inputs known positively negatively related output cases training model respect monotonic relationship provide regularization makes model interpretable however flexible monotonic functions computationally challenging learn beyond features break barrier learning ensembles monotonic calibrated interpolated look tables lattices key contribution automated algorithm selecting feature subsets ensemble base models demonstrate compared random forests ensembles produce similar better accuracy providing guaranteed monotonicity consistent prior knowledge smaller model size faster evaluation
architectural complexity measures recurrent neural networks paper systematically analyze connecting architectures recurrent neural networks rnns main contribution twofold first present rigorous graph theoretic framework describing connecting architectures rnns general second propose architecture complexity measures rnns recurrent depth captures rnn time nonlinear complexity feedforward depth captures local input output nonlinearity similar depth feedforward neural networks fnns recurrent skip coefficient captures rapidly information propagates time rigorously prove measure existence computability experimental results show rnns might benefit larger recurrent depth feedforward depth demonstrate increasing recurrent skip coefficient offers performance boosts long term dependency problems
online convex optimization unconstrained domains losses propose online convex optimization algorithm rescaledexp achieves optimal regret unconstrained setting without prior knowledge bounds loss functions prove lower bound showing exponential separation regret existing algorithms require known bound loss functions algorithm require knowledge rescaledexp matches lower bound asymptotically number iterations rescaledexp naturally hyperparameter free demonstrate empirically matches prior optimization algorithms require hyperparameter optimization
split lbi iterative regularization path structural sparsity iterative regularization path structural sparsity proposed paper based variable splitting linearized bregman iteration hence called emph split lbi despite simplicity split lbi outperforms popular generalized lasso theory experiments theory path consistency presented equipped proper early stopping split lbi achieve model selection consistency family irrepresentable conditions weaker necessary sufficient condition generalized lasso furthermore ell_2 error bounds also given minimax optimal rates utility benefit algorithm illustrated applications traditional image denoising novel example partial order ranking
variational autoencoder deep learning images labels captions novel variational autoencoder developed model images well associated labels captions deep generative deconvolutional network dgdn used decoder latent image features deep convolutional neural network cnn used image encoder cnn used approximate distribution latent dgdn features code latent code also linked generative models labels bayesian support vector machine captions recurrent neural network predicting label caption new image test averaging performed across distribution latent codes computationally efficient consequence learned cnn based encoder since framework capable modeling image presence absence associated labels captions new semi supervised setting manifested cnn learning images framework even allows unsupervised cnn learning based images alone
recovery guarantee non negative matrix factorization via alternating updates non negative matrix factorization popular tool decomposing data feature weight matrices non negativity constraints enjoys practical success poorly understood theoretically paper proposes algorithm alternates decoding weights updating features shows assuming generative model data provably recovers ground truth fairly mild conditions particular essential requirement features linear independence furthermore algorithm uses relu exploit non negativity decoding weights thus tolerate adversarial noise potentially large signal tolerate unbiased noise much larger signal analysis relies carefully designed coupling potential functions believe independent interest
proximal deep structured models many problems real world applications involve predicting continuous valued random variables statistically related paper propose powerful deep structured model able learn complex non linear functions encode dependencies continuous output variables show inference model using proximal methods efficiently solved feed foward pass special type deep recurrent neural network demonstrate effectiveness approach tasks image denoising depth refinement optical flow estimation
safe policy improvement minimizing robust baseline regret important problem sequential decision making uncertainty use limited data compute safe policy policy guaranteed perform least well given baseline strategy paper develop analyze new model based approach compute safe policy access inaccurate dynamics model system known accuracy guarantees proposed robust method uses inaccurate model directly minimize negative regret baseline policy contrary existing approaches minimizing regret allows improve baseline policy states accurate dynamics seamlessly fall back baseline policy otherwise show formulation hard propose approximate algorithm empirical results several domains show even relatively simple approximate algorithm significantly outperform standard approaches
pseudo bayesian algorithm robust pca commonly used many applications robust pca represents algorithmic attempt reduce sensitivity classical pca outliers basic idea learn decomposition data matrix interest low rank sparse components latter representing unwanted outliers although resulting problem typically hard convex relaxations provide computationally expedient alternative theoretical support however practical regimes performance guarantees break variety non convex alternatives including bayesian inspired models proposed boost estimation quality unfortunately though without additional priori knowledge none methods significantly expand critical operational range exact principal subspace recovery possible mix propose novel pseudo bayesian algorithm explicitly compensates design weaknesses many existing non convex approaches leading state art performance sound analytical foundation
learning values across many orders magnitude learning algorithms invariant scale signal approximated propose adaptively normalize targets used learning updates important value based reinforcement learning magnitude appropriate value approximations change time update policy behavior main motivation prior work learning play atari games rewards clipped predetermined range clipping facilitates learning across many different games single learning algorithm clipped reward function result qualitatively different behavior using adaptive normalization remove domain specific heuristic without diminishing overall performance
single pass pca matrix products paper present new algorithm computing low rank approximation product taking single pass matrices straightforward way first sketch individually find top components using pca sketch algorithm contrast retains additional summary information row column norms etc uses additional information obtain improved approximation sketches main analytical result establishes comparable spectral norm guarantee existing pass methods addition also provide results apache spark implementation shows better computational statistical performance real world synthetic evaluation datasets
convolutional neural fabrics despite success cnns selecting optimal architecture given task remains open problem instead aiming select single optimal architecture propose fabric embeds exponentially large number architectures fabric consists trellis connects response maps different layers scales channels sparse homogeneous local connectivity pattern hyper parameters fabric number channels layers individual architectures recovered paths fabric addition ensemble embedded architectures together sharing weights paths overlap parameters learned using standard methods based back propagation cost scales linearly fabric size present benchmark results competitive state art image classification mnist cifar10 semantic segmentation part labels dataset
generative shape models joint text recognition segmentation little training data demonstrate generative model object shapes achieve state art results challenging scene text recognition tasks orders magnitude fewer training images required competing discriminative methods addition transcribing text challenging images method performs fine grained instance segmentation characters show model robust affine transformations non affine deformations compared previous approaches
mixed vine copulas joint models spike counts local field potentials concurrent measurements neural activity multiple scales sometimes performed multimodal techniques become increasingly important studying brain function however statistical methods concurrent analysis currently lacking introduce techniques framework based vine copulas mixed margins construct multivariate stochastic models models describe detailed mixed interactions discrete variables neural spike counts continuous variables local field potentials propose efficient methods likelihood calculation inference sampling mutual information estimation within framework test methods simulated data demonstrate applicability mixed data generated biologically realistic neural network methods hold promise considerably improve statistical analysis neural data recorded simultaneously different scales
optimal black box reductions optimization objectives diverse world machine learning applications given rise plethora algorithms optimization methods finely tuned specific regression classification task hand reduce complexity algorithm design machine learning reductions develop reductions take method developed setting apply entire spectrum smoothness strong convexity applications furthermore unlike existing results new reductions optimal practical show new reductions give rise new faster running times training linear classifiers various families loss functions conclude experiments showing successes also practice
dialog based language learning long term goal machine learning research build intelligent dialog agent research natural language understanding focused learning fixed training sets labeled data supervision either word level tagging parsing tasks sentence level question answering machine translation kind supervision realistic humans learn language learned used communication work study dialog based language learning supervision given naturally implicitly response dialog partner conversation study setup domains babi dataset weston 2015 large scale question answering dodge 2015 evaluate set baseline learning strategies tasks show novel model incorporating predictive lookahead promising approach learning teacher response particular surprising result learn answer questions correctly without reward based supervision
online bayesian moment matching topic modeling unknown number topics latent dirichlet allocation lda popular model topic modeling well many problems latent groups simple effective number topics latent groups unknown hierarchical dirichlet process hdp provides elegant non parametric extension however complex model difficult incorporate prior knowledge since distribution topics implicit propose new models extend lda simple intuitive fashion directly expressing distribution number topics also propose new online bayesian moment matching technique learn parameters number topics models based streaming data approach achieves higher log likelihood batch online hdp fixed hyperparameters several corpora
sparse interactive model matrix completion side information matrix completion methods benefit side information besides partially observed matrix use side features describing row column entities matrix shown reduce sample complexity completing matrix propose novel sparse formulation explicitly models interaction row column side features approximate matrix entries unlike early methods model require low rank condition model parameter matrix prove side features span latent feature space matrix recovered number observed entries needed exact recovery log size matrix side features corrupted latent features matrix small perturbation method achieve epsilon recovery log sample complexity maintains rate similar classfic methods side information efficient linearized lagrangian algorithm developed strong guarantee convergence empirical results show approach outperforms state art methods simulations real world datasets
truncated variance reduction unified approach bayesian optimization level set estimation present new algorithm truncated variance reduction truvar treats bayesian optimization level set estimation lse gaussian processes unified fashion algorithm greedily shrinks sum truncated variances within set potential maximizers unclassified points lse updated based confidence bounds truvar effective several important settings typically non trivial incorporate myopic algorithms including pointwise costs heteroscedastic noise provide general theoretical guarantee truvar covering aspects use recover strengthen existing results lse moreover provide new result setting select number noise levels associated costs demonstrate effectiveness algorithm synthetic real world data sets
mixtures markov chains study problem reconstructing mixture markov chains trajectories generated random walks state space mild non degeneracy conditions show uniquely reconstruct underlying chains considering trajectories length represent triples states algorithm spectral nature easy implement
high dimensional structured superposition models high dimensional superposition models characterize observations using parameters written sum multiple component parameters structure sum low rank sparse matrices paper consider general superposition models allow sum number component parameters component structure characterized norm present simple estimator models give geometric condition components accurately estimated characterize sample complexity estimator give non asymptotic bounds componentwise estimation error use tools empirical processes generic chaining statistical analysis results substantially generalize prior work superposition models terms gaussian widths suitable spherical caps
finite sample prediction recovery bounds ordinal embedding goal ordinal embedding represent items points low dimensional euclidean space given set constraints like item closer item item ordinal constraints like often come human judgments classic approach solving problem known non metric multidimensional scaling account errors variation judgments consider noisy situation given constraints independently corrupted reversing correct constraint probability ordinal embedding problem studied decades past work pays little attention question whether accurate embedding possible apart empirical studies paper shows generative data model possible learn correct embedding noisy distance comparisons establishing fundamental result paper makes several new contributions first derive prediction error bounds embedding noisy distance comparisons exploiting fact rank distance matrix points bounds characterize well learned embedding predicts new comparative judgments second show underlying embedding recovered solving simple convex optimization result highly non trivial since show linear map corresponding distance comparisons non invertible exists nonlinear map invertible third new algorithms ordinal embedding proposed evaluated experiments
makes objects similar unified multi metric learning approach linkages essentially determined similarity measures derived multiple perspectives example spatial linkages usually generated based localities heterogeneous data whereas semantic linkages come various properties different physical meanings behind social relations many existing metric learning models focus spatial linkages leave rich semantic factors unconsidered similarities based models usually overdetermined linkages propose unified multi metric learning um2l framework exploit multiple types metrics um2l type combination operator introduced distance characterization multiple perspectives thus introduce flexibilities representing utilizing spatial semantic linkages besides propose uniform solver um2l guaranteed converge extensive experiments diverse applications exhibit superior classification performance comprehensibility um2l visualization results also validate ability physical meanings discovery
unsupervised learning spoken language visual context humans learn speak read write computers paper present deep neural network model capable rudimentary spoken language acquisition using untranscribed audio training data whose supervision comes form contextually relevant visual images describe collection data comprised 120 000 spoken audio captions places image dataset evaluate model image search annotation task also provide visualizations suggest model learning recognize meaningful words within caption spectrograms
cyclades conflict free asynchronous machine learning present cyclades general framework parallelizing stochastic optimization algorithms shared memory setting cyclades asynchronous model updates requires memory locking mechanisms similar hogwild type algorithms unlike hogwild cyclades introduces conflicts parallel execution offers black box analysis provable speedups across large family algorithms due inherent cache locality conflict free nature multi core implementation cyclades consistently outperforms hogwild type algorithms sufficiently sparse datasets leading speedup gains compared hogwild times gains asynchronous implementations variance reduction algorithms
disease trajectory maps medical researchers coming appreciate many diseases fact complex heterogeneous syndromes composed subpopulations express different variants related complication longitudinal data extracted individual electronic health records ehr offer exciting new way study subtle differences way diseases progress time paper focus answering questions asked using databases longitudinal ehr data first want understand whether individuals similar disease trajectories whether small number degrees freedom account differences trajectories across population second want understand important clinical outcomes associated disease trajectories answer questions propose disease trajectory map dtm novel probabilistic model learns low dimensional representations sparse irregularly sampled longitudinal data propose stochastic variational inference algorithm learning dtm allows model scale large modern medical datasets demonstrate dtm analyze data collected patients complex autoimmune disease scleroderma find dtm learns meaningful representations disease trajectories representations significantly associated important clinical outcomes
fast free inference simulation models bayesian conditional density estimation many statistical models simulated forwards intractable likelihoods approximate bayesian computation abc methods used infer properties models data traditionally methods approximate posterior parameters conditioning data inside ball around observed data correct limit monte carlo methods draw samples approximate posterior approximate predictions error bars parameters algorithms critically slow practice draw samples broader distribution posterior propose new approach likelihood free inference based bayesian conditional density estimation preliminary inferences based limited simulation data used guide later simulations cases learning accurate parametric representation entire true posterior distribution requires fewer model simulations monte carlo abc methods need produce single sample approximate posterior
stochastic structured prediction bandit feedback stochastic structured prediction bandit feedback follows learning protocol sequence iterations learner receives input predicts output structure receives partial feedback form task loss evaluation predicted structure present applications learning scenario convex non convex objectives structured prediction analyze stochastic first order methods present experimental evaluation problems natural language processing exponential output spaces compare convergence speed across different objectives practical criterion optimal task performance development data optimization theoretic criterion minimal squared gradient norm best results criteria obtained non convex objective pairwise preference learning bandit feedback
learning uncertainty comparison bayesian approach accurately differentiating truly unpredictably random systematic changes occur random profound effect affect cognition examine underlying computational principles guide different learning behavior uncertain environment compared model bayesian approach visual search task different volatility levels model bayesian approach reflected individual estimation environmental volatility strong correlation learning rate model belief stationarity bayesian approach different volatility conditions low volatility condition model indicates learning rate positively correlates lose shift rate choice optimality inverted shape bayesian approach indicates belief environmental stationarity positively correlates choice optimality lose shift rate inverted shape addition showed comparing expert learners individuals high lose shift rate sub optimal learners significantly higher learning rate estimated model lower belief stationarity bayesian model
minimax optimal alternating minimization kernel nonparametric tensor learning investigate statistical performance computational efficiency alternating minimization procedure nonparametric tensor learning tensor modeling widely used capturing higher order relations multimodal data sources addition linear model nonlinear tensor model received much attention recently high flexibility consider alternating minimization procedure general nonlinear model true function consists components reproducing kernel hilbert space rkhs paper show alternating minimization method achieves linear convergence optimization algorithm generalization error resultant estimator yields minimax optimality apply algorithm multitask learning problems show method actually shows favorable performances
recursive teaching dimension classes recursive teaching dimension rtd concept class subseteq introduced zilles zlhz11 complexity parameter measured worst case number labeled examples needed learn target concept recursive teaching model paper study quantitative relation rtd well known learning complexity measure dimension vcd improve best known upper worst case lower bounds recursive teaching dimension respect dimension given concept class subseteq vcd first show rtd first upper bound rtd depends vcd independent size concept class domain size work best known upper bound rtd log log obtained moran mswy15 remove log log factor also improve lower bound worst case ratio rtd vcd present family classes c_k vcd c_k rtd c_k implies ratio rtd vcd worst case large work largest ratio known obtained kuhlmann kuh99 since finite concept class known satisfy rtd vcd
dimension free iteration complexity finite sum optimization problems many canonical machine learning problems boil convex optimization problem finite sum structure however whereas much progress made developing faster algorithms setting inherent limitations problems satisfactorily addressed existing lower bounds indeed current bounds focus first order optimization algorithms apply often unrealistic regime number iterations less dimension number samples work extend framework arjevani cite arjevani2015lower arjevani2016iteration provide new lower bounds dimension free beyond assumptions current bounds thereby covering standard finite sum optimization methods sag saga svrg sdca without duality well stochastic coordinate descent methods sdca accelerated proximal sdca
gan training generative neural samplers using variational divergence minimization generative neural networks probabilistic models implement sampling using feedforward neural networks take random input vector produce sample probability distribution defined network weights models expressive allow efficient computation samples derivatives cannot used computing likelihoods marginalization generative adversarial training method allows train models use auxiliary discriminative neural network show generative adversarial approach special case existing general variational divergence estimation approach show divergence used training generative neural networks discuss benefits various choices divergence functions training complexity quality obtained generative models
low rank regression tensor responses paper proposes efficient algorithm holrr handle regression tasks outputs tensor structure formulate regression problem minimization least square criterion multilinear rank constraint difficult non convex problem holrr computes efficiently approximate solution problem solid theoretical guarantees kernel extension also presented experiments synthetic real data show holrr computes accurate solutions computationally competitive
double thompson sampling dueling bandits paper propose double thompson sampling algorithm dueling bandit problems name suggests selects first second candidates according thompson sampling specifically maintains posterior distribution preference matrix chooses pair arms comparison according sets samples independently drawn posterior distribution simple algorithm applies general copeland dueling bandits including condorcet dueling bandits special case general copeland dueling bandits show achieves log regret moreover using back substitution argument refine regret log log log condorcet dueling bandits many practical copeland dueling bandits addition propose enhancement referred reduces regret carefully breaking ties experiments based synthetic real world data demonstrate significantly improve overall performance terms regret robustness
linear dynamical neural population models nonlinear embeddings body recent work modeling neural activity focuses recovering low dimensional latent features capture statistical structure large scale neural populations approaches focused linear generative models inference computationally tractable propose flds general class nonlinear generative models permits firing rate neuron vary arbitrary smooth function latent linear dynamical state extra flexibility allows model capture richer set neural variability purely linear model retains easily visualizable low dimensional latent space fit class non conjugate models propose variational inference scheme along novel approximate posterior capable capturing rich temporal correlations across time show techniques permit inference wide class generative models also show application neural datasets compared state art neural population models flds captures much larger proportion neural variability small number latent dimensions providing superior predictive performance interpretability
regret bounds non decomposable metrics missing labels consider problem recommending relevant labels items given data point user particular interested practically important setting evaluation respect non decomposable labels performance metrics like f_1 measure emph training data missing labels end propose generic framework given performance metric psi devise regularized objective function threshold values predicted score vector threshold selected positive show regret generalization error given metric psi bounded ultimately estimation error certain underlying parameters particular derive regret bounds popular settings collaborative filtering multilabel classification positive unlabeled learning problems obtain precise non asymptotic regret bound small even large fraction labels missing empirical results synthetic benchmark datasets demonstrate explicitly modeling missing labels optimizing desired performance metric algorithm indeed achieves significantly better performance like f_1 score compared methods model missing label information carefully
dynamic matrix recovery incomplete observations exact low rank constraint low rank matrix factorizations arise wide variety applications including recommendation systems topic models source separation name many applications widely noted incorporating temporal information allowing possibility time varying models significant improvements possible practice however despite reported superior empirical performance dynamic models static counterparts limited theoretical justification introducing complex models paper aim address gap studying problem recovering dynamically evolving low rank matrix incomplete observations first propose locally weighted matrix smoothing lowems framework possible approach dynamic matrix recovery establish error bounds lowems matrix sensing matrix completion observation models results quantify potential benefits exploiting dynamic constraints terms recovery accuracy sample complexity illustrate benefits provide synthetic real world experimental results
nyi divergence variational inference paper introduces variational nyi bound extends traditional variational inference nyi alpha divergences new family variational methods unifies number existing approaches enables smooth interpolation evidence lower bound log marginal likelihood controlled value alpha parametrises divergence reparameterization trick monte carlo approximation stochastic optimisation methods deployed obtain tractable unified framework optimisation consider negative alpha values propose novel variational inference method new special case proposed framework experiments bayesian neural networks variational auto encoders demonstrate wide applicability bound
confusions time interpretable bayesian model characterize trends decision making propose confusions time cot novel generative framework facilitates multi granular analysis decision making process cot models confusions error properties individual decision makers evolution time also allows obtain diagnostic insights collective decision making process interpretable manner end cot models confusions decision makers evolution time via time dependent confusion matrices interpretable insights obtained grouping similar decision makers items judged clusters representing cluster appropriate prototype identifying important features characterizing cluster via subspace feature indicator vector experimentation real world data bail decisions asthma treatments insurance policy approval decisions demonstrates cot accurately model explain confusions decision makers evolution time
adaptive averaging accelerated descent dynamics study accelerated descent dynamics constrained convex optimization dynamics described naturally coupling dual variable accumulating gradients given rate eta primal variable obtained weighted average mirrored dual trajectory weights using lyapunov argument give sufficient conditions eta achieve desired convergence rate example show replicator dynamics example mirror descent simplex accelerated using simple averaging scheme propose adaptive averaging heuristic adaptively computes weights speed decrease lyapunov function provide guarantees adaptive averaging continuous time prove preserves quadratic convergence rate accelerated first order methods discrete time give numerical experiments compare existing heuristics adaptive restarting experiments indicate adaptive averaging performs least well adaptive restarting significant improvements cases
bayesian optimization probabilistic programs present first general purpose framework marginal maximum posteriori estimation probabilistic program variables using series code transformations evidence probabilistic program therefore graphical model optimized respect arbitrary subset sampled variables carry optimization develop first bayesian optimization package directly exploit source code target leading innovations problem independent hyperpriors unbounded optimization implicit constraint satisfaction delivering significant performance improvements prominent existing packages present applications method number tasks including engineering design parameter optimization
efficient globally convergent stochastic optimization canonical correlation analysis study stochastic optimization canonical correlation analysis cca whose objective nonconvex decouple training samples although several stochastic gradient based optimization algorithms recently proposed solve problem global convergence guarantee provided inspired alternating least squares power iterations formulation cca shift invert preconditioning method pca propose globally convergent meta algorithms cca transform original problem sequences least squares problems need solved approximately instantiate meta algorithms state art sgd methods obtain time complexities significantly improve upon previous work experimental results demonstrate superior performance
unified approach learning parameters sum product networks present unified approach learning parameters sum product networks spns prove complete decomposable spn equivalent mixture trees tree corresponds product univariate distributions based mixture model perspective characterize objective function learning spns based maximum likelihood estimation mle principle show optimization problem formulated signomial program construct parameter learning algorithms spns using sequential monomial approximations sma concave convex procedure cccp respectively proposed methods naturally admit multiplicative updates hence effectively avoiding projection operation help unified framework also show case spns cccp leads algorithm expectation maximization despite fact different general
feature distributed sparse regression screen clean approach existing approaches distributed sparse regression assume data partitioned samples however high dimensional data natural partition data features propose algorithm distributed sparse regression data partitioned features rather samples approach allows user tailor general method various distributed computing platforms trading total amount data bits sent communication network number rounds communication show implementation approach capable solving regularized regression problems millions features minutes
backprop learning discriminative deterministic state estimators generative state estimators based probabilistic filters smoothers popular classes state estimators robots autonomous vehicles however generative models limited capacity handle rich sensory observations camera images since must model entire distribution sensor readings discriminative models suffer limitation typically complex train latent variable models state estimation present alternative approach parameters latent state distribution directly optimized deterministic computation graph resulting simple effective gradient descent algorithm training discriminative state estimators show procedure used train state estimators use complex input raw camera images must processed using expressive nonlinear function approximators convolutional neural networks model viewed type recurrent neural network connection probabilistic filtering allows design network architecture particularly well suited state estimation evaluate approach synthetic tracking task raw image inputs visual odometry task kitti dataset results show significant improvement standard generative approaches regular recurrent neural networks
swapout learning ensemble deep architectures describe swapout new stochastic training method outperforms resnets identical network structure yielding impressive results cifar cifar 100 swapout samples rich set architectures including dropout stochastic depth residual architectures special cases viewed regularization method swapout inhibits adaptation units layer similar dropout also across network layers conjecture swapout achieves strong regularization implicitly tying parameters across layers viewed ensemble training method samples much richer set architectures existing methods dropout stochastic depth propose parameterization reveals connections exiting architectures suggests much richer set architectures explored show formulation suggests efficient training method validate conclusions cifar cifar 100 matching state art accuracy remarkably layer wider model performs similar 1001 layer resnet model
assortment optimization mallows model consider assortment optimization problem customer preferences follow mixture mallows distributions assortment optimization problem focuses determining revenue profit maximizing subset products large universe products important decision commonly faced retailers determining offer customers key challenges mallows distribution lacks closed form expression requires summing exponential number terms compute choice probability hence expected revenue profit per customer finding best subset require exhaustive search key contributions efficiently computable closed form expression choice probability mallows model compact mixed integer linear program mip formulation assortment problem
operator variational inference variational inference umbrella term algorithms cast bayesian inference optimization classically variational inference uses kullback leibler divergence define optimization though divergence widely used resultant posterior approximation suffer undesirable statistical properties address reexamine variational inference roots optimization problem use operators functions functions design variational objectives example design variational objective langevin stein operator develop black box algorithm operator variational inference opvi optimizing operator objective importantly operators enable make explicit statistical computational tradeoffs variational inference characterize different properties variational objectives objectives admit data subsampling allowing inference scale massive data well objectives admit variational programs rich class posterior approximations require tractable density illustrate benefits opvi mixture model generative model images
select sample spike slab sparse coding probabilistic inference serves popular model neural processing still unclear however approximate probabilistic inference accurate scalable high dimensional continuous latent spaces especially typical posteriors sensory data expected exhibit complex latent dependencies including multiple modes study approach efficiently scaled maintaining richly structured posterior approximation conditions example model use spike slab sparse coding processing combine latent subspace selection gibbs sampling select sample unlike factored variational approaches method maintain large numbers posterior modes complex latent dependencies unlike pure sampling method scalable high dimensional latent spaces among sparse coding approaches non trivial posterior approximations map ica like models report largest scale results applications firstly verify approach showing competitiveness standard denoising benchmarks secondly use scalability first time study highly overcomplete settings encoding using sophisticated posterior representations generally study shows accurate probabilistic inference multi modal posteriors complex dependencies tractable functionally desirable consistent models neural inference
ladder variational autoencoders variational autoencoders powerful models unsupervised learning however deep models several layers dependent stochastic variables difficult train limits improvements obtained using highly expressive models propose new inference model ladder variational autoencoder recursively corrects generative distribution data dependent approximate likelihood process resembling recently proposed ladder network show model provides state art predictive log likelihood tighter log likelihood lower bound compared purely bottom inference layered variational autoencoders generative models provide detailed analysis learned hierarchical latent representation show new inference model qualitatively different utilizes deeper distributed hierarchy latent variables finally observe batch normalization deterministic warm gradually turning term crucial training variational models many stochastic layers
spals fast alternating least squares via implicit leverage scores sampling tensor candecomp parafac decomposition powerful computationally challenging tool modern data analytics paper show ways sampling intermediate steps alternating minimization algorithms computing low rank tensor decompositions leading sparse alternating least squares spals method specifically sample khatri rao product arises intermediate object iterations alternating least squares product captures interactions different tensor modes form main computational bottleneck solving many tensor related tasks exploiting spectral structures matrix khatri rao product provide efficient access statistical leverage scores applied tensor decomposition method leads first algorithm runs sublinear time per iteration approximates output deterministic alternating least squares algorithms empirical evaluations approach show significantly speedups existing randomized deterministic routines performing decomposition tensor size 92k billion nonzeros formed amazon product reviews routine converges minutes error deterministic als
crf cnn modeling structured information human pose estimation deep convolutional neural networks cnn achieved great success hand modeling structural information proved critical many vision problems great interest integrate effectively classical neural network message passing neurons layer paper propose crf cnn framework simultaneously model structural information output hidden feature layers probabilistic way applied human pose estimation message passing scheme proposed various layers body joint receives messages others efficient way message passing implemented convolution features maps layer also integrated feedforward propagation neural networks finally neural network implementation end end learning crf cnn provided effectiveness demonstrated experiments benchmark datasets
consistent regularization approach structured prediction propose analyze regularization approach structured prediction problems characterize large class loss functions allows naturally embed structured outputs linear space exploit fact design learning algorithms using surrogate loss approach regularization techniques prove universal consistency finite sample bounds characterizing generalization properties proposed method experimental results provided demonstrate practical usefulness proposed approach
refined lower bounds adversarial bandits provide new lower bounds regret must suffered adversarial bandit algorithms new results show recent upper bounds either hold high probability depend total loss best arm depend quadratic variation losses close tight besides prove impossibility results first existence single arm optimal every round cannot improve regret worst case second regret cannot scale effective range losses contrast results possible full information setting
learning deep embeddings histogram loss suggest new loss learning deep embeddings key characteristics new loss absence tunable parameters good results obtained across range datasets problems loss computed estimating distribution similarities positive matching negative non matching point pairs computing probability positive pair lower similarity score negative pair based probability estimates show operations performed simple piecewise differentiable manner using histograms soft assignment operations makes proposed loss suitable learning deep embeddings using stochastic optimization experiments reveal favourable results compared recently proposed loss functions
kernel bayesian inference posterior regularization propose vector valued regression problem whose solution equivalent reproducing kernel hilbert space rkhs embedding bayesian posterior distribution equivalence provides new understanding kernel bayesian inference moreover optimization problem induces new regularization posterior embedding estimator faster comparable performance squared regularization kernel bayes rule regularization coincides former thresholding approach used kernel pomdps whose consistency remains established theoretical work solves open problem provides consistency analysis regression settings based optimizational formulation propose flexible bayesian posterior regularization framework first time enables put regularization distribution level apply method nonparametric state space filtering tasks extremely nonlinear dynamics show performance gains baselines
learning influence functions incomplete observations study problem learning influence functions incomplete observations node activations incomplete observations major concern online real world social networks fully observable establish proper improper pac learnability influence functions randomly missing observations proper pac learnability discrete time linear threshold dlt discrete time independent cascade dic models established reducing incomplete observations complete observations modified graph improper pac learnability result applies dlt dic models well continuous time independent cascade cic model based parametrization terms reachability features also gives rise efficient practical heuristic experiments synthetic real world datasets demonstrate ability method compensate even fairly large fraction missing observations
general tensor spectral clustering higher order data spectral clustering clustering well known techniques data analysis recent work extended spectral clustering square symmetric tensors hypermatrices derived network develop new tensor spectral clustering method simultaneously clusters rows columns slices nonnegative mode tensor generalizes tensors number modes algorithm based new random walk model call super spacey random surfer show method performs state art clustering methods several synthetic datasets ground truth clusters use algorithm analyze several real world datasets
bayesian latent structure discovery multi neuron recordings neural circuits contain heterogeneous groups neurons differ type location connectivity basic response properties however traditional methods dimensionality reduction clustering ill suited recovering structure underlying organization neural circuits particular take advantage rich temporal dependencies multi neuron recordings fail account noise neural spike trains describe new tools inferring latent structure simultaneously recorded spike train data using hierarchical extension multi neuron point process model commonly known generalized linear model glm approach combines glm flexible graph theoretic priors governing relationship latent features neural connectivity patterns fully bayesian inference via lya gamma augmentation resulting model allows classify neurons infer latent dimensions circuit organization correlated spike trains demonstrate effectiveness method applications synthetic data multi neuron recordings primate retina revealing latent patterns neural types locations spike trains alone
estimating size large network communities random sample real world networks large measured studied directly substantial interest estimating global network properties smaller sub samples important global properties number vertices nodes network estimating number vertices large network major challenge computer science epidemiology demography intelligence analysis paper consider population random graph stochastic block model sbm communities blocks sample obtained randomly choosing subset letting induced subgraph vertices addition observe total degree sampled vertex block membership given partial information propose efficient population size estimation algorithm called pulse accurately estimates size whole population well size community support theoretical analysis perform exhaustive set experiments study effects sample size sbm model parameters accuracy estimates experimental results also demonstrate pulse significantly outperforms widely used method called network scale estimator wide variety scenarios
wasserstein training restricted boltzmann machines boltzmann machines able learn highly complex multimodal structured multiscale real world data distributions parameters model usually learned minimizing kullback leibler divergence training samples learned model propose work novel approach boltzmann machine training assumes meaningful metric observations given metric represented wasserstein distance distributions derive gradient respect model parameters minimization new objective leads generative models different statistical properties demonstrate practical potential data completion denoising metric observations plays crucial role
deep admm net compressive sensing mri compressive sensing effective approach fast magnetic resonance imaging mri aims reconstructing image small number sampled data space accelerating data acquisition mri improve current mri system reconstruction accuracy computational speed paper propose novel deep architecture dubbed admm net admm net defined data flow graph derived iterative procedures alternating direction method multipliers admm algorithm optimizing based mri model training phase parameters net image transforms shrinkage functions etc discriminatively trained end end using bfgs algorithm testing phase computational overhead similar admm uses optimized parameters learned training data based reconstruction task experiments mri image reconstruction different sampling ratios space demonstrate significantly improves baseline admm algorithm achieves high reconstruction accuracies fast computational speed
maximization approximately submodular functions study problem maximizing function approximately submodular cardinality constraint approximate submodularity implicitly appears wide range applications many cases errors evaluation submodular function break submodularity say eps approximately submodular exists submodular function eps leq leq eps subsets interested characterizing query complexity maximizing subject cardinality constraint function error level eps provide lower upper bounds eps show exponential query complexity lower bound contrast eps stronger bounded curvature assumption give constant approximation algorithms
combining low density separators cnns work explores cnns recognition novel categories examples inspired transferability analysis cnns introduce additional unsupervised meta training stage exposes multiple top layer units large amount unlabeled real world images encouraging units learn diverse sets low density separators across unlabeled data capture generic richer description visual world decouples units ties specific set categories propose unsupervised margin maximization jointly estimates compact high density regions infers low density separators low density separator lds modules plugged top layers standard cnn architecture resulting cnns enhanced generality significantly improve performance scene classification fine grained recognition action recognition small training samples
learning sensor multiplexing design back propagation recent progress many imaging vision tasks driven use deep feed forward neural networks trained propagating gradients loss defined final output back network first layer operates directly image propose back propagating step learn camera sensor designs jointly networks carry inference images capture paper specifically consider design inference problems typical color camera sensor able measure color channel pixel location computational inference required reconstruct full color image learn camera sensor color multiplexing pattern encoding layer whose learnable weights determine color channel among fixed set measured location weights jointly trained reconstruction network operates corresponding sensor measurements produce full color image network achieves significant improvements accuracy traditional bayer pattern used color cameras automatically learns employ sparse color measurement approach similar recent design moreover improves upon design learning optimal layout measurements
high resolution neural connectivity incomplete tracing data using nonnegative spline regression whole brain neural connectivity data available viral tracing experiments reveal connections source injection site elsewhere brain hold promise revealing spatial patterns connectivity throughout mammalian brain achieve goal seek fit weighted nonnegative adjacency matrix among 100 brain voxels using viral tracer data despite multi year experimental effort injections provide incomplete coverage number voxels data orders magnitude larger number injections making problem severely underdetermined furthermore projection data missing within injection site local connections separable injection signal use novel machine learning algorithm meet challenges develop spatially explicit voxel scale connectivity map mouse visual system method combines features matrix completion loss missing data smoothing spline penalty regularize problem optionally low rank factorization demonstrate consistency estimator using synthetic data apply newly available allen mouse brain connectivity atlas data visual system algorithm significantly predictive current state art approaches assume regions homogeneous demonstrate efficacy low rank version visual cortex data discuss possibility extending whole brain connectivity matrix voxel scale
learning probabilistic latent space object shapes via generative adversarial modeling study problem object generation propose novel framework namely generative adversarial network gan generates objects probabilistic space leveraging recent advances volumetric convolutional networks generative adversarial nets benefits model fold first use adversarial criterion instead traditional heuristic criteria enables generator capture object structure implicitly synthesize high quality objects second generator establishes mapping low dimensional probabilistic space space objects sample objects without reference image cad models explore object manifold third adversarial discriminator provides powerful shape descriptor learned without supervision wide applications object recognition experiments demonstrate method generates high quality objects unsupervisedly learned features achieve impressive performance object recognition comparable supervised learning methods
learning sparse gaussian graphical models overlapping blocks present novel framework called grab graphical models overlapping blocks capture densely connected components network estimate grab takes input data matrix variables samples jointly learns network among variables densely connected groups variables called blocks grab major novelties compared existing network estimation methods require blocks given priori blocks overlap jointly learn network structure overlapping blocks solves joint optimization problem block coordinate descent method convex step show grab reveals underlying network structure substantially better state art competitors synthetic data applied cancer gene expression data grab outperforms competitors revealing known functional gene sets potentially novel genes drive cancer
multi step learning underlying structure statistical models multi step learning final learning task accomplished via sequence intermediate learning tasks intuition successive steps levels transform initial data representations suited final learning task related principle arises transfer learning baxter 2000 proposed theoretical framework study learning multiple tasks transforms inductive bias learner widespread multi step learning approach semi supervised learning steps unsupervised supervised several authors castelli cover 1996 balcan blum 2005 niyogi 2008 ben david 2008 urner 2011 analyzed ssl balcan blum 2005 proposing version pac learning framework augmented compatibility function link concept class unlabeled data distribution propose analyze ssl multi step learning approaches much spirit baxter framework defining learning problem generatively joint statistical model times determines natural way class conditional distributions possible marginal amounts abstract form compatibility function also allows analyze discrete non discrete settings tool analysis define notion gamma uniform shattering statistical models use give conditions marginal conditional models imply advantage multi step learning approaches particular recover general version result poggio 2012 mild hypotheses multi step approach learns features invariant successive factors finite group invariances sample complexity requirements additive rather multiplicative size subgroups
dynamic network surgery efficient dnns deep learning become ubiquitous technology improve machine intelligence however existing deep models structurally complex making difficult deployed mobile platforms limited computational power paper propose novel network compression method called dynamic network surgery remarkably reduce network complexity making fly connection pruning unlike previous methods accomplish task greedy way properly incorporate connection splicing whole process avoid incorrect pruning make continual network maintenance effectiveness method proved experiments without accuracy loss method efficiently compress number parameters lenet alexnet factor 108 times times respectively proving outperforms recent pruning method considerable margins code models available https github com yiwenguo dynamic network surgery
active nearest neighbor learning metric spaces propose pool based non parametric active learning algorithm general metric spaces called margin regularized metric active nearest neighbor marmann outputs nearest neighbor classifier give prediction error guarantees depend noisy margin properties input sample competitive obtained previously proposed passive learners prove label complexity marmann significantly lower passive learner similar error guarantees algorithm based generalized sample compression scheme new label efficient active model selection procedure
discriminative gaifman models present discriminative gaifman models novel family relational machine learning models gaifman models learn feature representations bottom representations locally connected bounded size regions knowledge bases kbs considering local bounded size neighborhoods knowledge bases renders logical inference learning tractable mitigates problem overfitting facilitates weight sharing gaifman models sample neighborhoods knowledge bases make learned relational models robust missing objects relations common situation open world kbs present core ideas gaifman models apply large scale relational learning problems also discuss ways gaifman models relate existing relational machine learning approaches
professor forcing new algorithm training recurrent networks teacher forcing algorithm trains recurrent networks supplying observed sequence values inputs training using network step ahead predictions multi step sampling introduce professor forcing algorithm uses adversarial domain adaptation encourage dynamics recurrent network training network sampling network multiple time steps apply professor forcing language modeling vocal synthesis raw waveforms handwriting generation image generation empirically find professor forcing acts regularizer improving test likelihood character level penn treebank sequential mnist also find model qualitatively improves samples especially sampling large number time steps supported human evaluation sample quality trade offs professor forcing scheduled sampling discussed produce snes showing professor forcing successfully makes dynamics network training sampling similar
pruning random forests prediction budget propose prune random forest resource constrained prediction first construct prune optimize expected feature cost accuracy pose pruning rfs novel integer program linear constraints encourages feature use establish total unimodularity constraint set prove corresponding relaxation solves original integer program exploit connections combinatorial optimization develop efficient primal dual algorithm scalable large datasets contrast bottom approach benefits good initialization conventional methods top acquiring features based utility value generally intractable requiring heuristics empirically pruning algorithm outperforms existing state art resource constrained algorithms
multistage campaigning social networks consider control problems multi stage campaigning social networks dynamic programming framework employed balance high present reward large penalty low future outcome presence extensive uncertainties particular establish theoretical foundations optimal campaigning social networks user activities modeled multivariate hawkes process derive time dependent linear relation intensity exogenous events several commonly used objective functions campaigning develop convex dynamic programming framework determining optimal intervention policy prescribes required level external drive stage desired campaigning result experiments synthetic data real world memetracker dataset show algorithm steer user activities optimal campaigning much accurately baselines
coevolutionary latent feature processes continuous time user item interactions matching users right items right time fundamental task recommendation systems users interact different items time users items feature evolve evolve time traditional models based static latent features discretizing time epochs become ineffective capturing fine grained temporal dynamics user item interactions propose coevolutionary latent feature process model accurately captures coevolving nature users items feature learn parameters design efficient convex optimization algorithm novel low rank space sharing constraints extensive experiments diverse real world datasets demonstrate significant improvements user behavior prediction compared state arts
coordinate wise power method paper propose coordinate wise version power method optimization viewpoint vanilla power method simultaneously updates coordinates iterate essential convergence analysis however different coordinates converge optimal value different speeds proposed algorithm call coordinate wise power method able select update important coordinates time iteration dimension matrix size active set inspired greedy nature method propose greedy coordinate descent algorithm applied non convex objective function specialized symmetric matrices provide convergence analyses methods experimental results synthetic real data show methods achieve times speedup basic power method meanwhile due coordinate wise nature methods suitable important case data cannot fit memory finally introduce coordinate wise mechanism could applied iterative methods used machine learning
barzilai borwein step size stochastic gradient descent major issues stochastic gradient descent sgd methods choose appropriate step size running algorithm since traditional line search technique apply stochastic optimization methods common practice sgd either use diminishing step size tune step size hand time consuming practice paper propose use barzilai borwein method automatically compute step sizes sgd variant stochastic variance reduced gradient svrg method leads algorithms sgd svrg prove svrg converges linearly strongly convex objective functions product prove linear convergence result svrg option proposed whose convergence result missing literature numerical experiments standard data sets show performance sgd svrg comparable sometimes even better sgd svrg best tuned step sizes superior advanced sgd variants
fast learning rates heavy tailed losses study fast learning rates losses necessarily bounded distribution heavy tails enable analyses introduce new conditions envelope function sup_ mathcal ell circ ell loss function mathcal hypothesis class exists integrable ell satisfies multi scale bernstein condition mathcal assumptions prove learning rate faster obtained depending multi scale bernstein powers arbitrarily close verify assumptions derive fast learning rates problem vector quantization means clustering heavy tailed distributions analyses enable obtain novel learning rates extend complement existing results literature theoretical practical viewpoints
cliquecnn deep unsupervised exemplar learning exemplar learning powerful paradigm discovering visual similarities unsupervised manner context however recent breakthrough deep learning could yet unfold full potential single positive sample great imbalance positive many negatives unreliable relationships samples training convolutional neural networks impaired given weak estimates local distance propose single optimization problem extract batches samples mutually consistent relations conflicting relations distributed different batches similar samples grouped compact cliques learning exemplar similarities framed sequence clique categorization tasks cnn consolidates transitivity relations within cliques learns single representation samples without need labels proposed unsupervised approach shown competitive performance detailed posture analysis object classification
guided policy search via approximate mirror descent guided policy search algorithms used optimize complex nonlinear policies deep neural networks without directly computing policy gradients high dimensional parameter space instead methods use supervised learning train policy mimic teacher algorithm trajectory optimizer trajectory centric reinforcement learning method guided policy search methods provide asymptotic local convergence guarantees construction clear much policy improves within small finite number iterations show guided policy search algorithms interpreted approximate variant mirror descent projection onto constraint manifold exact derive new guided policy search algorithm simpler provides appealing improvement convergence guarantees simplified convex linear settings show general nonlinear setting error projection step bounded provide empirical results several simulated robotic manipulation tasks show method stable achieves similar better performance compared prior guided policy search methods simpler formulation fewer hyperparameters
structured sparse regression via greedy hard thresholding several learning applications require solving high dimensional regression problems relevant features belong small number overlapping groups large datasets standard sparsity constraints hard thresholding methods proven extremely efficient methods require hard projections dealing overlapping groups paper show hard projections avoided appealing submodular optimization methods come strong theoretical guarantees even presence poorly conditioned data say features correlation geq existing analyses cannot handle methods exhibit interesting computation accuracy trade extended significantly harder problems sparse overlapping groups experiments real synthetic data validate claims demonstrate proposed methods orders magnitude faster greedy convex relaxation techniques learning group structured sparsity
learning games robustness fast convergence show learning algorithms satisfying low approximate regret property experience fast convergence approximate optimality large class repeated games property simply requires learner small regret compared eps multiplicative approximation best action hindsight ubiquitous among learning algorithms satisfied even vanilla hedge forecaster results improve upon recent work syrgkanis number ways require players observe payoffs players realized actions opposed expected payoffs show convergence occurs high probability show convergence bandit feedback finally improve upon speed convergence factor number players scope settings class algorithms analysis provides fast convergence considerably broader previous work framework applies dynamic population games via low approximate regret property shifting experts strengthen results lykouris ways allow players select learning algorithms larger class includes minor variant basic hedge algorithm increase maximum churn players approximate optimality achieved bandit setting present new algorithm provides small loss type bound improved dependence number actions utility settings simple efficient result independent interest
measuring reliability mcmc inference bidirectional monte carlo markov chain monte carlo mcmc main workhorses probabilistic inference notoriously hard measure quality approximate posterior samples challenge particularly salient black box inference methods hide details obscure inference failures work extend recently introduced bidirectional monte carlo technique evaluate mcmc based posterior inference algorithms running annealed importance sampling ais chains prior posterior vice versa simulated data upper bound expectation symmetrized divergence true posterior distribution distribution approximate samples integrate method probabilistic programming languages webppl stan validate several models datasets example method used guide design inference algorithms apply study effectiveness different model representations webppl stan
average case hardness rip certification restricted isometry property rip design matrices gives guarantees optimal recovery sparse linear models high interest compressed sensing statistical learning property particularly important computationally efficient recovery methods consequence even though general hard check rip holds substantial efforts find tractable proxies would allow construction rip matrices polynomial time verification rip given arbitrary matrix consider framework average case certifiers never wrongly declare matrix rip often correct random instances functions tractable suboptimal parameter regime show computationally hard task better regime results based new weaker assumption problem detecting dense subgraphs
provable efficient online matrix completion via non convex stochastic gradient descent matrix completion wish recover low rank matrix observing entries widely studied problem theory practice wide applications provable algorithms far problem restricted offline setting provide estimate unknown matrix using observations simultaneously however many applications online version observe entry time dynamically update estimate appealing existing algorithms efficient offline setting could highly inefficient online setting paper propose first provable efficient online algorithm matrix completion algorithm starts initial estimate matrix performs non convex stochastic gradient descent sgd every observation performs fast update involving row tall matrices giving near linear total runtime algorithm naturally used offline setting well gives competitive sample complexity runtime state art algorithms proofs introduce general framework show sgd updates tend stay away saddle surfaces could broader interests non convex problems
infinite hidden semi markov modulated interaction point process correlation events ubiquitous important temporal events modelling many cases correlation exists events emitted observations also arrival times state space models hidden markov model stochastic interaction point process models hawkes process studied extensively yet separately types correlations past paper propose bayesian nonparametric approach considers types correlations via unifying generalizing hidden semi markov model interaction point process model proposed approach simultaneously model observations arrival times temporal events determine number latent states data metropolis within particle gibbs sampler ancestor resampling developed efficient posterior inference approach tested synthetic real world data promising outcomes
linear contextual bandits knapsacks consider linear contextual bandit problem resource consumption addition reward generation round outcome pulling arm reward well vector resource consumptions expected values outcomes depend linearly context arm budget capacity constraints require sum vectors exceed budget dimension objective maximize total reward problem turns common generalization classic linear contextual bandits lincontextual bandits knapsacks bwk online stochastic packing problem ospp present algorithms near optimal regret bounds problem bounds compare favorably results unstructured version problem relation contexts outcomes could arbitrary algorithm competes fixed set policies accessible optimization oracle combine techniques work lincontextual bwk ospp nontrivial manner also tackling new difficulties present special cases
selective inference group sparse linear models develop tools selective inference setting group sparsity including construction confidence intervals values testing selected groups variables main technical result gives precise distribution magnitude projection data onto given subspace enables develop inference procedures broad class group sparse selection methods including group lasso iterative hard thresholding forward stepwise regression give numerical results illustrate tools simulated data health record data
deep neural networks inexact matching person identification person identification task matching images person across multiple camera views almost prior approaches address challenge attempting learn possible transformations relate different views person training corpora utilize transformation patterns matching query image gallery image bank test time necessitates learning good feature representations images robust feature matching technique deep learning approaches convolutional neural networks cnn simultaneously shown great promise recently work propose cnn based architectures person identification first given pair images extract feature maps images via multiple stages convolution pooling novel inexact matching technique matches pixels first representation second furthermore search across wider region second representation matching novel matching technique allows tackle challenges posed large viewpoint variations illumination changes partial occlusions approach shows promising performance requires half parameters current state art technique nonetheless also suffers false matches times order mitigate issue propose fused architecture combines inexact matching pipeline state art exact matching technique observe substantial gains fused model current state art multiple challenging datasets varying sizes gains
accelerating stochastic composition optimization consider stochastic composition optimization problem objective composition expected value functions propose new stochastic first order method namely accelerated stochastic compositional proximal gradient asc method updates based queries sampling oracle using different timescales asc first proximal gradient method stochastic composition problem deal nonsmooth regularization penalty show asc exhibits faster convergence best known algorithms achieves optimal sample error complexity several important special cases demonstrate application asc reinforcement learning conduct numerical experiments
learning bound parameter transfer learning consider transfer learning problem using parameter transfer approach suitable parameter feature mapping learned task applied another objective task introduce notion local stability parametric feature mapping parameter transfer learnability thereby derive learning bound parameter transfer algorithms application parameter transfer learning discuss performance sparse coding self taught learning although self taught learning algorithms plentiful unlabeled data often show excellent empirical performance theoretical analysis studied paper also provide first theoretical learning bound self taught learning
active memory replace attention several mechanisms focus attention neural network selected parts input memory used successfully deep learning models recent years attention improved image classification image captioning speech recognition generative models learning algorithmic tasks probably largest impact neural machine translation recently similar improvements obtained using alternative mechanisms focus single part memory operate parallel uniform way mechanism call active memory improved attention algorithmic tasks image processing generative modelling far however active memory improved attention natural language processing tasks particular machine translation analyze shortcoming paper propose extended model active memory matches existing attention models neural machine translation generalizes better longer sentences investigate model explain previous active memory models succeed finally discuss active memory brings benefits attention better choice
understanding effective receptive field deep convolutional neural networks study characteristics receptive fields units deep convolutional networks receptive field size crucial issue many visual tasks output must respond large enough areas image capture information large objects introduce notion effective receptive field size show gaussian distribution occupies fraction full theoretical receptive field size analyze effective receptive field several architecture designs effect sub sampling skip connections dropout nonlinear activations leads suggestions ways address tendency small
local similarity aware deep feature embedding existing deep embedding methods vision tasks capable learning compact euclidean space images euclidean distances correspond similarity metric make learning effective efficient hard sample mining usually employed samples identified computing euclidean feature distance however global euclidean distance cannot faithfully characterize true feature similarity complex visual feature space intraclass distance high density region larger interclass distance low density regions paper introduce position dependent deep metric pddm unit capable learning similarity metric adaptive local feature structure metric used select genuinely hard samples local neighborhood guide deep embedding learning online robust manner new layer appealing pluggable convolutional networks trained end end local similarity aware feature embedding demonstrates faster convergence boosted performance complex image retrieval datasets large margin nature also leads superior generalization results large open set scenarios transfer learning shot learning imagenet 2010 imagenet 10k datasets
end end kernel learning supervised convolutional kernel networks paper introduce new image representation based multilayer kernel machine unlike traditional kernel methods data representation decoupled prediction task learn shape kernel supervision proceed first proposing improvements recently introduced convolutional kernel networks ckns context unsupervised learning derive backpropagation rules take advantage labeled training data resulting model new type convolutional neural network optimizing filters layer equivalent learning linear subspace reproducing kernel hilbert space rkhs show method achieves reasonably competitive performance image classification standard deep learning datasets cifar svhn also image super resolution demonstrating applicability approach large variety image related tasks
single image depth perception wild paper studies single image depth perception wild recovering depth single image taken unconstrained settings introduce new dataset depth wild consisting images wild annotated relative depth pairs random points also propose new algorithm learns estimate metric depth using annotations relative depth compared state art algorithm simpler performs better experiments show algorithm combined existing rgb data new relative depth annotations significantly improves single image depth perception wild
toward deeper understanding neural networks power initialization dual view expressivity develop general duality neural networks compositional kernel hilbert spaces introduce notion computation skeleton acyclic graph succinctly describes family neural networks kernel space random neural networks generated skeleton node replication followed sampling normal distribution assign weights kernel space consists functions arise compositions averaging non linear transformations governed skeleton graph topology activation functions prove random networks induce representations approximate kernel space particular follows random weight initialization often yields favorable starting point optimization despite worst case intractability training neural networks
visual dynamics probabilistic future frame synthesis via cross convolutional networks study problem synthesizing number likely future frames single input image contrast traditional methods tackled problem deterministic non parametric way propose novel approach models future frames probabilistic manner proposed method therefore able synthesize multiple possible next frames using model solving challenging problem involves low high level image motion understanding successful image synthesis propose novel network structure namely cross convolutional network encodes images feature maps motion information convolutional kernels aid synthesizing future frames experiments model performs well synthetic data shapes animated game sprites well real wold video data show model also applied tasks visual analogy making present analysis learned network representations
fcn object detection via region based fully convolutional networks present region based fully convolutional networks accurate efficient object detection contrast previous region based detectors fast faster cnn apply costly per region subnetwork hundreds times region based detector fully convolutional almost computation shared entire image achieve goal propose position sensitive score maps address dilemma translation invariance image classification translation variance object detection method thus naturally adopt fully convolutional image classifier backbones latest residual networks resnets object detection show competitive results pascal voc datasets map 2007 set 101 layer resnet meanwhile result achieved test time speed 170ms per image times faster faster cnn counterpart code made publicly available https github com daijifeng001 fcn
learning draw generative adversarial networks gans recently demonstrated capability synthesize compelling real world images room interiors album covers manga faces birds flowers existing models synthesize images based global constraints class label caption provide control pose object location propose new model generative adversarial network gawwn synthesizes images given instructions describing content draw location show high quality 128 128 image synthesis caltech ucsd birds dataset conditioned informal text descriptions also object location system exposes control bounding box around bird constituent parts modeling conditional distributions part locations system also enables conditioning arbitrary subsets parts beak tail yielding efficient interface picking part locations
consistent estimation functions data missing non monotonically random missing records perennial problem analysis complex data types target inference function full data law simple cases data missing random completely random rubin 1976 well known adjustments exist result consistent estimators target quantities assumptions underlying estimators generally realistic practical missing data problems unfortunately consistent estimators complex cases data missing random ordering variables induces monotonicity missingness status known general notable exceptions robins 1997 tchetgen tchetgen 2016 sadinle reiter 2016 paper propose general class consistent estimators cases data missing random missingness status non monotonic estimators generalized inverse probability weighting estimators make assumptions underlying full data law instead place independence restrictions certain fairly mild assumptions distribution missingness status conditional data assumptions place distribution missingness status conditional data viewed version conditional markov random field mrf corresponding chain graph assumptions embedded model permit identification observed data law admit natural fitting procedure based pseudo likelihood approach besag 1975 illustrate approach simple simulation study analysis risk premature birth women botswana exposed highly active anti retroviral therapy
stochastic online auc maximization area roc auc metric widely used measuring classification performance imbalanced data theoretical practical interest develop online learning algorithms maximizes auc large scale data specific challenge developing online auc maximization algorithm learning objective function usually defined pair training examples opposite classes existing methods achieves line processing higher space time complexity work propose new stochastic online algorithm auc maximization particular show auc optimization equivalently formulated convex concave saddle point problem saddle representation stochastic online algorithm solam proposed time space complexity datum establish theoretical convergence solam high probability demonstrate effectiveness efficiency standard benchmark datasets
deep learning without poor local minima paper prove conjecture published 1989 also partially address open problem announced conference learning theory colt 2015 expected loss function deep nonlinear neural network prove following statements independence assumption adopted recent work function non convex non concave every local minimum global minimum every critical point global minimum saddle point property saddle points differs shallow networks layers deeper networks layers moreover prove statements hold deep linear neural networks depth widths unrealistic assumptions result present instance answer following question difficult directly train deep model theory difficult classical machine learning models non convexity difficult nonexistence poor local minima property saddle points note even though advanced theoretical foundations deep learning still gap theory practice
regularized nonlinear acceleration describe convergence acceleration technique generic optimization problems scheme computes estimates optimum nonlinear average iterates produced optimization method weights average computed via simple small linear system whose solution updated online acceleration scheme runs parallel base algorithm providing improved estimates solution fly original optimization method running numerical experiments detailed classical classification problems
learning poke poking experiential learning intuitive physics investigate experiential learning paradigm acquiring internal model intuitive physics model evaluated real world robotic manipulation task requires displacing objects target locations poking robot gathered 400 hours experience executing 50k pokes different objects propose novel approach based deep neural networks modeling dynamics robot interactions directly images jointly estimating forward inverse models dynamics inverse model objective provides supervision construct informative visual features forward model predict turn regularize feature space inverse model interplay objectives creates useful accurate models used multi step decision making formulation additional benefit possible learn forward models abstract feature space thus alleviate need predicting pixels experiments show joint modeling approach outperforms alternative methods also demonstrate active data collection using learned model improves performance
weight normalization simple reparameterization accelerate training deep neural networks present weight normalization reparameterization weight vectors neural network decouples length weight vectors direction reparameterizing weights way improve conditioning optimization problem speed convergence stochastic gradient descent reparameterization inspired batch normalization introduce dependencies examples minibatch means method also applied successfully recurrent models lstms noise sensitive applications deep reinforcement learning generative models batch normalization less well suited although method much simpler still provides much speed full batch normalization addition computational overhead method lower permitting optimization steps taken amount time demonstrate usefulness method applications supervised image recognition generative modelling deep reinforcement learning
linear memory decomposition invariant linearly convergent conditional gradient algorithm structured polytopes recently several works shown natural modifications classical conditional gradient method aka frank wolfe algorithm constrained convex optimization provably converge linear rate feasible set polytope objective smooth strongly convex however results suffer significant shortcomings large memory requirement due need store explicit convex decomposition current iterate consequence large running time overhead per iteration worst case convergence rate depends unfavorably dimension work present new conditional gradient variant corresponding analysis improves shortcomings particular memory computation overheads linear dimension addition case optimal solution sparse new convergence rate replaces factor least linear dimension previous works linear dependence number non zeros optimal solution heart method corresponding analysis novel way compute decomposition invariant away steps theoretical guarantees apply polytope apply several important structured polytopes capture central concepts paths graphs perfect matchings bipartite graphs marginal distributions arise structured prediction tasks theoretical findings complemented empirical evidence shows method delivers state art performance
achieving threshold general stochastic block model linearized acyclic belief propagation stochastic block model sbm long studied machine learning network science canonical model clustering community detection recent years new developments demonstrated presence threshold phenomena model set new challenges algorithms detection problem symmetric sbms decelle conjectured called kesten stigum threshold achieved efficiently proved communities remained open communities prove conjecture obtaining general result applies arbitrary sbms linear size communities developed algorithm linearized acyclic belief propagation abp algorithm mitigates effects cycles provably achieving threshold time extends prior methods achieving universally threshold reducing preserving computational complexity abp also connected power iteration method generalized nonbacktracking operator formalizing spectral message passing interplay described krzakala extending results bordenave
orthogonal random features present intriguing discovery related random fourier features replacing multiplication random gaussian matrix multiplication properly scaled random orthogonal matrix significantly decreases kernel approximation error call technique orthogonal random features orf provide theoretical empirical justification effectiveness motivated discovery propose structured orthogonal random features sorf uses class structured discrete orthogonal matrices speed computation method reduces time cost mathcal mathcal log data dimensionality almost compromise kernel approximation quality compared orf experiments several datasets verify effectiveness orf sorf existing methods also provide discussions using type discrete orthogonal structure broader range kernels applications
universal correspondence network present deep learning framework accurate visual correspondences demonstrate effectiveness geometric semantic matching spanning across rigid motions intra class shape appearance variations contrast previous cnn based approaches optimize surrogate patch similarity objective use deep metric learning directly learn feature space preserves either geometric semantic similarity fully convolutional architecture along novel correspondence contrastive loss allows faster training effective reuse computations accurate gradient computation use thousands examples per image pair faster testing feedforward passes keypoints instead typical patch similarity methods propose convolutional spatial transformer mimic patch normalization traditional features like sift shown dramatically boost accuracy semantic correspondences across intra class shape variations extensive experiments kitti pascal cub 2011 datasets demonstrate significant advantages features prior works use either hand constructed learned features
multiscale laplacian graph kernel many real world graphs graphs molecules exhibit structure multiple different scales existing kernels graphs either purely local purely global character contrast building hierarchy nested subgraphs multiscale laplacian graph kernels mlg kernels define paper account structure range different scales heart mlg construction another new graph kernel called feature space laplacian graph kernel flg kernel property lift base kernel defined vertices graphs kernel graphs mlg kernel applies flg kernels subgraphs recursively make mlg kernel computationally feasible also introduce randomized projection procedure similar nystro method rkhs operators
generalization erm stochastic convex optimization dimension strikes back stochastic convex optimization goal minimize convex function doteq sim convex set subset unknown distribution cdot support convex optimization based samples ldots common approach problems empirical risk minimization erm optimizes f_s doteq frac sum_ leq consider question many samples necessary erm succeed closely related question uniform convergence f_s demonstrate standard ell_p ell_q setting lipschitz bounded functions bounded radius erm requires sample size scales linearly dimension nearly matches standard upper bounds improves omega log dependence proved ell_2 ell_2 setting shalev shwartz 2009 stark contrast problems solved using dimension independent number samples ell_2 ell_2 setting log dependence ell_1 ell_ infty setting using approaches also demonstrate general class range bounded lipschitz bounded stochastic convex programs even stronger gap appears already dimension
large scale price optimization via network flow paper deals price optimization find best pricing strategy maximizes revenue profit basis demand forecasting models though recent advances regression technologies made possible reveal price demand relationship number multiple products existing price optimization methods mixed integer programming formulation cannot handle tens hundreds products high computational costs cope problem paper proposes novel approach based network flow algorithms reveal connection supermodularity revenue cross elasticity demand basis connection propose efficient algorithm employs network flow algorithms proposed algorithm handle hundreds thousands products returns exact optimal solution assumption regarding cross elasticity demand even case assumption hold proposed algorithm efficiently find approximate solutions good state art methods empirical results show
bayesian optimization robust bayesian neural networks bayesian optimization prominent method optimizing expensive evaluate black box functions prominently applied tuning hyperparameters machine learning algorithms despite successes prototypical bayesian optimization approach using gaussian process models scale well either many hyperparameters many function evaluations attacking lack scalability flexibility thus key challenges field present general approach using flexible parametric models neural networks bayesian optimization staying close truly bayesian treatment possible obtain scalability stochastic gradient hamiltonian monte carlo whose robustness improve via scale adaptation experiments including multi task bayesian optimization tasks parallel optimization deep neural networks deep reinforcement learning show power flexibility approach
protein contact prediction amino acid evolution using convolutional networks graph valued images proteins building blocks life abundant organic molecules central focus areas biomedicine protein structure strongly related protein function thus structure prediction crucial task way solve many biological questions contact map compact representation dimensional structure protein via pairwise contacts amino acid constituting protein use convolutional network calculate protein contact maps inferred statistical coupling positions protein sequence input network image like structure amenable convolutions every pixel instead color channels contains bipartite undirected edge weighted graph propose several methods treating graph valued images convolutional network proposed method outperforms state art methods large margin also allows great flexibility regard input data makes useful studying wide range problems
supervised word mover distance accurately measuring similarity text documents lies core many real world applications machine learning include web search ranking document recommendation multi lingual document matching article categorization recently new document metric word mover distance wmd proposed unprecedented results knn based document classification wmd elevates high quality word embeddings document metrics formulating distance documents optimal transport problem embedded words however document distances entirely unsupervised lack mechanism incorporate supervision available paper propose efficient technique learn supervised metric call supervised wmd wmd metric algorithm learns document distances measure underlying semantic differences documents leveraging semantic differences individual words discovered supervised training achieved linear transformation underlying word embedding space tailored word specific weights learned minimize stochastic leave nearest neighbor classification error per document level evaluate metric real world text classification tasks wmd consistently outperforms almost competitive baselines
beyond exchangeability chinese voting process many online communities present user contributed responses reviews products answers questions user provided helpfulness votes highlight useful responses voting social process gain momentum based popularity responses polarity existing votes propose chinese voting process cvp models evolution helpfulness votes self reinforcing process dependent position presentation biases evaluate model amazon product reviews stackexchange forums measuring intrinsic quality individual responses behavioral coefficients different communities
poisson gamma dynamical systems paper presents dynamical system based poisson gamma construction sequentially observed multivariate count data inherent model novel bayesian nonparametric prior ties shrinks parameters powerful way develop theory model infinite limit steady state model inductive bias demonstrated variety real world datasets shown learn interpretable structure superior predictive performance
interpretable distribution features maximum testing power semimetrics probability distributions proposed given sum differences expectations analytic functions evaluated spatial frequency locations features features chosen maximize distinguishability distributions optimizing lower bound test power statistical test using features result parsimonious interpretable indication distributions differ locally empirical estimate test power criterion converges increasing sample size ensuring quality returned features real world benchmarks high dimensional text image data linear time tests using proposed semimetrics achieve comparable performance state art quadratic time maximum mean discrepancy test returning human interpretable features explain test results
dense associative memory pattern recognition model associative memory studied stores reliably retrieves many patterns number neurons network propose simple duality dense associative memory neural networks commonly used deep learning associative memory side duality family models smoothly interpolates limiting cases constructed limit referred feature matching mode pattern recognition prototype regime deep learning side duality family corresponds feedforward neural networks hidden layer various activation functions transmit activities visible neurons hidden layer family activation functions includes logistics rectified linear units rectified polynomials higher degrees proposed duality makes possible apply energy based intuition associative memory analyze computational properties neural networks unusual activation functions higher rectified polynomials used deep learning utility dense memories illustrated test cases logical gate xor recognition handwritten digits mnist data set
relevant sparse codes variational information bottleneck many applications desirable extract relevant aspects data principled way information bottleneck method seeks code maximises information relevance variable constraining information encoded original data unfortunately however method computationally demanding data high dimensional non gaussian propose approximate variational scheme maximising lower bound objective analogous variational using method derive algorithm recover features relevant sparse finally demonstrate kernelised versions algorithm used address broad range problems non linear relation
examples enough learn criticize criticism interpretability example based explanations widely used effort improve interpretability highly complex distributions however prototypes alone rarely sufficient represent gist complexity order users construct better mental models understand complex data distributions also need criticism explain textit captured prototypes motivated bayesian model criticism framework develop texttt mmd critic efficiently learns prototypes criticism designed aid human interpretability human subject pilot study shows texttt mmd critic selects prototypes criticism useful facilitate human understanding reasoning also evaluate prototypes selected texttt mmd critic via nearest prototype classifier showing competitive performance compared baselines
showing versus teaching demonstration people often learn others demonstrations classic inverse reinforcement learning irl algorithms brought closer realizing capacity machines contrast teaching demonstration less well studied computationally develop novel bayesian model teaching demonstration stark differences arise demonstrators intentionally teaching task versus simply performing task experiments show human participants systematically modify teaching behavior consistent predictions model show even standard irl algorithms benefit learning behaviors intentionally pedagogical conclude discussing irl algorithms take advantage intentional pedagogy
learning active learning data paper suggest novel data driven approach active learning key idea train regressor predicts expected error reduction candidate sample particular learning state formulating query selection procedure regression problem restricted working existing heuristics instead learn strategies based experience previous outcomes show strategy learnt either simple synthetic datasets subset domain specific data method yields strategies work well real data wide range domains
scalable variational inference dynamical systems gradient matching promising tool learning parameters state dynamics ordinary differential equations grid free inference approach fully observable systems times competitive numerical integration however many real world applications sparse observations available even unobserved variables included model description cases gradient matching methods difficult apply simply provide satisfactory results despite high computational cost numerical integration still gold standard many applications using existing gradient matching approach propose scalable variational inference framework infer states parameters simultaneously offers computational speedups improved accuracy works well even model misspecifications partially observable system
active learning peers paper addresses challenge learning peers online multitask setting instead always requesting label human oracle proposed method first determines learner task acquire label sufficient confidence peers either task similarity weighted sum single similar task saves oracle query later use difficult cases queries human oracle paper develops new algorithm exhibit behavior proves theoretical mistake bound method compared best linear predictor hindsight experiments multitask learning benchmark datasets show clearly superior performance baselines assuming task independence learning oracle learning peer tasks
gradient episodic memory continuum learning major obstacle towards artificial intelligence poor ability current models reuse knowledge acquired past quickly learn new tasks forgetting previously learned work formalize emph continuum learning setting training examples emph iid generated continuous stream tasks unknown relationship first propose new set metrics continuum learning characterize learning systems terms average accuracy also terms ability transfer knowledge previous future tasks second propose model continuum learning termed gradient episodic memory gem reduces forgetting allows potential improvements performance previous tasks experiments variants mnist cifar100 datasets demonstrate strong performance model compared variety state art contenders
consistent multitask learning nonlinear output relations key multitask learning exploiting relationships different tasks improve prediction performance relations linear regularization approaches used successfully however practice assuming tasks linearly related might restrictive allowing nonlinear structures challenge paper tackle issue casting problem within framework structured prediction main contribution novel algorithm learning multiple tasks related system nonlinear equations joint outputs need satisfy show algorithm consistent efficiently implemented experimental results show potential proposed method
joint distribution optimal transportation domain adaptation paper deals unsupervised domain adaptation problem wants estimate prediction function given target domain without labeled sample exploiting knowledge available source domain labels known work makes following assumption exists non linear transformation joint feature label space distributions domain propose solution problem optimal transport allows recover estimated target optimizing simultaneously optimal coupling show method corresponds minimization bound target error provide efficient algorithmic solution convergence proved versatility approach terms class hypothesis loss functions demonstrated real world classification regression problems reach surpass state art results
learning multiple tasks deep relationship networks deep networks trained large scale data learn transferable features promote learning multiple tasks deep features eventually transition general specific along deep networks fundamental problem exploit relationship across different tasks improve feature transferability task specific layers paper propose deep relationship networks drn discover task relationship based novel tensor normal priors parameter tensors multiple task specific layers deep convolutional networks jointly learning transferable features task relationships drn able alleviate dilemma negative transfer feature layers transfer classifier layer extensive experiments show drn yields state art results standard multi task learning benchmarks
label efficient learning transferable representations acrosss domains tasks propose framework learns representation transferable across different domains tasks data efficient manner approach battles domain shift domain adversarial loss generalizes embedding novel task using metric learning based approach model simultaneously optimized labeled source data unlabeled sparsely labeled data target domain method shows compelling results novel classes within new domain even labeled examples per class available outperforming prevalent fine tuning approach addition demonstrate effectiveness framework transfer learning task image object recognition video action recognition
matching neural paths transfer recognition correspondence search many machine learning tasks require finding per part correspondences objects work focus low level correspondences highly ambiguous matching problem propose use hierarchical semantic representation objects coming convolutional neural network solve ambiguity training low level correspondence prediction directly might option domains ground truth correspondences hard obtain show transfer recognition used avoid training idea mark parts matching features close levels convolutional feature hierarchy neural paths although overall number paths exponential number layers propose polynomial algorithm aggregating single backward pass empirical validation done task stereo correspondence demonstrates achieve competitive results among methods use labeled target domain data
deep neural networks suffer crowding crowding visual effect suffered humans object recognized isolation longer recognized objects called clutter placed close work study effect crowding artificial deep neural networks dnns object recognition analyze deep convolutional neural networks dcnns well extension dcnns multi scale change receptive field size convolution filters position image called eccentricity dependent models latter networks recently proposed modeling feedforward path primate visual cortex results reveal incorporating clutter images training set learning dnns lead robustness clutter seen training also dnns trained objects isolation find recognition accuracy dnns falls closer clutter target object clutter find visual similarity target clutter also plays role pooling early layers dnn leads crowding finally show eccentricity dependent model trained objects isolation recognize target objects clutter objects near center image whereas dcnn cannot
svcca singular vector canonical correlation analysis deep understanding improvement continuing empirical successes deep networks becomes increasingly important develop better methods understanding training models representations learned within paper propose singular value canonical correlation analysis svcca tool quickly comparing representations way invariant affine transform allowing comparison different layers networks fast compute allowing comparisons calculated previous methods deploy tool measure intrinsic dimensionality layers showing cases needless parameterization probe learning dynamics throughout training finding networks converge final representations bottom show class specific information networks formed suggest new training regimes simultaneously save computation overfit less
neural expectation maximization many real world tasks reasoning physical interaction require iden tification manipulation conceptual entities first step towards solving tasks automated discovery symbol like representations distributed disentangled paper explicitly formalize problem inference spatial mixture model component parametrized neural network based expectation maximization framework derive differentiable clustering method simultaneously learns group represent individual entities evaluate method sequential perceptual grouping task find accurately able recover constituent objects demonstrate learned representations useful predictive coding
pointnet deep hierarchical feature learning point sets metric space prior works study deep learning point sets pointnet pioneer direction however design pointnet capture local structures induced metric space points live limiting ability recognize fine grained patterns generalizability complex scenes work introduce hierarchical neural network applies pointnet recursively nested partitioning input point set exploiting metric space distances network able learn local features increasing contextual scales observation point sets usually sampled varying densities results greatly decreased performance networks trained uniform densities propose novel set learning layers adaptively combine features multiple scales experiments show network called pointnet able learn deep point set features efficiently robustly particular results significantly better state art obtained challenging benchmarks point clouds
preserving proximity global ranking node embedding investigate unsupervised generative approach network embedding multi task siamese neural network structure formulated connect embedding vectors objective preserve global node ranking local proximity nodes provide deeper analysis connect proposed proximity objective link prediction community detection network show model satisfy following design properties scalability asymmetry unity simplicity experiment results verify design properties also demonstrate superior performance learning rank classification regression link prediction tasks
unsupervised transformation learning via convex relaxations goal extract meaningful transformations data thickness lines handwriting lighting portrait raw images work propose unsupervised approach learn transformations based reconstructing nearest neighbors using linear combination transformations derive new algorithm unsupervised linear transformation learning handwritten digits celebrity portrait datasets show even linear transformations method extracts meaningful transformations generates visually high quality transformed outputs moreover method semiparametric model data distribution allowing learned transformations extrapolate training data work new types images
hunt unique stable sparse fast feature learning graphs purpose learning graphs hunt graph representation exhibit certain uniqueness stability sparsity properties also amenable fast computation leads graph representation based discovery family graph spectral distances denoted fgsd prove possess desired properties evaluate quality graph features produced fgsd demonstrate utility apply graph classification problem extensive experiments show simple svm based classification algorithm driven powerful fgsd based graph features significantly outperforms sophisticated state art algorithms unlabeled node datasets terms accuracy speed also yields competitive results labeled datasets despite fact utilize node label information
deep subspace clustering network present novel deep neural network architecture unsupervised subspace clustering architecture built upon deep auto encoders non linearly map input data latent space key idea introduce novel self expressive layer encoder decoder mimic self expressiveness property proven effective traditional subspace clustering differentiable new self expressive layer provides simple effective way learn pairwise affinities data points standard back propagation procedure nonlinear neural network based method able cluster data points complex often nonlinear structures propose pre training fine tuning strategies let effectively learn parameters subspace clustering networks experiments show proposed method significantly outperforms state art unsupervised subspace clustering methods
learning graph embeddings embedding propagation propose embedding propagation unsupervised learning framework graph structured data learns vector representations graphs passing types messages neighboring nodes forward messages consist label representations representations words features associated nodes backward messages consist gradients results aggregating representations applying reconstruction loss node representations finally computed representation labels significantly fewer parameters hyperparameters instance competitive often outperforms state art unsupervised learning methods range benchmark data sets
unsupervised sequence classification using sequential output statistics consider learning sequence classifier without labeled data using sequential output statistics problem highly valuable since obtaining labels training data often costly sequential output statistics language models could obtained independently input data thus low cost address problem propose unsupervised learning cost function study properties show compared earlier works less inclined stuck trivial solutions avoids need strong generative model although harder optimize functional form stochastic primal dual gradient method developed effectively solve problem experiment results real world datasets demonstrate new unsupervised learning method gives drastically lower errors baseline methods specifically reaches test errors twice obtained fully supervised learning
context selection embedding models word embeddings effective tool analyze language recently extended model types data beyond text items recommendation systems embedding models consider probability target observation word item conditioned elements context words items paper show conditioning elements context optimal instead improve predictions quality embedding representations modeling probability target conditioned subset elements context develop model account use amortized variational inference automatically choose subset experiments demonstrate model outperforms standard embedding methods datasets different domains terms held predictions quality embedding representations
probabilistic rule realization selection abstraction realization bilateral processes key deriving intelligence creativity many domains processes approached emph rules high level principles reveal invariances within similar yet diverse examples probabilistic setting discrete input spaces focus rule realization problem generates input sample distributions follow given rules ambitiously beyond mechanical realization takes whatever given instead ask proactively selecting reasonable rules realize goal demanding practice since initial rule set always consistent thus intelligent compromises needed formulate rule realization selection strongly connected components within single symmetric convex problem derive efficient algorithm works large scale taking music compositional rules main example throughout paper demonstrate model efficiency music realization composition also music interpretation understanding analysis
trimmed density ratio estimation density ratio estimation become versatile tool machine learning community recently however due unbounded nature density ratio estimation vulnerable corrupted data points often pushes estimated ratio toward infinity paper present robust estimator automatically identifies trims outliers proposed estimator convex formulation global optimum obtained via subgradient descent analyze parameter estimation error estimator high dimensional settings experiments conducted verify effectiveness estimator
minimax optimal algorithm crowdsourcing consider problem accurately estimating reliability workers based noisy labels provide fundamental question crowdsourcing propose novel lower bound minimax estimation error applies estimation procedure propose triangular estimation algorithm estimating reliability workers low complexity implemented streaming setting labels provided workers real time rely iterative procedure prove minimax optimal matches lower bound conclude assessing performance state art algorithms synthetic real world data
introspective classification convolutional nets propose introspective convolutional networks icn emphasize importance convolutional neural networks empowered generative capabilities employ reclassification synthesis algorithm perform training using formulation stemmed bayes theory icn tries iteratively synthesize pseudo negative samples enhance improving classification single cnn classifier learned time generative able directly synthesize new samples within discriminative model conduct experiments benchmark datasets including mnist cifar svhn using state art cnn architectures observe improved classification results
adaptive classification prediction budget propose novel adaptive approximation approach test time resource constrained prediction given input instance test time gating function identifies prediction model input among collection models objective minimize overall average cost without sacrificing accuracy learn gating prediction models fully labeled training data means bottom strategy novel bottom method first trains high accuracy complex model low complexity gating prediction model subsequently learnt adaptively approximate high accuracy model regions low cost models capable making highly accurate predictions pose empirical loss minimization problem cost constraints jointly train gating prediction models number benchmark datasets method outperforms state art achieving higher accuracy cost
learning feature evolvable streams learning streaming data attracted much attention past years though studies consider data stream fixed features real practice features evolvable example features data gathered limited lifespan sensors change sensors substituted new ones paper propose novel learning paradigm feature evolvable streaming learning old features would vanish new features occur rather relying current features attempt recover vanished features exploit improve performance specifically learn models recovered features current features respectively benefit recovered features develop ensemble methods first method combine predictions models theoretically show assistance old features performance new features improved second approach dynamically select best single prediction establish better performance guarantee best model switches experiments synthetic real data validate effectiveness proposal
aggressive sampling multi class binary reduction applications text classification address problem multi class classification case number classes large propose double sampling strategy top multi class binary reduction strategy transforms original multi class problem binary classification problem pairs examples aim sampling strategy overcome curse long tailed class distributions exhibited majority large scale multi class classification problems reduce number pairs examples expanded data show strategy alter consistency empirical risk minimization principle defined double sample reduction experiments carried dmoz wikipedia collections 000 100 000 classes show efficiency proposed approach terms training prediction time memory consumption predictive performance respect state art approaches
adversarial surrogate losses ordinal regression ordinal regression seeks class label predictions penalty incurred mistakes increases according ordering labels absolute error canonical example many existing methods task reduce binary classification problems employ surrogate losses hinge loss instead derive uniquely defined surrogate ordinal regression loss functions seeking predictor robust worst case approximations training data labels subject matching certain provided training data statistics demonstrate advantages approach surrogate losses based hinge loss approximations using uci ordinal prediction tasks
formal guarantees robustness classifier adversarial manipulation recent work shown state art classifiers quite brittle sense small adversarial change originally high confidence correctly classified input leads wrong classification high confidence raises concerns classifiers vulnerable attacks calls question usage safety critical systems show paper first time formal guarantees robustness classifier giving instance specific emph lower bounds norm input manipulation required change classifier decision based analysis propose cross lipschitz regularization functional show using form regularization kernel methods resp neural networks improves robustness classifier without loss prediction performance
cost efficient gradient boosting many applications require learning classifiers regressors accurate cheap evaluate prediction cost drastically reduced learned predictor constructed majority inputs uses cheap features fast evaluations main challenge little loss accuracy work propose budget aware strategy based deep boosted regression trees contrast previous approaches learning cost penalties method grow deep trees average nonetheless cheap compute evaluate method number datasets find outperforms current state art large margin algorithm easy implement learning time comparable original gradient boosting source code made available acceptance
highly efficient gradient boosting decision tree gradient boosting decision tree gbdt popular machine learning algorithm quite effective implementations xgboost pgbrt although many engineering optimizations adopted implementations efficiency scalability still unsatisfactory feature dimension high data size large major reason feature need scan data instances estimate information gain possible split points time consuming tackle problem propose novel techniques emph gradient based side sampling goss emph exclusive feature bundling efb goss exclude significant proportion data instances small gradients use rest estimate information gain prove since data instances larger gradients play important role computation information gain goss obtain quite accurate estimation information gain much smaller data size efb bundle mutually exclusive features rarely take nonzero values simultaneously reduce number features prove finding optimal bundling exclusive features hard greedy algorithm achieve quite good approximation ratio thus effectively reduce number features without hurting accuracy split point determination much call new gbdt implementation goss efb emph lightgbm experiments multiple public datasets show lightgbm speeds training process conventional gbdt times achieving almost accuracy
estimating accuracy unlabeled data probabilistic logic approach propose efficient method estimate accuracy classifiers using unlabeled data consider setting multiple classification problems target classes tied together logical constraints example set classes mutually exclusive meaning data instance belong proposed method based intuition classifiers agree likely correct classifiers make prediction violates constraints least classifier must making error experiments real world data sets produce accuracy estimates within percent true accuracy using solely unlabeled data models also outperform existing state art solutions estimating accuracies combining multiple classifier outputs results emphasize utility logical constraints estimating accuracy thus validating intuition
inferring generative model structure static analysis obtaining enough labeled training data complex discriminative models major bottleneck machine learning pipeline popular solution combining multiple sources weak supervision using generative models structure models affects quality training labels difficult learn without ground truth labels instead rely weak supervision sources structure virtue encoded programmatically present coral paradigm infers generative model structure statically analyzing code heuristics thus reducing data required learn structure significantly prove coral sample complexity scales quasilinearly number heuristics number relations found improving standard sample complexity exponential identifying degree relations experimentally coral matches outperforms traditional structure learning approaches points using coral model dependencies instead assuming independence results performing better fully supervised model accuracy points heuristics used label radiology data without ground truth labels
scalable model selection belief networks propose scalable algorithm model selection sigmoid belief networks sbns based factorized asymptotic bayesian fab framework derive corresponding generalized factorized information criterion gfic sbn proven statistically consistent marginal log likelihood capture dependencies within hidden variables sbns recognition network employed model variational distribution resulting algorithm call fabia simultaneously execute model selection inference maximizing lower bound gfic synthetic real data experiments suggest fabia compared state art algorithms learning sbns produces concise model thus enabling faster testing improves predictive performance iii accelerates convergence prevents overfitting
time dependent spatially varying graphical models application brain fmri data analysis spatio temporal data often exhibits nonstationary changes spatial structure often masked strong temporal dependencies nonseparability work present additive model splits data temporally correlated signal spatially correlated noise model spatially correlated portion using time varying gaussian graphical model assumptions smoothness changes graphical model structure derive strong single sample convergence results confirming ability estimate track meaningful graphical models evolve time apply methodology discovery time varying spatial structure human brain fmri signals
bayesian data augmentation approach learning deep models data augmentation essential part training process applied deep learning models motivation robust training process deep learning models depends large annotated datasets expensive acquired stored processed therefore reasonable alternative able automatically generate new annotated training samples using process known data augmentation dominant data augmentation approach field assumes new training samples obtained via random geometric appearance transformations applied annotated training samples strong assumption unclear reliable generative model producing new training samples paper provide novel bayesian formulation data augmentation allowing introduce theoretically sound algorithm based extension generative adversarial network gan new annotated training points treated missing variables generated based distribution learned training set generalised monte carlo expectation maximisation process classification results mnist cifar cifar 100 show better performance proposed method compared current dominant data augmentation approach
union intersections uoi interpretable data driven discovery prediction increasing size complexity scientific data could dramatically enhance discovery prediction basic scientific applications neuroscience genetics systems biology etc realizing potential however requires novel statistical analysis methods interpretable predictive introduce union intersections uoi method flexible modular scalable framework enhanced model selection estimation method performs model selection model estimation intersection union operations respectively show uoi satisfy criteria low variance nearly unbiased estimation small number interpretable features maintaining high quality prediction accuracy perform extensive numerical investigation evaluate uoi algorithm uoi_ lasso synthetic real data demonstrate extraction interpretable functional networks human electrophysiology recordings well accurate prediction phenotypes genotype phenotype data reduced features also show uoi_ l1logistic uoi_ cur variants basic framework improved prediction parsimony classification matrix factorization several benchmark biomedical data sets results suggest methods based uoi framework could improve interpretation prediction data driven discovery across scientific fields
deep learning topological signatures inferring topological geometrical information data offer alternative perspective machine learning problems methods topological data analysis persistent homology enable obtain information typically form summary representations topological features however topological signatures often come unusual structure multisets intervals highly impractical machine learning techniques many strategies proposed map topological signatures machine learning compatible representations suffer agnostic target learning task contrast propose technique enables input topological signatures deep neural networks learn task optimal representation training approach realized novel input layer favorable theoretical properties classification experiments object shapes social network graphs demonstrate versatility approach case latter even outperform state art large margin
practical hash functions similarity estimation dimensionality reduction hashing basic tool dimensionality reduction employed several aspects machine learning however perfomance analysis often carried abstract assumption truly random unit cost hash functions used without concern concrete hash function employed concrete hash functions work fine sufficiently random input question trusted real world faced structured input paper focus prominent applications hashing namely similarity estimation permutation hashing oph scheme nips feature hashing weinberger icml found numerous applications approximate near neighbour search lsh classification svm consider recent mixed tabulation hash function dahlgaard focs proved theoretically perform like truly random hash function many applications including oph first show improved concentration bounds truly random hashing argue mixed tabulation performs similar input vectors dense main contribution however experimental comparison different hashing schemes inside applications find mixed tabulation hashing almost fast classic multiply mod prime scheme mod guaranteed work well sufficiently random data demonstrate applications lead bias poor concentration real world synthetic data also compare popular murmurhash3 proven guarantees mixed tabulation murmurhash3 perform similar truly random hashing experiments however mixed tabulation faster murmurhash3 proven guarantee good performance possible input making reliable
maxing ranking assumptions pac maximum maximum selection maxing ranking elements via random pairwise comparisons diverse applications studied many models assumptions simple natural assumption strong stochastic transitivity show maxing performed linearly many comparisons yet ranking requires quadratically many comparisons assumptions show borda score metric maximum selection performed linearly many comparisons ranking performed log comparisons
kernel functions based triplet comparisons propose ways defining kernel function data set available information data set consists similarity triplets form object similar object object machine learning problems based restricted information become popular recent years previous approaches construct low dimensional euclidean embedding data set reflects given similarity triplets aim defining kernel functions correspond high dimensional embeddings kernel functions subsequently used apply kernel method data set
learning structured optimal bipartite graph clustering clustering methods widely applied document clustering gene expression analysis methods make use duality features samples occurring structure sample feature clusters extracted graph based clustering methods bipartite graph constructed depict relation features samples existing clustering methods conduct clustering graph achieved original data matrix explicit cluster structure thus require post processing step obtain clustering results paper propose novel clustering method learn bipartite graph exactly connected components number clusters new bipartite graph learned model approximates original graph maintains explicit cluster structure immediately get clustering results without post processing extensive empirical results presented verify effectiveness robustness model
multi way interacting regression via factorization machines propose bayesian regression method accounts multi way interactions arbitrary orders among predictor variables model makes use factorization mechanism representing regression coefficients interactions among predictors interaction selection guided prior distribution random hypergraphs construction generalizes finite feature model present posterior inference algorithm based gibbs sampling establish posterior consistency regression model method evaluated extensive experiments simulated data demonstrated able identify meaningful interactions several applications genetics retail demand forecasting
maximum margin interval trees learning regression function using censored interval valued output data important problem fields genomics medicine goal learn real valued prediction function training output labels indicate interval possible values whereas existing algorithms task linear models paper investigate learning nonlinear tree models propose learn tree minimizing margin based discriminative objective function provide dynamic programming algorithm computing optimal solution log linear time show empirically algorithm achieves state art speed prediction accuracy benchmark several data sets
kernel feature selection via conditional covariance minimization propose framework feature selection employs kernel based measures independence find subset covariates maximally predictive response building past work kernel dimension reduction formulate approach constrained optimization problem involving trace conditional covariance operator
improved graph laplacian via geometric self consistency address problem setting kernel bandwidth epps used manifold learning algorithms construct graph laplacian exploiting connection manifold geometry represented riemannian metric laplace beltrami operator set epps optimizing laplacian ability preserve geometry data experiments show principled approach effective robust
mixture rank matrix approximation collaborative filtering low rank matrix approximation lrma methods achieved excellent accuracy among today collaborative filtering methods existing lrma methods rank user item feature matrices typically fixed rank adopted describe users items however studies show submatrices different ranks could coexist user item rating matrix approximations fixed ranks cannot perfectly describe internal structures rating matrix therefore leading inferior recommendation accuracy paper mixture rank matrix approximation mrma method proposed user item ratings characterized mixture lrma models different ranks meanwhile learning algorithm capitalizing iterated condition modes proposed tackle non convex optimization problem pertaining mrma experimental studies movielens netflix datasets demonstrate mrma outperform state art lrma based methods terms recommendation accuracy
predictive state recurrent neural networks present new model called predictive state recurrent neural networks psrnns filtering prediction dynamical systems psrnns draw insights recurrent neural networks rnns predictive state representations psrs inherit advantages types models like many successful rnn architectures psrnns use potentially deeply composed bilinear transfer functions combine information multiple sources source act gate another bilinear functions arise naturally connection state updates bayes filters like psrs observations viewed gating belief states show psrnns learned effectively combining backpropogation time bptt initialization based statistically consistent learning algorithm psrs called stage regression 2sr also show psrnns factorized using tensor decomposition reducing model size suggesting interesting theoretical connections existing multiplicative architectures lstms applied psrnns datasets showed outperform several popular alternative approaches modeling dynamical systems cases
hierarchical methods moments spectral methods moments provide powerful tool learning parameters latent variable models despite theoretical appeal applicability methods real data still limited due lack robustness model misspecification paper present hierarchical approach methods moments circumvent limitations method based replacing tensor decomposition step used previous algorithms approximate joint diagonalization experiments topic modeling show method outperforms previous tensor decomposition methods terms speed model quality
multitask spectral learning weighted automata consider problem estimating multiple related functions computed weighted automata wfa first present natural notion relatedness wfas considering extent several wfas share common underlying representation introduce model vector valued wfa conveniently helps formalize notion relatedness finally propose spectral learning algorithm vector valued wfas tackle multitask learning problem jointly learning multiple tasks form vector valued wfa algorithm enforces discovery representation space shared tasks benefits proposed multitask approach theoretically motivated showcased experiments synthetic real world datasets
generative local metric learning kernel regression paper shows metric learning used nadaraya watson kernel regression compared standard approaches bandwidth selection show metric learning significantly reduce mean square error mse kernel regression particularly high dimensional data propose method efficiently learning good metric function based upon analyzing performance estimator gaussian distributed data key feature approach estimator learned metric uses information global local structure training data theoretical empirical results confirm learned metric considerably reduce bias mse kernel regression
principles riemannian geometry neural networks study deals neural networks sense differential transformations systems differential equations forms part attempt construct formalized general theory neural networks branch riemannian geometry perspective following theoretical results developed proven feedforward networks limit number network layers goes infinity first shown residual neural networks dynamical systems first order differential equations opposed ordinary networks static implying network learning systems differential equations organize data second shown limit metric tensor residual networks converges smooth thus defines riemannian manifold third shown limit backpropagation graphs converges differentiable tensor fields results suggest analogy einstein general relativity particle trajectories geodesics curved space time manifolds neural networks learning curved space layer manifolds determine trajectory data moves network
subset selection sequential data subset selection task finding small subset informative items large ground set finds numerous applications different areas sequential data including time series ordered data contain important structural relationships among items imposed underlying dynamic models data play vital role selection representatives however nearly existing subset selection techniques ignore underlying dynamics data treat items independently leading incompatible set representatives paper develop new framework sequential subset selection takes advantage underlying dynamic models data promoting select set representatives high quality diversity also compatible according underlying dynamic models equip items transition dynamic models pose problem integer binary optimization assignments sequential items representatives leads high encoding diversity transition potentials proposed formulation non convex derive max sum message passing algorithm solve problem efficiently experiments synthetic real data including instructional video summarization motion capture segmentation show sequential subset selection framework achieves better encoding diversity state art also successfully incorporates dynamic data leading compatible representatives
quadratic convergence proximal newton algorithm nonconvex sparse learning propose proximal newton algorithm solving nonconvex regularized sparse learning problems high dimensions proposed algorithm integrates proximal newton algorithm multi stage convex relaxation based difference convex programming enjoys strong computational statistical guarantees specifically leveraging sophisticated characterization sparse modeling structures assumptions local restricted strong convexity hessian smoothness prove within stage convex relaxation proposed algorithm achieves local quadratic convergence eventually obtains sparse approximate local optimum optimal statistical properties convex relaxations numerical experiments provided support theory
support ordered weighted sparsity overlapping groups hardness algorithms support owl norms generalize norm providing better prediction accuracy better handling correlated variables study norms obtained extending support norm owl norms setting overlapping groups resulting norms general hard compute tractable certain collections groups demonstrate fact develop dynamic program problem projecting onto set vectors supported fixed number groups dynamic program utilizes tree decompositions complexity scales treewidth program converted extended formulation associated group structure models group support norms overlapping group variant ordered weighted norm numerical results demonstrate efficacy new penalties
parametric simplex method sparse learning high dimensional sparse learning imposed great computational challenge large scale data analysis paper investiage broad class sparse learning approaches formulated linear programs parametrized regularization factor solve parametric simplex method psm psm offers significant advantages competing methods psm naturally obtains complete solution path values regularization parameter psm provides high precision dual certificate stopping criterion psm yields sparse solutions iterations solution sparsity significantly reduces computational cost per iteration particularly demonstrate superiority psm various sparse learning approaches including dantzig selector sparse linear regression sparse support vector machine sparse linear classification sparse differential network estimation provide sufficient conditions psm always outputs sparse solutions computational performance significantly boosted thorough numerical experiments provided demonstrate outstanding performance psm method
learned amp principled neural network based compressive image recovery compressive image recovery challenging problem requires fast accurate algorithms recently neural networks applied problem promising results exploiting massively parallel gpu processing architectures oodles training data able run orders magnitude faster existing techniques unfortunately methods difficult train often times specific single measurement matrix largely unprincipled blackboxes recently demonstrated iterative sparse signal recovery algorithms unrolled form interpretable deep neural networks taking inspiration work develop novel neural network architecture mimics behavior denoising based approximate message passing amp algorithm call new network learned amp ldamp ldamp network easy train applied variety different measurement matrices comes state evolution heuristic accurately predicts performance importantly network outperforms state art bm3d amp nlr algorithms terms accuracy runtime high resolutions used matrices fast matrix multiply implementations ldamp runs times faster bm3d amp hundreds times faster nlr
falkon optimal large scale kernel method kernel methods provide principled way perform non linear nonparametric learning rely solid functional analytic foundations enjoy optimal statistical properties however least basic form limited applicability large scale scenarios stringent computational requirements terms time especially memory paper take substantial step scaling kernel methods proposing falkon novel algorithm allows efficiently process millions points falkon derived combining several algorithmic principles namely stochastic subsampling iterative solvers preconditioning theoretical analysis shows optimal statistical accuracy achieved requiring essentially memory sqrt time extensive experiments show state art results available large scale datasets achieved even single machine
efficient approximation algorithms strings kernel based sequence classification sequence classification algorithms svm require definition distance similarity measure sequences commonly used notion similarity number matches mers length subsequences sequences extending definition considering mers match distance yields better classification performance however makes problem computationally much complex known algorithms compute similarity computational complexity render applicable small values work develop novel techniques efficiently accurately estimate pairwise similarity score enables use much larger values get higher predictive accuracy opens broad avenue applying classification approach audio images text sequences algorithm achieves excellent approximation performance theoretical guarantees process solve open combinatorial problem posed major hindrance scalability existing solutions give analytical bounds quality runtime algorithm report empirical performance real world biological music sequences datasets
robust hypothesis test functional effect gaussian processes work constructs hypothesis test detecting whether data generating function real rightarrow real belongs specific reproducing kernel hilbert space mathcal structure mathcal partially known utilizing theory reproducing kernels reduce hypothesis simple sided score test scalar parameter develop testing procedure robust mis specification kernel functions also propose ensemble based estimator null model guarantee test performance small samples demonstrate utility proposed method apply test problem detecting nonlinear interaction groups continuous features evaluate finite sample performance test different data generating functions estimation strategies null model results revealed interesting connection notions machine learning model underfit overfit statistical inference type error power hypothesis test also highlighted unexpected consequences common model estimating strategies estimating kernel hyperparameters using maximum likelihood estimation model inference
invariance stability deep convolutional representations paper study deep signal representations near invariant groups transformations stable action diffeomorphisms without losing signal information achieved generalizing multilayer kernel introduced context convolutional kernel networks studying geometry corresponding reproducing kernel hilbert space show signal representation stable models functional space large class convolutional neural networks enjoy stability
testing learning distributions symmetric noise invariance kernel embeddings distributions maximum mean discrepancy mmd resulting distance distributions useful tools fully nonparametric sample testing learning distributions however rarely possible differences samples interest discovered differences due different types measurement noise data collection artefacts irrelevant sources variability propose distances distributions encode invariance additive symmetric noise aimed testing whether assumed true underlying processes differ moreover construct invariant features distributions leading learning algorithms robust impairment input distributions symmetric additive noise
empirical study properties random bases kernel methods kernel machines neural networks possess universal function approximation properties nevertheless practice way choosing appropriate function class differ thus limits usage emerge specifically neural networks learn representation adapting basis functions data task kernel methods typically use kernels adapted width rbf kernel change anymore contribute work contrasting neural network kernel methods empirical study analysis reveals random adaptive bases affect quality learning furthermore present kernel basis adaptation schemes make efficient usage features retaining universality properties
max margin invariant features transformed unlabelled data study representations invariant common transformations data important learning techniques focused local approximate invariance implemented within expensive optimization frameworks lacking explicit theoretical guarantees paper study kernels invariant unitary group theoretical guarantees addressing important practical issue unavailability transformed versions labelled data problem call unlabeled transformation problem special form semi supervised learning shot learning present theoretically motivated alternate approach invariant kernel svm based propose max margin invariant features mmif solve problem illustration design framework face recognition demonstrate efficacy approach large scale semi synthetic dataset 153 000 images new challenging protocol labelled faces wild lfw performing strong baselines
safetynets verifiable execution deep neural networks untrusted cloud inference using deep neural networks often outsourced cloud since computationally demanding task however raises fundamental issue trust client sure cloud performed inference correctly lazy cloud provider might use simpler less accurate model reduce computational load worse maliciously modify inference results sent client propose safetynets framework enables untrusted server cloud provide client short mathematical proof correctness inference tasks perform behalf client specifically safetynets develops implements specialized interactive proof protocol verifiable execution class deep neural networks represented arithmetic circuits empirical results layer deep neural networks demonstrate run time costs safetynets client server low safetynets detects incorrect computations neural network untrusted server high probability achieving state art accuracy mnist digit recognition timit speech recognition tasks
multi output polynomial networks factorization machines factorization machines polynomial networks supervised polynomial models based efficient low rank decomposition extend models multi output setting learning vector valued functions application multi class multi task problems cast problem learning way tensor whose slices share common decomposition propose convex formulation problem develop efficient conditional gradient algorithm prove global convergence despite fact involves non convex hidden unit selection step classification tasks show algorithm achieves excellent accuracy much sparser models existing methods recommendation system tasks show combine algorithm reduction ordinal regression multi output classification show resulting algorithm outperforms existing baselines terms ranking accuracy
neural hawkes process neurally self modulating multivariate point process many events occur world event types stochastically excited inhibited sense probabilities elevated decreased patterns sequence previous events discovering patterns help predict type event happen next propose model streams discrete events continuous time constructing neurally self modulating multivariate point process intensities multiple event types evolve according novel continuous time lstm generative model allows past events influence future complex realistic ways conditioning future event intensities hidden state recurrent neural network consumed stream past events model desirable qualitative properties achieves competitive likelihood predictive accuracy real synthetic datasets including missing data conditions
maximizing spread influence training data consider canonical problem influence maximization social networks since seminal work kempte kleinberg tardos largely disjoint efforts problem first studies problem associated learning generative model produces cascades second focuses algorithmic challenge identifying set influencers assuming generative model known recent results learning optimization imply general generative model known rather learned training data algorithm influence maximization yield constant factor approximation guarantee using polynomially many samples drawn distribution paper describe simple algorithm maximizing influence training data main idea behind algorithm leverage strong community structure social networks identify set individuals influentials whose communities little overlap although general approximation guarantee algorithm unbounded show algorithm performs well experimentally analyze performance prove algorithm obtains constant factor approximation guarantee graphs generated stochastic block model traditionally used model networks community structure
inductive representation learning large graphs low dimensional embeddings nodes large graphs proved extremely useful variety prediction tasks content recommendation identifying protein functions however existing approaches require nodes graph present training embeddings previous approaches inherently transductive naturally generalize unseen nodes present graphsage general inductive framework leverages node feature information text attributes efficiently generate node embeddings instead training individual embeddings node learn function generates embeddings sampling aggregating features node local neighborhood algorithm outperforms strong baselines inductive node classification benchmarks classify category unseen nodes evolving information graphs based citation reddit post data show algorithm generalizes completely unseen graphs using multi graph dataset protein protein interactions
meta learning perspective cold start recommendations items matrix factorization popular techniques product recommendation known suffer serious cold start problems item cold start problems particularly acute settings tweet recommendation new items arrive continuously paper present meta learning strategy address item cold start new items arrive continuously propose deep neural network architectures implement meta learning strategy first architecture learns linear classifier whose weights determined item history second architecture learns neural network whose biases instead adapted based item history evaluate techniques real world problem tweet recommendation production data twitter demonstrate proposed techniques significantly beat baseline lookup table based user embeddings also outperform state art production model tweet recommendation
dropoutnet addressing cold start recommender systems latent models become default choice recommender systems due performance scalability however research area primarily focused modeling user item interactions latent models developed cold start deep learning recently achieved remarkable success showing excellent results diverse input types inspired results propose neural network based latent model handle cold start recommender systems unlike existing approaches incorporate additional content based objective terms instead focus learning show neural network models explicitly trained handle cold start dropout model trained top existing latent model effectively providing cold start capabilities full power deep architectures empirically demonstrate state art accuracy publicly available benchmarks
federated multi task learning federated learning poses new statistical systems challenges training machine learning models distributed networks devices work show multi task learning naturally suited handle statistical challenges setting propose novel systems aware optimization method mocha robust practical systems issues method theory first time consider issues high communication cost stragglers fault tolerance distributed multi task learning resulting method achieves significant speedups compared alternatives federated setting demonstrate extensive simulations real world federated datasets
flexpoint adaptive numerical format efficient training deep neural networks deep neural networks commonly developed trained bit floating point format significant gains performance energy efficiency could realized training inference numerical formats optimized deep learning despite substantial advances limited precision inference recent years training neural networks low bit width remains challenging problem present flexpoint data format aiming complete replacement bit floating point format training inference designed support deep network topologies without modifications flexpoint tensors shared exponent dynamically adjusted minimize overflows maximizing available dynamic range validate flexpoint training alexnet deep residual network generative adversarial network using simulator implemented emph neon deep learning framework demonstrate bit flexpoint closely matches bit floating point training models without need tuning model hyper parameters results suggest flexpoint promising numerical format future hardware training inference
bayesian inference individualized treatment effects using multi task gaussian processes predicated increasing abundance electronic health records investigate problem inferring individualized treatment effects using observational data stemming potential outcomes model propose novel multi task learning framework factual counterfactual outcomes modeled outputs function vector valued reproducing kernel hilbert space vvrkhs develop nonparametric bayesian method learning treatment effects using multi task gaussian process linear coregionalization kernel prior vvrkhs bayesian approach allows compute individualized measures confidence estimates via pointwise credible intervals crucial realizing full potential precision medicine impact selection bias alleviated via risk based empirical bayes method adapting multi task prior jointly minimizes empirical error factual outcomes uncertainty unobserved counterfactual outcomes conduct experiments observational datasets interventional social program applied premature infants left ventricular assist device applied cardiac patients wait listed heart transplant experiments show method significantly outperforms state art
tomography london underground scalable model origin destination data paper addresses classical network tomography problem inferring local traffic given origin destination observations focussing large complex public transportation systems build scalable model exploits input output information estimate unobserved link station loads users path preferences based reconstruction users travel time distribution model flexible enough capture possible different path choice strategies correlations users travelling similar paths similar times corresponding likelihood function intractable medium large scale networks propose distinct strategies namely exact maximum likelihood inference approximate tractable model variational inference original intractable model application approach consider emblematic case london underground network tap tap system tracks start exit time location journeys day set synthetic simulations real data provided transport london used validate test model predictions observable unobservable quantities
matching balanced nonlinear representations treatment effects estimation estimating treatment effects observational data challenging problem due missing counterfactuals matching effective strategy tackle problem widely used matching estimators nearest neighbor matching nnm pair treated units similar control units terms covariates estimate treatment effects accordingly however existing matching estimators poor performance distributions control treatment groups unbalanced moreover theoretical analysis suggests bias causal effect estimation would increase dimension covariates paper aim address problems learning low dimensional balanced nonlinear representations bnr observational data particular convert counterfactual prediction classification problem develop kernel learning model domain adaptation constraint design novel matching estimator dimension covariates significantly reduced projecting data low dimensional subspace experiments several synthetic real world datasets demonstrate effectiveness approach
moleculenet continuous filter convolutional neural network modeling quantum interactions deep learning potential revolutionize quantum chemistry ideally suited learn representations structured data speed exploration chemical space convolutional neural networks proven first choice images audio video data atoms molecules restricted grid instead precise locations contain essential physical information would get lost discretized thus propose use textit continuous filter convolutional layers able model local correlations without requiring data lie grid apply layers moleculenet novel deep learning architecture modeling quantum interactions molecules obtain joint model total energy interatomic forces follows fundamental quantum chemical principles includes rotationally invariant energy predictions smooth differentiable potential energy surface architecture achieves state art performance benchmarks equilibrium molecules molecular dynamics trajectories finally introduce challenging benchmark chemical structural variations suggests path work
hiding images plain sight deep steganography steganography practice concealing secret message within another ordinary message commonly steganography used unobtrusively hide small message within noisy regions larger image study attempt place full size color image within another image size deep neural networks simultaneously trained create hiding revealing processes designed specifically work pair system trained images drawn randomly imagenet database works well natural images wide variety sources beyond demonstrating successful application deep learning hiding images carefully examine result achieved explore extensions unlike many popular steganographic methods encode secret message within least significant bits carrier image approach compresses distributes secret image representation across available bits
universal style transfer via feature transforms universal style transfer aims transfer arbitrary visual styles content images existing feed forward based methods enjoying inference efficiency mainly limited inability generalizing unseen styles compromised visual quality paper present simple yet effective method tackles limitations without training pre defined styles key ingredient method pair feature transforms whitening coloring embedded image reconstruction network whitening coloring transforms reflect direct matching feature covariance content image given style image shares similar spirits optimization gram matrix based cost neural style transfer demonstrate effectiveness algorithm generating high quality stylized images comparisons number recent methods also analyze method visualizing whitened features synthesizing textures simple feature coloring
attend predict understanding gene regulation selective attention chromatin past decade seen revolution genomic technologies enable flood genome wide profiling chromatin marks recent literature tried understand gene regulation predicting gene expression large scale chromatin measurements fundamental challenges exist learning tasks genome wide chromatin signals spatially structured high dimensional highly modular core aim understand relevant factors work together previous studies either failed model complex dependencies among input signals relied separate feature analysis explain decisions paper presents attention based deep learning approach call chromattention uses unified architecture model interpret dependencies among chromatin factors controlling gene regulation chromattention uses hierarchy multiple long short term memory lstm modules encode input signals model various chromatin marks cooperate automatically chromattention trains levels attention jointly target prediction enabling attend differentially relevant marks locate important positions per mark evaluate model across different cell types tasks human proposed architecture accurate attention scores also provide better interpretation state art feature visualization methods saliency map
unbounded cache model online language modeling open vocabulary propose extension recurrent networks language modeling adapt prediction changes data distribution associate non parametric large scale memory component stores hidden activations seen past approach seen unbounded continuous cache make use modern approximate search quantization algorithms stores millions representations searching efficiently show approach helps adapting pretrained neural networks novel data distribution tackle called rare word problem
deconvolutional paragraph representation learning learning latent representations long text sequences important first step many natural language processing applications recurrent neural networks rnns become cornerstone challenging task however quality sentences rnn based decoding reconstruction decreases length text propose sequence sequence purely convolutional deconvolutional autoencoding framework free issue also computationally efficient proposed method simple easy implement leveraged building block many applications show empirically compared rnns framework better reconstructing correcting long paragraphs quantitative evaluation semi supervised text classification summarization tasks demonstrate potential better utilization long unlabeled text data
analyzing hidden representations end end automatic speech recognition systems neural models become ubiquitous automatic speech recognition systems neural networks typically used acoustic models complex systems recent studies explored end end speech recognition systems based neural networks trained directly predict text input acoustic features although systems conceptually elegant simpler traditional systems less obvious interpret trained models work analyze speech representations learned deep end end model based convolutional recurrent layers trained connectionist temporal classification ctc loss use pre trained model generate frame level features given classifier trained frame classification phones evaluate representations different layers deep model compare quality predicting phone labels experiments shed light important aspects end end model layer depth model complexity design choices
best worlds transferring knowledge discriminative learning generative visual dialog model present novel training framework neural sequence models particularly grounded dialog generation standard training paradigm models maximum likelihood estimation mle minimizing cross entropy human responses across variety domains recurring problem mle trained generative neural dialog models tend produce safe generic responses like know tell contrast discriminative dialog models trained rank list candidate human responses outperform generative counterparts terms automatic metrics diversity informativeness responses however useful practice since deployed real conversations users work aims achieve best worlds practical usefulness strong performance via knowledge transfer primary contribution end end trainable generative visual dialog model receives gradients perceptual adversarial loss sequence sampled leverage recently proposed gumbel softmax approximation discrete distribution specifically rnn augmented sequence samplers coupled straight gradient estimator enables end end differentiability also introduce stronger encoder visual dialog employ self attention mechanism answer encoding along metric learning loss aid better capturing semantic similarities answer responses overall proposed model outperforms state art visdial dataset significant margin recall
teaching machines describe images natural language feedback robots eventually part every household thus critical enable algorithms learn guided non expert users paper bring human loop enable human teacher give feedback learning agent form natural language descriptive sentence provide stronger learning signal numeric reward easily point mistakes correct focus problem image captioning quality output easily judged non experts propose phrase based captioning model trained policy gradients design critic provides reward learner conditioning human provided feedback show exploiting descriptive feedback model learns perform better given independently written human captions
high order attention models visual question answering quest algorithms enable cognitive abilities important part machine learning common trait recent cognitive like tasks take account different data modalities visual lingual paper propose novel generally applicable form attention mechanism learns high order correlations various data modalities show high order correlations effectively direct appropriate attention relevant elements different data modalities required solve joint task demonstrate effectiveness high order attention mechanism task visual question answering vqa achieve state art performance standard vqa dataset
visual reference resolution using attention memory visual dialog visual dialog task answering series inter dependent questions given input image often requires resolve visual references among questions problem different visual question answering vqa relies spatial attention visual grounding estimated image question pair propose novel attention mechanism exploits visual attentions past resolve current reference visual dialog scenario proposed model equipped associative attention memory storing sequence previous attention key pairs memory model retrieves previous attention taking account recency relevant current question order resolve potentially ambiguous reference model merges retrieved attention tentative obtain final attention current question specifically use dynamic parameter prediction combine attentions conditioned question extensive experiments new synthetic visual dialog dataset show model significantly outperforms state art points situation visual reference resolution plays important role moreover proposed model presents superior performance points improvement visual dialog dataset despite significantly fewer parameters baselines
semi supervised learning optical flow generative adversarial networks convolutional neural networks cnns recently applied optical flow estimation problem training cnns requires sufficiently large ground truth training data existing approaches resort synthetic unrealistic datasets hand unsupervised methods capable leveraging real world videos training ground truth flow fields available methods however rely fundamental assumptions brightness constancy spatial smoothness priors hold near motion boundaries paper propose exploit unlabeled videos semi supervised learning optical flow generative adversarial network key insight adversarial loss capture structural patterns flow warp errors without making explicit assumptions extensive experiments benchmark datasets demonstrate proposed semi supervised algorithm performs favorably purely supervised semi supervised learning schemes
associative embedding end end learning joint detection grouping introduce associative embedding novel method supervising convolutional neural networks task detection grouping number computer vision problems framed manner including multi person pose estimation instance segmentation multi object tracking usually grouping detections achieved multi stage pipelines instead propose approach teaches network simultaneously output detections group assignments technique easily integrated state art network architecture produces pixel wise predictions show apply method multi person pose estimation report state art performance multi person pose mpii dataset coco dataset
learning deep structured multi scale features using attention gated crfs contour prediction recent works shown exploiting multi scale representations deeply learned via convolutional neural networks cnn tremendous importance accurate contour detection paper presents novel approach predicting contours advances state art fundamental aspects multi scale feature generation fusion different previous works directly considering multi scale feature maps obtained inner layers primary cnn architecture introduce hierarchical deep model produces rich complementary representations furthermore refine robustly fuse representations learned different scales novel attention gated conditional random fields crfs proposed experiments ran publicly available datasets bsds500 nyudv2 demonstrate effectiveness latent crf model overall hierarchical framework
incorporating side information adaptive convolution computer vision tasks often side information available helpful solve task example crowd counting camera perspective camera angle height gives clue appearance scale people scene side information shown useful counting systems using traditional hand crafted features fully utilized counting systems based deep learning order incorporate available side information propose adaptive convolutional neural network acnn convolution filter weights adapt current scene context via side information particular model filter weights low dimensional manifold within high dimensional space filter weights filter weights generated using learned filter manifold sub network whose input side information help side information adaptive weights acnn disentangle variations related side information extract discriminative features related current context camera perspective noise level blur kernel parameters demonstrate effectiveness acnn incorporating side information tasks crowd counting corrupted digit recognition image deblurring experiments show acnn improves performance compared plain cnn similar number parameters since existing crowd counting datasets contain ground truth side information collect new dataset ground truth camera angle height side information
learning multi view stereo machine show learn multi view stereopsis system contrast recent learning based methods reconstruction leverage underlying geometry problem feature projection unprojection along viewing rays formulating operations differentiable manner able learn system end end task metric reconstructions end end learning allows utilize priors object shapes enabling reconstruction objects much fewer images even single image required classical approaches well completion unseen surfaces thoroughly evaluate approach shapenet dataset demonstrate benefits classical approaches recent learning based methods
working hard know neighbor margins local descriptor learning loss introduce novel loss learning local feature descriptors inspired sift matching scheme show proposed loss relies maximization distance closest positive closest negative patches could replace complex regularization methods used local descriptor learning works well shallow deep convolution network architectures resulting descriptor compact dimensionality sift 128 shows state art performance matching patch verification retrieval benchmarks fast compute gpu
multimodal image image translation enforcing cycle consistency many image image translation problems ambiguous single input image corresponding multiple possible outputs work aim model distribution possible outputs conditional generative modeling setting ambiguity mapping encoded low dimensional latent vector randomly sampled test time generator learns map input along latent code output explicitly enforce cycle consistency latent code output encouraging invertibility helps prevent many mapping latent code output training also known problem mode collapse helps produce diverse results evaluate relationship perceptual realism diversity images generated method test variety domains
deep supervised discrete hashing rapid growth image video data web hashing extensively studied image video search recent years benefit recent advances deep learning deep hashing methods achieved promising results image retrieval however limitations previous deep hashing methods semantic information fully exploited paper develop deep supervised discrete hashing algorithm based assumption learned binary codes ideal classification pairwise label information classification information used learn hash codes within stream framework constrain outputs last layer binary codes directly rarely investigated deep hashing algorithm discrete nature hash codes alternating minimization method used optimize objective function experimental results shown method outperforms current state art methods benchmark datasets
svd softmax fast softmax approximation large vocabulary neural networks propose fast approximation method softmax function large vocabulary using singular value decomposition svd svd softmax targets fast accurate probability estimation topmost probable words inference recurrent neural network language models proposed method transforms weight matrix used calculation logits using svd approximate probability word estimated fraction svd transformed matrix apply technique language modeling neural machine translation present guideline good approximation algorithm requires arithmetic operations 800k vocabulary case shows speedup gpu
hash embeddings efficient word representations present hash embeddings efficient method representing words continuous vector form hash embedding seen interpolation standard word embedding word embedding created using random hash function hashing trick hash embeddings token represented dimensional embeddings vectors dimensional weight vector final dimensional representation token product rather fitting embedding vectors token selected hashing trick shared pool embedding vectors experiments show hash embeddings easily deal huge vocabularies consisting millions tokens using hash embedding need create dictionary training perform kind vocabulary pruning training show models trained using hash embeddings exhibit least level performance models trained using regular embeddings across wide range tasks furthermore number parameters needed embedding fraction required regular embedding since standard embeddings embeddings constructed using hashing trick actually special cases hash embedding hash embeddings considered extension improvement existing regular embedding types
regularized framework sparse structured neural attention modern neural networks often augmented attention mechanism tells network focus within input propose paper new framework sparse structured attention building upon max operator regularized strongly convex function show operator differentiable gradient defines mapping real values probabilities suitable attention mechanism framework includes softmax slight generalization recently proposed sparsemax special cases however also show framework incorporate modern structured penalties resulting new attention mechanisms focus entire segments groups input encouraging parsimony interpretability derive efficient algorithms compute forward backward passes attention mechanisms enabling use neural network trained backpropagation showcase potential drop replacement existing attention mechanisms evaluate large scale tasks textual entailment machine translation sentence summarization attention mechanisms improve interpretability without sacrificing performance notably textual entailment summarization outperform existing attention mechanisms based softmax sparsemax
attentional pooling action recognition introduce simple yet surprisingly powerful model incorporate attention action recognition human object interaction tasks proposed attention module trained without extra supervision gives sizable boost accuracy keeping network size computational cost nearly leads significant improvements state art base architecture standard action recognition benchmarks across still images videos establishes new state art mpii relative improvement hmdb rgb datasets also perform extensive analysis attention module empirically analytically terms latter introduce novel derivation bottom top attention low rank approximations bilinear pooling methods typically used fine grained classification perspective attention formulation suggests novel characterization action recognition fine grained recognition problem
plan attend generate planning sequence sequence models investigate integration planning mechanism encoder decoder architectures attention present model plans ahead computes alignments input output sequences constructing matrix proposed future alignments commitment vector governs whether follow recompute plan mechanism inspired strategic attentive reader writer straw model proposed model end end trainable fully differentiable operations show outperforms strong baseline character level translation tasks wmt algorithmic task finding eulerian circuits graphs among others analysis demonstrates model computes qualitatively intuitive alignments converges faster baselines achieves superior performance fewer parameters
dilated recurrent neural networks notoriously learning recurrent neural networks rnns long sequences difficult task major challenges extracting complex dependencies vanishing exploding gradients efficient parallelization paper introduce simple yet effective rnn connection structure dilatedrnn simultaneously tackles challenges proposed architecture characterized multi resolution dilated recurrent skip connections combined flexibly different rnn cells moreover dilatedrnn reduces number parameters enhances training efficiency significantly matching state art performance even vanilla rnn cells tasks involving long term dependencies provide theory based quantification architecture advantages introduce memory capacity measure mean recurrent length suitable rnns long skip connections existing measures rigorously prove advantages dilatedrnn recurrent neural architectures
thalamus gated recurrent modules propose deep learning model inspired neuroscience theories communication within neocortex model consists recurrent modules send features via routing center endowing neural modules flexibility share features multiple time steps show model learns route information hierarchically processing input data chain modules observe common architectures feed forward neural networks skip connections emerging special cases architecture novel connectivity patterns learned text8 compression task model outperforms multi layer recurrent networks sequential tasks
wasserstein learning deep generative point process models point processes becoming popular modeling asynchronous sequential data due sound mathematical foundation strength modeling variety real world phenomena currently often characterized via intensity function limits model expressiveness due unrealistic assumptions parametric form used practice furthermore learned via maximum likelihood approach prone failure multi modal distributions sequences paper propose intensity free approach point processes modeling transforms nuisance processes target furthermore train model using likelihood free leveraging wasserstein distance point processes experiments various synthetic real world data substantiate superiority proposed point process model conventional ones
stabilizing training generative adversarial networks regularization deep generative models based generative adversarial networks gans demonstrated impressive sample quality order work require careful choice architecture parameter initialization selection hyper parameters fragility part due dimensional mismatch model distribution true distribution causing density ratio associated divergence undefined overcome fundamental limitation propose new regularization approach low computational cost yields stable gan training procedure demonstrate effectiveness approach several datasets including common benchmark image generation tasks approach turns gan models reliable building blocks deep learning
neural variational inference learning undirected graphical models many problems machine learning naturally expressed language undirected graphical models propose learning inference algorithms undirected models optimize variational approximation log likelihood model central approach upper bound log partition function parametrized function express flexible neural network bound enables accurately track partition function learning speed sampling train broad class powerful hybrid directed undirected models via unified variational inference framework empirically demonstrate effectiveness method several popular generative modeling datasets
adversarial symmetric variational autoencoder new form variational autoencoder vae developed joint distribution data codes considered symmetric forms observed data fed encoder yield codes latent codes drawn simple prior propagated decoder manifest data lower bounds learned marginal log likelihood fits observed data latent codes learning variational bound seeks minimize symmetric kullback leibler divergence joint density functions simultaneously seeking maximize marginal log likelihoods facilitate learning new form adversarial training developed extensive set experiments performed demonstrate state art data reconstruction generation several image benchmarks datasets
diverse accurate image description using variational auto encoder additive gaussian encoding space paper proposes method generate image descriptions using conditional variational auto encoder cvae data dependent gaussian prior encoding space standard cvaes fixed gaussian prior easily collapse generate descriptions little variability approach addresses problem linearly combining multiple gaussian priors based semantic content image increasing flexibility representational power generative model evaluate additive gaussian cvae cvae approach mscoco dataset show produces captions diverse accurate strong lstm baseline cvae variants
forcing training stochastic recurrent networks many efforts devoted incorporate stochastic latent variables sequential neural models recurrent neural networks rnns rnns latent variables successful capturing variability observed natural structured data speech work propose novel recurrent latent variable model unifies successful ideas recently proposed architectures model step sequence associated latent variable used condition recurrent dynamics future steps model trained amortised variational inference inference network augmented rnn runs backward sequence addition next step prediction add auxiliary cost latent variables forces reconstruct state backward recurrent network provides latent variables task independent objective enhances performance overall model although conceptually simple model achieves state art results standard speech benchmarks timit blizzard finally apply model language modeling imdb dataset auxiliary cost crucial learning interpretable latent variables setting show regular evidence lower bound significantly underestimates log likelihood model thus encouraging future works compare likelihoods methods using tighter bounds
shot imitation learning imitation learning commonly applied solve different tasks isolation usually requires either careful feature engineering significant number samples far desire ideally robots able learn demonstrations given task instantly generalize new situations task without requiring task specific engineering paper propose meta learning framework achieving capability call shot imitation learning specifically consider setting large maybe infinite set tasks task many instantiations example task could stack blocks table single tower another task could place blocks table block towers etc case different instances task would consist different sets blocks different initial states training time algorithm presented pairs demonstrations subset tasks neural net trained takes input demonstration current state initially initial state demonstration pair outputs action goal resulting sequence states actions matches closely possible second demonstration test time demonstration single instance new task presented neural net expected perform well new instances new task experiments show use soft attention allows model generalize conditions tasks unseen training data anticipate training model much greater variety tasks settings obtain general system turn demonstrations robust policies accomplish overwhelming variety tasks
reconstruct crush network article introduces energy based model adversarial regarding data minimizes energy given data distribution positive samples maximizing energy another given data distribution negative unlabeled samples model especially instantiated autoencoders energy represented reconstruction error provides general distance measure unknown data resulting neural network thus learns reconstruct data first distribution crushing data second distribution solution handle different problems positive unlabeled learning covariate shift especially imbalanced data using autoencoders allows handling large variety data images text even dialogues experiments show flexibility proposed approach dealing different types data different settings images cifar cifar 100 training setting text amazon reviews learning dialogues facebook babi next response classification dialogue completion
fader networks generating image variations sliding attribute values paper introduces new encoder decoder architecture trained reconstruct images disentangling salient information image values attributes directly latent space result training model generate different realistic versions input image varying attribute values using continuous attribute values choose much specific attribute perceivable generated image property could allow applications users modify image using sliding knobs like faders mixing console change facial expression portrait update color objects compared state art mostly relies training adversarial networks pixel space altering attribute values train time approach results much simpler training schemes nicely scales multiple attributes present evidence model significantly change perceived value attributes preserving naturalness images
predrnn recurrent neural networks video prediction using spatiotemporal lstms predictive learning video sequences aims generate future images learning historical frames spatial appearance temporal variations crucial structures paper models structures presenting predictive recurrent neural network predrnn architecture enlightened idea video prediction system memorize spatial appearance temporal variations unified memory pool concretely memory states longer constrained inside lstm unit instead allowed zigzag directions across stacked rnn layers vertically time steps horizontally core network new spatiotemporal lstm lstm unit extracts memorizes spatial temporal video representations simultaneously predrnn achieves state art prediction performance standard video datasets believed general framework extended predictive learning tasks beyond video prediction
multi agent predictive modeling attentional commnets multi agent predictive modeling essential step understanding physical social team play systems recently interaction networks ins proposed task modeling multi agent physical systems ins scale number interactions system typically quadratic higher order number agents paper introduce vain attentional commnet multi agent predictive modeling scales linearly number agents show vain effective multi agent predictive modeling representation learned transferable learning new data poor tasks method evaluated tasks challenging multi agent prediction domains chess soccer outperforms competing multi agent approaches
real time image saliency black box classifiers work develop fast saliency detection method applied differentiable image classifier train masking model manipulate scores classifier masking salient parts input image model generalises well unseen images requires single forward pass perform saliency detection therefore suitable use real time systems test approach cifar imagenet datasets show produced saliency maps easily interpretable sharp free artifacts suggest new metric saliency test method imagenet object localisation task achieve results outperforming weakly supervised methods
prototypical networks shot learning propose prototypical networks problem shot classification classifier must generalize new classes seen training set given small number examples new class prototypical networks learn metric space classification performed computing distances prototype representations class compared recent approaches shot learning reflect simpler inductive bias beneficial limited data regime achieve excellent results provide analysis showing simple design decisions yield substantial improvements recent approaches involving complicated architectural choices meta learning extend prototypical networks shot learning achieve state art results birds dataset
shot learning information retrieval lens shot learning refers understanding new concepts examples propose information retrieval inspired approach problem motivated increased importance maximally leveraging available information low data regime define training objective aims extract much information possible training batch effectively optimizing relative orderings batch points simultaneously particular view batch point query ranks remaining ones based predicted relevance define model framework structured prediction optimize mean average precision rankings method produces state art results standard benchmarks shot learning
reversible residual network backpropagation without storing activations residual networks resnets demonstrated significant improvement traditional convolutional neural networks cnns image classification increasing performance networks grow deeper wider however memory consumption becomes bottleneck needs store intermediate activations calculating gradients using backpropagation work present reversible residual network revnet variant resnets layer activations reconstructed exactly next layer therefore activations layers need stored memory backprop demonstrate effectiveness revnets cifar imagenet establishing nearly identical performance equally sized resnets activation storage requirements independent depth
gated recurrent convolution neural network ocr optical character recognition ocr aims recognize text natural images widely researched computer vision community paper present new architecture named gated recurrent convolution layer grcl challenge grcl constructed adding gate recurrent convolution layer rcl find equipped gate control context modulation rcl balancing feed forward component well recurrent component addition build bidirectional long short term memory blstm sequence modelling test several variants blstm find suitable architecture ocr finally combine gated recurrent convolution neural network grcnn blstm recognize text natural image grcnn blstm trained end end outperforms benchmark datasets terms state art results including iiit street view text svt icdar
learning efficient object detection models knowledge distillation despite significant accuracy improvement convolutional neural networks cnn based object detectors often require prohibitive runtimes process image real time applications state art models often use deep networks large number floating point operations efforts model compression learn compact models fewer number parameters much reduced accuracy work propose new framework learn compact fast object detection networks improved accuracy using knowledge distillation cite hinton2015distilling hint learning cite romero2014fitnets although knowledge distillation demonstrated excellent improvements simpler classification setups complexity detection poses new challenges form regression region proposals less voluminous labels address several innovations weighted cross entropy loss address class imbalance teacher bounded loss handle regression component adaptation layers better learn intermediate teacher distributions conduct comprehensive empirical evaluation different distillation configurations multiple datasets including pascal kitti ilsvrc coco results show consistent improvement accuracy speed trade offs modern multi class detection models
active bias training accurate neural network emphasizing high variance samples self paced learning hard example mining weight training instances improve learning accuracy paper presents improved alternatives based lightweight estimates sample uncertainty stochastic gradient descent sgd variance predicted probability correct class across iterations mini batch sgd proximity correct class probability decision threshold extensive experimental results datasets show methods reliably improve accuracy various network architectures including additional gains top popular training techniques residual learning momentum adam batch normalization dropout distillation
langevin dynamics continuous tempering training deep neural networks minimizing non convex high dimensional objective functions challenging especially training modern deep neural networks paper novel approach proposed divides training process consecutive phases obtain better generalization performance bayesian sampling stochastic optimization first phase explore energy landscape capture fat modes second fine tune parameter learned first phase bayesian learning phase apply continuous tempering stochastic approximation langevin dynamics create efficient effective sampler temperature adjusted automatically according designed temperature dynamics strategies overcome challenge early trapping bad local minima achieved remarkable improvements various types neural networks shown theoretical analysis empirical experiments
differentiable learning logical rules knowledge base reasoning study problem learning probabilistic first order logical rules knowledge base reasoning learning problem difficult requires learning parameters continuous space well structure discrete space propose framework neural logic programming combines parameter structure learning first order logical rules end end differentiable model approach inspired recently developed differentiable logic called tensorlog inference tasks compiled sequences differentiable operations design neural controller system learns compose operations empirically method obtains state art results multiple knowledge base benchmark datasets including freebase wikimovies
deliberation networks sequence generation beyond pass decoding encoder decoder framework achieved promising progress many sequence generation tasks including machine translation text summarization dialog system image captioning etc framework adopts pass forward process decoding generating sequence lacks deliberation process generated sequence directly used final output without polishing however deliberation common behavior human daily life like reading news writing papers articles books work introduce deliberation process encoder decoder framework propose deliberation networks sequence generation deliberation network levels decoders first pass decoder generates raw sequence second pass decoder polishes refines raw sentence deliberation since second pass deliberation decoder overall picture sequence generated might potential generate better sequence looking future words raw sentence experiments neural machine translation text summarization demonstrate effectiveness proposed deliberation networks
neural program meta induction recently proposed methods neural program induction work assumption large set input output examples learning given input output mapping paper aims address problem data computation efficiency program induction leveraging information related tasks specifically propose novel approaches cross task knowledge transfer improve program induction limited data scenarios first proposal portfolio adaptation set induction models pretrained set related tasks best model adapted towards new task using transfer learning second approach meta program induction shot learning approach used make model generalize new tasks without additional training test efficacy methods constructed new benchmark programs written karel programming language using extensive experimental evaluation karel benchmark demonstrate proposals dramatically outperform baseline induction method use knowledge transfer also analyze relative performance approaches study conditions perform best particular meta induction outperforms existing approaches extreme data sparsity small number examples available fewer number available examples increase thousand portfolio adapted program induction becomes best approach intermediate data sizes demonstrate combined method adapted meta program induction strongest performance
saliency based sequential image attention multiset prediction humans process visual scenes selectively sequentially using attention central models human visual attention saliency map propose hierarchical visual architecture operates saliency map uses novel attention mechanism sequentially focus salient regions take additional glimpses within regions architecture motivated human visual attention used multi label image classification novel multiset task demonstrating achieves high precision recall localizing objects attention unlike conventional multi label image classification models model supports multiset prediction due reinforcement learning based training process allows arbitrary label permutation multiple instances per label
protein interface prediction using graph convolutional networks present general framework graph convolution classification tasks labeled graphs node edge features performing convolution operation neighborhood node interest able stack multiple layers convolution learn effective latent representations integrate information across input graph demonstrate effectiveness approach prediction interfaces proteins challenging problem important applications drug discovery design proposed approach achieves accuracy better state art svm method task also outperforms recently proposed diffusion convolution form graph convolution
dual agent gans photorealistic identity preserving profile face synthesis synthesizing realistic profile faces promising efficiently training deep pose invariant models large scale unconstrained face recognition populating samples extreme poses avoiding tedious annotations however learning synthetic faces achieve desired performance due discrepancy distributions synthetic real face images narrow gap propose dual agent generative adversarial network gan model improve realism face simulator output using unlabeled real faces preserving identity information realism refinement dual agents specifically designed distinguishing real fake identities simultaneously particular employ shelf face model simulator generate profile face images varying poses gan leverages fully convolutional network generator generate high resolution images auto encoder discriminator dual agents besides novel architecture make several key modifications standard gan preserve pose texture preserve identity stabilize training process pose perception loss identity perception loss iii adversarial loss boundary equilibrium regularization term experimental results show gan presents compelling perceptual results also significantly outperforms state arts large scale challenging nist ijb unconstrained face recognition benchmark addition proposed gan also promising new approach solving generic transfer learning problems effectively
toward robustness label noise training deep discriminative neural networks collecting large training datasets annotated high quality labels costly process paper proposes novel framework training deep convolutional neural networks noisy labeled datasets problem formulated using undirected graphical model represents relationship noisy clean labels trained semi supervised setting proposed structure inference latent clean labels tractable regularized training using auxiliary sources information proposed model applied image labeling problem shown effective labeling unseen images well reducing label noise training cifar coco datasets
soft hard vector quantization end end learning compressible representations present new approach learn compressible representations deep architectures end end training strategy method based soft continuous relaxation quantization entropy anneal discrete counterparts throughout training showcase method challenging applications image compression neural network compression tasks typically approached different methods soft hard quantization approach gives results competitive state art
selective classification deep neural networks selective classification techniques also known reject option yet considered context deep neural networks dnns techniques potentially significantly improve dnns prediction performance trading coverage paper propose method construct selective classifier given trained neural network method allows user set desired risk level test time classifier rejects instances needed grant desired risk high probability empirical results cifar imagenet convincingly demonstrate viability method opens possibilities operate dnns mission critical applications example using method unprecedented error top imagenet classification guaranteed probability almost test coverage
deep lattice networks partial monotonic functions propose learning deep models monotonic respect user specified set inputs alternating layers linear embeddings ensembles lattices calibrators piecewise linear functions appropriate constraints monotonicity jointly training resulting network implement layers projections new computational graph nodes tensorflow use adam optimizer batched stochastic gradients experiments benchmark real world datasets show layer monotonic deep lattice networks achieve state art performance classification regression monotonicity guarantees
learning prune deep neural networks via layer wise optimal brain surgeon develop slim accurate deep neural networks become crucial real world applications especially employed embedded systems though previous work along research line shown promising results existing methods either fail significantly compress well trained deep network require heavy retraining process pruned deep network boost prediction performance paper propose new layer wise pruning method deep neural networks proposed method parameters individual layer pruned independently based second order derivatives layer wise error function respect corresponding parameters prove final prediction performance drop pruning bounded linear combination reconstructed errors caused layer therefore guarantee needs perform light retraining process pruned network resume original prediction performance conduct extensive experiments benchmark datasets demonstrate effectiveness pruning method compared several state art baseline methods
bayesian compression deep learning compression computational efficiency deep learning become problem great significance work argue principled effective way attack problem taking bayesian point view sparsity inducing priors prune large parts network introduce novelties paper use hierarchical priors prune nodes instead individual weights use posterior uncertainties determine optimal fixed point precision encode weights factors significantly contribute achieving state art terms compression rates still staying competitive methods designed optimize speed energy efficiency
lower bounds robustness adversarial perturbations input output mappings learned state art neural networks significantly discontinuous possible cause neural network used image recognition misclassify input applying specific hardly perceptible perturbations input called adversarial perturbations many hypotheses proposed explain existence peculiar samples well several methods mitigate proven explanation remains elusive however work take steps towards formal characterization adversarial perturbations deriving lower bounds magnitudes perturbations necessary change classification neural networks bounds experimentally verified mnist cifar data sets
sobolev training neural networks heart deep learning aim use neural networks function approximators training produce outputs inputs emulation ground truth function data creation process many cases access input output pairs ground truth however becoming common access derivatives target output respect input example ground truth function neural network network compression distillation generally target derivatives computed ignored paper introduces sobolev training neural networks method incorporating target derivatives addition target values training optimising neural networks approximate function outputs also function derivatives encode additional information target function within parameters neural network thereby improve quality predictors well data efficiency generalization capabilities learned function approximation provide theoretical justifications approach well examples empirical evidence distinct domains regression classical optimisation datasets distilling policies agent playing atari large scale applications synthetic gradients domains use sobolev training employing target derivatives addition target values results models higher accuracy stronger generalisation
structured bayesian pruning via log normal multiplicative noise dropout based regularization methods regarded injecting random noise pre defined magnitude different parts neural network training recently shown bayesian dropout procedure improves generalization also leads extremely sparse neural architectures automatically setting individual noise magnitude per weight however sparsity hardly used acceleration since unstructured paper propose new bayesian model takes account computational structure neural networks provides structured sparsity removes neurons convolutional channels cnns inject noise neurons outputs keeping weights unregularized establish probabilistic model proper truncated log uniform prior noise truncated log normal variational approximation ensures term evidence lower bound computed closed form model leads structured sparsity removing elements low snr computation graph provides significant acceleration number deep neural architectures model easy implement corresponds addition dropout like layer computation graph
population matching discrepancy applications deep learning differentiable estimation distance distributions based samples important many deep learning tasks estimation maximum mean discrepancy mmd however mmd suffers sensitive kernel bandwidth hyper parameter weak gradients large mini batch size used training objective paper propose population matching discrepancy pmd estimating distribution distance based samples well algorithm learn parameters distributions using pmd objective pmd defined minimum weight matching sample populations distribution prove pmd strongly consistent estimator first wasserstein metric apply pmd deep learning tasks domain adaptation generative modeling empirical results demonstrate pmd overcomes aforementioned drawbacks mmd outperforms mmd tasks terms performance well convergence speed
investigating learning dynamics deep neural networks using random matrix theory evidence well conditioned singular value distribution input output jacobian lead substantial improvements training performance deep neural networks deep linear networks conclusive evidence initializing using orthogonal random matrices lead dramatic improvements training however benefit initialization strategies proven much less obvious realistic nonlinear networks use random matrix theory study conditioning jacobian nonlinear neural networks random initialization show singular value distribution jacobian sensitive distribution weights also nonlinearity surprisingly find benefit orthogonal initialization negligible rectified linear networks substantial tanh networks provide rule thumb initializing tanh networks display dynamical isometry full depth finally perform experiments mnist cifar10 using wide array optimizers show conclusively singular value distribution jacobian intimately related learning dynamics finally show spectral density jacobian evolves relatively slowly training good initialization affects learning dynamics far initial setting weights
robust imitation diverse behaviors deep generative models recently shown great promise imitation learning motor control given enough data even supervised approaches shot imitation learning however vulnerable cascading failures agent trajectory diverges demonstrations compared purely supervised methods generative adversarial imitation learning gail learn robust controllers fewer demonstrations inherently mode seeking difficult train paper show combine favourable aspects approaches base model new type variational autoencoder demonstration trajectories learns semantic policy embeddings show embeddings learned dof jaco robot arm reaching tasks smoothly interpolated resulting smooth interpolation reaching behavior leveraging policy representations develop new version gail much robust purely supervised controller especially demonstrations avoids mode collapse capturing many diverse behaviors gail demonstrate approach learning diverse gaits demonstration biped dof humanoid mujoco physics environment
question asking program generation hallmark human intelligence ability ask rich creative revealing questions introduce cognitive model capable constructing human like questions approach treats questions formal programs executed state world output answer model specifies probability distribution complex compositional space programs favoring concise programs help agent learn current context evaluate approach modeling types open ended questions generated humans attempting learn ambiguous situation game find model predicts questions people ask generalize novel situations creative ways addition compare number model variants assess features critical producing human like questions
variational laws visual attention dynamic scenes computational models visual attention crossroad disciplines like cognitive science computational neuroscience computer vision paper proposes approach based principle foundational laws drive emergence visual attention devise variational laws eye movement rely generalized view least action principle physics potential energy captures details well peripheral visual features kinetic energy corresponds classic interpretation analytic mechanics addition lagrangian contains brightness invariance term characterizes significantly scanpath trajectories obtain differential equations visual attention stationary point generalized action propose algorithm estimate model parameters finally report experimental results validate model tasks saliency detection
flexible statistical inference mechanistic models neural dynamics mechanistic models single neuron dynamics extensively studied computational neuroscience however identifying models quantitatively reproduce empirically measured data challenging propose overcome limitation using likelihood free inference approaches also known approximate bayesian computation abc perform full bayesian inference single neuron models approach builds recent advances abc learning neural network maps features observed data posterior distribution parameters learn bayesian mixture density network approximating posterior multiple rounds adaptively chosen simulations furthermore propose efficient approach handling missing features parameter settings simulator fails prevalent issues models neural dynamics well strategy automatically learning relevant features using recurrent neural networks synthetic data approach efficiently estimates posterior distributions recovers ground truth parameters vitro recordings membrane voltages recover multivariate posteriors biophysical parameters yield model predicted voltage traces accurately match empirical data approach enable neuroscientists perform bayesian inference complex neuron models without design model specific algorithms closing gap mechanistic statistical approaches single neuron modelling
training recurrent networks generate hypotheses brain solves hard navigation problems self localization navigation noisy sensors ambiguous world computationally challenging yet animals humans excel robotics simultaneous location mapping slam algorithms solve problem though joint sequential probabilistic inference coordinates external spatial landmarks generate first neural solution slam problem training recurrent lstm networks perform set hard navigation tasks require generalization completely novel trajectories environments goal make sense diverse phenomenology brain spatial navigation circuits related function show hidden unit representations exhibit several key properties hippocampal place cells including stable tuning curves remap environments result also proof concept end end learning slam algorithm using recurrent networks demonstration approach advantages robotic slam
neural system identification large populations separating neuroscientists classify neurons different types perform similar computations different locations visual field traditional neural system identification methods capitalize separation learning deep convolutional feature spaces shared among many neurons provides exciting path forward architectural design needs account data limitations new experimental techniques enable recordings thousands neurons experimental time limited sample small fraction neuron response space show major bottleneck fitting convolutional neural networks cnns neural data estimation individual receptive field locations problem scratched surface thus far propose cnn architecture sparse pooling layer factorizing spatial feature dimensions network scales well thousands neurons short recordings trained end end explore architecture ground truth data explore challenges limitations cnn based system identification moreover show network model outperforms current state art system identification model mouse primary visual cortex publicly available dataset
simple model recognition recall memory show several striking differences memory performance recognition recall tasks explained ecological bias endemic classic memory experiments experiments universally involve stimuli retrieval cues show sensible think recall simply retrieving items probed cue typically item list better think recognition retrieving cues probed items test theory manipulating number items cues memory experiment show crossover effect memory performance within subjects recognition performance superior recall performance number items greater number cues recall performance better recognition converse holds build simple computational model around theory using sampling approximate ideal bayesian observer encoding retrieving situational occurrence frequencies stimuli retrieval cues model robustly reproduces number dissociations recognition recall previously used argue dual process accounts declarative memory
gaussian process based nonlinear latent structure discovery multivariate spike train data large body recent work focused methods identifying low dimensional latent structure multi neuron spike train data methods employed either linear latent dynamics linear log linear mappings latent space spike rates propose doubly nonlinear latent variable model population spike trains identify nonlinear low dimensional structure underlying apparently high dimensional spike train data model poisson gaussian process latent variable model gplvm defined low dimensional latent variable governed gaussian process nonlinear tuning curves parametrized exponentiated samples second gaussian process poisson observations nonlinear tuning curves allow discovery low dimensional latent embeddings even spike rates span high dimensional subspace hippocampal place cell codes learn model introduce decoupled laplace approximation fast approximate inference method allows efficiently maximize marginal likelihood latent path integrating tuning curves show method outperforms previous approaches maximizing laplace approximation based marginal likelihoods convergence speed value final objective apply model spike trains recorded hippocampal place cells show outperforms variety previous methods latent structure discovery including variational auto encoder based methods parametrize nonlinear mapping latent space spike rates deep neural network
deep adversarial neural decoding present novel approach solve problem reconstructing perceived stimuli brain responses combining probabilistic inference deep learning approach first inverts linear transformation latent features brain responses maximum posteriori estimation inverts nonlinear transformation perceived stimuli latent features adversarial training convolutional neural networks test approach functional magnetic resonance imaging experiment show generate state art reconstructions perceived faces brain activations
cross spectral factor analysis neuropsychiatric disorders schizophrenia depression often disruption way different regions brain communicate another order build greater understanding neurological basis disorders introduce novel model multisite local field potentials lfps low frequency voltage oscillations measured electrodes implanted many brain regions simultaneously proposed model called cross spectral factor analysis csfa breaks observed lfps electrical functional connectomes electomes defined differing spatiotemporal properties electome defined unique frequency power phase coherence patterns many brain regions properties granted features via gaussian process formulation multiple kernel learning framework critically electomes interpretable used design follow causal studies furthermore using formulation lfp signals mapped lower dimensional space better traditional approaches remarkably addition interpretability proposed approach achieves state art predictive ability compared black box approaches looking behavioral paradigms genotype prediction tasks mouse model demonstrating feature basis capturing neural dynamics related outcomes conclude discussion csfa analysis used conjunction experiments design causal studies provide gold standard validation inferred neural relationships
cognitive impairment prediction alzheimer disease regularized modal regression accurate automatic predictions cognitive assessment via neuroimaging markers critical early detection alzheimer disease linear regression models successfully used association study neuroimaging features cognitive performance alzheimer disease study however existing methods built least squares mean square error mse criterion sensitive outliers performance degraded heavy tailed noise complex brain disorder data paper beyond criterion investigating regularized modal regression statistical learning viewpoint new regularized scheme based modal regression proposed estimation variable selection robust outliers heavy tailed noise skewed noise conduct theoretical analysis establish approximation bound learning conditional mode function sparsity analysis variable selection robustness characterization experimental evaluations simulated data adni cohort data provided support promising performance proposed algorithm
stochastic submodular maximization case coverage functions continuous optimization techniques sgd extensions main workhorse modern machine learning nevertheless variety important machine learning problems require solving discrete optimization problems submodular objectives goal paper unleash toolkit modern continuous optimization discrete problems first introduce framework emph stochastic submodular optimization instead emph oracle access underlying objective explicitly considers statistical computational aspects evaluating objective provide formalization emph stochastic submodular maximization class important discrete optimization problems show state art techniques continuous optimization lifted realm discrete optimization extensive experimental evaluation demonstrate practical impact proposed approach
gradient methods submodular maximization paper study problem maximizing continuous submodular functions naturally arise many learning applications involving utility functions active learning sensing matrix approximations network inference despite apparent lack convexity functionals prove stochastic projected gradient methods provide strong approximation guarantees maximizing continuous submodular functions convex constraints specifically prove monotone continuous submodular functions fixed points projected gradient ascent provide factor approximation global maxima also study stochastic gradient mirror methods show mathcal epsilon iterations methods reach solutions achieve expectaion objective values exceeding frac text opt epsilon immediate implication result bridge discrete continuous submodular maximization finally experiments real data demonstrate projected gradient methods consistently achieve best utility compared continuous baselines remaining competitive terms computational effort
non convex finite sum optimization via scsg methods develop class algorithms variants stochastically controlled stochastic gradient scsg methods smooth nonconvex finite sum optimization problem assuming smoothness component complexity scsg reach stationary point nabla epsilon min epsilon epsilon strictly outperforms stochastic gradient descent moreover scsg never worse state art methods based variance reduction significantly outperforms target accuracy low similar acceleration also achieved functions satisfy polyak lojasiewicz condition empirical experiments demonstrate scsg outperforms stochastic gradient methods training multi layers neural networks terms training validation loss
influence maximization varepsilon almost submodular threshold function influence maximization problem selecting nodes social network maximize influence spread problem extensively studied works focus submodular influence diffusion models paper motivated empirical evidences explore influence maximization non submodular regime particular study general threshold model fraction nodes non submodular threshold functions threshold functions closely upper lower bounded submodular functions call varepsilon almost submodular first show strong hardness result frac gamma approximation influence maximization unless networks gamma varepsilon almost submodular nodes gamma parameter depending varepsilon although threshold function close submodular influence maximization still hard approximate provide varepsilon ell frac approximation algorithms number varepsilon almost submodular nodes ell finally conduct experiments number real world datasets results demonstrate approximation algorithms outperform benchmark algorithms
subset selection noise problem selecting best element subset universe involved many applications previous studies assumed noise free environment noisy monotone submodular objective function paper considers realistic general situation evaluation subset noisy monotone function necessarily submodular multiplicative additive noises understand impact noise firstly show approximation ratio greedy algorithm poss powerful algorithms noise free subset selection noisy environments propose incorporate noise aware strategy poss resulting new ponss algorithm better approximation ratio empirical results influence maximization sparse regression problems show superior performance ponss
polynomial time algorithms dual volume sampling study dual volume sampling method selecting columns short wide matrix probability selection proportional volume spanned rows induced submatrix method proposed avron boutsidis 2013 showed promising method column subset selection multiple applications however wider adoption hampered lack polynomial time sampling algorithms remove hindrance developing exact randomized polynomial time sampling algorithm well derandomization thereafter study dual volume sampling via theory real stable polynomials prove distribution satisfies strong rayleigh property result remarkable consequences especially implies provably fast mixing markov chain sampler makes dual volume sampling much attractive practitioners sampler closely related classical algorithms popular experimental design methods date lacking theoretical analysis known empirically work well
lookahead bayesian optimization inequality constraints consider task optimizing objective function subject inequality constraints objective constraints expensive evaluate bayesian optimization popular way tackle optimization problems expensive objective function evaluations mostly applied unconstrained problems several approaches proposed address expensive constraints limited greedy strategies maximizing immediate reward address limitation propose lookahead approach selects next evaluation order maximize long term feasible reduction objective function present numerical experiments demonstrating performance improvements lookahead approach compared greedy algorithms constrained expected improvement eic predictive entropy search constraint pesc
non monotone continuous submodular maximization structure algorithms submodular continuous functions important objectives wide real world applications spanning map inference determinantal point processes dpps mean field inference probabilistic submodular models amongst others submodularity captures subclass non convex functions enables exact minimization approximate maximization polynomial time work study problem maximizing non monotone submodular continuous functions general closed convex constraints start investigating several properties underlie objectives used devise optimization algorithms provable guarantees concretely first devise phase algorithm approximation guarantee algorithm allows use existing methods ensured find approximate stationary points subroutine thus enabling utilize recent progress non convex optimization present non monotone frank wolfe variant approximation guarantee sublinear convergence rate finally extend approach broader class generalized submodular continuous functions captures wider spectrum applications theoretical findings validated several synthetic real world problem instances
solving almost systems random quadratic equations paper deals finding dimensional solution system quadratic equations y_i langle rangle general known hard put forth novel procedure starts emph weighted maximal correlation initialization obtainable power iterations followed successive refinements based emph iteratively reweighted gradient type iterations novel techniques distinguish prior works inclusion fresh weighting regularization certain random measurement models proposed procedure returns true solution high probability time proportional reading data y_i provided number equations constant times number unknowns namely empirically upshots contribution perfect signal recovery high dimensional regime given information theoretic limit number equations near optimal statistical accuracy presence additive noise extensive numerical tests using synthetic data real images corroborate improved signal recovery performance computational efficiency relative state art approaches
learning relus via gradient descent paper study problem learning rectified linear units relus functions form vct mapsto max langle vct vct rangle vct denoting weight vector study problem high dimensional regime number observations fewer dimension weight vector assume weight vector belongs closed set convex nonconvex captures known side information structure focus realizable model inputs chosen gaussian distribution labels generated according planted weight vector show projected gradient descent initialization vct converges linear rate planted model number samples optimal numerical constants results dynamics convergence shallow neural nets provide insights towards understanding dynamics deeper architectures
stochastic mirror descent non convex optimization paper examine class non convex stochastic programs call emph variationally coherent properly includes quasi pseudo convex optimization problems establish convergence class problems study well known smd method show algorithm last iterate converges problem global optimum probability results contribute landscape non convex optimization clarifying convexity quasi convexity essential global convergence rather variational coherence much weaker requirement suffices localize class account locally variationally coherent problems show last iterate stochastic mirror descent converges local optima high probability finally consider last iterate convergence rates problems sharp minima derive special case conclusion probability last iterate stochastic gradient descent reaches exact global optimum finite number steps result contrasted existing work linear programs exhibit asymptotic convergence rates
accelerated first order methods geodesically convex optimization riemannian manifolds paper propose accelerated first order method geodesically convex optimization generalization standard nesterov accelerated method euclidean space nonlinear riemannian space first derive equations approximate linearization gradient like updates euclidean space geodesically convex optimization particular analyze global convergence properties accelerated method geodesically strongly convex problems show method improves convergence rate sqrt moreover method also improves global convergence rate geodesically general convex problems finally give specific iterative scheme matrix karcher mean problems validate theoretical results experiments
fine grained complexity empirical risk minimization kernel methods neural networks empirical risk minimization erm ubiquitous machine learning underlies supervised learning methods large body work algorithms various erm problems exact computational complexity erm still understood address issue multiple popular erm problems including kernel svms kernel ridge regression training final layer neural network particular give conditional hardness results problems based complexity theoretic assumptions strong exponential time hypothesis assumptions show algorithms solve aforementioned erm problems high accuracy sub quadratic time also give similar hardness results computing gradient empirical loss main computational burden many non convex learning tasks
large scale quadratically constrained quadratic program via low discrepancy sequences consider problem solving large scale quadratically constrained quadratic program problems occur naturally many scientific web applications although efficient methods tackle problem mostly scalable paper develop method transforms quadratic constraint linear form sampling set low discrepancy points transformed problem solved applying state art large scale solvers show convergence approximate solution true solution well finite sample error bounds experimental results also shown prove scalability practice
new alternating direction method linear programming well known linear program constraint matrix mathbf mathbb times alternating direction method multiplier converges globally linearly rate mathbf log epsilon however rate related problem dimension algorithm exhibits slow fluctuating tail convergence practice paper propose new variable splitting method prove method convergence rate mathbf log epsilon proof based simultaneously estimating distance pair primal dual iterates optimal primal dual solution set certain residuals practice result new first order solver exploit sparsity specific structure matrix mathbf significant speedup important problems basis pursuit inverse covariance matrix estimation svm nonnegative matrix factorization problem compared current fastest solvers
dykstra algorithm admm coordinate descent connections insights extensions study connections dykstra algorithm projecting onto intersection convex sets augmented lagrangian method multipliers admm block coordinate descent prove coordinate descent regularized regression problem separable penalty functions seminorms exactly equivalent dykstra algorithm applied dual problem admm dual problem also seen equivalent special case sets linear subspace connections aside interesting right suggest new ways analyzing extending coordinate descent example existing convergence theory dykstra algorithm polyhedra discern coordinate descent lasso problem converges asymptotically linear rate also develop parallel versions coordinate descent based dykstra admm connections
smooth primal dual coordinate descent algorithms nonsmooth convex optimization propose new randomized coordinate descent method convex optimization template broad applications analysis relies novel combination ideas applied primal dual gap function smoothing acceleration homotopy non uniform sampling result method features first convergence rate guarantees best known variety common structure assumptions template provide numerical evidence support theoretical results comparison state art algorithms
first order adaptive sample size methods reduce complexity empirical risk minimization paper studies empirical risk minimization erm problems large scale datasets incorporates idea adaptive sample size methods improve guaranteed convergence bounds first order stochastic deterministic methods contrast traditional methods attempt solve erm problem corresponding full dataset directly adaptive sample size schemes start small number samples solve corresponding erm problem statistical accuracy sample size grown geometrically scaling factor use solution previous erm warm start new erm theoretical analyses show use adaptive sample size methods reduces overall computational cost achieving statistical accuracy whole dataset broad range deterministic stochastic first order methods gains specific choice method particularized accelerated gradient descent stochastic variance reduce gradient computational cost advantage logarithm number training samples numerical experiments various datasets confirm theoretical claims showcase gains using proposed adaptive sample size scheme
accelerated consensus via min sum splitting apply min sum message passing protocol solve consensus problem distributed optimization show ordinary min sum algorithm converge modified version known splitting yields convergence problem solution prove proper choice tuning parameters allows min sum splitting yield subdiffusive accelerated convergence rates matching rates obtained shift register methods acceleration scheme embodied min sum splitting consensus problem bears similarities lifted markov chains techniques multi step first order methods convex optimization
integration methods optimization algorithms show accelerated optimization methods seen particular instances multi step integration schemes numerical analysis applied gradient flow equation compared recent advances vein differential equation considered basic gradient flow derive class multi step schemes includes accelerated algorithms using classical conditions numerical analysis multi step schemes integrate differential equation using larger step sizes intuitively explains acceleration phenomenon
efficient use limited memory resources accelerate linear learning work propose generic approach efficiently use compute accelerators gpus fpgas training large scale machine learning models training data exceeds memory capacity technique builds upon primal dual coordinate selection uses duality gaps selection criteria dynamically decide part data made available fast processing provide strong theoretical guarantees motivating gap based selection scheme provide efficient practical implementation thereof illustrate power approach demonstrate performance training generalized linear models large scale datasets exceeding memory size modern gpu showing order magnitude speedup existing approaches
screening rule regularized ising model estimation discover screening rule regularized ising model estimation simple closed form screening rule necessary sufficient condition exactly recovering blockwise structure solution given regularization parameters enough sparsity screening rule combined exact inexact optimization procedures deliver solutions efficiently practice screening rule especially suitable large scale exploratory data analysis number variables dataset thousands interested relationship among handful variables within moderate size clusters interpretability experimental results various datasets demonstrate efficiency insights gained introduction screening rule
uprooting rerooting higher order graphical models idea uprooting rerooting graphical models introduced specifically binary pairwise models weller way transform model whole equivalence class related models inference model yields inference results others helpful since inference relevant bounds much easier obtain accurate model class introduce methods extend approach models higher order potentials develop theoretical insights example demonstrate triplet consistent polytope tri unique universally rooted demonstrate empirically rerooting significantly improve accuracy methods inference higher order models negligible computational cost
concentration multilinear functions ising model applications network data prove near tight concentration measure polynomial functions ising model high temperature improving radius concentration guaranteed known results polynomial factors dimension number nodes ising model show results optimal logarithmic factors dimension obtain results extending strengthening exchangeable pairs approach used prove concentration measure setting chatterjee demonstrate efficacy functions statistics testing strength interactions social networks synthetic real world data
inference graphical models via semidefinite programming hierarchies maximum posteriori probability map inference graphical models amounts solving graph structured combinatorial optimization problem popular inference algorithms belief propagation generalized belief propagation gbp intimately related linear programming relaxation within sherali adams hierarchy despite popularity algorithms well understood sum squares sos hierarchy based semidefinite programming sdp provide superior guarantees unfortunately sos relaxations graph vertices require solving sdp theta variables degree hierarchy practice approach scale beyond tens variables paper propose sdp relaxations map inference using sos hierarchy innovations focused computational efficiency firstly analogy variants introduce decision variables corresponding contiguous regions graphical model secondly solve resulting sdp using non convex burer monteiro style method develop sequential rounding procedure demonstrate resulting algorithm solve problems tens thousands variables within minutes significantly outperforms gbp practical problems image denoising ising spin glasses finally specific graph types establish sufficient condition tightness proposed partial sos relaxation
beyond normality learning sparse probabilistic graphical models non gaussian setting present algorithm identify sparse dependence structure continuous non gaussian probability distributions given corresponding set data conditional independence structure arbitrary distribution represented undirected graph markov random field algorithms learning structure restricted discrete gaussian cases new approach allows realistic accurate descriptions distribution question turn better estimates sparse markov structure sparsity graph interest accelerate inference improve sampling methods reveal important dependencies variables algorithm relies exploiting connection sparsity graph sparsity transport maps deterministically couple probability measure another
dynamic importance sampling anytime bounds partition function computing partition function key inference task many graphical models paper propose dynamic importance sampling scheme provides anytime finite sample bounds partition function algorithm balances advantages major inference strategies heuristic search variational bounds monte carlo methods blending sampling search refine variationally defined proposal algorithm combines generalizes recent work anytime search probabilistic bounds partition function using intelligently chosen weighted average samples construct unbiased estimator partition function strong finite sample confidence intervals inherit rapid early improvement rate sampling long term benefits improved proposal search gives significantly improved anytime behavior flexible trade offs memory time solution quality demonstrate effectiveness approach empirically real world problem instances taken recent uai competitions
nonbacktracking bounds influence independent cascade models paper develops upper lower bounds influence measure network precisely expected number nodes seed set influence independent cascade model particular bounds exploit nonbacktracking walks fortuin kasteleyn ginibre fkg type inequalities computed message passing implementation nonbacktracking walks recently allowed headways community detection paper shows use also impact influence computation provide knob control trade efficiency accuracy bounds finally tightness bounds illustrated simulations various network models
rigorous dynamics consistent estimation arbitrarily conditioned linear systems problem estimating random vector noisy linear measurements unknown parameters distributions must also learned arises wide range statistical learning linear inverse problems show computationally simple iterative message passing algorithm provably obtain asymptotically consistent estimates certain high dimensional large system limit lsl general parameterizations previous message passing techniques required sub gaussian matrices often fail matrix ill conditioned proposed algorithm called adaptive vector approximate message passing adaptive vamp auto tuning applies right rotationally random importantly class includes matrices arbitrarily bad conditioning show parameter estimates mean squared error mse iteration converge deterministic limits precisely predicted simple set state evolution equations addition simple testable condition provided mse matches bayes optimal value predicted replica method paper thus provides computationally simple method provable guarantees optimality consistency large class linear inverse problems
learning disentangled representations semi supervised deep generative models variational autoencoders vaes learn representations data jointly training probabilistic encoder decoder network typically models encode features data single variable interested learning disentangled representations encode distinct aspects data separate variables propose learn representations using model architectures generalize standard vaes employing general graphical model structure encoder decoder allows train partially specified models make relatively strong assumptions subset interpretable variables rely flexibility neural networks learn representations remaining variables define general objective semi supervised learning model class approximated using importance sampling procedure applies generally class models evaluate framework ability learn disentangled representations qualitative exploration generative capacity quantitative evaluation discriminative ability variety models datasets
gauging variational inference computing partition function important statistical inference task arising applications graphical models since computationally intractable approximate methods used resolve issue practice mean field belief propagation arguably popular successful approaches variational type paper propose new variational schemes coined gauged gauged improving respectively provide lower bounds partition function utilizing called gauge transformation modifies factors keeping partition function invariant moreover prove exact gms single loop special structure even though bare perform badly case extensive experiments complete gms relatively small size large 300 variables confirm newly proposed algorithms outperform generalize
variational inference via chi upper bound minimization variational inference widely used efficient alternative mcmc posits family approximating distributions finds member closest true posterior closeness usually measured via divergence though successful approach also problems notably typically leads underestimation posterior variance paper propose chivi new black box variational inference algorithm minimizes chi chi divergence chivi minimizes upper bound model evidence term cubo minimizing cubo leads better estimates posterior used classical lower bound elbo provide sandwich estimate marginal likelihood study chivi models probit regression gaussian process classification cox process model basketball plays compared classical chivi produces better error rates accurate estimates posterior variance
collapsed variational bayes markov jump processes markov jump processes continuous time stochastic processes widely used statistical applications natural sciences recently machine learning inference models typically proceeds via markov chain monte carlo suffer various computational challenges work propose novel collapsed variational inference algorithm address issue work leverages ideas discrete time markov chains exploits connection idea called uniformization algorithm proceeds marginalizing parameters markov jump process approximating distribution trajectory factored distribution segments piecewise constant function unlike mcmc schemes marginalize transition times piecewise constant process scheme optimizes discretization time resulting significant computational savings apply ideas synthetic data well dataset check recordings demonstrate superior performance state art mcmc methods
bayesian dyadic trees histograms regression many machine learning tools regression based recursive partitioning covariate space smaller regions regression function estimated locally among regression trees ensembles demonstrated impressive empirical performance work shed light machinery behind bayesian variants methods particular study bayesian regression histograms bayesian dyadic trees simple regression case predictor focus reconstruction regression surfaces piecewise constant number jumps unknown show suitably designed priors posterior distributions concentrate around true step regression function minimax rate log factor results require knowledge true number steps width true partitioning cells thus bayesian dyadic regression trees fully adaptive recover true piecewise regression function nearly well knew exact number location jumps results constitute first step towards understanding bayesian trees ensembles worked well practice aside discuss prior distributions balanced interval partitions relate problem geometric probability namely quantify probability covering circumference circle random arcs whose endpoints confined grid new variant original problem
differentially private bayesian learning distributed data many applications machine learning example health care would benefit methods guarantee privacy data subjects differential privacy become established standard protecting learning results standard algorithms require single trusted party access entire data clear weakness consider bayesian learning distributed setting party holds single sample samples data propose learning strategy based secure multi party sum function aggregating summaries data holders gaussian mechanism method builds asymptotically optimal practically efficient bayesian inference rapidly diminishing extra cost
model powered conditional independence test consider problem non parametric conditional independence testing testing continuous random variables given samples joint distribution continuous random vectors determine whether independent vert approach converting conditional independence test classification problem allows harness powerful classifiers like gradient boosted trees deep neural networks models handle complex probability distributions allow perform significantly better compared prior state art high dimensional testing main technical challenge classification problem need samples conditional product distribution joint distribution independent vert given access samples true joint distribution tackle problem propose novel nearest neighbor bootstrap procedure theoretically show generated samples indeed close terms total variational distance develop theoretical results regarding generalization bounds classification problem translate error bounds testing provide novel analysis rademacher type classification bounds presence non textit near independent samples empirically validate performance algorithm simulated real datasets show performance gains previous methods
worlds collide integrating different counterfactual assumptions fairness machine learning used make crucial decisions people lives nearly decisions risk individuals certain race gender sexual orientation subpopulation unfairly discriminated recent method demonstrated use techniques counterfactual inference make predictions fair across different subpopulations method requires provides causal model generated data hand genera validating causal model impossible using observational data alone without assumptions hence desirable integrate competing causal models provide counterfactually fair decisions regardless world correct paper show possible make predictions approximately fair respect multiple possible causal models thus bypassing problem exact causal specification frame goal learning fair classifier optimization problem fairness constraints provide techniques relaxations solve optimization problem demonstrate flexibility model real world fair classification problems show model seamlessly balance fairness multiple worlds prediction accuracy
lda uncovering latent patterns text based sequential decision processes sequential decision making often important useful end users understand underlying patterns causes lead corresponding decisions however typical deep reinforcement learning algorithms seldom provide information due black box nature paper present probabilistic model lda uncover latent patterns text based sequential decision processes model understood variant latent topic models tailored maximize total rewards draw interesting connection approximate maximum likelihood estimation lda celebrated learning algorithm demonstrate text game domain proposed method provides viable mechanism uncover latent patterns decision processes also obtains state art rewards games
probabilistic models integration error assessment functional cardiac models paper studies numerical computation integrals representing estimates predictions output computational model respect distribution mathrm uncertain inputs model functional cardiac models motivate work neither possess closed form expression evaluation either requires approx 100 cpu hours precluding standard numerical integration methods proposal treat integration estimation problem joint model priori unknown function priori unknown distribution result posterior distribution integral explicitly accounts dual sources numerical approximation error due severely limited computational budget construction applied account statistically principled manner impact numerical errors present confounding factors functional cardiac model assessment
expectation propagation exponential family using algebra exponential family distributions highly useful machine learning since calculation performed efficiently natural parameters exponential family recently extended emph exponential family contains student distributions family members thus allows handle noisy data well however since exponential family defined emph deformed exponential cannot derive efficient learning algorithm exponential family expectation propagation paper borrow mathematical tools algebra statistical physics show pseudo additivity distributions allows perform calculation exponential family distributions natural parameters develop expectation propagation algorithm exponential family provides deterministic approximation posterior predictive distribution simple moment matching finally apply proposed algorithm bayes point machine student process classification demonstrate performance numerically
probabilistic framework nonlinearities stochastic neural networks present probabilistic framework nonlinearities based doubly truncated gaussian distributions setting truncation points appropriately able generate various types nonlinearities within unified framework including sigmoid tanh relu commonly used nonlinearities neural networks framework readily integrates existing stochastic neural networks hidden units characterized random variables allowing first time learn nonlinearities alongside model weights networks extensive experiments demonstrate performance improvements brought proposed framework integrated restricted boltzmann machine rbm temporal rbm truncated gaussian graphical model tggm
clone mcmc parallel high dimensional gaussian gibbs sampling propose generalized gibbs sampler algorithm obtaining samples approx imately distributed high dimensional gaussian distribution similarly hogwild methods approach target original gaussian distribution interest approximation contrary hogwild methods single parameter allows trade bias variance show empirically method flexible performs well compared hogwild type algorithms
learning spatiotemporal piecewise geodesic trajectories longitudinal manifold valued data introduce hierarchical model allows estimate group average piecewise geodesic trajectory riemannian space measurements individual variability model falls well defined mixed effect models subject specific trajectories defined spatial temporal transformations group average piecewise geodesic path component component thus apply model wide variety situations due non linearity model use stochastic approximation expectation maximization algorithm estimate model parameters experiments synthetic data validate choice model applied metastatic renal cancer chemotherapy monitoring run estimations recist scores treated patients estimate time escape treatment experiments highlight role different parameters response treatment
scalable levy process priors spectral kernel learning gaussian processes rich distributions functions generalisation properties determined kernel function propose distribution kernels formed modelling spectral density levy process resulting distribution support stationary covariances including popular rbf periodic matern kernels combined inductive biases enable automatic data efficient learning long range extrapolation state art predictive performance posterior inference develop reversible jump mcmc approach includes automatic selection model order exploit algebraic structure proposed process training predictions show proposed model empirically recover flexible ground truth covariances demonstrate extrapolation several benchmarks
inferring latent structure human decision making raw visual inputs goal imitation learning match example expert behavior without access reinforcement signal expert demonstrations provided humans however often show significant variability due latent factors explicitly modeled introduce extension generative adversarial imitation learning method infer latent structure human decision making unsupervised way method imitate complex behaviors also learn interpretable meaningful representations demonstrate approach applicable high dimensional environments including raw visual inputs highway driving domain show model learned demonstrations able produce different driving styles accurately anticipate human actions method surpasses various baselines terms performance functionality
hybrid reward architecture reinforcement learning main challenges reinforcement learning generalisation typical deep methods achieved approximating optimal value function low dimensional representation using deep network approach works well many domains domains optimal value function cannot easily reduced low dimensional representation learning slow unstable paper contributes towards tackling challenging domains proposing new method called hybrid reward architecture hydra hydra takes input decomposed reward function learns separate value function component reward function component typically depends subset features overall value function much smoother easier approximated low dimensional representation enabling effective learning demonstrate hydra toy problem atari game pac man hydra achieves human performance
shallow updates deep reinforcement learning deep reinforcement learning drl methods deep network dqn achieved state art results variety challenging high dimensional domains success mainly attributed power deep neural networks learn rich domain representations approximating value function policy batch reinforcement learning methods linear representations hand stable require less hyper parameter tuning yet substantial feature engineering necessary achieve good results work propose hybrid approach least squares deep network dqn combines rich feature representations learned drl algorithm stability linear least squares method periodically training last hidden layer drl network batch least squares update key approach bayesian regularization term least squares update prevents fitting recent data tested dqn atari games demonstrate significant improvement vanilla dqn double dqn also investigated reasons superior performance method interestingly found performance improvement attributed large batch size used method optimizing last layer
towards generalization simplicity continuous control remarkable successes deep learning speech recognition computer vision motivated efforts adapt similar techniques problem domains including reinforcement learning consequently methods produced rich motor behaviors simulated robot tasks success largely attributed use multi layer neural networks work among first carefully study might responsible recent advancements main result calls emerging narrative question showing much simpler architectures based linear rbf parameterizations achieve comparable performance state art results study different policy representations regard performance measures hand also towards robustness external perturbations find learned neural network policies standard training scenarios robust linear rbf policies fact remarkably brittle finally directly modify training scenarios order favor robust policies find compelling case favor multi layer architectures overall study suggests multi layer architectures default choice unless side side comparison simpler architectures shows otherwise generally hope results lead interest carefully studying architectural choices associated trade offs training generalizable robust policies
interpolated policy gradient merging policy policy gradient estimation deep reinforcement learning policy model free deep reinforcement learning methods using previously collected data improve sample efficiency policy policy gradient techniques hand policy algorithms often stable easier use paper examines theoretically empirically approaches merging policy updates deep reinforcement learning theoretical results show policy updates value function estimator interpolated policy policy gradient updates whilst still satisfying performance bounds analysis uses control variate methods produce family policy gradient algorithms several recently proposed algorithms special cases family provide empirical comparison techniques remaining algorithmic details fixed show different mixing policy gradient estimates policy samples contribute improvements empirical performance final algorithm provides generalization unification existing deep policy gradient techniques theoretical guarantees bias introduced policy updates improves state art model free deep methods number openai gym continuous control benchmarks
scalable planning tensorflow hybrid nonlinear domains given recent deep learning results demonstrate ability effectively optimize high dimensional non convex functions gradient descent optimization gpus ask paper whether symbolic gradient optimization tools tensorflow effective planning hybrid mixed discrete continuous nonlinear domains high dimensional state action spaces end demonstrate hybrid planning tensorflow rmsprop gradient descent competitive mixed integer linear program milp based optimization piecewise linear planning domains compute optimal solutions substantially outperforms state art interior point methods nonlinear planning domains furthermore remark tensorflow highly scalable converging strong policy large scale concurrent domain total 576 000 continuous actions horizon time steps minutes provide number insights clarify strong performance including observations despite long horizons rmsprop avoids vanishing exploding gradients problem together results suggest new frontier highly scalable planning nonlinear hybrid domains leveraging gpus power recent advances gradient descent highly optmized toolkits like tensorflow
task based end end model learning stochastic optimization machine learning techniques becoming widespread become common see prediction algorithms operating within larger process however criteria train algorithms often differ ultimate criteria evaluate paper proposes end end approach learning probabilistic machine learning models within context stochastic programming manner directly captures ultimate task based objective used present experimental evaluations proposed approach classical inventory stock problem real world electrical grid scheduling task cases show proposed approach outperform traditional modeling purely black box policy optimization approaches
value prediction network paper proposes novel deep reinforcement learning approach called value prediction network vpn integrates model free model based methods single neural network contrast typical model based methods vpn learns dynamics model whose abstract states trained make option conditional predictions future values rather future observations experimental results show vpn several advantages model free model based baselines stochastic environment careful planning required building accurate observation prediction model difficult furthermore vpn outperforms deep network dqn several atari games even short lookahead planning demonstrating potential new way learning good state representation
variable importance using decision trees decision trees random forests well established models offer good predictive performance also provide rich feature importance information practitioners often employ variable importance methods rely impurity based information methods remain poorly characterized theoretical perspective provide novel insights performance methods deriving finite sample performance guarantees high dimensional setting various modeling assumptions demonstrate effectiveness impurity based methods via extensive set simulations
expressive power neural networks view width expressive power neural networks important understanding deep learning existing works consider problem view depth network paper study width affects expressiveness neural networks classical results state emph depth bounded depth networks suitable activation functions universal approximators show universal approximation theorem emph width bounded relu networks width relu networks input dimension universal approximators moreover except measure set functions cannot approximated width relu networks exhibits phase transition several recent works demonstrate benefits depth proving depth efficiency neural networks classes deep networks cannot realized shallow network whose size emph exponential bound pose dual question width efficiency relu networks wide networks cannot realized narrow networks whose size substantially larger show exist classes wide networks cannot realized narrow network whose depth emph polynomial bound hand demonstrate extensive experiments narrow networks whose depth exceed polynomial bound constant factor approximate wide shallow network high accuracy results provide comprehensive evidence depth effective width expressiveness relu networks
sgd learns conjugate kernel class network show standard stochastic gradient decent sgd algorithm guaranteed learn polynomial time function competitive best function conjugate kernel space network defined daniely frostig singer result holds log depth networks rich family architectures best knowledge first polynomial time guarantee standard neural network learning algorithm networks depth corollaries follows neural networks depth log sgd guaranteed learn polynomial time constant degree polynomials polynomially bounded coefficients likewise follows sgd large enough networks learn continuous function polynomial time complementing classical expressivity results
radon machines effective parallelisation machine learning order simplify adaptation learning algorithms growing amounts data well growing need accurate confident predictions critical applications paper propose novel provably effective parallelisation scheme contrast parallelisation techniques scheme applied broad class learning algorithms without mathematical derivations without writing single line additional code achieve treating learning algorithm black box applied parallel random data subsets resulting hypotheses assigned leaves aggregation tree bottom replaces set hypotheses corresponding inner node tree radon point considering confidence parameters epsilon delta input learning algorithm efficient sample complexity polynomial epsilon delta time complexity polynomial sample complexity parallelisation scheme algorithm achieve guarantees applied polynomial number cores polylogarithmic time result allows effective parallelisation broad class learning algorithms intrinsically related nick class decision problems well learnability exact learning cost parallelisation form slightly larger sample complexity empirical study confirms potential parallisation scheme range data sets several learning algorithms
noise tolerant interactive learning using pairwise comparisons study problem interactively learning binary classifier using noisy labeling pairwise comparison oracles comparison oracle answers given instances likely positive learning oracles multiple applications obtaining direct labels harder pairwise comparisons easier algorithm leverage types oracles paper attempt characterize access easier comparison oracle helps improving label total query complexity show comparison oracle reduces learning problem learning threshold function present algorithm interactively queries label comparison oracles characterize query complexity tsybakov adversarial noise conditions comparison labeling oracles lower bounds show label total query complexity almost optimal
pac bayesian analysis randomized learning application stochastic gradient descent analyze generalization properties randomized learning algorithms focusing stochastic gradient descent sgd using novel combination pac bayes algorithmic stability importantly risk bounds hold posterior distributions algorithm hyperparameters including distributions depend training data inspires adaptive sampling algorithm sgd optimizes posterior runtime analyze algorithm context risk bounds evaluate empirically benchmark dataset
revisiting perceptron efficient label optimal learning halfspaces long standing problem efficiently learn linear separator using labels possible presence noise work propose efficient perceptron based algorithm actively learning homogeneous linear separators uniform distribution bounded noise label flipped probability eta algorithm achieves near optimal tilde frac 2eta frac epsilon label complexity time tilde frac epsilon 2eta adversarial noise tilde omega epsilon fraction labels flipped algorithm achieves near optimal tilde frac epsilon label complexity time tilde frac epsilon furthermore show active learning algorithm converted efficient passive learning algorithm near optimal sample complexity respect epsilon
sample computationally efficient learning algorithms concave distributions provide new results noise tolerant sample efficient learning algorithms concave distributions new class concave distributions broad natural generalization log concavity includes many important additional distributions pareto distribution distribution class studied context efficient sampling integration optimization much remains unknown geometry class distributions applications context learning challenge unlike commonly used distributions learning uniform generally log concave distributions broader class closed marginalization operator many distributions fat tailed work introduce new convex geometry tools study properties concave distributions use properties provide bounds quantities interest learning including probability disagreement halfspaces disagreement outside band disagreement coefficient use results significantly generalize prior results margin based active learning disagreement based active learning passively learning intersections halfspaces analysis geometric properties concave distributions might independent interest optimization broadly
nearest neighbor sample compression efficiency consistency infinite dimensions examine bayes consistency recently proposed nearest neighbor based multiclass learning algorithm algorithm derived sample compression bounds enjoys statistical advantages tight fully empirical generalization bounds well algorithmic advantages runtime memory savings prove algorithm strongly bayes consistent metric spaces finite doubling dimension first consistency result efficient nearest neighbor sample compression scheme rather surprisingly discover algorithm continues bayes consistent even certain infinite dimensional setting basic measure theoretic conditions classic consistency proofs hinge violated surprising since known bayes consistent setting pose several challenging open problems future research
learning identifiable gaussian bayesian networks polynomial time sample complexity learning directed acyclic graph dag structure bayesian network observational data notoriously difficult problem many non identifiability hardness results known paper propose provably polynomial time algorithm learning sparse gaussian bayesian networks equal noise variance class bayesian networks dag structure uniquely identified observational data high dimensional settings show log number samples suffices method recover true dag structure high probability number variables maximum markov blanket size obtain theoretical guarantees condition called emph restricted strong adjacency faithfulness rsaf strictly weaker strong faithfulness condition methods based conditional independence testing need success sample complexity method matches information theoretic limits terms dependence validate theoretical findings synthetic experiments
world graph discovering statistical structure links fundamental problem analysis social networks choosing misspecified model equivalently incorrect inference algorithm result invalid analysis even falsely uncover patterns fact artifacts model work focuses unifying widely used link formation models stochastic block model sbm small world latent space model swm integrating techniques kernel learning spectral graph theory nonlinear dimensionality reduction develop first statistically sound polynomial time algorithm discover latent patterns sparse graphs models network comes sbm algorithm outputs block structure swm algorithm outputs estimates node latent position
learning uncertain curves wasserstein metric gaussian processes introduce novel framework statistical analysis populations non degenerate gaussian processes gps natural representations uncertain curves allows inherent variation uncertainty function valued data properly incorporated population analysis using wasserstein metric geometrize space gps mean covariance functions compact index spaces prove existence uniqueness barycenter population gps well convergence metric barycenter finite dimensional counterparts justifies practical computations finally demonstrate framework experimental validation datasets representing brain connectivity climate change source code released upon publication
clustering network valued data community detection focuses clustering nodes detecting communities mostly single network problem considerable practical interest received great deal attention research community able cluster within network important emerging needs able cluster multiple networks largely motivated routine collection network data generated potentially different populations networks node correspondence node correspondence present cluster summarizing network graphon estimate whereas node correspondence present propose novel solution clustering networks associating computationally feasible feature vector network based trace powers adjacency matrix illustrate methods simulated real data sets theoretical justifications given terms consistency
power truncated svd general high rank matrix estimation problems show given estimate widehat mat close general high rank positive semi definite psd matrix mat spectral norm widehat mat mat leq delta simple truncated singular value decomposition widehat mat produces multiplicative approximation mat frobenius norm observation leads many interesting results general high rank matrix estimation problems high rank matrix completion show possible recover general high rank matrix mat varepsilon relative error frobenius norm partial observations sample complexity independent spectral gap mat high rank matrix denoising design algorithms recovers matrix mat relative error frobenius norm noise perturbed observations without assuming mat exactly low rank low dimensional estimation high dimensional covariance given samples dimension mathcal n_n mat mat show possible estimate covariance matrix mat relative error frobenius norm approx improving classical covariance estimation results requires approx
adagan boosting generative models generative adversarial networks gan effective method training generative models complex data natural images however notoriously hard train suffer problem missing modes model able produce examples certain regions space propose iterative procedure called adagan every step add new component mixture model running gan algorithm weighted sample inspired boosting algorithms many potentially weak individual predictors greedily aggregated form strong composite predictor prove analytically incremental procedure leads convergence true distribution finite number steps step optimal convergence exponential rate otherwise also illustrate experimentally procedure addresses problem missing modes
discovering potential influence via information bottleneck discovering potential influence variable another variable fundamental scientific practical interest existing correlation measures suitable discovering average influence fail discover potential influences bridge gap postulate set natural axioms expect measure potential influence satisfy show rate information bottleneck hypercontractivity coefficient satisfies proposed axioms iii provide novel estimator estimate hypercontractivity coefficient samples numerical experiments demonstrate proposed estimator discovers potential influence various indicators datasets robust discovering gene interactions gene expression time series data statistically powerful estimators correlation measures binary hypothesis testing canonical potential influences
phase transitions pooled data problem paper study pooled data problem identifying labels associated large collection items based sequence pooled tests revealing counts label within pool noiseless setting exact recovery identify exact asymptotic threshold required number tests optimal decoding prove phase transition complete success complete failure addition present novel noisy variation problem provide information theoretic framework characterizing required number tests general noise models results reveal noise make problem considerably difficult strict increases scaling laws even low noise levels
coded distributed computing inverse problems computationally intensive distributed parallel computing often bottlenecked small set slow workers known stragglers paper utilize emerging idea coded computation design novel error correcting code inspired technique solving linear inverse problems specific iterative methods parallelized implementation affected stragglers example applications include inverse problems personalized pagerank sampling graphs provably show coded computation technique reduce mean squared error computational deadline constraint fact ratio mean squared error replication based coded techniques diverges infinity deadline increases experiments personalized pagerank performed real systems real social networks show ratio large unlike coded computation techniques proposed thus far strategy combines outputs workers including stragglers produce accurate estimates computational deadline also ensures accuracy degrades gracefully event number stragglers large
revisit fuzzy neural network demystifying batch normalization relu generalized hamming network revisit fuzzy neural network cornerstone notion textit generalized hamming distance provides novel theoretically justified approach rectifying understanding traditional neural computing turns many useful neural network methods batch normalization rectified linear units could interpreted new framework rectified generalized hamming network gnn proposed accordingly ghn lends rigiour analysis within fuzzy logics theory also demonstrates superior performances variety learning tasks terms fast learning speed well controlled behaviour simple parameter settings
posterior sampling reinforcement learning worst case regret bounds present algorithm based posterior sampling aka thompson sampling achieves near optimal worst case regret bounds underlying markov decision process mdp communicating finite though unknown diameter main result high probability regret upper bound tilde sqrt sat communicating mdp states actions diameter regret compares total reward achieved algorithm total expected reward optimal infinite horizon undiscounted average reward policy time horizon result improves best previously known upper bound tilde sqrt achieved algorithm setting matches dependence established lower bound omega sqrt dsat problem
framework multi rmed andit testing online fdr control propose alternative framework existing setups controlling false alarms multiple tests run time setup arises many practical applications pharmaceutical companies test new treatment options control pills different diseases internet companies test default webpages versus various alternatives time framework proposes replace sequence tests sequence best arm mab instances continuously monitored data scientist interleaving mab tests online false discovery rate fdr algorithm obtain best worlds low sample complexity time online fdr control main contributions propose reasonable definitions null hypothesis mab instances demonstrate derive always valid sequential value allows continuous monitoring mab test iii show using rejection thresholds online fdr algorithms confidence levels mab algorithms results sample optimality high power low fdr point time run extensive simulations verify claims also report results real data collected new yorker cartoon caption contest
monte carlo tree search best arm identification recent advances bandit tools techniques sequential learning steadily enabling new applications promising resolution range challenging related problems study game tree search problem goal quickly identify optimal move given game tree sequentially sampling stochastic payoffs develop new algorithms trees arbitrary depth operate summarizing deeper levels tree confidence intervals depth applying best arm identification procedure root prove new sample complexity guarantees refined dependence problem instance show experimentally algorithms outperform existing elimination based algorithms match previous special purpose methods depth trees
minimal exploration structured stochastic bandits paper introduces addresses wide class stochastic bandit problems function mapping arm corresponding reward exhibits known structural properties existing structures linear lipschitz unimodal combinatorial dueling covered framework derive asymptotic instance specific regret lower bound problems develop ossb algorithm whose regret matches fundamental limit ossb based classical principle optimism face uncertainty thompson sampling rather aims matching minimal exploration rates sub optimal arms characterized derivation regret lower bound illustrate efficiency ossb using numerical experiments case linear bandit problem show ossb outperforms existing algorithms including thompson sampling
regret analysis continuous dueling bandit dueling bandit learning framework feedback information learning process restricted noisy comparison pair actions paper address dueling bandit problem based cost function continuous space propose stochastic mirror descent algorithm show algorithm achieves sqrt log regret bound strong convexity smoothness assumptions cost function clarify equivalence regret minimization dueling bandit convex optimization cost function moreover considering lower bound convex optimization turned algorithm achieves optimal convergence rate convex optimization optimal regret dueling bandit except logarithmic factor
elementary symmetric polynomials optimal experimental design revisit classical problem optimal experimental design oed new mathematical model grounded geometric motivation specifically introduce models based elementary symmetric polynomials polynomials capture partial volumes offer graded interpolation widely used optimal optimal design models obtaining special cases analyze properties models derive greedy convex relaxation algorithms computing associated designs analysis establishes approximation guarantees algorithms empirical results substantiate claims demonstrate curious phenomenon concerning greedy algorithm finally byproduct obtain new results theory elementary symmetric polynomials independent interest
online learning linear dynamical systems present efficient practical algorithm online prediction discrete time linear dynamical systems despite non convex optimization problem using improper learning convex relaxation algorithm comes provable guarantees near optimal regret bounds compared best lds hindsight overparameterizing small logarithmic factor analysis brings together ideas improper learning convex relaxations online regret minimization spectral theory hankel matrices
efficient flexible inference stochastic systems many real world dynamical systems described stochastic differential equations thus parameter inference challenging important problem many disciplines provide grid free flexible algorithm offering parameter state inference stochastic systems compare approch based variational approximations state art methods showing significant advantages runtime accuracy
group sparse additive machine family learning algorithms generated additive models attracted much attention recently flexibility interpretability high dimensional data analysis among learning models grouped variables shown competitive performance prediction variable selection however previous works mainly focus least squares regression problem classification task thus desired design new additive classification model variable selection capability many real world applications focus high dimensional data classification address challenging problem paper investigate classification group sparse additive models reproducing kernel hilbert spaces novel classification method called emph group sparse additive machine groupsam proposed explore utilize structure information among input variables generalization error bound derived proved integrating sample error analysis empirical covering numbers hypothesis error estimate stepping stone technique new bound shows groupsam achieve satisfactory learning rate polynomial decay experimental results synthetic data benchmark datasets consistently show effectiveness new approach
bregman divergence stochastic variance reduction saddle point adversarial prediction adversarial machines learner competes adversary gained much recent interest machine learning naturally form saddle point optimization often separable structure sometimes also unmanageably large dimension work show adversarial prediction multivariate losses solved much faster used first reduce problem size exponentially using appropriate sufficient statistics adapt new stochastic variance reduced algorithm balamurugan bach 2016 allow bregman divergence prove linear rate convergence retained show adversarial prediction using divergence achieve speedup example times compared euclidean alternative verify theoretical findings extensive experiments example applications adversarial prediction lpboosting
online multiclass boosting recent work extended theoretical analysis boosting algorithms multiclass problems online settings however multiclass extension batch setting online extensions consider binary classification fill gap literature defining justifying weak learning condition online multiclass boosting condition leads optimal boosting algorithm requires minimal number weak learners achieve certain accuracy additionally propose adaptive algorithm near optimal enjoys excellent performance real data due adaptive property
universal consistency minimax rates online mondrian forest establish consistency algorithm mondrian forest cite lakshminarayanan2014mondrianforests lakshminarayanan2016mondrianuncertainty randomized classification algorithm implemented online first amend original mondrian forest algorithm proposed cite lakshminarayanan2014mondrianforests considers emph fixed lifetime parameter indeed fact parameter fixed actually hinders statistical consistency original procedure modified mondrian forest algorithm grows trees increasing lifetime parameters lambda_n uses alternative updating rule allowing work also online fashion second provide theoretical analysis establishing simple conditions consistency theoretical analysis also exhibits surprising fact algorithm achieves minimax rate optimal rate estimation lipschitz regression function strong extension previous results cite arlot2014purf_bias emph arbitrary dimension
mean teachers better role models weight averaged consistency targets improve semi supervised deep learning results recently proposed temporal ensembling achieved state art results several semi supervised learning benchmarks maintains exponential moving average label predictions training example penalizes predictions inconsistent target however targets change per epoch temporal ensembling becomes unwieldy learning large datasets overcome problem propose mean teacher method averages model weights instead label predictions additional benefit mean teacher improves test accuracy enables training fewer labels temporal ensembling mean teacher achieves error rate svhn 250 labels better temporal ensembling 1000 labels
learning complementary labels collecting labeled data costly thus critical bottleneck real world classification tasks mitigate problem consider complementary label specifies class pattern belong collecting complementary labels would less laborious ordinary labels since users carefully choose correct class many candidate classes however complementary labels less informative ordinary labels thus suitable approach needed better learn complementary labels paper show unbiased estimator classification risk obtained complementary labels loss function satisfies particular symmetric condition theoretically prove estimation error bounds proposed method experimentally demonstrate usefulness proposed algorithms
positive unlabeled learning non negative risk estimator emph positive emph unlabeled data binary classifier trained learning state art emph unbiased learning however model flexible empirical risk training data negative suffer serious overfitting paper propose emph non negative risk estimator learning minimized robust overfitting thus able train flexible models given limited data moreover analyze emph bias emph consistency emph mean squared error reduction proposed risk estimator emph estimation error corresponding risk minimizer experiments show proposed risk estimator successfully fixes overfitting problem unbiased counterparts
semisupervised clustering queries locally encodable source coding source coding canonical problem data compression information theory locally encodable source coding compressed bit depends bits input paper show recently popular model semisupervised clustering equivalent locally encodable source coding model task perform multiclass labeling unlabeled elements beginning ask parallel set simple queries oracle provides possibly erroneous binary answers queries queries cannot involve fixed constant number delta elements labeling elements clustering must done based noisy query answers goal recover correct labelings minimizing number queries equivalence locally encodable source codes leads find lower bounds number queries required variety scenarios also able show fundamental limitations pairwise cluster queries propose pairwise queries provably performs better
learning errors structured prediction approximate inference work try understand differences exact approximate inference algorithms structured prediction compare estimation approximation error underestimate overestimate models result shows perspective learning errors performances approximate inference could good exact inference error analyses also suggest new margin existing learning algorithms empirical evaluations text classification sequential labelling dependency parsing witness success approximate inference benefit proposed margin
optimal generalizability parametric learning consider parametric learning problem objective learner determined parametric loss function employing empirical risk minimization possibly regularization inferred parameter vector biased toward training samples bias measured cross validation procedure practice data set partitioned training set used training validation set used training left measure sample performance classical cross validation strategy leave cross validation loocv sample left validation training done rest samples presented learner process repeated samples loocv rarely used practice due high computational complexity paper first develop computationally efficient approximate loocv aloocv provide theoretical guarantees performance use aloocv provide optimization algorithm finding optimal regularizer empirical risk minimization framework numerical experiments illustrate accuracy efficiency aloocv well proposed framework optimal regularizer
multi objective non parametric sequential prediction online learning research mainly focusing minimizing objective function many real world applications however several objective functions considered simultaneously recently algorithm dealing several objective functions case presented paper extend multi objective framework case stationary ergodic processes thus allowing dependencies among observations first identify asymptomatic lower bound prediction strategy present algorithm whose predictions achieve optimal solution fulfilling continuous convex constraining criterion
fixed rank approximation positive semidefinite matrix streaming data several important applications streaming pca semidefinite programming involve large scale positive semidefinite psd matrix presented sequence linear updates storage limitations possible retain sketch psd matrix paper develops new algorithm fixed rank psd approximation sketch approach combines nystr approximation novel mechanism rank truncation theoretical analysis establishes proposed method achieve prescribed relative error schatten norm exploits spectral decay input matrix computer experiments show proposed method dominates alternative techniques fixed rank psd matrix approximation across wide range examples
communication efficient stochastic gradient descent applications neural networks parallel implementations stochastic gradient descent sgd received significant research attention thanks excellent scalability properties fundamental barrier parallelizing sgd high bandwidth cost communicating gradient updates nodes consequently several lossy compresion heuristics proposed nodes communicate quantized gradients although effective practice heuristics always guarantee convergence clear whether improved paper propose quantized sgd qsgd family compression schemes gradient updates provides convergence guarantees qsgd allows user smoothly trade emph communication bandwidth emph convergence time nodes adjust number bits sent per iteration cost possibly higher variance show trade inherent sense improving past threshold would violate information theoretic lower bounds qsgd guarantees convergence convex non convex objectives asynchrony extended stochastic variance reduced techniques applied training deep neural networks image classification automated speech recognition qsgd leads significant reductions end end training time example 16gpus train resnet152 network full accuracy imagenet faster full precision variant
machine learning adversaries byzantine tolerant gradient descent study resilience byzantine failures distributed implementations stochastic gradient descent sgd far distributed machine learning frameworks largely ignored possibility failures especially arbitrary byzantine ones causes failures include software bugs network asynchrony biases local datasets well attackers trying compromise entire system assuming set workers byzantine ask resilient sgd without limiting dimension size parameter space first show gradient aggregation rule based linear combination vectors proposed workers current approaches tolerates single byzantine failure formulate resilience property aggregation rule capturing basic requirements guarantee convergence despite byzantine workers propose emph krum aggregation rule satisfies resilience property argue first provably byzantine resilient algorithm distributed sgd also report experimental evaluations krum
ranking data continuous labels oriented recursive partitions formulate supervised learning problem referred continuous ranking continuous real valued label assigned observable taking values feature space goal order possible observations means scoring function tend increase decrease together highest probability problem generalizes multi partite ranking certain extent task finding optimal scoring functions naturally cast optimization dedicated functional cri terion called iroc curve maximization kendall related pair theoretical side describe optimal elements problem provide statistical guarantees empirical kendall maximiza tion appropriate conditions class scoring function candidates also propose recursive statistical learning algorithm tailored empirical iroc curve optimization producing piecewise constant scoring function fully described oriented binary tree preliminary numerical experiments highlight difference nature regression continuous ranking provide strong empirical evidence performance empirical optimizers criteria proposed
practical data dependent metric compression provable guarantees introduce new distance preserving compact representation multi dimensional point sets given points dimensional space coordinate represented using bits bits per point produces representation size log epsilon log bits per point approximate distances factor epsilon algorithm almost matches recent bound indyk 2017 much simpler compare algorithm product quantization jegou 2011 state art heuristic metric compression method evaluate algorithms several data sets sift mnist new york city taxi time series synthetic dimensional data set embedded high dimensional space algorithm produces representations comparable better produced provable guarantees performance
clustering stable instances euclidean means euclidean means problem arguably widely studied clustering problem machine learning means objective hard worst case practitioners enjoyed remarkable success applying heuristics like lloyd algorithm problem address disconnect study following question properties real world instances enable design efficient algorithms prove guarantees finding optimal clustering consider natural notion called additive perturbation stability believe captures many practical instances euclidean means clustering stable instances unique optimal means solutions change even point perturbed little euclidean distance captures property means optimal solution tolerant measurement errors uncertainty points design efficient algorithms provably recover optimal clustering instances additive perturbation stable instance additional separation design simple efficient algorithm provable guarantees also robust outliers also complement results studying amount stability real datasets demonstrating algorithm performs well benchmark datasets
distributed hierarchical clustering graph clustering fundamental task many data mining machine learning pipelines particular identifying good hierarchical structure time fundamental challenging problem several applications amount data analyze increasing astonishing rate day hence need new solutions efficiently compute effective hierarchical clusterings huge data main focus paper minimum spanning tree mst based clusterings particular propose affinity novel hierarchical clustering based boruvka mst algorithm prove certain theoretical guarantees affinity well classic algorithms show practice superior several state art clustering algorithms furthermore present mapreduce algorithms affinity first works case input graph dense takes constant rounds based mst algorithm dense graphs improves upon prior work karloff second algorithm assumption density input graph finds affinity clustering log rounds using distributed hash tables dhts show experimentally algorithms scalable huge data sets
sparse means embedding means clustering algorithm ubiquitous tool data mining machine learning shows promising performance however high computational cost hindered applications broad domains researchers successfully addressed obstacles dimensionality reduction methods recently cite dblp journals tit boutsidiszmd15 develop state art random projection method faster means clustering method delivers many improvements dimensionality reduction methods example compared advanced singular value decomposition based feature extraction approach cite dblp journals tit boutsidiszmd15 reduce running time factor min epsilon log data matrix mathbb times data points features losing factor approximation accuracy unfortunately still require mathcal frac ndk epsilon 2log matrix multiplication cost prohibitive large values break bottleneck carefully build sparse embedded means clustering algorithm requires mathcal nnz nnz denotes number non zeros fast matrix multiplication moreover proposed algorithm improves cite dblp journals tit boutsidiszmd15 results approximation accuracy factor empirical studies corroborate theoretical findings demonstrate approach able significantly accelerate means clustering achieving satisfactory clustering performance
medoids means seeding show experimentally algorithm clarans han 1994 finds better medoids solutions voronoi iteration algorithm hastie 2001 finding along similarity voronoi iteration algorithm lloyd means algorithm motivates use clarans means initializer show clarans outperforms algorithms datasets mean decrease means initialization mean squared error mse final mse introduce algorithmic improvements clarans improve complexity runtime making extremely viable initialization scheme large datasets
applied algorithmic foundation hierarchical clustering hierarchical clustering data analysis method used decades despite widespread use lack analytical foundation method foundation would support methods currently used guide future improvements paper gives applied algorithmic foundation hierarchical clustering goal paper give analytic framework supporting observations seen practice paper considers dual problem framework hierarchical clustering introduced dasgupta main results popular algorithms used practice average linkage agglomerative clustering small constant approximation ratio paper establishes using recursive means divisive clustering poor lower bound approximation ratio perhaps explaining popular practice motivated poor performance means seek find divisive algorithms perform well theoretically paper gives constant approximation algorithms paper represents first work giving foundation hierarchical clustering algorithms used practice
inhomogoenous hypergraph clustering applications hypergraph partitioning important problem machine learning computer vision network analytics widely used method hypergraph partitioning relies minimizing normalized sum costs partitioning hyperedges across clusters algorithmic solutions based approach assume different partitions hyperedge incur cost however assumption fails leverage fact different subsets vertices within hyperedge different structural importance hence propose new hypergraph clustering technique termed inhomogeneous hypergraph partitioning assigns different costs different hyperedge cuts prove inhomogeneous partitioning produces quadratic approximation optimal solution inhomogeneous costs satisfy submodularity constraints moreover demonstrate inhomogenous partitioning offers significant performance improvements applications structure learning rankings subspace segmentation motif clustering
subspace clustering via tangent cones given samples lying near number fixed subspaces subspace clustering task grouping samples based corresponding subspaces many subspace clustering methods operate assigning affinity pair points feeding affinities common clustering algorithm paper proposes new paradigm subspace clustering computes affinities based underlying conic geometry union subspaces proposed conic subspace clustering csc approach considers convex hull collection normalized data points tangent cones sample union subspaces underlying data imposes strong association tangent cone point original subspace containing addition describing novel geometric perspective paper provides practical algorithm subspace clustering leverages perspective tangent cone membership test estimate affinities algorithm accompanied deterministic stochastic guarantees properties learned affinity matrix directly translate overall clustering accuracy
tensor biclustering consider dataset data collected multiple features multiple individuals multiple times type data represented dimensional individual feature time tensor become increasingly prominent various areas science tensor biclustering problem computes subset individuals subset features whose signal trajectories time lie low dimensional subspace modeling similarity among signal trajectories allowing different scalings across different individuals different features study information theoretic limit problem generative model moreover propose efficient spectral algorithm solve tensor biclustering problem analyze achievability bound asymptotic regime finally show efficiency proposed method several synthetic real datasets
unified approach interpreting model predictions understanding model made certain prediction crucial many applications however large modern datasets best accuracy often achieved complex models even experts struggle interpret ensemble deep learning models creates tension accuracy interpretability response variety methods recently proposed help users interpret predictions complex models present unified framework interpreting predictions namely shap shapley additive explanations assigns feature importance particular prediction key components shap framework identification class additive feature importance measures theoretical results unique solution class set desired properties class unifies existing methods several recent methods class desired properties means framework inform development new methods explaining prediction models demonstrate several new methods presented paper based shap framework show better computational performance better consistency human intuition existing methods
efficient sublinear regret algorithms online sparse linear regression online sparse linear regression task applying linear regression analysis examples arriving sequentially subject resource constraint limited number features examples observed despite importance many practical applications recently shown polynomial time sublinear regret algorithm unless subseteq bpp exponential time sublinear regret algorithm known paper introduce mild assumptions solve problem assumptions present polynomial time sublinear regret algorithms online sparse linear regression addition thorough experiments publically available data demonstrate algorithms outperform known algorithms
unbiased estimates linear regression via volume sampling given full rank matrix columns rows consider task estimating pseudo inverse based pseudo inverse sampled subset columns size least number rows show possible subset columns chosen proportional squared volume spanned rows chosen submatrix volume sampling resulting estimator unbiased surprisingly covariance estimator also closed form equals specific factor times top pseudo inverse plays important part solving linear least squares problem try predict label column assume labels expensive given labels small subset columns sample using methods show weight vector solution sub problem unbiased estimator optimal solution whole problem based column labels believe new formulas establish fundamental connection linear least squares volume sampling use methods obtain algorithm volume sampling faster state art obtaining bounds total loss estimated least squares solution labeled columns
separability loss functions revisiting discriminative generative models revisit classical analysis generative discriminative models general exponential families high dimensional settings towards develop novel technical machinery including notion separability general loss functions allow provide general framework obtain convergence rates general estimators use machinery analyze convergence rates generative discriminative models provide insights nuanced behaviors high dimensions results also applicable differential parameter estimation quantity interest difference generative model parameters
generalized linear model regression distance set penalties estimation generalized linear models glm complicated presence constraints handle constraints maximizing penalized log likelihood penalties lasso effective high dimensions often lead severe shrinkage paper explores instead penalizing squared distance constraint sets distance penalties flexible algebraic regularization penalties avoid drawback shrinkage optimize distance penalized objectives make use majorization minimization principle resulting algorithms constructed within framework amenable acceleration come global convergence guarantees applications shape constraints sparse regression rank restricted matrix regression synthetic real data showcase strong empirical performance distance penalization even non convex constraints
group additive structure identification kernel nonparametric regression additive model popularly used models high dimensional nonparametric regression analysis however main drawback neglects possible interactions predictor variables paper reexamine group additive model proposed literature rigorously define intrinsic group additive structure relationship response variable predictor vector vect develop effective structure penalized kernel method simultaneous identification intrinsic group additive structure nonparametric function estimation method utilizes novel complexity measure derive group additive structures show proposed method consistent identifying intrinsic group additive structure simulation study real data applications demonstrate effectiveness proposed method general tool high dimensional nonparametric regression
learning overcomplete hmms study basic problem learning overcomplete hmms many hidden states small output alphabet despite significant practical importance hmms poorly understood known positive negative results efficient learning paper present several new results positive negative help define boundaries tractable learning setting intractable setting show positive results large subclass hmms whose transition matrices sparse well conditioned small probability mass short cycles also show learning impossible given polynomial number samples hmms small output alphabet whose transition matrices random regular graphs large degree
matrix norm estimation entries singular values data matrix form provide insights structure data effective dimensionality choice hyper parameters higher level data analysis tools however many practical applications collaborative filtering network analysis get partial observation scenarios consider fundamental problem recovering various spectral properties underlying matrix sampling entries propose framework first estimating schatten norms matrix several values using surrogates estimating spectral properties interest spectrum rank paper focuses technical challenges accurately estimating schatten norms sampling matrix introduce novel unbiased estimator based counting small structures graph provide guarantees match empirical performances theoretical analysis shows schatten norms recovered accurately strictly smaller number samples compared needed recover underlying low rank matrix numerical experiments suggest significantly improve upon competing approach using matrix completion methods
optimal shrinkage singular values random data contamination low rank matrix contaminated uniformly distributed noise missing values outliers corrupt entries reconstruction singular values singular vectors contaminated matrix key problem machine learning computer vision data science paper show common contamination models including arbitrary combinations uniform noise missing values outliers corrupt entries described efficiently using single framework develop asymptotically optimal algorithm estimates manipulation singular values applies contamination models considered finally find explicit signal noise cutoff estimation singular value decomposition must fail well defined sense
new theory nonconvex matrix completion prevalent matrix completion theories reply assumption locations missing data distributed uniformly randomly uniform sampling nevertheless reason observations missing often depends unseen observations thus missing data practice usually occurs nonuniform fashion rather randomly break limits randomness assumption paper introduces new hypothesis called isomeric condition provably weaker randomness assumption arguably holds even missing data placed irregularly equipped new tool prove series theorems missing data recovery matrix completion particular prove exact solutions identify target matrix included critical points commonly used nonconvex programs unlike existing nonconvex theories use condition convex programs theories show nonconvex programs work much weaker condition comparing existing theories nonuniform sampling theories flexible powerful
learning low dimensional metrics paper investigates theoretical foundations metric learning focused key questions fully addressed prior work consider learning general low dimensional low rank metrics well sparse metrics develop upper lower minimax bounds generalization error quantify sample complexity metric learning terms dimension feature space dimension rank underlying metric also bound accuracy learned metric relative underlying true generative metric results involve novel mathematical approaches metric learning problem also shed new light special case ordinal embedding aka non metric multidimensional scaling
fast alternating minimization algorithms dictionary learning present theoretical guarantees alternating minimization algorithm dictionary learning sparse coding problem dictionary learning problem factorize samples appropriate basis dictionary times sparse vector algorithm simple alternating minimization procedure switching gradient descent ell_1 minimization every step dictionary learning specifically alternating minimization algorithms dictionary learning well studied theoretically empirically however contrast previous theoretical analysis problem replace condition operator norm true underlying dictionary condition matrix infinity norm allows get convergence rates terms error estimated dictionary infinity norm also allows initialize randomly converge globally optimum guarantees reasonable generative model allow dictionaries growing operator norms handle arbitrary level overcompleteness sparsity information theoretically optimal incoherent dictionaries also present statistical guarantees present sample complexity guarantees algorithm
consistent robust regression present first efficient provably consistent estimator robust regression problem area robust learning optimization generated significant amount interest learning statistics communities recent years owing applicability scenarios corrupted data well handling model mis specifications particular special interest devoted fundamental problem robust linear regression estimators tolerate corruption constant fraction response variables widely studied surprisingly however date aware polynomial time estimator offers consistent estimate presence dense unbounded corruptions work present estimator called crr solves open problem put forward work bhatia 2015 consistency analysis requires novel stage proof technique involving careful analysis stability ordered lists independent interest show crr offers consistent estimates empirically far superior several recently proposed algorithms robust regression problem including extended lasso torrent algorithm comparison crr offers comparable better model recovery runtimes faster order magnitude
partial hard thresholding towards unified analysis support recovery machine learning compressed sensing central importance understand tractable algorithm recovers support sparse signal compressed measurements paper present towards principled analysis support recovery performance family hard thresholding algorithms end appeal partial hard thresholding pht operator proposed recently jain ieee trans information theory 2017 show proper conditions pht recovers arbitrary sparse signal within kappa log kappa iterations kappa condition number specializing pht operator obtain best known result hard thresholding pursuit orthogonal matching pursuit replacement experiments simulated data complement theoretical findings also illustrate interesting phase transition iteration number cannot significantly reduced
minimax estimation bandable precision matrices inverse covariance matrix provides considerable insight understanding statistical models multivariate setting particular distribution variables assumed multivariate normal sparsity pattern inverse covariance matrix commonly referred precision matrix corresponds adjacency matrix representation gauss markov graph encodes conditional independence statements variables minimax results spectral norm previously established covariance matrices sparse banded sparse precision matrices establish minimax estimation bounds estimating banded precision matrices spectral norm results greatly improve upon existing bounds particular find minimax rate estimating banded precision matrices matches estimating banded covariance matrices key insight analysis able obtain barely noisy estimates times subblocks precision matrix inverting slightly wider blocks empirical covariance matrix along diagonal theoretical results complemented experiments demonstrating sharpness bounds
diffusion approximations online principal component estimation global convergence paper propose adopt diffusion approximation tools study dynamics oja iteration online stochastic gradient method principal component analysis oja iteration maintains running estimate true principal component streaming data enjoys less temporal spatial complexities show oja iteration top eigenvector generates continuous state discrete time markov chain unit sphere characterize oja iteration phases using diffusion approximation weak convergence tools phase analysis provides finite sample error bound running estimate matches minimax information lower bound pca bounded noise
learning koopman invariant subspaces dynamic mode decomposition spectral decomposition koopman operator attracting attention tool analysis nonlinear dynamical systems dynamic mode decomposition popular numerical algorithm koopman spectral analysis however often need prepare nonlinear observables manually according underlying dynamics always possible since priori knowledge paper propose fully data driven method koopman spectral analysis based principle learning koopman invariant subspaces observed data end propose minimization residual sum squares linear least squares regression estimate set functions transforms data form linear regression fits well introduce implementation neural networks evaluate performance empirically using nonlinear dynamical systems applications
stochastic approximation canonical correlation analysis propose novel first order stochastic approximation algorithms canonical correlation analysis cca algorithms presented instances noisy matrix stochastic gradient msg noisy matrix exponential gradient meg achieve epsilon suboptimality population objective time poly epsilon delta probability least delta input dimensionality also consider practical variants proposed algorithms compare methods cca theoretically empirically
diving shallows computational perspective large scale shallow learning remarkable recent success deep neural networks easy analyze theoretically particularly hard disentangle relative significance architecture optimization achieving accurate classification large datasets flip side shallow methods kernel methods encountered obstacles scaling large data practical methods variants gradient descent used successfully deep learning seem perform par applied kernel methods difficulty sometimes attributed limitations shallow architecture paper first identify basic limitation gradient descent based optimization methods used conjunctions smooth kernels analysis demonstrates vanishingly small fraction function space reachable polynomial number gradient descent iterations drastically limits approximating power gradient descent fixed computational budget leading serious regularization issue purely algorithmic persisting even limit infinite data address shortcoming practice introduce eigenpro iteration based simple direct preconditioning scheme using small number approximate eigenvectors also viewed learning new kernel optimized gradient descent turns injecting small amount approximate second order information leads major improvements convergence large data translates significant performance boost state art kernel methods particular able match improve results recently reported literature small fraction computational budget finally feel results show need broader computational perspective modern large scale learning complement traditional statistical convergence analyses
unreasonable effectiveness structured random orthogonal embeddings examine class embeddings based structured random matrices orthogonal rows applied many machine learning applications including dimensionality reduction kernel approximation johnson lindenstrauss transform angular kernel show select matrices yielding guaranteed improved performance accuracy speed compared earlier methods introduce matrices complex entries give significant accuracy improvement provide geometric markov chain based perspectives help understand benefits empirical results suggest approach helpful wider range applications
generalization properties learning random features study generalization properties ridge regression random features statistical learning framework show first time sqrt learning bounds achieved sqrt log random features rather suggested previous results prove faster learning rates show might require random features unless sampled according possibly problem dependent distribution results shed light statistical computational trade offs large scale kernelized learning showing potential effectiveness random features reducing computational complexity keeping optimal generalization properties
gaussian quadrature kernel features kernel methods recently attracted resurgent interest matching performance deep neural networks tasks speech recognition random fourier features map technique commonly used scale kernel machines employing randomized feature map means epsilon samples required achieve approximation error epsilon paper investigate alternative schemes constructing feature maps deterministic rather random approximating kernel frequency domain using gaussian quadrature show deterministic feature maps constructed gamma achieve error epsilon gamma epsilon gamma samples epsilon goes validate methods datasets different domains mnist timit showing deterministic features faster generate achieve comparable accuracy state art kernel methods based random fourier features
linear time kernel goodness fit test propose novel adaptive test goodness fit computational cost linear number samples learn test features best indicate differences observed samples reference model minimizing false negative rate features constructed via stein method meaning necessary compute normalising constant model analyse asymptotic bahadur efficiency new test prove mean shift alternative test always greater relative efficiency previous linear time kernel test regardless choice parameters test experiments performance method exceeds earlier linear time test matches exceeds power quadratic time kernel test high dimensions model structure exploited goodness fit test performs far better quadratic time sample test based maximum mean discrepancy samples drawn model
convergence rates partition based bayesian multivariate density estimation method study class non parametric density estimators bayesian settings estimators obtained adaptively partitioning sample space suitable prior analyze concentration rate posterior distribution demonstrate rate directly depend dimension problem several special cases another advantage class bayesian density estimators adapt unknown smoothness true density function thus achieving optimal convergence rate without artificial conditions density also validate theoretical results variety simulated data sets
power absolute discounting dimensional distribution estimation categorical models natural fit many problems learning distribution categories samples high dimensionality dilute data minimax optimality pessimistic remedy issue serendipitously discovered estimator absolute discounting corrects empirical frequencies subtracting constant observed categories redistributes among unobserved outperforms classical estimators empirically used extensively natural language modeling paper rigorously explain prowess estimator using less pessimistic notions show absolute discounting recovers classical minimax risk rates emph adaptive effective dimension rather true dimension strongly related good turing estimator inherits emph competitive properties use power law distributions corner stone results validate theory via synthetic data application global terrorism database
optimally learning populations parameters consider following fundamental estimation problem entities unknown parameter p_i observe independent random variables x_1 ldots x_n x_i sim binomial p_i accurately recover histogram cumulative density function p_i empirical estimates would recover histogram earth mover distance theta frac sqrt equivalently ell_1 distance cdfs show provided sufficiently large achieve error frac information theoretically optimal also extend results multi dimensional parameter case capturing settings member population multiple associated parameters beyond theoretical results demonstrate recovery algorithm performs well practice variety datasets providing illuminating insights several domains including politics sports analytics
communication efficient distributed learning discrete distributions initiate systematic study distribution learning density estimation distributed model problem data drawn unknown distribution partitioned across multiple machines machines must succinctly communicate referee end referee estimate underlying distribution data problem motivated pressing need build communication efficient protocols various distributed systems power consumption limited bandwidth impose stringent communication constraints give first upper lower bounds communication complexity nonparametric density estimation discrete probability distributions distances specifically results include following case unknown distribution arbitrary machine sample show interactive protocol learns distribution must essentially communicate entire sample case structured distributions histograms monotone design distributed protocols achieve better communication guarantees trivial ones show tight bounds regimes
improved dynamic regret non degeneracy functions recently growing research interest analysis dynamic regret measures performance online learner sequence local minimizers exploiting strong convexity previous studies shown dynamic regret upper bounded path length comparator sequence paper illustrate dynamic regret improved allowing learner query gradient function multiple times meanwhile strong convexity weakened non degeneracy conditions specifically introduce squared path length could much smaller path length new regularity comparator sequence multiple gradients accessible learner first demonstrate dynamic regret strongly convex functions upper bounded minimum path length squared path length extend theoretical guarantee functions semi strongly convex self concordant best knowledge first time semi strong convexity self concordance utilized tighten dynamic regret
parameter free online learning via model selection introduce new framework deriving efficient algorithms obtain model selection oracle inequalities adversarial online learning setting also sometimes described parameter free online learning work area focused specific highly structured function classes nested balls hilbert space eschew approach propose generic meta algorithm framework achieves oracle inequalities minimal structural assumptions allows derive new computationally efficient algorithms oracle bounds wide range settings results previously unavailable give first computationally efficient algorithms work arbitrary banach spaces mild smoothness assumptions previous results applied hilbert case derive new oracle inequalities various matrix classes non nested convex sets generic regularizers finally generalize providing oracle inequalities arbitrary non linear classes contextual learning model particular give new algorithms learning multiple kernels results derived unified meta algorithm scheme based novel multi scale algorithm prediction expert advice based random playout independent interest
fast rates bandit optimization upper confidence frank wolfe consider problem bandit optimization inspired stochastic optimization online learning problems bandit feedback problem objective minimize global loss function actions necessarily cumulative loss framework allows study general class problems applications statistics machine learning fields solve problem analyze upper confidence frank wolfe algorithm inspired techniques bandits convex optimization give theoretical guarantees performance algorithm various classes functions discuss optimality results
online learning transductive regret study online learning general notion transductive regret regret modification rules applying expert sequences opposed single experts representable weighted finite state transducers show transductive regret generalizes existing notions regret including external regret internal regret swap regret conditional swap regret present general online learning algorithm minimizing transductive regret extend work design efficient algorithms time selection sleeping expert settings product study algorithm swap regret mild assumptions efficient existing methods
multi armed bandits metric movement costs consider non stochastic multi armed bandit problem setting fixed known metric action space determines cost switching pair actions loss online learner components first usual loss selected actions second additional loss due switching actions main contribution gives tight characterization expected minimax regret setting terms complexity measure mathcal underlying metric depends covering numbers finite metric spaces actions give efficient algorithm achieves regret form widetilde max set mathcal sqrt show best possible regret bound generalizes previous known regret bounds special cases unit switching cost regret widetilde theta max set sqrt mathcal theta interval metric regret widetilde theta max set sqrt mathcal theta infinite metrics spaces lipschitz loss functions derive tight regret bound widetilde theta frac minkowski dimension space known tight even switching costs
differentially private empirical risk minimization revisited faster general paper study differentially private empirical risk minimization erm different settings smooth strongly convex loss function without non smooth regularization give algorithms achieve either optimal near optimal utility bound less gradient complexity compared previous work erm smooth convex loss function high dimension setting give algorithm achieves upper bound less gradient complexity previous ones last generalize expected excess empirical risk convex polyak lojasiewicz condition give tighter upper bound utility comparing result cite dblp journals corr zhangzmw17
certified defenses data poisoning attacks machine learning systems trained user provided data susceptible data poisoning attacks whereby malicious users inject data aim corrupting learned model recent work proposed number attacks defenses little understood worst case performance defense face determined attacker remedy constructing upper bounds loss across broad family attacks defenders operate via outlier removal followed empirical risk minimization bound comes paired candidate attack nearly realizes upper bound giving powerful tool quickly assessing defense given dataset empirically find even simple defense mnist dogfish datasets certifiably resilient attack contrast imdb sentiment dataset driven test error adding poisoned data
sparse approximate conic hulls consider problem computing restricted nonnegative matrix factorization nmf times matrix specifically seek factorization approx columns subset re_ geq times equivalently given matrix consider problem finding small subset columns conic hull eps approximates conic hull columns distance every column conic hull columns eps fraction angular diameter size smallest eps approximation produce eps sized eps approximation yielding first provable polynomial time eps approximation class nmf problems also desirably approximation independent furthermore prove approximate conic caratheodory theorem general sparsity result shows column eps approximated eps sparse combination results facilitated reduction problem approximating convex hulls prove convex conic hull variants sum hard resolving open problem finally provide experimental results convex conic algorithms variety feature selection tasks
tensor train rank minimization statistical efficiency scalable algorithm tensor train decomposition provides space efficient representation higher order tensors despite advantage face crucial limitations apply decomposition machine learning problems lack statistical theory scalable algorithms paper address limitations first introduce convex relaxation decomposition problem derive error bound tensor completion task next develop alternating optimization method randomization technique time complexity efficient space complexity experiments numerically confirm derived bounds empirically demonstrate performance method real higher order tensor
sparse convolutional coding neuronal assembly detection cell assemblies originally proposed donald hebb 1949 subsets neurons firing temporally coordinated way gives rise repeated motifs supposed underly neural representations information processing although hebb original proposal dates back many decades detection assemblies role coding still open current research topic partly simultaneous recordings large populations neurons became feasible relatively recently current easy apply computational techniques focus identification strictly synchronously spiking neurons paper propose new algorithm based sparse convolutional coding detecting recurrent motifs arbitrary structure given length testing algorithm synthetically generated datasets shows outperforms established methods accurately identifies temporal structure embedded assemblies even contain overlapping neurons strong background noise present evaluation experimental datasets hippocampal slices cortical neuron cultures known ground truth provided promising results related neurophysiological phenomena
estimating high dimensional non gaussian multiple index models via stein lemma consider estimating parametric components semi parametric multiple index models high dimensional non gaussian setting estimators leverage score function based second order stein lemma require gaussian elliptical symmetry assumptions made literature show estimator achieves near optimal statistical rate convergence even score function response variable heavy tailed utilize data driven truncation argument based required concentration results established supplement theoretical results via simulation experiments confirm theory
solid harmonic wavelet scattering predicting quantum molecular energy invariant descriptors electronic densities introduce solid harmonic wavelet scattering representation invariant rigid movements stable deformations regression classification images solid harmonic wavelets computed multiplying solid harmonic functions gaussian windows dilated different scales invariant scattering coefficients obtained cascading wavelet transforms complex modulus nonlinearity study application solid harmonic scattering invariants estimation quantum molecular energies also invariant rigid movements stable respect deformations introduce neural network multiplicative non linearity regression scattering invariants provide close state art results database organic molecules
clustering billions reads dna data storage storing data synthetic dna offers possibility improving information density durability several orders magnitude compared current storage technologies however dna data storage requires computationally intensive process retrieve data particular crucial step data retrieval pipeline involves clustering billions strings respect edit distance observe datasets domain many notable properties containing large number small clusters well separated edit distance metric space regime existing algorithms unsuitable either long running time low accuracy address issue present novel distributed algorithm approximately computing underlying clusters algorithm converges efficiently dataset satisfies certain separability properties coming dna storage systems also prove assumptions algorithm robust outliers high levels noise provide empirical justification accuracy scalability convergence algorithm real synthetic data compared state art algorithm clustering dna sequences algorithm simultaneously achieves higher accuracy 1000x speedup real datasets
deep recurrent neural network based identification precursor micrornas micrornas mirnas small non coding ribonucleic acids rnas play key roles post transcriptional gene regulation direct identification mature mirnas infeasible due short lengths researchers instead aim identifying precursor mirnas pre mirnas many known pre mirnas distinctive stem loop secondary structure structure based filtering usually first step predict possibility given sequence pre mirna identify new pre mirnas often non canonical structure however need consider additional features structure obtain additional characteristics existing computational methods rely manual feature extraction inevitably limits efficiency robustness generalization computational identification address limitations existing approaches propose pre mirna identification method incorporates deep recurrent neural network rnn automated feature learning classification multimodal architecture seamless integration prior knowledge secondary structure attention mechanism improving long term dependence modeling rnn based class activation mapping highlighting learned representations contrast pre mirnas non pre mirnas experiments recent benchmarks proposed approach outperformed compared state art alternatives terms various performance metrics
decoding value networks neural machine translation neural machine translation nmt become popular technology recent years beam search facto decoding method due shrunk search space reduced computational complexity issue beam search since searches local optima time step step forward looking usually cannot output best target sentence inspired success methodology alphago paper propose using prediction network improve beam search takes source sentence currently available decoding output y_1 cdots candidate word step inputs predicts long term value bleu score partial target sentence completed nmt model following practice reinforcement learning call prediction network emph value network specifically propose recurrent structure value network train parameters bilingual data test time choosing word decoding consider conditional probability given nmt model long term value predicted value network experiments show approach significantly improve translation accuracy translation tasks english french translation chinese english translation
towards imagenet cnn nlp pretraining sentence encoders machine translation computer vision benefited initializing multiple deep layers weights pre trained large supervised training sets like imagenet contrast deep models language tasks currently benefit transfer unsupervised word vectors randomly initialize higher layers paper use encoder attentional sequence sequence model trained machine translation initialize models different language tasks show transfer improves performance using word vectors wide variety common nlp tasks sentiment analysis sst imdb question classification entailment snli question answering squad
deep voice multi speaker neural text speech introduce technique augmenting neural text speech tts low dimensional trainable speaker embeddings generate different voices single model starting point show improvements state art approaches single speaker neural tts deep voice tacotron introduce deep voice based similar pipeline deep voice constructed higher performance building blocks demonstrates significant audio quality improvement deep voice improve tacotron introducing post processing neural vocoder demonstrate significant audio quality improvement demonstrate technique multi speaker speech synthesis deep voice tacotron multi speaker tts datasets show single neural tts system learn hundreds unique voices less half hour data per speaker achieving high audio quality synthesis preserving speaker identities almost perfectly
modulating early visual processing language commonly assumed language refers high level visual concepts leaving low level visual processing unaffected view dominates current literature computational models language vision tasks visual linguistic input mostly processed independently fused single representation paper deviate classic pipeline propose modulate emph entire visual processing linguistic input specifically condition batch normalization parameters pretrained residual network language embedding approach call modulated residual networks mrn significantly improves strong baselines visual question answering tasks ablation study shows modulating early stages visual processing beneficial
multimodal learning reasoning visual question answering reasoning entities relationships multimodal data key goal artificial general intelligence visual question answering vqa problem excellent way test reasoning capabilities model multimodal representation learning however current vqa models oversimplified deep neural networks comprised long short term memory lstm unit question comprehension convolutional neural network cnn learning single image representation argue single visual representation contains limited general information image contents thus limit model reasoning capabilities work introduce modular neural network model learns multimodal multifaceted representation image question proposed model learns use multimodal representation reason image entities achieves new state art performance vqa benchmark datasets vqa wide margin
interpretable globally optimal prediction textual grounding using image concepts textual grounding important challenging task human computer interaction robotics knowledge mining existing algorithms generally formulate task selection solution set bounding box proposals obtained deep net based systems work demonstrate cast problem textual grounding unified framework permits efficient search possible bounding boxes hence able consider significantly proposals due unified formulation approach rely successful first stage beyond demonstrate trained parameters model used word embeddings capture spatial image relationships provide interpretability lastly approach outperforms current state art methods flickr 30k entities referitgame dataset respectively
multiscale quantization fast similarity search propose multiscale quantization approach fast similarity search large high dimensional datasets key insight approach quantization methods particular product quantization perform poorly variance norm data points common scenario real world datasets especially product quantization residuals obtained coarse vector quantization address issue propose multiscale formulation learn separate scalar quantizer residual norms parameters learned jointly stochastic gradient descent framework minimize overall quantization error provide theoretical motivation proposed technique conduct comprehensive experiments large scale public datasets demonstrating substantial improvements recall existing state art methods
maskrnn instance level video object segmentation instance level video object segmentation important technique video editing compression capture temporal coherence paper develop maskrnn recurrent neural net approach fuses frame output deep nets object instance binary segmentation net providing mask localization net providing bounding box due recurrent component localization component method able take advantage long term temporal structures video data well rejecting outliers validate proposed algorithm challenging benchmark datasets davis 2016 dataset davis 2017 dataset segtrack dataset achieving state art performance
flat2sphere learning spherical convolution fast features 360 imagery 360 cameras offer tremendous new possibilities vision graphics augmented reality spherical images produce make core feature extraction non trivial convolutional neural networks cnns trained images perspective cameras yield flat filters yet 360 images cannot projected single plane without significant distortion naive solution repeatedly projects viewing sphere tangent planes accurate much computationally intensive real problems propose learn spherical convolutional network translates planar cnn process 360 imagery directly equirectangular projection approach learns reproduce flat filter outputs 360 data sensitive varying distortion effects across viewing sphere key benefits efficient feature extraction 360 images video ability leverage powerful pre trained networks researchers carefully honed together massive labeled image training sets perspective images validate approach compared several alternative methods terms raw cnn output accuracy well applying state art flat object detector 360 data method yields accurate results saving orders magnitude computation versus existing exact reprojection solution
deep mean shift priors image restoration paper introduce natural image prior directly represents gaussian smoothed version natural image distribution include prior formulation image restoration bayes estimator also allows solve noise blind image restoration problems gradient bound estimator involves gradient logarithm prior gradient corresponds mean shift vector natural image distribution learn mean shift vector field using denoising autoencoders demonstrate competitive results noise blind deblurring super resolution demosaicing
pixels graphs associative embedding graphs useful abstraction image content graphs represent details individual objects scene capture interactions pairs objects present method training convolutional neural network takes input image produces full graph definition done end end single stage use associative embeddings network learns simultaneously identify elements make graph piece together benchmark visual genome dataset demonstrate state art performance challenging task scene graph generation
shape reconstruction modeling sketch object reconstruction single image highly determined problem requiring strong prior knowledge plausible shapes introduces challenge learning based approaches object annotations real images scarce previous work chose train synthetic data ground truth information suffered domain adaptation issue tested real data work propose end end trainable framework sequentially estimating sketch object shape disentangled step formulation advantages first compared full shape sketch much easier recovered image transfer synthetic real images second reconstruction sketch easily transfer learned model synthetic data real images rendered sketches invariant object appearance variations real images including lighting texture etc relieves domain adaptation problem third derive differentiable projective functions shapes sketch making framework end end trainable real images requiring real image annotations framework achieves state art performance shape reconstruction
temporal coherency based criteria predicting video frames using deep multi stage generative adversarial networks predicting future sequence video frames recently sought yet challenging task field computer vision machine learning although efforts tracking using motion trajectories flow features complex problem generating unseen frames studied extensively paper deal problem using convolutional models within multi stage generative adversarial networks gan framework proposed method uses stages gans generate crisp clear set future frames although gans used past predicting future none works consider relation subsequent frames temporal dimension main contribution lies formulating objective functions based normalized cross correlation ncc pairwise contrastive divergence pcd solving problem method coupled traditional loss experimented real world video datasets viz sports ucf 101 kitti performance analysis reveals superior results recent state art methods
learning generalize intrinsic images structured disentangling autoencoder intrinsic decomposition single image highly challenging task due inherent ambiguity scarcity training data contrast traditional fully supervised learning approaches paper propose learning intrinsic image decomposition explaining input image model rendered intrinsics network rin joins together image decomposition pipeline predicts reflectance shape lighting conditions given single image recombination function learned shading model used recompose original input based intrinsic image predictions network use unsupervised reconstruction error additional signal improve intermediate representations allows large scale unlabeled data useful training also enables transferring learned knowledge images unseen object categories lighting conditions shapes extensive experiments demonstrate method performs well intrinsic image decomposition knowledge transfer
unsupervised object learning dense equivariant image labelling key challenges visual perception extract abstract models objects object categories visual measurements affected complex nuisance factors viewpoint occlusion motion deformations starting recent idea viewpoint factorization propose new approach given large number images object supervision extract dense object centric coordinate frame coordinate frame invariant deformations images comes dense equivariant labelling neural network map image pixels corresponding object coordinates demonstrate applicability method simple articulated objects deformable objects human faces learning embeddings random synthetic transformations optical flow correspondences without manual supervision
sided unsupervised domain mapping unsupervised domain mapping learner given unmatched datasets goal learn mapping translates sample analog sample recent approaches shown learning simultaneously inverse mapping convincing mappings obtained work present method learning without learning done learning mapping maintains distance pair samples moreover good mappings obtained even maintaining distance different parts sample mapping present experimental results new method allows sided mapping learning also leads preferable numerical results existing circularity based constraint entire code made publicly available
contrastive learning image captioning image captioning popular topic computer vision achieved substantial progress recent years however distinctiveness natural descriptions often overlooked previous work closely related quality captions distinctive captions likely describe images unique aspects work propose new learning method contrastive learning image captioning specifically via constraints formulated top reference model proposed method encourage distinctiveness maintaining overall quality generated captions tested method challenging datasets improves baseline model significant margins also showed studies proposed method generic used models various structures
dynamic routing capsules capsule group neurons whose activity vector represents instantiation parameters specific type entity object object part use length activity vector represent probability entity exists orientation represent instantiation paramters active capsules level make predictions via transformation matrices instantiation parameters higher level capsules multiple predictions agree higher level capsule becomes active show discrimininatively trained multi layer capsule system achieves state art performance mnist considerably better convolutional net recognizing highly overlapping digits achieve results use iterative routing agreement mechanism lower level capsule prefers send output higher level capsules whose activity vectors big scalar product prediction coming lower level capsule
uncertainties need bayesian deep learning computer vision major types uncertainty model aleatoric uncertainty captures noise inherent observations hand epistemic uncertainty accounts uncertainty model uncertainty explained away given enough data traditionally difficult model epistemic uncertainty computer vision new bayesian deep learning tools possible study benefits modeling epistemic aleatoric uncertainty bayesian deep learning models vision tasks present bayesian deep learning framework combining input dependent aleatoric uncertainty together epistemic uncertainty study models framework per pixel semantic segmentation depth regression tasks explicit uncertainty formulation leads new loss functions tasks interpreted learned attenuation makes loss robust noisy data also giving new state art results segmentation depth regression benchmarks
efficient optimization linear dynamical systems applications clustering sparse coding linear dynamical systems ldss fundamental tools modeling spatio temporal data various disciplines though rich modeling analyzing ldss free difficulty mainly ldss comply euclidean geometry hence conventional learning techniques applied directly paper propose efficient projected gradient descent method minimize general form loss function demonstrate clustering sparse coding ldss solved proposed method efficiently end first derive novel canonical form representing parameters lds show gradient descent updates projection space ldss achieved dexterously contrast previous studies solution avoids approximation lds modeling optimization process extensive experiments reveal superior performance proposed method terms convergence classification accuracy state art techniques
label distribution learning forests label distribution learning ldl general learning framework assigns instance distribution set labels rather single label multiple labels current ldl methods either restricted assumptions expression form label distribution limitations representation learning learn deep features end end manner paper presents label distribution learning forests ldlfs novel label distribution learning algorithm based differentiable decision trees several advantages decision trees potential model general form label distributions mixture leaf node predictions learning differentiable decision trees combined representation learning define distribution based loss function forest enabling trees learned jointly show update function leaf node predictions guarantees strict decrease loss function derived variational bounding effectiveness proposed ldlfs verified several ldl tasks computer vision application showing significant improvements state art ldl methods
graph matching via multiplicative update algorithm graph matching fundamental problem computer vision machine learning area problem usually formulated quadratic programming problem doubly stochastic discrete integer constraints since hard approximate algorithms required paper present new algorithm called multiplicative update graph matching mpgm develops multiplicative update technique solve matching problem mpgm main benefits theoretically mpgm solves general problem doubly stochastic constraint naturally directly whose convergence kkt optimality guaranteed empirically mpgm generally returns sparse solution thus also incorporate discrete constraint approximately optimization efficient simple implement experiments synthetic real world matching tasks show benefits mpgm algorithm
training quantized nets deeper understanding currently deep neural networks deployed low power embedded devices first training full precision model using powerful computing hardware deriving corresponding low precision model efficient inference systems however training models directly coarsely quantized weights key step towards learning embedded platforms limited computing resources memory capacity power consumption numerous recent publications studied methods training quantized network weights studies mostly empirical work investigate training methods quantized neural networks theoretical viewpoint first explore accuracy guarantees training methods convexity assumptions look behavior algorithms non convex problems show training algorithms exploit high precision representations important annealing property purely quantized training methods lack explains many observed empirical differences types algorithms
inner loop free admm using auxiliary deep neural networks propose new method uses apply deep learning techniques accelerate popular alternating direction method multipliers admm solution inverse problems admm updates consist proximity operator least squares regression includes big matrix inversion explicit solution updating dual variables typically inner loops required solve first sub minimization problems due intractability prior matrix inversion avoid drawbacks limitations propose textit inner loop free update rule pre trained deep convolutional architectures specifically learn conditional denoising auto encoder imposes implicit data dependent prior regularization ground truth first sub minimization problem design follows empirical bayesian strategy leading called amortized inference matrix inversion second sub problem learn convolutional neural network approximate matrix inversion inverse mapping learned feeding input learned forward network note training neural network require ground truth measurements data independent extensive experiments synthetic data real datasets demonstrate efficiency accuracy proposed method compared conventional admm solution using inner loops solving inverse problems
towards accurate binary convolutional neural network introduce novel scheme train binary convolutional neural networks cnns cnns weights activations constrained run time known using binary weights activations drastically reduce memory size accesses replace arithmetic operations efficient bitwise operations leading much faster test time inference lower power consumption however previous works binarizing cnns usually result severe prediction accuracy degradation paper address issue major innovations approximating full precision weights linear combination multiple binary weight bases employing multiple binary activations alleviate information loss implementation resulting binary cnn denoted abc net shown achieve much closer performance full precision counterpart even reach comparable prediction accuracy imagenet forest trail datasets given adequate binary weight bases activations
runtime neural pruning paper propose runtime neural pruning rnp framework prunes deep neural network dynamically runtime unlike existing neural pruning methods produce fixed pruned model deployment method preserves full ability original network conducts pruning according input image current feature maps adaptively pruning performed bottom layer layer manner model markov decision process use reinforcement learning training agent judges importance convolutional kernel conducts channel wise pruning conditioned different samples network pruned image easier task since ability network fully preserved balance point easily adjustable according available resources method applied shelf network structures reach better tradeoff speed accuracy especially large pruning rate
structured embedding models grouped data word embeddings powerful approach analyzing language exponential family embeddings efe extend types data develop structured exponential family embeddings efe method discovering embeddings vary across related groups data study word usage congressional speeches varies across states party affiliation words used differently across sections arxiv purchase patterns groceries vary across seasons key success method groups share statistical information develop sharing strategies hierarchical modeling amortization demonstrate benefits approach empirical studies speeches abstracts shopping baskets show sefe enables group specific interpretation word usage outperforms efe predicting held data
poincar embeddings learning hierarchical representations representation learning become invaluable approach learning symbolic data text graphs however complex symbolic datasets often exhibit latent hierarchical structure state art methods typically learn embeddings euclidean vector spaces account property purpose introduce new approach learning hierarchical representations symbolic data embedding hyperbolic space precisely dimensional poincar ball due underlying hyperbolic geometry allows learn parsimonious representations symbolic data simultaneously capturing hierarchy similarity introduce efficient algorithm learn embeddings based riemannian optimization show experimentally poincar embeddings outperform euclidean embeddings significantly data latent hierarchies terms representation capacity terms generalization ability
language modeling recurrent highway hypernetworks provide extensive experimental theoretical support efficacy recurrent highway networks rhns recurrent hypernetworks complimentary original works demonstrate experimentally rhns benefit far better gradient flow lstms coupled greatly improved task accuracy raise provide solutions several theoretical issues hypernetworks believe yield gains future along dramatically reduced computational cost combining rhns hypernetworks make significant improvement current state art language modeling performance penn treebank relying much simpler regularization finally argue rhns drop replacement lstms analogous lstms vanilla rnns hypernetworks facto augmentation analogous attention recurrent architectures
preventing gradient explosions gated recurrent units gated recurrent unit gru successful recurrent neural network architecture time series data gru typically trained using gradient based method subject exploding gradient problem gradient increases significantly problem caused abrupt change dynamics gru due small variation parameters paper find condition dynamics gru changes drastically propose learning method address exploding gradient problem method constrains dynamics gru drastically change evaluated method experiments language modeling polyphonic music modeling experiments showed method prevent exploding gradient problem improve modeling accuracy
wider deeper cheaper faster tensorized lstms sequence learning long short term memory lstm popular approach boosting ability recurrent neural networks store longer term temporal information capacity lstm network increased widening adding layers however former introduces additional parameters latter increases runtime alternative propose tensorized lstm hidden states represented tensors updated via cross layer convolution increasing tensor size network widened efficiently without additional parameters since parameters shared across different locations tensor delaying output network deepened implicitly little additional runtime since deep computations timestep merged temporal computations sequence experiments conducted challenging sequence learning tasks show potential proposed model
fast slow recurrent neural networks processing sequential data variable length major challenge wide range applications speech recognition language modeling generative image modeling machine translation address challenge proposing novel recurrent neural network rnn architecture fast slow rnn rnn rnn incorporates strengths multiscale rnns deep transition rnns processes sequential data different timescales learns complex transition functions time step next evaluate rnn character based language modeling data sets penn treebank hutter prize wikipedia improve state art results bits per character bpc respectively addition ensemble rnns achieves bpc hutter prize wikipedia outperforming best known compression algorithm respect bpc measure also present empirical investigation learning network dynamics rnn explains improved performance compared rnn architectures approach general kind rnn cell possible building block rnn architecture thus flexibly applied different tasks
cold start reinforcement learning softmax policy gradients present learning algorithm targeted efficiently solving fundamental problems structured output prediction exposure bias problem model exposed training data distribution fail exposed predictions wrong objective problem training model convenient objective functions gives suboptimal performance method based policy gradient approach reinforcement learning succeeds avoiding common overhead procedures associated approaches namely warm start training variance reduction policy updates proposed cold start reinforcement learning method based new softmax policy gradient softmax policy combines efficiency simplicity maximum likelihood approach effectiveness reward based signal empirical evidence validates method structured output predictions automatic summarization image captioning tasks
deep learning precipitation nowcasting benchmark new model goal making high resolution forecasts regional rainfall precipitation nowcasting become important fundamental technology underlying various public services ranging rainfall alerts flight safety recently convolutional lstm convlstm model shown outperform traditional optical flow based methods precipitation nowcasting suggesting deep learning models huge potential solving problem however convolutional recurrence structure convlstm based models location invariant natural motion transformation rotation location variant general furthermore since deep learning based precipitation nowcasting newly emerging area clear evaluation protocols yet established address problems propose new model benchmark precipitation nowcasting specifically beyond convlstm propose trajectory gru trajgru model actively learn location variant structure recurrent connections besides provide benchmark includes real world large scale dataset hong kong observatory new training loss comprehensive evaluation protocol facilitate future research gauge state art
recurrent ladder networks propose recurrent extension ladder network cite ladder motivated inference required hierarchical latent variable models demonstrate recurrent ladder able handle wide variety complex learning tasks need iterative inference temporal modeling architecture shows close optimal results temporal modeling video data competitive results music modeling improved perceptual grouping based higher order abstractions stochastic textures motion cues present results fully supervised semi supervised unsupervised tasks results suggest proposed architecture principles powerful tools learning hierarchy abstractions handling temporal information modeling relations interactions objects
predictive state decoders encoding future recurrent networks recurrent neural networks rnns vital modeling technique rely internal states learned indirectly optimization supervised unsupervised reinforcement training loss rnns used model dynamic processes characterized underlying latent states whose form often unknown precluding analytic representation inside rnn predictive state representation psr literature latent state processes modeled internal state representation directly models distribution future observations recent work area relied explicitly representing targeting sufficient statistics probability distribution seek combine advantages rnns psrs augmenting existing state art recurrent neural networks predictive state decoders psds add supervision network internal state representation target predicting future observations psds simple implement easily incorporated existing training pipelines via additional loss regularization demonstrate effectiveness psds experimental results different domains probabilistic filtering imitation learning reinforcement learning method improves statistical performance state art recurrent baselines fewer iterations less data
qmdp net deep learning planning partial observability paper introduces qmdp net neural network architecture planning partial observability qmdp net combines strengths model free learning model based planning recurrent policy network represents policy connecting model planning algorithm solves model thus embedding solution structure planning network learning architecture qmdp net fully differentiable allows end end training train qmdp net set different environments generalize new ones transfer larger environments well preliminary experiments qmdp net showed strong performance several robotic tasks simulation interestingly qmdp net encodes qmdp algorithm sometimes outperforms qmdp algorithm experiments qmdp net increased robustness end end learning
filtering variational objectives evidence lower bound elbo appears many algorithms maximum likelihood estimation mle latent variables sharp lower bound marginal log likelihood neural latent variable models optimizing elbo jointly variational posterior model parameters produces state art results inspired success elbo surrogate mle objective consider extension elbo family lower bounds defined monte carlo estimator marginal likelihood show tightness bounds asymptotically related variance underlying estimator introduce special case filtering variational objectives takes arguments elbo passes particle filter form tighter bound filtering variational objectives optimized tractably stochastic gradients particularly suited mle sequential latent variable models standard sequential generative modeling tasks present uniform improvements computational budget models trained elbo iwae objectives include whole nat per timestep improvements
unsupervised learning disentangled latent representations sequential data present factorized hierarchical variational autoencoder learns disentangled representations sequential data without supervision specifically exploit multi scale nature information sequential data formulating explicitly within factorized hierarchical graphical model imposes sequence specific priors global priors different sets latent variables model evaluated speech corpora demonstrate qualitatively ability transform speakers linguistic content manipulating different sets latent variables quantitatively ability outperform vector baseline speaker verification reduce word error rate much mismatched train test scenarios automatic speech recognition tasks
neural discrete representation learning learning useful representations without supervision remains key challenge machine learning paper propose simple yet powerful generative model learns discrete representations model vector quantised variational autoencoder vae differs vaes key ways encoder network outputs discrete rather continuous codes prior learnt rather static order learn discrete latent representation incorporate ideas vector quantisation using method allows model circumvent issues posterior collapse latents ignored paired powerful autoregressive decoder typically observed vae framework pairing representations autoregressive prior model generate high quality images videos speech well high quality speaker inpainting providing evidence utility learnt representations
variational memory addressing generative models aiming augment generative models external memory interpret output memory module stochastic addressing conditional mixture distribution read operation corresponds sampling discrete memory address retrieving corresponding content memory perspective allows apply variational inference memory addressing enables effective training memory module using target information guide memory lookups stochastic addressing particularly well suited generative models naturally encourages multimodality prominent aspect high dimensional datasets treating chosen address latent variable also allows quantify amount information gained memory lookup measure contribution memory module generative process illustrate advantages approach incorporate variational autoencoder apply resulting model task generative shot learning intuition behind architecture memory module pick relevant template memory continuous part model concentrate modeling remaining variations demonstrate empirically model able identify access relevant memory contents even hundreds unseen omniglot characters memory
cortical microcircuits gated recurrent neural networks cortical circuits exhibit intricate recurrent architectures remarkably similar across different brain areas stereotyped structure suggests existence common computational principles remained largely elusive inspired gated memory networks namely long short term memory lstm nets introduce recurrent neural network rnn information gated inhibitory units subtractive balanced subrnn propose subrnns natural mapping onto known canonical excitatory inhibitory cortical microcircuits show networks subtractive gating easier optimise standard multiplicative gates moreover subrnns yield near exact solution standard long term dependency task temporal addition task empirical results across several long term dependency tasks generalised temporal addition multiplication temporal mnist word level language modelling show subrnns outperform achieve similar performance lstm networks tested work suggests novel view cortex solves complex contextual problems provides first step towards unifying machine learning recurrent networks biological counterparts
continual learning deep generative replay attempts train comprehensive artificial intelligence capable solving multiple tasks impeded chronic problem called catastrophic forgetting although simply replaying previous data alleviates problem requires large memory even worse often infeasible real world applications access past data limited inspired generative nature hippocampus short term memory system primate brain propose deep generative replay novel framework cooperative dual model architecture consisting deep generative model generator task solving model solver models training data previous tasks easily sampled interleaved new task test methods several sequential learning settings involving image classification tasks
vae learning via stein variational gradient descent new method learning variational autoencoders vaes developed based stein variational gradient descent key advantage approach need make parametric assumptions form encoder distribution performance enhanced integrating proposed encoder importance sampling excellent performance demonstrated across multiple unsupervised semi supervised problems including semi supervised analysis imagenet data demonstrating scalability model large datasets
learning inpaint image compression study design deep architectures lossy image compression present architectural recipes context multi stage progressive encoders empirically demonstrate importance compression performance specifically show predicting original image data residuals multi stage progressive architecture facilitates learning leads improved performance approximating original content learning inpaint neighboring image pixels performing compression reduces amount information must stored achieve high quality approximation incorporating design choices baseline progressive encoder yields average reduction file size similar quality compared original residual encoder
visual interaction networks glance humans make rich predictions future state wide range physical systems modern approaches engineering robotics graphics often restricted narrow domains require direct measurements underlying states introduce visual interaction network general purpose model learning dynamics physical system raw visual observations predicting future states model consists perceptual front end based convolutional neural networks dynamics predictor based interaction networks joint training perceptual front end learns parse dynamic visual scene set factored latent object representations dynamics predictor learns roll states forward time computing interactions dynamics producing predicted physical trajectory arbitrary length found input video frames visual interaction network generate accurate future trajectories hundreds time steps wide range physical systems model also applied scenes invisible objects inferring future states effects visible objects implicitly infer unknown mass objects results demonstrate perceptual module object based dynamics predictor module induce factored latent representations support accurate dynamical predictions work opens new opportunities model based decision making planning raw sensory observations complex physical environments
neuralfdr learning discovery thresholds hypothesis features datasets grow richer important challenge leverage full features data maximize number useful discoveries controlling false positives address problem context multiple hypotheses testing hypothesis observe value along set features specific hypothesis example genetic association studies hypothesis tests correlation variant trait rich set features variant location conservation epigenetics etc could inform likely variant true association however popular testing approaches benjamini hochberg procedure independent hypothesis weighting ihw either ignore features assume features categorical propose new algorithm neuralfdr automatically learns discovery threshold function hypothesis features parametrize discovery threshold neural network enables flexible handling multi dimensional discrete continuous features well efficient end end optimization prove neuralfdr strong false discovery rate fdr guarantees show makes substantially discoveries synthetic real datasets moreover demonstrate learned discovery threshold directly interpretable
eigen distortions hierarchical representations develop method comparing hierarchical image representations terms ability explain perceptual sensitivity humans specifically utilize fisher information establish model derived prediction local sensitivity perturbations around given natural image given image compute eigenvectors fisher information matrix largest smallest eigenvalues corresponding model predicted least noticeable image distortions respectively human subjects measure amount distortion reliably detected added image compare thresholds predictions corresponding model use method test ability variety representations mimic human perceptual sensitivity find early layers vgg16 deep neural network optimized object recognition provide better match human perception later layers better match stage convolutional neural network cnn trained database human ratings distorted image quality hand find simple models early visual processing incorporating stages local gain control trained database distortion ratings predict human sensitivity significantly better cnn layers vgg16
fly operation batching dynamic computation graphs dynamic neural networks toolkits pytorch dynet chainer offer flexibility implementing models cope data varying dimensions structure relative toolkits operate statically declared computations tensorflow cntk theano however existing toolkits static dynamic require developer organize computations batches necessary exploiting high performance data parallel algorithms hardware batching task generally difficult becomes major hurdle architectures become complex paper present algorithm implementation dynet toolkit automatically batching operations developers simply write minibatch computations aggregations single instance computations batching algorithm seamlessly executes fly computationally efficient batches variety tasks obtain throughput similar manual batches well comparable speedups single instance learning architectures impractical batch manually
learning affinity via spatial propagation networks paper propose spatial propagation networks learning affinity matrix show constructing row column linear propagation model spatially variant transformation matrix constitutes affinity matrix models dense global pairwise similarities image specifically develop way connection linear propagation model formulates sparse transformation matrix elements output deep cnn results dense affinity matrix effective model task specific pairwise similarity instead designing similarity kernels according image features points directly output similarities pure data driven manner spatial propagation network generic framework applied numerous tasks traditionally benefit designed affinity image matting colorization guided filtering name furthermore model also learn semantic aware affinity high level vision tasks due learning capability deep model validate proposed framework refinement object segmentation experiments helen face parsing pascal voc 2012 semantic segmentation tasks show spatial propagation network provides general effective efficient solutions generating high quality segmentation results
supervised adversarial domain adaptation work provides framework addressing problem supervised domain adaptation deep models main idea exploit adversarial learning learn embedded subspace simultaneously maximizes confusion domains semantically aligning embedded versions supervised setting becomes attractive especially target data samples need labeled scenario alignment separation semantic probability distributions difficult lack data found carefully designing training scheme whereby typical binary adversarial discriminator augmented distinguish different classes possible effectively address supervised adaptation problem addition approach high speed adaptation requires extremely low number labeled target training samples even per category effective extensively compare approach state art domain adaptation experiments using datasets handwritten digit recognition using datasets visual object recognition
deep hyperspherical learning convolution inner product founding basis convolutional neural networks cnns key end end visual representation learning benefiting deeper architectures recent cnns demonstrated increasingly strong representation abilities despite improvement increased depth larger parameter space also led challenges properly training network light challenges propose hyperspherical convolution sphereconv novel learning framework gives angular representations hyperspheres introduce spherenet deep hyperspherical convolution networks distinct conventional inner product based convolutional networks particular spherenet adopts sphereconv basic convolution operator supervised generalized angular softmax loss natural loss formulation sphereconv show spherenet effectively encode discriminative representation alleviate training difficulty leading easier optimization faster convergence better classification performance convolutional counterparts also provide theoretical justifications advantages hyperspherical optimization experiments ablation studies verified conclusion
riemannian approach batch normalization batch normalization proven effective algorithm deep neural network training normalizing input neuron reducing internal covariate shift space weight vectors layer naturally interpreted riemannian manifold invariant linear scaling weights following intrinsic geometry manifold provides new learning rule efficient easier analyze also propose intuitive effective gradient clipping regularization methods proposed algorithm utilizing geometry manifold resulting algorithm consistently outperforms original various types network architectures datasets
backprop without learning rates coin betting deep learning methods achieve state art performance many application scenarios yet methods require significant amount hyperparameters tuning order achieve best results particular tuning learning rates stochastic optimization process still main bottlenecks paper propose new stochastic gradient descent procedure deep networks require learning rate setting contrary previous methods adapt learning rates make use assumed curvature objective function instead reduce optimization process game betting coin propose learning rate free optimal algorithm scenario theoretical convergence proven convex quasi convex functions empirical evidences show advantage algorithm popular stochastic gradient algorithms
convergence block coordinate descent training dnns tikhonov regularization lifting relu function higher dimensional space develop smooth multi convex formulation training feed forward deep neural networks dnns allows develop block coordinate descent bcd training algorithm consisting sequence numerically well behaved convex optimizations using ideas proximal point methods convex analysis prove bcd algorithm converge globally stationary point linear convergence rate order experiments mnist database dnns trained bcd algorithm consistently yielded better test set error rates identical dnn architectures tarined via stochastic gradient descent sgd variants caffe toolbox
collaborative deep learning fixed topology networks significant recent interest parallelize deep learning algorithms order handle enormous growth data model sizes advances focus model parallelization engaging multiple computing agents via using central parameter server aspect data parallelization along decentralized computation explored sufficiently context paper presents new consensus based distributed sgd cdsgd momentum variant cdmsgd algorithm collaborative deep learning fixed topology networks enables data parallelization well decentralized computation framework extremely useful learning agents access local private data communication constrained environment analyze convergence properties proposed algorithm strongly convex nonconvex objective functions fixed diminishing step sizes using concepts lyapunov function construction demonstrate efficacy algorithms comparison baseline centralized sgd recently proposed federated averaging algorithm also enables data parallelism based benchmark datasets mnist cifar cifar 100
regularization affects critical points linear networks paper concerned problem representing learning linear transformation using linear neural network recent years growing interest study networks part due successes deep learning main question body research also paper pertains existence optimality properties critical points mean squared loss function primary concern robustness critical points regularization loss function optimal control model introduced purpose learning algorithm regularized form backprop derived using hamilton formulation optimal control formulation used provide complete characterization critical points terms solutions nonlinear matrix valued equation referred characteristic equation analytical numerical tools bifurcation theory used compute critical points via solutions characteristic equation main conclusion critical point diagram fundamentally different even arbitrary small amounts regularization
predicting organic reaction outcomes weisfeiler lehman network prediction organic reaction outcomes fundamental problem computational chemistry since reaction involve hundreds atoms fully exploring space possible transformations intractable current solution utilizes reaction templates limit space suffers coverage efficiency issues paper propose template free approach efficiently explore space product molecules first pinpointing reaction center set nodes edges graph edits occur since small number atoms contribute reaction center directly enumerate candidate products generated candidates scored weisfeiler lehman difference network models high order interactions changes occurring nodes across molecule framework outperforms top performing template based approach margin running orders magnitude faster finally demonstrate model accuracy rivals performance domain experts
predicting scene parsing motion dynamics future important intelligent systems textit autonomous vehicles robotics anticipate future order plan early make decisions accordingly predicting future scene parsing motion dynamics helps agents understand visual environment better former provides dense semantic segmentations textit objects present later provides dense motion information textit objects move future paper propose novel model predict future scene parsing motion dynamics unobserved video frames simultaneously using history information preceding frames corresponding scene parsing results input model able predict scene parsing motion arbitrary time steps ahead importantly model superior compared methods predict parsing motion individually solve prediction tasks jointly fully exploit complementary relationship best knowledge paper first aiming learn predict future scene parsing motion dynamics simultaneously large scale cityscapes dataset demonstrated model produces significantly better parsing motion prediction compared well established baselines addition also present predict steering angle vehicles using model good results verify capability model learn underlying latent parameters
houdini democratizing adversarial examples generating adversarial examples critical step evaluating improving robustness learning machines far existing methods work classification designed alter true performance measure problem hand introduce novel flexible approach named houdini generating adversarial specifically tailored final performance measure task considered successfully apply houdini range applications speech recognition pose estimation
geometric matrix completion recurrent multi graph neural networks matrix completion models among common formulations recommender systems recent works showed boost performance techniques introducing pairwise relationships users items form graphs imposing smoothness priors graphs however techniques fully exploit local stationary structures user item graphs number parameters learn linear number users items propose novel approach overcome limitations using geometric deep learning graphs matrix completion architecture combines novel multi graph convolutional neural network learn meaningful statistical graph structured patterns users items recurrent neural network applies learnable diffusion score matrix neural network system computationally attractive requires constant number parameters independent matrix size apply method several standard datasets showing outperforms state art matrix completion techniques
compression aware training deep neural networks recent years great progress made variety application domains thanks development increasingly deeper neural networks unfortunately huge number units networks makes expensive computationally memory wise overcome exploiting fact deep networks parametrized several compression strategies proposed methods however typically start network trained standard manner without considering future compression paper propose explicitly account compression training process end introduce regularizer encourages parameter matrix layer low rank training show allows learn much compact yet least effective models state art compression techniques
non parametric neural networks deep neural networks dnns probabilistic graphical models pgms main tools statistical modeling dnns provide ability model rich complex relationships input independent output variables pgms provide ability encode dependencies among output variables end end training models structured graphical dependencies top independent neural predictions recently emerged principled ways combining paradigms types models proven powerful discriminative settings discrete outputs extensions structured continuous spaces well performing efficient inference spaces lacking propose non parametric neural networks n3s modular approach cleanly separates non parametric structured posterior representation discriminative inference scheme allows end end training components experiments evaluate ability n3s capture structured posterior densities modeling compute complex statistics densities inference compare model number baselines including popular variational sampling based inference schemes terms accuracy speed
gibbsnet iterative adversarial inference deep graphical models directed latent variable models formulate joint distribution advantage sampling fast exact yet weakness need specify often simple fixed prior limits expressiveness model undirected latent variable models discard requirement specified prior yet sampling generally requires iterative procedure blocked gibbs sampling require many steps achieve samples joint distribution propose novel approach learning joint distribution data latent code uses adversarially learned iterative procedure gradually refine joint distribution better match data distribution step gibbsnet best worlds theory practice achieving speed simplicity directed latent variable model guaranteed assuming adversarial game reaches virtual training criteria global minimum produce samples sampling iterations achieving expressiveness flexibility undirected latent variable model gibbsnet away need explicit ability classification class conditional generation joint image attribute modeling single model trained specific tasks show empirically gibbsnet able learn complex show leads improved inpainting iterative refinement dozens steps stable generation without collapse thousands steps despite trained steps
exploring generalization deep learning goal understanding drives generalization deep networks consider several recently suggested explanations including norm based control sharpness robustness study measures ensure generalization highlighting importance scale normalization making connection sharpness pac bayes theory investigate well measures explain different observed phenomena
regularizing deep neural networks noise interpretation optimization overfitting critical challenges deep neural networks various types regularization methods improve generalization performance injecting noises hidden units training dropout known successful regularizer still clear enough training techniques work well practice maximize benefit presence conflicting objectives optimizing true data distribution preventing overfitting regularization paper addresses issues interpreting conventional training methods regularization noise injection optimize lower bound true objective proposing technique achieve tighter lower bound using multiple noise samples per mini batch demonstrate effectiveness idea several computer vision applications
extracting low dimensional dynamics multiple large scale neural population recordings learning predict correlations powerful approach understanding neural population dynamics extract low dimensional trajectories population recordings using dimensionality reduction methods current approaches dimensionality reduction neural data limited single population recordings identify dynamics embedded across multiple measurements propose approach extracting low dimensional dynamics multiple sequential recordings algorithm scales data comprising millions observed dimensions making possible access dynamics distributed across large populations multiple brain areas building subspace identification approaches dynamical systems perform parameter estimation minimizing moment matching objective using scalable stochastic gradient descent algorithm model optimized predict temporal covariations across neurons across time show approach naturally handles missing data multiple partial recordings identify dynamics predict correlations even presence severe subsampling small overlap recordings demonstrate effectiveness approach simulated data whole brain larval zebrafish imaging dataset
adaptive sampling population neurons adaptive sampling methods neuroscience primarily focused maximizing firing rate single recorded neuron recording neurons usually possible find single stimulus maximizes firing rates neurons motivates objective function takes account recorded population neurons together propose adept adaptive sampling method optimize population objective functions simulated experiments first confirmed population objective functions elicited varied stimulus responses single neuron objective functions tested adept closed loop electrophysiological experiment population activity recorded macaque cortical area known mid level visual processing adept uses outputs deep convolutional neural network model feature embeddings predict neural responses adept elicited mean stimulus responses larger randomly chosen natural images well larger scatter stimulus responses adaptive sampling methods enable new scientific discoveries recording population neurons heterogeneous response properties
onacid online analysis calcium imaging data real time optical imaging methods using calcium indicators critical monitoring activity large neuronal populations vivo imaging experiments typically generate large amount data needs processed extract activity imaged neuronal sources deriving processing algorithms active area research existing methods require processing large amounts data time rendering vulnerable volume recorded data preventing real time experimental interrogation introduce onacid online framework analysis streaming calcium imaging data including motion artifact correction neuronal source extraction iii activity denoising deconvolution approach combines extends previous work online dictionary learning calcium imaging data analysis deliver automated pipeline discover track activity hundreds cells real time thereby enabling new types closed loop experiments apply algorithm large scale experimental datasets benchmark performance manually annotated data show outperforms popular offline approach
detrended partial cross correlation brain connectivity analysis brain connectivity analysis critical component ongoing human connectome projects decipher healthy diseased brain recent work highlighted power law multi time scale properties brain signals however remains lack methods specifically quantify short long range brain connections paper using detrended partial cross correlation analysis dpcca propose novel functional connectivity measure delineate brain interactions multiple time scales controlling covariates use rich simulated fmri data validate proposed method apply real fmri data cocaine dependence prediction task show compared extant methods dpcca based approach distinguishes short long range functional connectivity also improves feature extraction subsequently increasing classification accuracy together paper contributes broadly new computational methodologies understand neural information processing
practical bayesian optimization model fitting bayesian adaptive direct search computational models fields computational neuroscience often evaluated via stochastic simulation numerical approximation fitting models implies difficult optimization problem complex possibly noisy parameter landscapes bayesian optimization successfully applied solving expensive black box problems engineering machine learning explore whether applied general tool model fitting first present novel algorithm bayesian adaptive direct search bads achieves competitive performance affordable computational overhead running time typical models perform extensive benchmark bads many common state art nonconvex derivative free optimizers set model fitting problems real data models studies behavioral cognitive computational neuroscience default settings bads consistently finds comparable better solutions methods showing great promise bads particular general model fitting tool
error detection correction framework connectomics significant advances made recent years problem neural circuit reconstruction electron microscopic imagery improvements image acquisition image alignment boundary detection greatly reduced achievable error rate order make progress argue automated error detection essential focussing effort attention human machine paper report use automated error detection attention signal flood filling error correction module demonstrate significant improvements upon state art segmentation performance
cake effective brain connectivity causal kernels fundamental goal network neuroscience understand activity region drives activity elsewhere process referred effective connectivity propose model causal interaction using integro differential equations causal kernels allow rich analysis effective connectivity approach combines tractability flexibility autoregressive modeling biophysical interpretability dynamic causal modeling causal kernels learned nonparametrically using gaussian process regression yielding efficient framework causal inference construct novel class causal covariance functions enforce desired properties causal kernels approach call cake construction model hyperparameters biophysical meaning therefore easily interpretable demonstrate efficacy cake number simulations give example realistic application magnetoencephalography meg data
learning neural representations human cognition across many fmri studies cognitive neuroscience enjoying rapid increase extensive public brain imaging datasets opening door design deploy large scale statistical models targeting unified perspective available data implies finding scalable automated solutions old challenge aggregate heterogeneous information brain function universal cognitive system relates psychological behavior brain networks cast challenge machine learning approach predict conditions statistical brain maps across different studies leverage multi task learning multi scale dimension reduction learn low dimensional representations brain images carry robust cognitive information robustly associated psychological stimuli multi dataset classification model achieves best prediction performance several large reference datasets compared models forgo learning cognitive aware low dimension representation brings substantial performance boost analysis small datasets introspected identify universal template cognitive concepts
mapping distinct timescales functional interactions among brain networks brain processes occur various timescales ranging milliseconds neurons minutes hours behavior characterizing functional coupling among brain regions diverse timescales key understanding brain produces behavior apply instantaneous lag based measures conditional linear dependence based granger geweke causality infer network connections distinct timescales functional magnetic resonance imaging fmri data due slow sampling rate fmri widely held produces spurious unreliable estimates functional connectivity applied fmri data challenge claim combining simulations novel machine learning approach first show simulated fmri data instantaneous lag based identify distinct timescales complementary patterns functional connectivity next analyzing fmri recordings 500 human subjects show linear classifier trained either instantaneous lag based connectivity reliably distinguishes task versus rest brain states cross validation accuracy importantly instantaneous lag based exploit markedly different spatial temporal patterns connectivity achieve robust classification approach provides novel framework uncovering validating functionally connected networks operate distinct timescales brain
robust estimation neural signals calcium imaging calcium imaging prominent technology neuroscience research allows simultaneous recording large numbers neurons awake animals automated extraction neurons temporal activity imaging datasets important step path producing neuroscience results however nearly imaging datasets typically contain gross contaminating sources could contributed technology used underlying biological tissue although attempts made better extract neural signals limited gross contamination scenarios effort address contamination full generality statistical estimation work proceed new direction propose extract cells activity using robust estimation derive optimal robust loss based simple abstraction calcium imaging data also find simple practical optimization routine loss provably fast convergence use proposed robust loss matrix factorization framework extract neurons temporal activity calcium imaging datasets demonstrate superiority robust estimation approach existing methods simulated real datasets
learning morphology brain signals using alpha stable convolutional sparse coding neural time series data contain wide variety prototypical signal waveforms atoms significant importance clinical cognitive research goals analyzing data hence extract shift invariant atoms even though success reported existing algorithms limited applicability due heuristic nature moreover often vulnerable artifacts impulsive noise typically present raw neural recordings study address issues propose novel probabilistic convolutional sparse coding csc model learning shift invariant atoms raw neural signals containing potentially severe artifacts core model call alpha csc lies family heavy tailed distributions called alpha stable distributions develop novel computationally efficient monte carlo expectation maximization algorithm inference maximization step boils weighted csc problem develop computationally efficient optimization algorithm results show proposed algorithm achieves state art convergence speeds besides alpha csc significantly robust artifacts compared competing algorithms extract spike bursts oscillations even reveal subtle phenomena cross frequency coupling applied noisy neural time series
streaming weak submodularity interpreting neural networks fly many machine learning applications important explain predictions black box classifier example deep neural network assign image particular class cast interpretability black box classifiers combinatorial maximization problem propose efficient streaming algorithm solve subject cardinality constraints extending ideas badanidiyuru 2014 provide constant factor approximation guarantee algorithm case random stream order weakly submodular objective function first theoretical guarantee general class functions also show algorithm exists worst case stream order algorithm obtains similar explanations inception predictions times faster state art lime framework ribeiro 2016
decomposable submodular function minimization discrete continuous paper investigates connections discrete continuous approaches decomposable submodular function minimization provide improved running time estimates state art continuous algorithms problem using combinatorial arguments also provide systematic experimental comparison types methods based clear distinction level level algorithms
differentiable learning submodular functions incorporate discrete optimization algorithms within modern machine learning models example possible use deep architectures layer whose output minimal cut parametrized graph given models trained end end leveraging gradient information introduction layers seems challenging due non continuous output paper focus problem submodular minimization show layers indeed possible key idea continuously relax output without sacrificing guarantees provide easily computable approximation jacobian complemented complete theoretical analysis finally contributions let experimentally learn probabilistic log supermodular models via level variational inference formulation
robust optimization non convex objectives consider robust optimization problems goal optimize worst case class objective functions develop reduction robust improper optimization bayesian optimization given oracle returns alpha approximate solutions distributions objectives compute distribution solutions alpha approximate worst case show derandomizing solution hard general done broad class statistical learning tasks apply results robust neural network training submodular optimization evaluate approach experimentally character classification task subject adversarial distortion robust influence maximization large networks
optimization landscape tensor decompositions non convex optimization local search heuristics widely used machine learning achieving many state art results becomes increasingly important understand work hard problems typical data landscape many objective functions learning conjectured geometric property local optima approximately global optima thus solved efficiently local search algorithms however establishing property difficult paper analyze optimization landscape random complete tensor decomposition problem many applications unsupervised leaning especially learning latent variable models practice efficiently solved gradient ascent non convex objective show small constant epsilon among set points function values epsilon factor larger expectation function local maxima approximate global maxima previously best known result characterizes geometry small neighborhoods around true components result implies even initialization barely better random guess gradient ascent algorithm guaranteed solve problem main technique uses kac rice formula random matrix theory best knowledge first time kac rice formula successfully applied counting number local minima highly structured random polynomial dependent coefficients
gradient descent take exponential time escape saddle points although gradient descent almost always escapes saddle points asymptotically lee 2016 paper shows even fairly natural random initialization schemes non pathological functions significantly slowed saddle points take exponential time escape hand gradient descent perturbations 2015 jin 2017 slowed saddle points find approximate local minimizer polynomial time result concludes gradient descent inherently slower justifies importance adding perturbations efficient non convex optimization experiments also provided demonstrate theoretical findings
convolutional phase retrieval study convolutional phase retrieval problem asks recover unknown signal mathbf length measurements consisting magnitude cyclic convolution known kernel mathbf length model motivated applications channel estimation optics underwater acoustic communication signal interest acted given channel filter phase information difficult impossible acquire show mathbf random geq omega frac mathbf mathbf mathbf mathrm poly log mathbf efficiently recovered global phase using combination spectral initialization generalized gradient descent main challenge coping dependencies measurement operator overcome challenge using ideas decoupling theory suprema chaos processes restricted isometry property random circulant matrices recent analysis alternating minimizing methods
implicit regularization matrix factorization study implicit regularization optimizing underdetermined quadratic objective matrix gradient descent factorization conjecture provide empirical theoretical evidence small enough step sizes initialization close enough origin gradient descent full dimensional factorization converges minimum nuclear norm solution
near linear time approximation algorithms optimal transport via sinkhorn iteration computing optimal transport distances earth mover distance fundamental problem machine learning statistics computer vision despite recent introduction several algorithms good empirical performance unknown whether general optimal transport distances approximated near linear time paper demonstrates ambitious goal fact achieved cuturi sinkhorn distances provides guidance towards parameter tuning algorithm result relies new analysis sinkhorn iterations also directly suggests new algorithm greenkhorn theoretical guarantees numerical simulations clearly illustrate greenkhorn significantly outperforms classical sinkhorn algorithm practice
frank wolfe equilibrium computation consider frank wolfe method constrained convex optimization first order projection free procedure show algorithm recast different light emerging special case particular meta algorithm computing equilibria saddle points convex concave sum games equilibrium computation trick relies existence regret online learning generate sequence iterates also provide proof convergence vanishing regret show stated equivalence several nice properties particularly exhibits modularity gives rise various old new algorithms explore resulting methods provide experimental results demonstrate correctness efficiency
greedy algorithms cone constrained optimization convergence guarantees greedy optimization methods matching pursuit frank wolfe algorithms regained popularity recent years due simplicity effectiveness theoretical guarantees address optimization textit linear span textit convex hull set atoms respectively paper consider intermediate case optimization textit convex cone parametrized conic hull generic atom set leading first principled definitions non negative algorithms give explicit convergence rates demonstrate excellent empirical performance novel algorithms analysis tailored particular function atom set particular derive sublinear mathcal convergence general smooth convex objectives linear convergence mathcal strongly convex objectives cases general sets atoms furthermore establish clear correspondence algorithms known algorithms literature novel algorithms analyses target general atom sets general objective functions hence directly applicable large variety learning settings
cyclic coordinate descent beats randomized coordinate descent coordinate descent methods seen resurgence recent interest applicability machine learning well large scale data analysis superior empirical performance methods variants cyclic coordinate descent ccd randomized coordinate descent rcd deterministic randomized versions methods light recent results literature common perception rcd always dominates ccd terms performance paper question perception provide examples generally problem classes ccd deterministic order faster rcd terms asymptotic worst case convergence furthermore provide lower upper bounds amount improvement rate deterministic relative rcd amount improvement depend deterministic order used also provide characterization best deterministic order leads maximum improvement convergence rate terms combinatorial properties hessian matrix objective function
linear convergence frank wolfe type algorithm trace norm balls propose rank variant classical frank wolfe algorithm solve convex minimization trace norm ball algorithm replaces top singular vector computation svd frank wolfe top singular vector computation svd done repeatedly applying svd times algorithm linear convergence rate objective function smooth strongly convex optimal solution rank improves convergence rate total complexity frank wolfe method variants
adaptive accelerated gradient converging method lderian error bound condition recent studies shown proximal gradient method accelerated gradient method apg restarting enjoy linear convergence weaker condition strong convexity namely quadratic growth condition qgc however faster convergence restarting apg method relies potentially unknown constant qgc appropriately restart apg restricts applicability address issue developing novel adaptive gradient converging methods leveraging magnitude proximal gradient criterion restart termination analysis extends much general condition beyond qgc namely lderian error bound heb condition key technique development novel synthesis adaptive regularization conditional restarting scheme extends previous work focusing strongly convex problems much broader family problems furthermore demonstrate results important implication applications machine learning objective function coercive semi algebraic convergence speed essentially frac total number iterations objective function consists ell_1 ell_ infty ell_ infty huber norm regularization convex smooth piecewise quadratic loss square loss squared hinge loss huber loss proposed algorithm parameter free enjoys faster linear convergence without assumptions restricted eigen value condition notable linear convergence results aforementioned problems global instead local best knowledge improved results first shown work
searching dark practical svrg methods error bound conditions guarantee paper develops practical stochastic variance reduced gradient svrg methods error bound conditions theoretical guarantee error bound conditions inherent property optimization problem recently revived optimization developing fast algorithms improved global convergence without strong convexity particular condition interest quadratic error bound aka second order growth condition weaker strong convexity leveraged developing linear convergence many gradient proximal gradient methods several recent studies also derived linear convergence quadratic error bound condition stochastic variance reduced gradient method important milestone stochastic optimization solving machine learning problems however studies overlooked critical issue algorithmic dependence unknown parameter analogous strong convexity modulus error bound conditions usually difficult estimate therefore makes algorithm practical solving many interesting machine learning problems address issue propose novel techniques automatically search unknown parameter fly optimization maintaining almost convergence rate oracle setting assuming involved parameter given
geometric descent method convex composite minimization paper extend geometric descent method recently proposed bubeck lee singh tackle nonsmooth strongly convex composite problems prove proposed algorithm dubbed geometric proximal gradient method geopg converges linear rate sqrt kappa thus achieves optimal rate among first order methods kappa condition number problem numerical results linear regression logistic regression elastic net regularization show geopg compares favorably nesterov accelerated proximal gradient method especially problem ill conditioned
faster non ergodic stochastic alternating direction method multipliers study stochastic convex optimization subjected linear equality constraints traditional stochastic alternating direction method multipliers nesterov acceleration scheme achieve ergodic sqrt convergence rates number iteration introducing variance reduction techniques convergence rates improve ergodic paper propose new stochastic admm elaborately integrates nesterov extrapolation techniques nesterov extrapolation algorithm achieve non ergodic convergence rate optimal separable linearly constrained non smooth convex problems convergence rates based admm methods actually tight sqrt non ergodic sense best knowledge first work achieves truly accelerated stochastic convergence rate constrained convex problems experimental results demonstrate algorithm significantly faster existing state art stochastic admm methods
doubly accelerated stochastic variance reduced dual averaging method regularized empirical risk minimization develop new accelerated stochastic gradient method efficiently solving convex regularized empirical risk minimization problem mini batch settings use mini batches becoming golden standard machine learning community mini batch settings stabilize gradient estimate easily make good use parallel computing core proposed method incorporation new double acceleration technique variance reduction technique theoretically analyze proposed method show method much improves mini batch efficiencies previous accelerated stochastic methods essentially needs size sqrt mini batches achieving optimal iteration complexities non strongly strongly convex objectives training set size show even non mini batch settings method surpasses best known convergence rate non strongly convex objectives achieves strongly convex objectives
limitations variance reduction acceleration schemes finite sums optimization study conditions able efficiently apply variance reduction acceleration schemes finite sums problems first show perhaps surprisingly finite sum structure sufficient obtaining complexity bound tilde epsilon smooth strongly convex finite sums must also know exactly individual function referred oracle iteration next show broad class first order coordinate descent finite sums algorithms including sdca svrg sag possible get accelerated complexity bound tilde sqrt epsilon unless strong convexity parameter given explicitly lastly show class algorithms used minimizing smooth non strongly convex finite sums optimal complexity bound tilde epsilon assuming average update rule used iteration tilde sqrt epsilon otherwise
nonlinear acceleration stochastic algorithms extrapolation methods use last iterates optimization algorithm produce better estimate optimum shown achieve optimal convergence rates deterministic setting using simple gradient iterates study extrapolation methods stochastic setting iterates produced either simple accelerated stochastic gradient algorithm first derive convergence bounds arbitrary potentially biased perturbations produce asymptotic bounds using ratio variance noise accuracy current point finally apply acceleration technique stochastic algorithms sgd saga svrg katyusha different settings show significant performance gains
acceleration averaging stochastic descent dynamics formulate study general family continuous time stochastic dynamics accelerated first order minimization smooth convex functions building averaging formulation accelerated mirror descent propose stochastic variant gradient contaminated noise study resulting stochastic differential equation prove bound rate change energy function associated problem use derive estimates convergence rates function values expectation persistent asymptotically vanishing noise discuss interaction parameters dynamics learning rate averaging weights variation noise process show particular asymptotic rate variation affects choice parameters ultimately convergence rate
multiscale semi markov dynamics intracortical brain computer interfaces intracortical brain computer interfaces allow people tetraplegia control computer cursor imagining motion paralyzed limbs standard decoders derived kalman filter assumes markov dynamics angle intended movement unimodal likelihood channel neural activity due errors made decoding noisy neural data user attempts move cursor goal angle cursor goal positions change rapidly propose dynamic bayesian network includes screen goal position part latent state thus allows motion cues aggregated much longer history neural activity multiscale model explicitly captures relationship instantaneous angles motion long term goals incorporates semi markov dynamics motion trajectories also propose flexible likelihood model recordings neural populations offline experiments recorded neural data demonstrate significantly improved prediction motion directions compared kalman filter baselines derive efficient online inference algorithm enabling clinical trial participant tetraplegia control computer cursor neural activity real time
eeg graph factor graph based model capturing spatial temporal observational relationships electroencephalograms paper reports factor graph based model brain activity jointly describes instantaneous observation based temporal spatial dependencies factor functions represent dependencies defined manually based domain knowledge model validated using clinically collected intracranial electroencephalogram eeg data epilepsy patients application seizure onset localization results indicate model outperforms conventional approaches devised using observational dependency alone better auc furthermore also show manual definitions factor functions allow solve graph inference exactly using graph cut algorithm experiments show proposed inference technique provides gain auc compared sampling based alternatives
asynchronous parallel coordinate minimization map inference finding maximum posteriori map assignment central task graphical models since modern applications give rise large problem instances increasing need efficient solvers work propose improve efficiency coordinate minimization based dual decomposition solvers running updates asynchronously parallel case message passing inference performed multiple processing units simultaneously without coordination reading writing shared memory analyze convergence properties resulting algorithms identify settings speedup gains expected numerical evaluations show approach indeed achieves significant speedups common computer vision tasks
speeding latent variable gaussian graphical model estimation via nonconvex optimization study estimation latent variable gaussian graphical model lvggm precision matrix superposition sparse matrix low rank matrix order speed estimation sparse plus low rank components propose sparsity constrained maximum likelihood estimator based matrix factorization efficient alternating gradient descent algorithm hard thresholding solve algorithm orders magnitude faster convex relaxation based methods lvggm addition prove algorithm guaranteed linearly converge unknown sparse low rank components optimal statistical precision experiments synthetic genomic data demonstrate superiority algorithm state art algorithms corroborate theory
expxorcist nonparametric graphical models via conditional exponential densities non parametric multivariate density estimation faces strong statistical computational bottlenecks practical approaches impose near parametric assumptions form density functions paper leverage recent developments propose class non parametric models attractive computational statistical properties approach relies simple function space assumption conditional distribution variable conditioned variables non parametric exponential family form
reducing reparameterization gradient variance optimization noisy gradients become ubiquitous statistics machine learning reparameterization gradients gradient estimates computed via reparameterization trick represent class noisy gradients often used monte carlo variational inference mcvi however gradient estimators noisy optimization procedure slow fail converge way reduce noise use samples gradient estimate computationally expensive instead view noisy gradient random variable form inexpensive approximation generating procedure gradient sample approximation high correlation noisy gradient construction making useful control variate variance reduction demonstrate approach non conjugate multi level hierarchical models bayesian neural net observed gradient variance reductions multiple orders magnitude 000 times
robust conditional probabilities conditional probabilities core concept machine learning example optimal prediction label given input corresponds maximizing conditional probability given common approach inference tasks learning model conditional probabilities however models often based strong assumptions log linear models hence estimate conditional probabilities robust highly dependent validity assumptions propose framework reasoning conditional probabilities without assuming anything underlying distributions except knowledge second order marginals estimated data show setting leads guaranteed bounds conditional probabilities calculated efficiently variety settings including structured prediction finally apply semi supervised deep learning obtaining results competitive variational autoencoders
stein variational gradient descent gradient flow stein variational gradient descent svgd deterministic sampling algorithm iteratively transports set particles approximate given distributions based efficient gradient based update guarantees optimally decrease divergence within function space paper develops first theoretical analysis svgd establish empirical measures svgd samples weakly converges target distribution show asymptotic behavior svgd characterized nonlinear fokker planck equation known vlasov equation physics develop geometric perspective views svgd gradient flow divergence functional new metric structure space distributions induced stein operator
parallel streaming wasserstein barycenters efficiently aggregating data different sources challenging problem particularly samples source distributed differently differences inherent inference task present reasons sensors sensor network placed far apart affecting individual measurements conversely computationally advantageous split bayesian inference tasks across subsets data data need identically distributed across subsets principled way fuse probability distributions via lens optimal transport wasserstein barycenter single distribution summarizes collection input measures respecting geometry however computing barycenter scales poorly requires discretization input distributions barycenter improving situation present scalable communication efficient parallel algorithm computing wasserstein barycenter arbitrary distributions algorithm operate directly continuous input distributions optimized streaming data method even robust nonstationary input distributions produces barycenter estimate tracks input measures time algorithm semi discrete needing discretize barycenter estimate best knowledge also provide first bounds quality approximate barycenter discretization becomes finer finally demonstrate practical effectiveness method tracking moving distributions sphere well large scale bayesian inference task
aide algorithm measuring accuracy probabilistic inference algorithms approximate probabilistic inference algorithms central many fields examples include sequential monte carlo inference robotics variational inference machine learning markov chain monte carlo inference statistics key problem faced practitioners measuring accuracy approximate inference algorithm specific dataset existing techniques measuring inference accuracy often brittle specialized type inference algorithm paper introduces auxiliary inference divergence estimator aide algorithm measuring accuracy approximate inference algorithms aide based observation inference algorithms treated probabilistic models random variables used within inference algorithm viewed auxiliary variables view leads new estimator symmetric divergence output distributions inference algorithms paper illustrates application aide algorithms inference regression hidden markov dirichlet process mixture models experiments show aide captures qualitative behavior broad class inference algorithms detect failure modes inference algorithms missed standard heuristics
deep dynamic poisson factorization model new model named deep dynamic poisson factorization model analyzing sequential count vectors proposed paper model based poisson factor analysis method captures dependence among time steps neural networks representing implicit distributions local complicated relationship obtained local implicit distribution deep latent structure exploited get long time dependence variational inference latent variables gradient descent based loss functions derived variational distribution performed inference synthetic dataset real world dataset applied proposed model results show good predicting fitting performance interpretable latent structure
model shrinkage effect gamma process edge partition models edge partition model epm fundamental bayesian nonparametric model extracting overlapping structure binary matrix epm adopts gamma process gamma prior automatically shrink number active atoms however empirically found model shrinkage epm typically work appropriately leads overfitted solution analysis expectation epm intensity function suggested gamma priors epm hyperparameters disturb model shrinkage effect internal gamma order ensure model shrinkage effect epm works appropriate manner proposed novel generative constructions epm cepm incorporating constrained gamma priors depm incorporating dirichlet priors instead gamma priors furthermore depm model parameters including infinite atoms gamma prior could marginalized thus possible derive truly infinite depm idepm efficiently inferred using collapsed gibbs sampler experimentally confirmed model shrinkage proposed models works well idepm indicated state art performance generalization ability link prediction accuracy mixing efficiency convergence speed
model evidence nonequilibrium simulations marginal likelihood model evidence key quantity bayesian parameter estimation model comparison many probabilistic models computation marginal likelihood challenging involves sum integral enormous parameter space markov chain monte carlo mcmc powerful approach compute marginal likelihoods various mcmc algorithms evidence estimators proposed literature discuss use nonequilibrium techniques estimating marginal likelihood nonequilibrium estimators build recent developments statistical physics known annealed importance sampling ais reverse ais probabilistic machine learning introduce new estimators model evidence combine forward backward simulations show various challenging models new evidence estimators outperform forward reverse ais
nice adversarial training mcmc existing markov chain monte carlo mcmc methods either based general purpose domain agnostic schemes lead slow convergence require hand crafting problem specific proposals expert propose nice novel method train flexible parametric markov chain kernels produce samples desired properties first propose efficient likelihood free adversarial training method train markov chain mimic given data distribution leverage flexible volume preserving flows obtain parametric kernels mcmc using bootstrap approach show train efficient markov chains sample prescribed posterior distribution iteratively improving quality model samples nice provides first framework automatically design efficient domain specific mcmc proposals empirical results demonstrate nice combines strong guarantees mcmc expressiveness deep neural networks able significantly outperform competing methods hamiltonian monte carlo
identification gaussian process state space models gaussian process state space model gpssm non linear dynamical system unknown transition measurement mappings described gps research gpssms focussed state estimation problem however key challenge gpssms satisfactorily addressed yet system identification address challenge impose structured gaussian variational posterior distribution latent states parameterised recognition model form directional recurrent neural network inference structure allows recover posterior smoothed entire sequence data provide practical algorithm efficiently computing lower bound marginal likelihood using reparameterisation trick additionally allows arbitrary kernels used within gpssm demonstrate efficiently generate plausible future trajectories system seek model gpssm requiring small number interactions true system
streaming sparse gaussian process approximations sparse approximations gaussian process models provide suite methods enable models deployed large data regime enable analytic intractabilities sidestepped however field lacks principled method handle streaming data posterior distribution function values hyperparameters updated online fashion small number existing approaches either use suboptimal hand crafted heuristics hyperparameter learning suffer catastrophic forgetting slow updating new data arrive paper develops new principled framework deploying gaussian process probabilistic models streaming setting providing principled methods learning hyperparameters optimising pseudo input locations proposed framework experimentally validated using synthetic real world datasets
bayesian optimization gradients bayesian optimization shown success global optimization expensive evaluate multimodal objective functions however unlike optimization methods bayesian optimization typically use derivative information paper show bayesian optimization exploit derivative information find good solutions fewer objective function evaluations particular develop novel bayesian optimization algorithm derivative enabled knowledge gradient dkg step bayes optimal asymptotically consistent provides greater step value information derivative free setting dkg accommodates noisy incomplete derivative information comes sequential batch forms optionally reduce computational cost inference automatically selected retention single directional derivative also compute dkg acquisition function gradient using novel fast discretization free technique show dkg provides state art performance compared wide range optimization procedures without gradients benchmarks including logistic regression deep learning kernel learning nearest neighbors
variational inference gaussian process models linear complexity large scale gaussian process inference long faced practical challenges due time space complexity superlinear dataset size sparse variational gaussian process models capable learning large scale data standard strategies sparsifying model prevent approximation complex functions work propose novel variational gaussian process model decouples representation mean covariance functions reproducing kernel hilbert space show new parametrization generalizes previous models yields variational inference problem solved stochastic gradient ascent time space complexity linear number mean function parameters strategy makes adoption large scale expressive gaussian process models possible run several experiments regression tasks show decoupled approach greatly outperforms previous sparse variational gaussian process inference procedures
efficient modeling latent information supervised learning using gaussian processes often machine learning data collected combination multiple conditions voice recordings multiple persons labeled could build model captures latent information related conditions generalize new data present new model called latent variable multiple output gaussian processes lvmogp allows jointly model multiple conditions regression generalize new condition data points test time lvmogp infers posteriors gaussian processes together latent space representing information different conditions derive efficient variational inference method lvmogp computational complexity low sparse gaussian processes show lvmogp significantly outperforms related gaussian process methods various tasks synthetic real data
non stationary spectral kernels propose non stationary spectral kernels gaussian process regression propose model spectral density non stationary kernel function mixture input dependent gaussian process frequency density surfaces solve generalised fourier transform model present family non stationary non monotonic kernels learn input dependent potentially long range non monotonic covariances inputs derive efficient inference using model whitening marginalized posterior show case studies kernels necessary modelling even rather simple time series image geospatial data non stationary characteristics
scalable log determinants gaussian process kernel learning applications varied bayesian neural networks determinantal point processes elliptical graphical models kernel learning gaussian processes gps must compute log determinant positive definite matrix derivatives leading prohibitive computations propose novel approaches estimating quantities fast matrix vector multiplications mvms stochastic approximations based chebyshev lanczos surrogate models converge quickly even kernel matrices challenging spectra leverage approximations develop scalable gaussian process approach kernel learning find lanczos generally superior chebyshev kernel learning surrogate approach highly efficient accurate popular kernels
spectral mixture kernels multi output gaussian processes initially multiple output gaussian processes mogps models relied linear transformations independent latent single output gaussian processes gps resulted cross covariance functions limited parametric interpretation thus conflicting single output gps intuitive understanding lengthscales frequencies magnitudes name contrary current approaches mogp able better interpret relationship different channels directly modelling cross covariances spectral mixture kernel phase shift propose parametric family complex valued cross spectral densities build cramer theorem multivariate version bochner theorem provide principled approach design multivariate covariance functions constructed kernels able model delays among channels addition phase differences thus expressive previous methods also providing full parametric interpretation relationship across channels proposed method validated synthetic data compared existing mogp methods real world examples
linearly constrained gaussian processes consider modification covariance function gaussian processes correctly account known linear constraints modelling target function transformation underlying function constraints explicitly incorporated model guaranteed fulfilled sample drawn prediction made also propose constructive procedure designing transformation operator illustrate result simulated real data examples
hindsight experience replay dealing sparse rewards biggest challenges reinforcement learning present novel technique called hindsight experience replay allows sample efficient learning rewards sparse binary therefore avoid need complicated reward engineering combined arbitrary policy algorithm seen form implicit curriculum demonstrate approach task manipulating objects robotic arm particular run experiments different tasks pushing sliding pick place case using binary rewards indicating whether task completed ablation studies show hindsight experience replay crucial ingredient makes training possible challenging environments show policies trained physics simulation deployed physical robot successfully complete task video presenting experiments available https goo smrqni
log normality skewness estimated state action values reinforcement learning overestimation state action values harmful reinforcement learning agents paper show state action value estimated using bellman equation decomposed weighted sum path wise values follow log normal distributions since log normal distributions skewed distribution estimated values also skewed leading imbalanced likelihood overestimation degree imbalance vary greatly among actions policies within problem instance making agent prone select actions policies inferior expected return higher likelihood overestimation present comprehensive analysis skewness examine factors impacts theoretical empirical results discuss possible ways reduce undesirable effect skewness
finite sample analysis gtd policy evaluation algorithms markov setting reinforcement learning key component policy evaluation aims estimate value function expected long term accumulated reward starting state given policy good policy evaluation method algorithms estimate value functions given policy accurately find better policy state space large continuous emph gradient based temporal difference gtd algorithms linear function approximation value function widely used considering collection evaluation data likely time reward consuming get clear understanding finite sample performance gtd algorithms important efficiency policy evaluation entire algorithms previous work converted gtd algorithms convex concave saddle point problem provided finite sample analysis gtd algorithms constant step size assumption data generated however know problems data generated markov processes rather step size set different ways paper realistic markov setting derive finite sample bounds expectation high probability general convex concave saddle point problem hence gtd algorithms bounds show markov setting variants step size gtd algorithms converge convergence rate determined step size related mixing time markov process explain experience reply trick effective since improve mixing property markov process best knowledge analysis first provide finite sample bounds gtd algorithms markov setting
inverse filtering hidden markov models paper considers related inverse filtering problems hidden markov models hmms given sequence state posteriors system dynamics estimate corresponding sequence observations estimate observation likelihoods iii jointly estimate observation likelihoods observation sequence problems motivated challenges reverse engineering sensors including calibration diagnostics show avoid computationally expensive mixed integer linear program milp exploiting structure hmm filter provide conditions quantities uniquely recovered finally also consider case posteriors corrupted noise shown problem naturally posed clustering problem proposed algorithm evaluated real world polysomnographic data used automatic sleep staging
safe model based reinforcement learning stability guarantees reinforcement learning powerful paradigm learning optimal policies experimental data however find optimal policies reinforcement learning algorithms explore possible actions harmful real world systems consequence learning algorithms rarely applied safety critical systems real world paper present learning algorithm explicitly considers safety terms stability guarantees specifically extend control theoretic results lyapunov stability verification show use statistical models dynamics obtain high performance control policies provable stability certificates moreover additional regularity assumptions terms gaussian process prior prove effectively safely collect data order learn dynamics thus improve control performance expand safe region state space experiments show resulting algorithm safely optimize neural network policy simulated inverted pendulum without pendulum ever falling
data efficient reinforcement learning continuous state action gaussian pomdps present data efficient reinforcement learning method continuous state action systems significant observation noise data efficient solutions small noise exist pilco learns cartpole swing task 30s pilco evaluates policies planning state trajectories using dynamics model however pilco applies policies observed state therefore planning observation space extend pilco filtering instead plan belief space consistent partially observable markov decisions process pomdp planning enables data efficient learning significant observation noise outperforming naive methods post hoc application filter policies optimised original unfiltered pilco algorithm test method cartpole swing task involves nonlinear dynamics requires nonlinear control
linear regression without correspondence article considers algorithmic statistical aspects linear regression correspondence covariates responses unknown first fully polynomial time approximation scheme given natural least squares optimization problem constant dimension next average case noise free setting responses exactly correspond linear function draws standard multivariate normal distribution efficient algorithm based lattice basis reduction shown exactly recover unknown linear function arbitrary dimension finally lower bounds signal noise ratio established approximate recovery unknown linear function
complexity learning neural networks stunning empirical successes neural networks currently lack rigorous theoretical eplanation form would explanation take face existing complexity theoretic lower bounds first step might show data generated neural networks single hidden layer smooth activation functions benign input distributions learned efficiently demonstrate comprehensive lower bound ruling possibility wide class activation functions including currently used inputs drawn logconcave distribution family hidden layer functions whose output sum gate hard learn precise sense statistical query algorithm includes known variants stochastic gradient descent loss function needs exponential number queries even using tolerance inversely proportional input dimensionality moreover hard family functions realizable small sublinear dimension number activation units single hidden layer lower bound also robust small perturbations true weights systematic experiments illustrate phase transition training error predicted analysis
near optimal sketching low rank tensor regression study least squares regression problem min_ theta p_1 times cdots times p_d theta theta low rank tensor defined theta sum_ theta_1 circ cdots circ theta_d vectors theta_d mathbb p_d small compared p_1 ldots p_d circ denotes outer product vectors theta linear function theta problem motivated fact number parameters theta cdot sum_ p_d significantly smaller prod_ p_d number parameters ordinary least squares regression consider decomposition model tensors theta well tucker decomposition models show apply data dimensionality reduction techniques based sparse random projections phi times reduce problem much smaller problem min_ theta phi theta phi phi theta phi varepsilon theta holds simultaneously theta obtain significantly smaller dimension sparsity randomized linear mapping phi possible ordinary least squares regression finally give number numerical simulations supporting theory
input sparsity time possible kernel low rank approximation low rank approximation common tool used accelerate kernel methods times kernel matrix approximated via rank matrix tilde stored much less space processed quickly work study limits computationally efficient low rank kernel approximation show broad class kernels including popular gaussian polynomial kernels computing relative error rank approximation least difficult multiplying input data matrix times arbitrary matrix times barring breakthrough fast matrix multiplication large requires omega nnz time nnz number non zeros lower bound matches many parameter regimes recent work subquadratic time algorithms low rank approximation general kernels mm16 mm17 demonstrating algorithms unlikely significantly improved particular nnz input sparsity runtimes time hope show first time nnz time approximation possible general radial basis function kernels gaussian kernel closely related problem low rank approximation kernelized dataset
higher order total variation classes grids minimax theory trend filtering methods consider problem estimating values function nodes dimensional grid graph equal side lengths smash noisy observations function assumed smooth allowed exhibit different amounts smoothness different regions grid heterogeneity eludes classical measures smoothness nonparametric statistics holder smoothness meanwhile total variation smoothness classes allow heterogeneity restrictive another sense constant functions counted perfectly smooth achieve move past consider higher order classes based ways compiling discrete derivatives parameter across nodes relate classes holder classes derive minimax error rates higher order classes analyze naturally associated trend filtering methods seen optimal appropriate class
alternating estimation structured high dimensional multi response models consider problem learning high dimensional multi response linear models structured parameters exploiting noise correlations among different responses propose alternating estimation altest procedure estimate model parameters based generalized dantzig selector gds suitable sample size resampling assumptions show error estimates generated altest high probability converges linearly certain minimum achievable level tersely expressed geometric measures gaussian width sets related parameter structure best knowledge first non asymptotic statistical guarantee altest type algorithm applied estimation general structures
compressing gram matrix learning neural networks polynomial time consider problem learning function classes computed neural networks various activations relu sigmoid task believed intractable worst case major open problem understand minimal assumptions classes admit efficient algorithms work show natural distributional assumption eigenvalue decay gram matrix yields polynomial time algorithms non realizable setting expressive classes networks feed forward networks relus make assumptions network architecture labels given sufficiently strong polynomial eigenvalue decay obtain fully polynomial time algorithms parameters respect square loss milder decay also leads improved algorithms aware prior work assumption marginal distribution alone leads polynomial time algorithms networks relus even hidden layer unlike prior assumptions marginal distribution gaussian eigenvalue decay observed practice common data sets algorithm applies function class embedded suitable rkhs main technical contribution new approach proving generalization bounds kernelized regression using compression schemes opposed rademacher bounds general known sample complexity bounds kernel methods must depend norm corresponding rkhs quickly become large depending kernel function employed sidestep worst case bounds sparsifying gram matrix using recent work recursive nystrom sampling due musco musco prove approximate sparse hypothesis admits compression scheme whose true error depends rate eigenvalue decay
learning average top loss work introduce average top atk loss new ensemble loss supervised learning atk loss provides natural generalization widely used ensemble losses namely average loss maximum loss furthermore atk loss combines advantages alleviate corresponding drawbacks better adapt different data distributions show atk loss affords intuitive interpretation reduces penalty continuous convex individual losses correctly classified data atk loss lead convex optimization problems solved effectively conventional sub gradient based method study statistical learning theory matk establishing classification calibration statistical consistency matk provide useful insights practical choice parameter demonstrate applicability matk learning combined different individual loss functions binary multi class classification regression using synthetic real datasets
hierarchical clustering beyond worst case hiererachical clustering computing recursive partitioning dataset obtain clusters increasingly finer granularity fundamental problem data analysis although hierarchical clustering mostly studied procedures linkage algorithms top heuristics rather optimization problems recently dasgupta proposed objective function hierarchical clustering initiated line work developing algorithms explicitly optimize objective see also paper consider fairly general random graph model hierarchical clustering called hierarchical stochastic blockmodel hsbm show certain regimes svd approach mcsherry combined specific linkage methods results clustering give approximation dasgupta cost function also show approach based sdp relaxations balanced cuts based work makarychev combined recursive sparsest cut algorithm dasgupta yields approximation slightly larger regimes also semi random setting adversary remove edges random graph generated according hsbm finally report empirical evaluation synthetic real world data showing proposed svd based method indeed achieve better cost widely used heurstics also results better classification accuracy underlying problem multi class classification
net trim convex pruning deep neural networks performance guarantee introduce analyze new technique model reduction deep neural networks large networks theoretically capable learning arbitrarily complex models overfitting model redundancy negatively affects prediction accuracy model variance net trim algorithm prunes sparsifies trained network layer wise removing connections layer solving convex optimization program program seeks sparse set weights layer keeps layer inputs outputs consistent originally trained model algorithms associated analysis applicable neural networks operating rectified linear unit relu nonlinear activation present parallel cascade versions algorithm latter achieve slightly simpler models generalization performance former computed distributed manner cases net trim significantly reduces number connections network also providing enough regularization slightly reduce generalization error also provide mathematical analysis consistency initial network retrained model analyze model sample complexity derive general sufficient conditions recovery sparse transform matrix single layer taking independent gaussian random vectors inputs show network response described using maximum number non weights per node weights learned log samples
graph theoretic approach multitasking key feature neural network architectures ability support simultaneous interaction among large numbers units learning processing representations however richness interactions trades ability network simultaneously carry multiple independent processes salient limitation many domains human cognition remains largely unexplored paper use graph theoretic analysis network architecture address question tasks represented edges bipartite graph cup define new measure multitasking capacity networks based assumptions tasks emph need multitasked rely independent resources form matching tasks emph performed without interference form induced matching main result inherent tradeoff multitasking capacity average degree network holds emph regardless network architecture results also extended networks depth greater positive side demonstrate networks random like locally sparse desirable multitasking properties results shed light parallel processing limitations neural systems provide insights useful analysis design parallel architectures
information theoretic analysis generalization capability learning algorithms derive upper bounds generalization error learning algorithm terms mutual information input output upper bounds provide theoretical guidelines striking right balance data fit generalization controlling input output mutual information learning algorithm results also used analyze generalization capability learning algorithms adaptive composition bias accuracy tradeoffs adaptive data analytics work extends leads nontrivial improvements recent results russo zou
independence clustering without matrix independence clustering problem considered following formulation given set random variables required find finest partitioning u_1 dots u_k clusters clusters u_1 dots u_k mutually independent since mutual independence target pairwise similarity measurements use thus traditional clustering algorithms inapplicable distribution random variables general unknown sample available thus problem cast terms time series forms sampling considered stationary time series main emphasis latter general case consistent computationally tractable algorithm settings proposed number fascinating open directions research outlined
polynomial codes optimal design high dimensional coded matrix multiplication consider large scale matrix multiplication problem computation carried using distributed system master node multiple worker nodes worker store parts input matrices propose computation strategy leverages ideas coding theory design intermediate computations worker nodes order efficiently deal straggling workers proposed strategy named emph polynomial codes achieves optimum recovery threshold defined minimum number workers master needs wait order compute output furthermore leveraging algebraic structure polynomial codes map reconstruction problem final output polynomial interpolation problem solved efficiently polynomial codes provide order wise improvement state art terms recovery threshold also optimal terms several metrics furthermore extend code distributed convolution show order wise optimality
estimating mutual information discrete continuous mixtures estimation mutual information observed samples basic primitive machine learning useful several learning tasks including correlation mining information bottleneck chow liu tree conditional independence testing causal graphical models mutual information quantity well defined general probability spaces estimators developed special case discrete continuous pairs random variables estimators operate using principle calculating differential entropies pair however general mixture spaces individual entropies well defined even though mutual information paper develop novel estimator estimating mutual information discrete continuous mixtures prove consistency estimator theoretically well demonstrate excellent empirical performance problem relevant wide array applications variables discrete continuous others mixture continuous discrete components
best response regression regression task predictor given set instances along real value point subsequently identify value new instance accurately possible work initiate study strategic predictions machine learning consider regression task tackled players payoff player proportion points predicts accurately player first revise probably approximately correct learning framework deal case duel predictors devise algorithm finds linear regression predictor best response necessarily linear regression algorithm show linearithmic sample complexity polynomial time complexity dimension instances domain fixed also test approach high dimensional setting show significantly defeats classical regression algorithms prediction duel together work introduces novel machine learning task lends well current competitive online settings provides theoretical foundations illustrates applicability
statistical cost sharing study cost sharing problem cooperative games situations cost function available via oracle queries must instead learned samples drawn distribution represented tuples different subsets players formalize approach call statistical cost sharing consider computation core shapley value expanding work balcan 2015 give precise sample complexity bounds computing cost shares satisfy core property high probability function non empty core shapley value never studied setting show submodular cost functions curvature bounded curvature kappa approximated samples uniform distribution sqrt kappa factor bound tight define statistical analogues shapley axioms derive notion statistical shapley value approximated arbitrarily well samples distribution function
sample complexity measure applications learning optimal auctions introduce new sample complexity measure refer split sample growth rate hypothesis sample size split sample growth rate hat tau counts many different hypotheses empirical risk minimization output sub sample size show expected generalization error upper bounded left sqrt frac log hat tau right result enabled strengthening rademacher complexity analysis expected generalization error show sample complexity measure greatly simplifies analysis sample complexity optimal auction design many auction classes studied literature sample complexity derived solely noticing auction classes erm sample sub sample pick parameters equal points sample
multiplicative weights update constant step size congestion games convergence limit cycles chaos multiplicative weights update mwu method ubiquitous meta algorithm works follows distribution maintained certain set step probability assigned action gamma multiplied epsilon gamma gamma cost action gamma rescaled ensure new values form distribution analyze mwu congestion games agents use textit arbitrary admissible constants learning rates epsilon prove convergence textit exact nash equilibria interestingly convergence result carry nearly homologous mwu variant step probability assigned action gamma multiplied epsilon gamma even innocuous case agent strategy load balancing games dynamics provably lead limit cycles even chaotic behavior
efficiency guarantees data analysis efficiency outcomes game theoretic settings main item study intersection economics computer science notion price anarchy takes worst case stance efficiency analysis considering instance independent guarantees efficiency propose data dependent analog price anarchy refines worst case assuming access samples strategic behavior focus auction settings latter non trivial due private information held participants approach bounding efficiency data robust statistical errors mis specification unlike traditional econometrics seek learn private information players observed behavior analyze properties outcome directly quantify inefficiency without going private information apply approach datasets sponsored search auction system find empirical results significant improvement bounds worst case analysis
safe nested subgame solving imperfect information games unlike perfect information games imperfect information games cannot solved decomposing game subgames solved independently thus computationally intensive equilibrium finding techniques used decisions must consider strategy game whole possible solve imperfect information game exactly decomposition possible approximate solutions improve existing solutions solving disjoint subgames process referred subgame solving introduce subgame solving techniques outperform prior methods theory practice also show adapt past subgame solving techniques respond opponent actions outside original action abstraction significantly outperforms prior state art approach action translation finally show subgame solving repeated game progresses tree leading significantly lower exploitability applied techniques develop first defeat top humans heads limit texas hold poker
deep reinforcement learning human preferences sophisticated reinforcement learning systems interact usefully real world environments need communicate complex goals systems work explore goals defined terms non expert human preferences pairs trajectory segments approach separates learning goal learning behavior achieve show approach effectively solve complex tasks without access reward function including atari games simulated robot locomotion providing feedback agent interactions environment reduces cost human oversight far enough practically applied state art systems demonstrate flexibility approach show successfully train complex novel behaviors hour human time behaviors environments considerably complex previously learned human feedback
multi modal imitation learning unstructured demonstrations using generative adversarial nets imitation learning traditionally applied learn single task demonstrations thereof requirement structured isolated demonstrations limits scalability imitation learning approaches difficult apply real world scenarios robots able execute multitude tasks paper propose multi modal imitation learning framework able segment imitate skills unlabelled unstructured demonstrations learning skill segmentation imitation learning jointly extensive simulation results indicate method efficiently separate demonstrations individual skills learn imitate using single multi modal policy
ex2 exploration exemplar models deep reinforcement learning deep reinforcement learning algorithms shown learn complex tasks using highly general policy classes however sparse reward problems remain significant challenge exploration methods based novelty detection particularly successful settings typically require generative predictive models observations difficult train observations high dimensional complex case raw images propose novelty detection algorithm exploration based entirely discriminatively trained exemplar models classifiers trained discriminate visited state others intuitively novel states easier distinguish states seen training show kind discriminative modeling corresponds implicit density estimation combined count based exploration produce competitive results range popular benchmark tasks including state art results challenging egocentric observations vizdoom benchmark
exploration study count based exploration deep reinforcement learning count based exploration algorithms known perform near optimally used conjunction tabular reinforcement learning methods solving small discrete markov decision processes mdps generally thought count based methods cannot applied high dimensional state spaces since states occur recent deep exploration strategies able deal high dimensional continuous state spaces complex heuristics often relying optimism face uncertainty intrinsic motivation work describe surprising finding simple generalization classic count based approach reach near state art performance various high dimensional continuous deep benchmarks states mapped hash codes allows count occurrences hash table counts used compute reward bonus according classic count based exploration theory find simple hash functions achieve surprisingly good results many challenging tasks furthermore show domain dependent learned hash code improve results detailed analysis reveals important aspects good hash function appropriate granularity encoding information relevant solving mdp exploration strategy achieves near state art performance continuous control tasks atari 2600 games hence providing simple yet powerful baseline solving mdps require considerable exploration
thinking fast slow deep learning tree search solving sequential decision making problems text parsing robotic control game playing requires combination planning policies generalisation plans paper present expert iteration novel algorithm decomposes problem separate planning generalisation tasks planning new policies performed tree search deep neural network generalises plans contrast standard deep reinforcement learning algorithms rely neural network generalise plans discover show method substantially outperforms policy gradients board game hex winning games trained equal time
natural value approximators learning trust past estimates neural networks smooth initial inductive bias small changes input lead large changes output however reinforcement learning domains sparse rewards value functions non smooth structure characteristic asymmetric discontinuity whenever rewards arrive propose mechanism learns interpolation direct value estimate projected value estimate computed encountered reward previous estimate reduces need learn discontinuities thus improves value function approximation furthermore interpolation learned state dependent method deal heterogeneous observability demonstrate change leads significant improvements multiple atari games applied state art a3c algorithm
active exploration learning symbolic representations introduce online active exploration algorithm data efficiently learning abstract symbolic model environment algorithm divided parts first part quickly generates intermediate bayesian symbolic model data agent collected far agent use along second part guide future exploration towards regions state space model uncertain show algorithm outperforms random greedy exploration policies different computer game domains first domain asteroids inspired game complex dynamics basic logical structure second treasure game simpler dynamics complex logical structure
state aware imitation learning imitation learning study learning act given set demonstrations provided human expert intuitively apparent learning take optimal actions simpler undertaking situations similar ones shown teacher however imitation learning approaches tend use insight directly paper introduce state aware imitation learning sail imitation learning algorithm allows agent learn remain states confidently take correct action recover lead astray key algorithm gradient learned using temporal difference update rule leads agent prefer states similar demonstrated states show estimating linear approximation gradient yields similar theoretical guarantees online temporal difference learning approaches empirically show sail effectively used imitation learning continuous domains non linear function approximators used policy representation gradient estimate
successor features transfer reinforcement learning transfer reinforcement learning refers notion generalization occur within task also across tasks propose transfer framework scenario reward function changes task environment dynamics remain approach rests key ideas successor features value function representation decouples dynamics environment rewards generalized policy improvement generalization dynamic programming policy improvement step considers set policies rather single put together ideas lead approach integrates seamlessly within reinforcement learning framework allows free exchange information tasks proposed method also provides performance guarantees transferred policy even learning taken place derive theorems set approach firm theoretical ground present experiments show successfully promotes transfer practice significantly outperforming alternative methods sequence navigation tasks control simulated joint robotic arm
bridging gap value policy based reinforcement learning establish new connection value policy based reinforcement learning based relationship softmax temporal value consistency policy optimality entropy regularization specifically show softmax consistent action values satisfy strong consistency property optimal entropy regularized policy probabilities along action sequence regardless provenance observation develop new algorithm path consistency learning pcl minimizes inconsistency measured along multi step action sequences extracted policy traces subsequently deepen relationship showing single model used represent policy softmax action values beyond eliminating need separate critic unification demonstrates policy gradients stabilized via self bootstrapping policy data experimental evaluation demonstrates algorithms significantly outperform strong actor critic learning baselines across several benchmark tasks
using options covariance testing long horizon policy policy evaluation evaluating policy deploying real world risky costly policy policy evaluation ope algorithms use historical data collected running previous policy evaluate new policy provides means evaluating policy without requiring ever deployed importance sampling popular ope method robust partial observability works continuous states actions however amount historical data required importance sampling scale exponentially horizon problem number sequential decisions made propose using policies temporally extended actions called options show combining policies importance sampling significantly improve performance long horizon problems addition take advantage special cases arise due options based policies furthermore improve performance importance sampling generalize special cases general covariance testing rule used decide weights drop estimate derive new algorithm called incremental importance sampling provide significantly accurate estimates broad class domains
compatible reward inverse reinforcement learning inverse reinforcement learning irl effective approach recover reward function explains behavior expert observing set demonstrations paper novel model free irl approach differently existing irl algorithms require specify function space search expert reward function leveraging fact policy gradient needs optimal policy algorithm generates set basis functions span subspace reward functions make policy gradient vanish within subspace using second order criterion search reward function penalizes deviation expert policy introducing approach finite domains extend continuous ones proposed approach empirically compared irl methods finite taxi domain continuous linear quadratic gaussian lqg car hill environments
adaptive batch size safe policy gradients policy gradient methods among best reinforcement learning techniques solve complex control problems real world applications common good initial policy whose performance needs improved acceptable try bad policies learning process although several methods choosing step size exist parameter significantly affects speed stability gradient methods research paid less attention determine number samples used estimate gradient direction batch size update policy parameters paper propose set methods jointly optimize step batch sizes guarantee high probability improve policy performance update besides providing theoretical guarantees show numerical simulations analyze behavior methods
regret minimization mdps options without prior knowledge option framework integrates temporal abstraction reinforcement learning model introduction macro actions options recent works leveraged mapping markov decision processes mdps options semi mdps smdps introduced smdp versions exploration exploitation algorithms rmaxsmdp ucrlsmdp analyze impact options learning performance nonetheless pac smdp sample complexity rmaxsmdp hardly translated equivalent pac mdp theoretical guarantees ucrlsmdp requires prior knowledge parameters characterizing distributions cumulative reward duration option hardly available practice paper remove limitation combining smdp view together inner markov structure options novel algorithm whose regret performance matches ucrlsmdp additive regret term show scenarios term negligible advantage temporal abstraction preserved also report preliminary empirical result supporting theoretical findings
bellman residual bad proxy paper aims theoretically empirically comparing standard optimization criteria reinforcement learning maximization mean value minimization bellman residual purpose place framework policy search algorithms usually designed maximize mean value derive method minimizes residual policies theoretical analysis shows good proxy policy optimization notably better value based counterpart also propose experiments randomly generated generic markov decision processes specifically designed studying influence involved concentrability coefficient show bellman residual generally bad proxy policy optimization directly maximizing mean value much better despite current lack deep theoretical analysis might seem obvious directly addressing problem interest usually better given prevalence projected bellman residual minimization value based reinforcement learning believe question worth considered
learning unknown markov decision processes thompson sampling approach consider problem learning unknown markov decision process mdp weakly communicating infinite horizon setting propose thompson sampling based reinforcement learning algorithm dynamic episodes tsde beginning episode algorithm generates sample posterior distribution unknown model parameters follows optimal stationary policy sampled model rest episode duration episode dynamically determined stopping criteria first stopping criterion controls growth rate episode length second stopping criterion happens number visits state action pair doubled establish tilde sqrt bounds expected regret bayesian setting sizes state action spaces time bound span regret bound matches best available bound weakly communicating mdps numerical results show perform better existing algorithms infinite horizon mdps
online reinforcement learning stochastic games study online reinforcement learning average reward stochastic games sgs models player sum game markov environment state transitions step payoffs determined simultaneously learner adversary propose textsc ucsg algorithm achieves sublinear regret compared game value competing arbitrary opponent result improves previous ones setting regret bound dependency textit diameter intrinsic value related mixing property sgs slightly extended textsc ucsg finds varepsilon maximin stationary policy sample complexity tilde mathcal left text poly varepsilon right varepsilon error parameter best knowledge extended result first average reward setting analysis develop markov chain perturbation bounds mean first passage times techniques deal non stationary opponents interest right
reinforcement learning model mismatch study reinforcement learning model misspecification access true environment reasonably close approximation address problem extending framework robust mdps model free reinforcement learning setting access model parameters sample states define robust versions learning sarsa learning prove convergence approximately optimal robust policy approximate value function respectively scale robust algorithms large mdps via function approximation prove convergence different settings prove convergence robust approximate policy iteration robust approximate value iteration linear architectures mild assumptions also define robust loss function mean squared robust projected bellman error give stochastic gradient descent algorithms guaranteed converge local minimum
zap learning zap learning algorithm introduced paper improvement watkins original algorithm recent competitors several respects matrix gain algorithm designed asymptotic variance optimal moreover ode analysis suggests transient behavior close match deterministic newton raphson implementation made possible time scale update equation matrix gain sequence analysis suggests approach lead stable efficient computation even non ideal parameterized settings numerical experiments confirm quick convergence even non ideal cases
ensemble sampling thompson sampling emerged effective heuristic broad range online decision problems basic form algorithm requires computing sampling posterior distribution models tractable simple special cases paper develops ensemble sampling aims approximate thompson sampling maintaining tractability even face complex models neural networks ensemble sampling dramatically expands range applications thompson sampling viable establish theoretical basis supports approach present computational results offer insight
action centered contextual bandits contextual bandits become quite popular offer middle ground simple approaches based multi armed bandits complex approaches using full power reinforcement learning demonstrated success web applications rich body associate theoretical guarantees linear models well understood theoretically preferred practitioners easy understand reason also simple implement debug furthermore linear model true get strong performance guarantees unfortunately emerging application mobile health time invariant linear model assumption untenable provide extension linear model contextual bandit parts baseline reward treatment effect allow former complex keep latter simple argue model plausible mobile health applications time leads algorithms strong performance guarantees linear model setting theory also supported experiments data gathered recently concluded mobile health study
conservative contextual linear bandits safety desirable property immensely increase applicability learning algorithms real world decision making problems much easier company deploy algorithm safe guaranteed perform least well baseline paper study issue safety contextual linear bandits application many different fields including personalized recommendation online marketing formulate notion safety class algorithms develop safe contextual linear bandit algorithm called conservative linear ucb clucb simultaneously minimizes regret satisfies safety constraint maintains performance fixed percentage performance baseline strategy uniformly time prove upper bound regret clucb show decomposed terms upper bound regret standard linear ucb algorithm grows time horizon constant term accounts loss conservative order satisfy safety constraint empirically show algorithm safe validate theoretical analysis
rotting bandits multi armed bandits mab framework highlights trade acquiring new knowledge exploration leveraging available knowledge exploitation classical mab problem decision maker must choose arm time step upon receives reward decision maker objective maximize cumulative expected reward time horizon mab problem studied extensively specifically assumption arms rewards distributions stationary quasi stationary time consider variant mab framework termed rotting bandits arm expected reward decays function number times pulled motivated many real world scenarios online advertising content recommendation crowdsourcing present algorithms accompanied simulations derive theoretical guarantees
identifying outlier arms multi armed bandit study novel problem lying intersection areas multi armed bandit outlier detection multi armed bandit useful tool model process incrementally collecting data multiple objects decision space outlier detection powerful method narrow attention objects data collected however studied detect outlier objects incrementally collecting data necessary data collection expensive formalize problem identifying outlier arms multi armed bandit propose algorithms theoretical guarantee analyze sampling efficiency experimental results synthetic real data show solution saves cost baseline nearly perfect accuracy
multi task learning contextual bandits contextual bandits form multi armed bandit agent access predictive side information known context arm time step used model personalized news recommendation placement applications work propose multi task learning framework contextual bandit problems like multi task learning batch setting goal leverage similarities contexts different arms improve agent ability predict rewards contexts propose upper confidence bound based multi task learning algorithm contextual bandits establish corresponding regret bound interpret bound quantify advantages learning presence high task arm similarity also describe effective scheme estimating task similarity data demonstrate algorithm performance several data sets
boltzmann exploration done right boltzmann exploration classic strategy sequential decision making uncertainty standard tools reinforcement learning despite widespread use virtually theoretical understanding limitations actual benefits exploration scheme drive exploration meaningful way prone misidentifying optimal actions spending much time exploring suboptimal ones right tuning learning rate paper address several questions classic setup stochastic multi armed bandits main results showing boltzmann exploration strategy monotone learning rate sequence induce suboptimal behavior remedy offer simple non monotone schedule guarantees near optimal performance albeit given prior access key problem parameters typically available practical situations like time horizon suboptimality gap delta importantly propose novel variant uses different learning rates different arms achieves distribution dependent regret bound order frac log delta distribution independent bound order sqrt log without requiring prior knowledge demonstrate flexibility technique also propose variant guarantees performance bounds even rewards heavy tailed
improving expected improvement algorithm expected improvement algorithm popular strategy information collection optimization uncertainty algorithm widely known greedy nevertheless enjoys wide use due simplicity ability handle uncertainty noise coherent decision theoretic framework provide rigorous insight study properties simple setting bayesian optimization domain consists finite grid points called best arm identification problem goal allocate measurement effort wisely confidently identify best arm using small number measurements framework show formally far optimal overcome shortcoming introduce simple modification expected improvement algorithm surprisingly simple change results algorithm asymptotically optimal gaussian best arm identification problems provably outperforms standard order magnitude
lucb algorithm large scale crowdsourcing paper focuses best arm identification multi armed bandits bounded rewards develop algorithm fusion lil ucb lucb offering best qualities algorithms method achieved proving novel anytime confidence bound mean bounded distributions analogue lil type bounds recently developed sub gaussian distributions corroborate theoretical results numerical experiments based new yorker cartoon caption contest
scalable generalized linear bandits online computation hashing generalized linear bandits glbs natural extension stochastic linear bandits popular successful recent years however scalability existing glbs poor limiting applicability paper proposes new scalable solutions glb problem respects first unlike existing glbs whose per time step space time complexity grow least linearly time propose new algorithm performs online computations enjoy constant space time complexity heart novel generalized linear extension online confidence set conversion trick gloc takes emph online learning algorithm turns glb algorithm special case apply gloc online newton step algorithm results low regret glb algorithm second case number arms large propose new algorithms next arm selected via inner product search methods implemented hashing algorithms hash amenable result time complexity sublinear thompson sampling extension gloc hash amenable regret bound dimensional arm sets scales worse scaling gloc towards closing gap propose new hash amenable algorithm whose regret bound scales additionally propose fast approximate hash key computation inner product better accuracy state art independent interest conclude paper preliminary experimental results confirming merits methods
bandits dueling partially ordered sets address problem dueling bandits defined partially ordered sets posets setting arms comparable several incomparable optimal arms propose algorithm unchainedbandits efficiently finds set optimal arms pareto front poset even pairs comparable arms cannot priori distinguished pairs incomparable arms set minimal assumptions means unchainedbandits require information comparability used limited knowledge poset achieve algorithm relies concept decoys stems social psychology also provide theoretical guarantees regret incurred number comparison required unchainedbandits report compelling empirical results
position based multiple play multi armed bandit problem unknown position bias study multiple play multi armed bandit problem position bias involves several slots latter slots yield fewer rewards characterize hardness problem deriving asymptotic regret bound propose permutation minimum empirical divergence algorithm derive asymptotically optimal regret bound uncertainty position bias optimal algorithm problem requires non convex optimizations different usual partial monitoring semi bandit problems propose cutting plane method related convex relaxation optimizations using auxiliary variables
online influence maximization independent cascade model semi bandit feedback study stochastic online problem learning influence social network semi bandit feedback observe users influence problem combines challenges limited feedback learning agent observes influenced portion network combinatorial number actions cardinality feasible set exponential maximum number influencers propose computationally efficient ucb like algorithm imlinucb analyze regret bounds polynomial quantities interest reflect structure network probabilities influence moreover depend inherently large quantities cardinality action set best knowledge first results imlinucb permits linear generalization therefore suitable large scale problems experiments show regret imlinucb scales suggested upper bounds several representative graph topologies based linear generalization imlinucb significantly reduce regret real world influence maximization semi bandits
scale free algorithm stochastic bandits bounded kurtosis existing strategies finite armed stochastic bandits mostly depend parameter scale must known advance sometimes form bound payoffs knowledge variance subgaussian parameter notable exceptions analysis gaussian bandits unknown mean variance cowan katehakis 2015a uniform distributions unknown support cowan katehakis 2015b results derived specialised cases generalised non parametric setup learner knows bound kurtosis noise scale free measure extremity outliers
adaptive active hypothesis testing limited information consider problem active sequential hypothesis testing bayesian decision maker must infer true hypothesis set hypotheses decision maker choose set actions outcome action corrupted independent noise paper consider special case decision maker limited knowledge distribution observations action binary value observed objective infer true hypothesis low error minimizing number action sampled main results include derivation lower bound sample size system limited knowledge design active learning policy matches lower bound outperforms similar known algorithms
near optimal edge evaluation explicit generalized binomial graphs robotic motion planning problems uav flying fast partially known environment robot arm moving around cluttered objects require finding collision free paths quickly typically solved constructing graph vertices represent robot configurations edges represent potentially valid movements robot theses configurations main computational bottlenecks expensive edge evaluations check collisions state art planning methods reason optimal sequence edges evaluate order find collision free path quickly paper drawing novel equivalence motion planning bayesian active learning paradigm decision region determination drd unfortunately straight application isting methods requires computation exponential number edges graph present bisect efficient near optimal algorithm solve drd problem edges independent bernoulli random variables leveraging property able significantly reduce computational complexity exponential linear number edges show bisect outperforms several state art algorithms spectrum planning problems mobile robots manipulators real flight data collected full scale helicopter
robust efficient transfer learning hidden parameter markov decision processes introduce new formulation hidden parameter markov decision process hip mdp framework modeling families related tasks using low dimensional latent embeddings replace original gaussian process based model bayesian neural network new framework correctly models joint uncertainty latent weights state space scalable inference thus expanding scope hip mdp applications higher dimensions complex dynamics
overcoming catastrophic forgetting incremental moment matching catastrophic forgetting problem neural networks loses information first task training second task propose incremental moment matching imm resolve problem imm incrementally matches moment posterior distribution neural networks trained first second task respectively make search space posterior parameter smooth imm procedure complemented various transfer learning techniques including weight transfer norm old new parameter variant dropout old parameter analyze approach various datasets including mnist cifar caltech ucsd birds lifelog datasets experimental results show imm achieves state art performance variety datasets balance information old new network
hypothesis transfer learning via transformation functions consider hypothesis transfer learning htl problem incorporates hypothesis trained source domain learning procedure target domain existing theoretical analysis either studies specific algorithms presents upper bounds generalization error excess risk paper propose unified algorithm dependent framework htl novel notion transformation functions characterizes relation source target domains conduct general risk analysis framework particular show first time domains related htl enjoys faster convergence rates excess risks kernel smoothing kernel ridge regression classical non transfer learning settings accompany framework analysis cross validation htl search best transfer technique gracefully reduce non transfer learning htl helpful experiments robotics neural imaging data demonstrate effectiveness framework
learning multiple visual domains residual adapters growing interest learning data representations work well many different types problems data paper look particular task learning single visual representation successfully utilized analysis different types images dog breeds stop signs digits inspired recent work learning networks predict parameters another develop tunable deep network architecture means adapter residual modules steered fly diverse visual domains method achieves high degree parameter sharing maintaining even improving accuracy domain specific representations also introduce visual decathlon challenge benchmark evaluates ability representations capture simultaneously different visual domains measures ability recognize well uniformly
self supervised learning motion capture propose learning based end end motion capture model monocular videos wild current state art solutions motion capture single camera optimization driven optimize parameters human model projection matches measurements video person segmentation optical flow keypoint detections etc optimization models susceptible local minima bottleneck forced using clean green screen like backgrounds capture time manual initialization switching multiple cameras input resource instead optimizing mesh skeleton parameters directly model optimizes neural network weights predict shape skeleton configurations given monocular rgb video model trained using combination strong supervision synthetic data self supervision differentiable rendering skeletal keypoints dense mesh motion human background segmentation end end trainable framework empirically show model combines best worlds supervised learning test time optimization supervised learning initializes model parameters right regime ensuring good pose surface initialization test time without manual effort self supervision back propagating differentiable rendering allows unsupervised adaptation model test data offers much tighter fit pretrained fixed model show proposed model improves experience converges low error solutions previous optimization methods fail
information theoretic properties markov random fields algorithmic applications markov random fields popular model high dimensional probability distributions years many mathematical statistical algorithmic problems studied recently known algorithms provably learning relied exhaustive search correlation decay various incoherence assumptions bresler gave algorithm learning general ising models bounded degree graphs approach based structural result mutual information ising models take conceptual approach proving lower bounds mutual information proof generalizes well beyond ising models arbitrary markov random fields higher order interactions application obtain algorithms learning markov random fields bounded degree graphs nodes order interactions time log sample complexity algorithms also extend various partial observation models
maximizing subset accuracy recurrent neural networks multi label classification multi label classification task predicting set labels given input instance classifier chains state art method tackling problems essentially converts problem sequential prediction problem labels first ordered arbitrary fashion task predict sequence binary values labels paper replace classifier chains recurrent neural networks sequence sequence prediction algorithm recently successfully applied sequential prediction tasks many domains key advantage approach allows share parameters across classifiers prediction chain key property multi target prediction problems classifier chains recurrent neural networks depend fixed ordering labels typically part multi label problem specification also compare different ways ordering label set give recommendations suitable ordering strategies
local aggregative games aggregative games provide rich abstraction model strategic multi agent interactions focus learning local aggregative games payoff player function action aggregate behavior neighbors digraph show existence pure strategy epsilon nash equilibrium games payoff functions convex sub modular prove information theoretic lower bound value oracle model approximating structure digraph non negative monotone sub modular cost functions edge set cardinality also introduce gamma aggregative games generalize local aggregative games admit epsilon nash equilibrium stable respect small changes specified graph property moreover provide estimation algorithms game theoretic model meaningfully recover underlying structure payoff functions real voting data
empirical bayes approach optimizing machine learning algorithms rapidly growing interest using bayesian optimization tune model inference hyperparameters machine learning algorithms take long time run example spearmint popular software package selecting optimal number layers learning rate neural networks given uncertainty hyperparameters give best predictive performance given fitting model choice hyperparameters costly arguably wasteful throw away best result per bayesian optimization related issue danger overfitting validation data optimizing many hyperparameters paper consider alternative approach uses samples hyperparameter selection procedure average uncertainty model hyperparameters resulting approach empirical bayes hyperparameter averaging hyp predicts held data better bayesian optimization experiments latent dirichlet allocation deep latent gaussian models hyp suggests simpler approach evaluating deploying machine learning algorithms require separate validation data set hyperparameter selection procedure
learning chordal markov networks via branch bound present new algorithmic approach computationally hard task finding chordal markov network structure maximizes given scoring function algorithm based branch bound integrates dynamic programming domain pruning obtaining strong bounds search space pruning empirically show approach dominates recent integer programming approach bartlett cussens uai 2013 thereby also constraint optimization approach corander nips 2013 furthermore algorithm scales times wrt number variables state art dynamic programming algorithm kangas nips 2014 potential reaching variables time circumventing tight exponential lower bounds memory consumption pure dynamic programming approach
optimal sample complexity wise data top ranking explore top rank aggregation problem aims recover consistent ordering focuses top ranked items based partially revealed preference information examine wise comparison model builds plackett luce model sample items ranked according perceived utilities modeled noisy observations underlying true utilities result characterize minimax optimality sample size top ranking optimal sample size turns inversely proportional devise algorithm effectively converts wise samples pairwise ones employs spectral method using refined data demonstrating optimality develop novel technique deriving tight ell_ infty estimation error bounds key accurately analyzing performance top ranking algorithms challenging recent work relied additional maximum likelihood estimation mle stage merged spectral method attain good estimates ell_ infty error achieve limit pairwise model contrast although valid slightly restricted regimes result demonstrates spectral method alone sufficient general wise model run numerical experiments using synthetic data confirm optimal sample size decreases rate moreover running algorithm real world data find applicability extends settings fit model
translation synchronization via truncated least squares paper introduce robust algorithm textsl transync translation synchronization problem aims recover global coordinates set nodes noisy relative measurements along pre defined observation graph basic idea transync apply truncated least squares solution step used gradually prune noisy measurements analyze transync deterministic noisy model demonstrating robustness stability experimental results synthetic real datasets show transync superior state art convex formulations terms efficiency accuracy
bayesian sparsity gated recurrent nets iterations many first order algorithms applied minimizing common regularized regression functions often resemble neural network layers pre specified weights observation prompted development learning based approaches purport replace iterations enhanced surrogates forged dnn models available training data example important hard sparse estimation problems recently benefitted genre upgrade simple feedforward recurrent networks ousting proximal gradient based iterations analogously paper demonstrates powerful bayesian algorithms promoting sparsity rely complex multi loop majorization minimization techniques mirror structure sophisticated long short term memory lstm networks alternative gated feedback networks previously designed sequence prediction part development examine parallels latent variable trajectories operating across multiple time scales optimization activations within deep network structures designed adaptively model characteristic sequences resulting insights lead novel sparse estimation system granted training data estimate optimal solutions efficiently regimes algorithms fail including practical direction arrival doa geometry recovery problems underlying principles expose also suggestive learning process richer class multi loop algorithms domains
online learning multivariate hawkes processes develop nonparametric online learning algorithm estimates triggering functions multivariate hawkes process mhp approach take approximates triggering function functions reproducing kernel hilbert space rkhs maximizes time discretized version log likelihood tikhonov regularization theoretically algorithm achieves calo log regret bound numerical results show algorithm offers competing performance nonparametric batch learning algorithm run time comparable parametric online learning algorithm
efficient second order online kernel learning adaptive embedding online kernel learning okl flexible framework approach prediction problems since large approximation space provided reproducing kernel hilbert spaces contain accurate function problem nonetheless optimizing space computationally expensive first order methods accumulate sqrt loss optimal function curse kernelization results per step complexity second order methods get closer optimum much faster suffering log regret second order updates even expensive per step cost existing approximate okl methods try reduce complexity either limiting support vectors introduced predictor avoiding kernelization process altogether using embedding nonetheless long size approximation space number grow time adversary always exploit approximation process paper propose pros kons method combines nystrom sketching project input point small accurate embedded space performs efficient second order updates space embedded space continuously updated guarantee embedding remains accurate show per step cost grows effective dimension problem moreover second order updated allows achieve logarithmic regret empirically compare algorithm recent large scales benchmarks show performs favorably
online offline conversions adaptive minibatch sizes present approach towards convex optimization relies novel scheme converts adaptive online algorithms offline methods offline optimization setting derived methods shown obtain favourable adaptive guarantees depend emph harmonic sum queried gradients show methods implicitly adapt objective structure smooth case fast convergence rates ensured without prior knowledge smoothness parameter still maintaining guarantees non smooth setting approach natural extension stochastic setting resulting lazy version sgd stochastic minibathces chosen emph adaptively depending magnitude gradients thus providing principled approach towards choosing minibatch sizes
nonparametric online regression learning metric study algorithms online nonparametric regression learn directions along regression function smoother algorithm learns mahalanobis metric based gradient outer product matrix regression function automatically adapting effective rank matrix simultaneously bounding regret data sequence terms spectrum preliminary step analysis generalize nonparametric online learning algorithm hazan megiddo enabling compete functions whose lipschitzness measured respect arbitrary mahalanobis metric
stochastic adversarial online learning without hyperparameters online optimization algorithms focus things performing well adversarial settings adapting unknown data parameters lipschitz constants typically achieving sqrt regret performing well stochastic settings leverage structure losses strong convexity typically achieving log regret algorithms focus former problem hitherto achieved sqrt stochastic setting rather log introduce online optimization algorithm achieves log regret wide class stochastic settings gracefully degrading optimal sqrt regret adversarial settings logarithmic factors algorithm require prior knowledge data tuning parameters achieve superior performance
affine invariant online optimization present new affine invariant optimization algorithm called emph online lazy newton algorithm modification online newton step algorithm convergence rate online lazy newton independent conditioning algorithm performance depends best possible preconditioning problem retrospect emph intrinsic dimensionality application show online lazy newton achieve optimal widetilde theta sqrt regret low rank experts problem improving sqrt factor previously best known bound resolving open problem posed hazan 2016
online convex optimization stochastic constraints paper considers online convex optimization oco stochastic constraints generalizes zinkevich oco known simple fixed set introducing multiple stochastic functional constraints generated round disclosed decision maker decision made formulation arises naturally decisions restricted stochastic environments deterministic environments noisy information also includes many important problems oco long term constraints stochastic constrained convex optimization deterministic constrained convex optimization special case solve problem paper proposes new algorithm achieves sqrt expected regret constraint violations sqrt log high probability regret constraint violations experiments real world data center scheduling problem verify performance new algorithm
online learning hint study variant online linear optimization player receives hint loss function beginning round hint given form vector weakly correlated loss vector round show player benefit hint set feasible actions sufficiently round specifically set strongly convex hint used guarantee regret log set uniformly convex hint used guarantee regret sqrt contrast establish omega sqrt lower bounds regret set feasible actions polyhedron
efficient online linear optimization approximation algorithms revisit problem online linear optimization case set feasible actions accessible approximated linear optimization oracle factor alpha multiplicative approximation guarantee setting particular interesting since captures natural online extensions well studied offline linear optimization problems hard yet admit efficient approximation algorithms goal minimize alpha regret natural extension standard regret online learning setting present new algorithms significantly improved oracle complexity full information bandit variants problem mainly variants present alpha regret bounds number prediction rounds using log calls approximation oracle per iteration average first results obtain average oracle complexity log even poly logarithmic alpha regret bound positive constant variants
random permutation online isotonic regression revisit isotonic regression linear orders problem fitting monotonic functions best explain data online settings previously shown online isotonic regression unlearnable fully adversarial model lead study fixed design model instead develop practical random permutation model show regret bounded excess leave loss develop efficient algorithms matching lower bounds also analyze class simple popular forward algorithms make recommendations look algorithms online isotonic regression partial orders
minimax optimal players finite time expert prediction problem study minimax strategies online prediction problem expert advice conjectured simple adversary strategy called comb optimal game number experts results new insights make progress direction showing small additive term comb minimax optimal finite time expert problem addition provide setting new minimax optimal comb based learner prior work fundamental learning problem optimal learners known rightarrow infty regret previous state art learner scales sqrt log larger regret learner expert case also characterize regret game scaling sqrt log gives first time optimal constant leading sqrt term regret
online learning optimal bidding strategy repeated multi commodity auctions study online learning problem bidder participates repeated auctions goal maximizing period payoff bidder determines optimal allocation budget among bids goods period bidding strategy propose polynomial time algorithm inspired dynamic programming approach knapsack problem referred dynamic programming discrete set dpds proposed algorithm achieves regret order sqrt log showing regret lower bounded omega sqrt strategy conclude dpds order optimal sqrt log term evaluate performance dpds empirically context virtual trading wholesale electricity markets using historical data new york market empirical results show dpds consistently outperforms benchmark heuristic methods derived machine learning online learning approaches
online prediction selfish experts consider problem binary prediction expert advice settings experts agency seek maximize credibility paper makes main contributions first defines model reason formally settings selfish experts demonstrates incentive compatible algorithms closely related design proper scoring rules second design algorithms good performance guarantees absolute loss function third give formal separation power online prediction selfish experts online prediction honest experts proving lower bounds non algorithms particular selfish experts absolute loss function randomized algorithm online prediction otherwise asymptotically vanishing regret
real time bidding side information consider problem repeated bidding online advertising auctions side information browser cookies available ahead submitting bid form dimensional vector goal advertiser maximize total utility total number clicks derived displaying ads given limited budget allocated given time horizon optimizing bids modeled linear contextual multi armed bandit mab problem knapsack constraint continuum arms develop ucb type algorithms combine streams literature confidence set approach linear contextual mabs probabilistic bisection search method stochastic root finding mild assumptions underlying unknown distribution establish distribution independent regret bounds order tilde cdot sqrt either infty scales linearly
general framework robust interactive learning propose general framework interactively learning models binary non binary classifiers orderings rankings items clusterings data points framework based generalization angluin equivalence query model iteration algorithm proposes model user either accepts reveals specific mistake proposal feedback correct probability adversarially incorrect probability algorithm must able learn presence arbitrary noise algorithm goal learn ground truth model using iterations general framework based graph representation models user feedback able learn efficiently sufficient graph whose nodes models weighted edges capture user feedback property proposed target models correct user feedback must lie shortest path assumption natural algorithm reminiscent multiplicative weights update algorithm efficiently learn even presence random noise user feedback general result rederive barely extra effort classic results learning classifiers recent result interactive clustering addition easily obtain new interactive learning algorithms ordering ranking
practical locally private heavy hitters present new practical local differentially private heavy hitters algorithms achieving optimal near optimal worst case error treehist bithist algorithms server running time tilde user running time tilde hence improving prior state art result bassily smith stoc 2015 requiring server time user time typically large number participants local algorithms millions reduction time complexity particular user side crucial use algorithms practice implemented algorithm treehist verify theoretical analysis compared performance performance google rappor code
deanonymization bitcoin p2p network recent attacks bitcoin peer peer p2p network demonstrated transaction flooding protocols used ensure network consistency enable user deanonymization linkage user address pseudonym bitcoin network 2015 bitcoin community responded attacks changing network flooding mechanism different protocol known diffusion however unclear diffusion actually improves system anonymity paper model bitcoin networking stack analyze anonymity properties pre post 2015 core problem epidemic source inference graphs observational model spreading mechanisms informed bitcoin implementation notably models studied epidemic source detection literature identify analyze near optimal source estimators analysis suggests bitcoin networking protocols pre post 2015 offer poor anonymity properties networks regular tree topology confirm claim simulation 2015 snapshot real bitcoin p2p network topology
accuracy first selecting differential privacy level accuracy constrained erm traditional approaches differential privacy assume fixed privacy requirement computation attempt maximize accuracy computation subject privacy constraint differential privacy increasingly deployed practical settings often instead fixed accuracy requirement given computation data analyst would like maximize privacy computation subject accuracy constraint raises question find run maximally private empirical risk minimizer subject given accuracy requirement propose general noise reduction framework apply variety private empirical risk minimization erm algorithms using search space privacy levels find empirically strongest meets accuracy constraint incurring logarithmic overhead number privacy levels searched privacy analysis algorithm leads naturally version differential privacy privacy parameters dependent data term post privacy related recently introduced notion privacy odometers also give post privacy analysis classical abovethreshold privacy tool modifying allow queries chosen depending database finally apply approach common objective functions regularized linear logistic regression empirically compare noise reduction methods inverting theoretical utility guarantees standard private erm algorithms stronger empirical baseline based binary search
renyi differential privacy mechanisms posterior sampling newly proposed privacy definition nyi differential privacy rdp mironov 2017 examine inherent privacy releasing single sample posterior distribution exploit impact prior distribution mitigating influence individual data points particular focus sampling exponential family specific generalized linear models logistic regression propose novel rdp mechanisms well offering new rdp analysis existing method order add value rdp framework method capable achieving arbitrary rdp privacy guarantees offer experimental results efficacy
collecting telemetry data privately collection analysis telemetry data user devices routinely performed many software companies telemetry collection leads improved user experience poses significant risks users privacy locally differentially private ldp algorithms recently emerged main tool allows data collectors estimate various population statistics preserving privacy guarantees provided algorithms typically strong single round telemetry collection degrade rapidly telemetry collected regularly particular existing ldp algorithms suitable repeated collection counter data daily app usage statistics paper develop new ldp mechanisms geared towards repeated collection counter data formal privacy guarantees even executed arbitrarily long period time basic analytical tasks mean estimation histogram estimation ldp mechanisms repeated data collection provide estimates comparable even accuracy existing single round ldp collection mechanisms conduct empirical evaluation real world counter datasets verify theoretical results mechanisms deployed fortune 500 company collect telemetry across millions devices
generating steganographic images via adversarial training adversarial training proved competitive supervised learning methods computer vision tasks however studies mainly confined generative tasks image synthesis paper apply adversarial training techniques discriminative task learning steganographic algorithm steganography collection techniques concealing existence information embedding within non secret medium cover texts images show adversarial training produce robust steganographic techniques unsupervised training scheme produces steganographic algorithm competes state art steganographic techniques also show supervised training adversarial model produces robust steganalyzer performs discriminative task deciding image contains secret information define game parties alice bob eve order simultaneously train steganographic algorithm steganalyzer alice bob attempt communicate secret message contained within image eve eavesdrops conversation attempts determine secret information embedded within image represent alice bob eve neural networks validate scheme independent image datasets showing novel method studying steganographic problems surprisingly competitive established steganographic techniques
fitting low rank tensors constant time paper develop algorithm approximates residual error tucker decomposition popular tensor decomposition methods provable guarantee given order tensor mathbb n_1 times cdots times n_k algorithm randomly samples constant number indices mode creates mini tensor tilde mathbb times cdots times whose elements given intersection sampled indices show residual error tucker decomposition tilde sufficiently close high probability result implies figure much fit low rank tensor emph constant time regardless size useful guessing favorable rank tucker decomposition finally demonstrate sampling method works quickly accurately using multiple real datasets
thy friend friend iterative collaborative filtering sparse matrix estimation sparse matrix estimation problem consists estimating distribution times matrix sparsely observed single instance matrix entries independent random variables captures wide array problems special instances include matrix completion context recommendation systems graphon estimation community detection mixed membership stochastic block models inspired classical collaborative filtering recommendation systems propose novel iterative collaborative filtering style algorithm matrix estimation generic setting show mean squared error mse estimator goes long omega random entries total entries observed uniformly sampled rank entries bounded support maximum squared error across entries converges high probability long observe little omega entries results best known sample complexity results generality intuitive easy implement iterative nearest neighbor style algorithm matches conjectured sample complexity lower bound computationally efficient algorithm detection mixed membership stochastic block model
fair clustering fairlets study question fair clustering disparate impact doctrine protected class must approximately equal representation every cluster formulate fair clustering problem center median objectives show even protected classes problem challenging optimum solution violates common conventions instance point longer assigned nearest cluster center route introduce concept fairlets minimal sets satisfy fair representation approximately preserving clustering objective show fair clustering problem decomposed first finding appropriate fairlets using existing machinery traditional clustering algorithms finding good fairlets hard proceed obtain efficient approximation algorithms based minimum cost flow empirically demonstrate emph price fairness comparing value fair clustering real world datasets sensitive attributes
fairness calibration machine learning community become increasingly concerned potential bias discrimination predictive models motivated growing line work means classification procedure fair particular investigate tension minimizing error disparity across different population groups maintaining calibrated probability estimates show calibration compatible single error constraint equal false negatives rates across groups show algorithm satisfies relaxation better randomizing percentage predictions existing classifier unsettling findings extend generalize existing results empirically confirmed several datasets
avoiding discrimination causal reasoning recent work fairness machine learning focused various statistical discrimination criteria trade criteria observational depend joint distribution predictor protected attribute features outcome convenient work observational criteria severe inherent limitations prevent resolving matters fairness conclusively going beyond observational criteria frame problem discrimination based protected attributes language causal reasoning viewpoint shifts attention right fairness criterion want assume causal data generating process lens causality make several contributions first crisply articulate observational criteria fail thus formalizing matter opinion second approach exposes previously ignored subtleties fundamental problem finally put forward natural causal non discrimination criteria develop algorithms satisfy
optimized pre processing discrimination prevention non discrimination recognized objective algorithmic decision making paper introduce novel probabilistic formulation data pre processing reducing discrimination propose convex optimization learning data transformation goals controlling discrimination limiting distortion individual data samples preserving utility characterize impact limited sample size accomplishing objective instances proposed optimization applied datasets including real world criminal recidivism results show discrimination greatly reduced small cost classification accuracy
recycling fairness learning conditional distribution matching constraints equipping machine learning models ethical legal constraints serious issue without future machine learning risk paper takes step forward direction focuses ensuring machine learning models deliver fair decisions legal scholarships notion fairness evolving multi faceted set overarching goal develop unified machine learning framework able handle definitions fairness combinations also new definitions might stipulated future achive goal recycle well established machine learning techniques privileged learning distribution matching harmonize satisfying multi faceted fairness definitions consider protected characteristics race gender privileged information accelerates model training delivers fairness unawareness cast demographic parity equalized odds equal opportunity classical sample problem conditional distributions solved general form using distance measures hilbert space finally show several existing models special cases
parity preference learning cost effective notions fairness adoption automated data driven decision making ever expanding range applications raised concerns potential unfairness towards certain social groups context number recent studies focused defining detecting removing unfairness data driven decision systems however existing notions fairness based parity equality treatment outcomes different social groups tend needlessly stringent limiting overall decision making accuracy paper draw inspiration fair division envy freeness literature economics game theory propose preference based notions fairness given choice various sets decision treatments outcomes group users would collectively prefer treatment outcomes regardless dis parity compared groups introduce tractable proxies design convex margin based classifiers satisfy preference based notions fairness finally experiment variety synthetic real world datasets show preference based fairness allows greater decision accuracy parity based fairness
beyond parity fairness objectives collaborative filtering study fairness collaborative filtering recommender systems sensitive discrimination exists historical data biased data lead collaborative filtering methods make unfair predictions users minority groups identify insufficiency existing fairness metrics propose new metrics address different forms unfairness fairness metrics optimized adding fairness terms learning objective experiments synthetic real data show new metrics better measure fairness baseline fairness objectives effectively help reduce unfairness
multi view matrix factorization linear dynamical system estimation consider maximum likelihood estimation linear dynamical systems generalized linear observation models maximum likelihood typically considered hard setting since latent states transition parameters must inferred jointly given expectation maximization scale prone local minima moment matching approaches subspace identification literature become standard despite known statistical efficiency issues paper instead reconsider likelihood maximization develop optimization based strategy recovering latent states transition parameters key approach view reformulation maximum likelihood estimation linear dynamical systems enables use global optimization algorithms matrix factorization show proposed estimation strategy outperforms widely used identification algorithms subspace identification methods terms accuracy runtime
random projection filter bank time series data propose random projection filter bank rpfb generic simple approach extract features time series data rpfb set randomly generated stable autoregressive filters convolved input time series generate features features used conventional machine learning algorithm solving tasks time series prediction classification time series data etc different filters rpfb extract different aspects time series together provide reasonably good summary time series rpfb easy implement fast compute parallelizable provide finite sample error upper bound shows rpfb provides reasonable approximation class dynamical systems empirical results series synthetic real world problems show rpfb effective way extract features time series
dirichlet mixture model hawkes processes event sequence clustering cluster event sequences generated via different point processes interesting important problem statistical machine learning solve problem propose discuss effective model based clustering method based novel dirichlet mixture model special significant type point processes hawkes process proposed model generates event sequences different clusters hawkes processes different parameters uses dirichlet process prior distribution clusters prove identifiability mixture model propose effective variational bayesian inference algorithm learn model adaptive inner iteration allocation strategy designed accelerate convergence algorithm moreover investigate sample complexity computational complexity learning algorithm depth experiments synthetic real world data show clustering method based model learn structural triggering patterns hidden asynchronous event sequences robustly achieve superior performance clustering purity consistency compared existing methods
predicting user activity level point process models mass transport equation point processes powerful tools model user activities plethora applications social sciences predicting user activities based point processes central problem however existing works mostly problem specific use heuristics simplify stochastic nature point processes paper propose framework provides unbiased estimator probability mass function point processes particular design key reformulation prediction problem derive differential difference equation compute conditional probability mass function framework applicable general point processes prediction tasks achieves superb predictive efficiency performance diverse real world applications compared state arts
policy evaluation slate recommendation paper studies evaluation policies recommend ordered set items ranking based context common scenario web search ads recommendation build techniques combinatorial bandits introduce new practical estimator thorough empirical evaluation real world data reveals estimator accurate variety settings including subroutine learning rank task achieves competitive performance derive conditions estimator unbiased conditions weaker prior heuristics slate evaluation experimentally demonstrate smaller bias parametric approaches even conditions violated finally theory experiments also show exponential savings amount required data compared general unbiased estimators
expectation propagation stochastic kinetic model complex interaction systems technological breakthroughs allow collect data increasing spatio temporal resolution complex systems combination high resolution data strong theoretical models dynamic belief networks bethe free energy formulation lead crucial understanding complex interaction dynamics functions systems paper formulate dynamics complex interacting network stochastic process driven sequence events develop expectation propagation algorithms make inference noisy observations avoid getting stuck local optimum formulate problem minimizing bethe free energy constrained optimization problem local optimum also global optimum expectation propagation algorithms show good performance inferring interaction dynamics complex transportation networks competing models particle filter extended kalman filter deep neural networks
multi agent reinforcement learning model common pool resource appropriation note reviewer due mistake final submission process figs wrong however correct versions appendix intended fig shown fig first last note also appendix figs also mislabeled apologize confusion correct final version humanity faces numerous problems common pool resource appropriation class multi agent social dilemma includes problems ensuring sustainable use fresh water common fisheries grazing pastures irrigation systems abstract models common pool resource appropriation based non cooperative game theory predict self interested agents generally fail find socially positive equilibria phenomenon called tragedy commons however reality human societies sometimes able discover implement stable cooperative solutions decades behavioral game theory research sought uncover aspects human behavior make possible work based laboratory experiments participants make single choice much appropriate recognizing importance spatial temporal resource dynamics recent trend toward experiments complex real time video game like environments however standard methods non cooperative game theory longer used generate predictions case show deep reinforcement learning used instead end study emergent behavior groups independently learning agents partially observed markov game modeling cpr appropriation experiments highlight importance trial error learning common pool resource appropriation shed light relationship exclusion sustainability inequality
balancing information exposure social networks social media brought revolution people consuming news beyond undoubtedly large number advantages brought social media platforms point criticism creation echo chambers filter bubbles caused social homophily algorithmic personalization paper address problem balancing information exposure social network assume opposing campaigns viewpoints present network network nodes different preferences towards campaigns goal find sets nodes employ respective campaigns overall information exposure campaigns balanced formally define problem characterize hardness develop approximation algorithms present experimental evaluation results model inspired literature influence maximization offer significant novelties first balance information exposure modeled symmetric difference function neither monotone submodular thus amenable existing approaches second previous papers consider setting selfish agents provide bounds best response strategies move last player consider setting centralized agent provide bounds global objective function
scalable demand aware recommendation recommendation commerce mix durable nondurable goods characteristics distinguish well studied media recommendation problem demand items combined effect form utility time utility product must intrinsically appealing consumer time must right purchase particular durable goods time utility function inter purchase duration within product category consumers unlikely purchase items category close temporal succession moreover purchase data contrast ratings data implicit non purchases necessarily indicating dislike together issues give rise positive unlabeled demand aware recommendation problem pose via joint low rank tensor completion product category inter purchase duration vector estimation relax problem propose highly scalable alternating minimization approach solve problems millions users millions items single thread also show superior prediction accuracies multiple real world data sets
greedy approach budgeted maximum inner product search maximum inner product search mips important task many machine learning applications prediction phase low rank matrix factorization model recommender system recently substantial research perform mips sub linear time recently however existing work flexibility control trade search efficiency search quality paper study important problem mips computational budget carefully studying problem structure mips develop novel greedy mips algorithm handle budgeted mips design simple intuitive greedy mips yields surprisingly superior performance compared state art approaches specific example candidate set containing half million vectors dimension 200 greedy mips runs 200x faster naive approach yielding search results top precision greater
dpscreen dynamic personalized screening screening important diagnosis treatment wide variety diseases good screening policy personalized disease features patient dynamic history patient including history screening growth electronic health records data led development many models predict onset progression different diseases however limited work address personalized screening different diseases work develop first framework construct screening policies large class disease models disease modeled finite state stochastic process absorbing disease state patient observes external information process instance self examinations discovering comorbidities etc trigger patient arrive clinician earlier scheduled screenings clinician carries tests based test results external information schedules next arrival computing exactly optimal screening policy balances delay detection frequency screenings computationally intractable paper provides computationally tractable construction approximately optimal policy illustration make use large breast cancer data set constructed policy screens patients less often according initial risk personalized features patient according results previous screens personalized history patient comparison existing clinical policies constructed policy leads large reductions number screens performed achieving expected delays disease detection
deep multi task gaussian processes survival analysis competing risks designing optimal treatment plans patients comorbidities requires accurate cause specific mortality prognosis motivated recent availability linked electronic health records develop nonparametric bayesian model survival analysis competing risks used jointly assessing patient risk multiple competing adverse outcomes model views patient survival times respect competing risks outputs deep multi task gaussian process dmgp inputs patients covariates unlike parametric survival analysis methods based cox weibull models model uses dmgps capture complex non linear interactions patients covariates cause specific survival times thereby learning flexible patient specific cause specific survival curves data driven fashion without explicit parametric assumptions hazard rates propose variational inference algorithm capable learning model parameters time event data handling right censoring experiments synthetic real data show model outperforms state art survival models
premise selection theorem proving deep graph embedding propose deep learning approach premise selection selecting relevant mathematical statements automated proof given conjecture represent higher order logic formula graph invariant variable renaming time fully preserves syntactic semantic information embed graph continuous vector via novel embedding method preserves information edge ordering approach achieves state art results holstep dataset improving classification accuracy
gradients generative models improved discriminative analysis tandem mass spectra tandem mass spectrometry high throughput technology used identify proteins complex biological sample drop blood collection spectra generated output process spectrum representative peptide protein subsequence present original complex sample work leverage log likelihood gradients generative models improve identification spectra particular show gradient recently proposed dynamic bayesian network dbn naturally employed kernel based discriminative classifier resulting fisher kernel substantially improves upon recent attempts combine generative discriminative models post processing analysis outperforming methods evaluated datasets extend improved accuracy offered fisher kernel framework search algorithms introducing theseus dbn representating large number widely used scoring functions furthermore gradient ascent max product inference hand use theseus learn model parameters without supervision
style transfer non parallel text cross alignment paper focuses style transfer basis paired text instance broader family problems including machine translation decipherment sentiment modification key technical challenge separate content desired text characteristics sentiment leverage refined cross alignment latent representations across mono lingual text corpora different characteristics deliberately modify encoded examples according characteristics requiring reproduced instances match population available examples altered characteristics demonstrate effectiveness method tasks sentiment modification decipherment word substitution cyphers recovery word reodering
emergence language multi agent games learning communicate sequences symbols learning communicate interaction rather relying explicit supervision often considered prerequisite developing general study setting agents engage playing referential game scratch develop communication protocol necessary succeed game unlike previous work require messages exchange train test time form language sequences discrete symbols compare reinforcement learning approach using differentiable relaxation straight gumbel softmax estimator observe latter much faster converge results effective protocols interestingly also observe protocol induce optimizing communication success exhibits degree compositionality variability information phrased different ways properties characteristic natural languages ultimate goal ensure communication accomplished natural language also perform experiments inject prior information natural language model study properties resulting protocol
elf extensive lightweight flexible research platform real time strategy games paper propose elf extensive lightweight flexible platform fundamental reinforcement learning research using elf implement highly customizable real time strategy rts engine game environments mini rts capture flag tower defense mini rts miniature version starcraft captures key game dynamics runs 165k frame per second fps macbook pro notebook coupled modern reinforcement learning methods system train full game bot built ais end end day cpus gpu addition platform flexible terms environment agent communication topologies choices methods changes game parameters host existing based game environments like ale using elf thoroughly explore training parameters show network leaky relu batch normalization coupled long horizon training progressive curriculum beats rule based built time full game mini rts strong performance also achieved games game replays show agents learn interesting strategies elf along platform open sourced
extremeweather large scale climate dataset semi supervised detection localization understanding extreme weather events detection identification extreme weather events large scale climate simulations important problem risk management informing governmental policy decisions advancing basic understanding climate system recent work shown fully supervised convolutional neural networks cnns yield acceptable accuracy classifying well known types extreme weather events large amounts labeled data available however many different types spatially localized climate patterns interest including hurricanes extra tropical cyclones weather fronts blocking events etc simulation data labels available large scale additionally labelled data exist incomplete various ways covering certain years geographic areas false negatives etc climate data thus poses number interesting machine learning challenges show current techniques able cope varying degrees success present multichannel spatiotemporal encoder decoder cnn architecture semi supervised bounding box prediction exploratory data analysis propose novel bounding box prediction method combining improving aspects state art methods demonstrate approach able leverage temporal information unlabelled data improve localization extreme weather events explore representations learned model order better understand important data present dataset extremeweather encourage research challenging machine learning topics facilitating work understanding mitigating effects climate change
approximation convergence properties generative adversarial learning generative adversarial networks gan approximate target data distribution jointly optimizing objective function player game generator discriminator despite empirical success however basic questions well approximate target distribution remain unanswered first known restricting discriminator family affects approximation quality second number different objective functions proposed understand convergence global minima objective function leads convergence target distribution various notions distributional convergence paper address questions broad unified setting defining notion adversarial divergences includes number recently proposed objective functions show objective function adversarial divergence additional conditions using restricted discriminator family moment matching effect additionally show objective functions strict adversarial divergences convergence objective function implies weak convergence thus generalizing previous results
gradient descent gan optimization locally stable despite growing prominence optimization generative adversarial networks gans still poorly understood topic paper analyze gradient descent form gan optimization natural setting simultaneously take small gradient steps generator discriminator parameters show even though gan optimization emph correspond convex concave game even simple parameterizations proper conditions equilibrium points optimization procedure still emph locally asymptotically stable traditional gan formulation hand show recently proposed wasserstein gan non convergent limit cycles near equilibrium motivated stability analysis propose additional regularization term gradient descent gan updates emph able guarantee local stability wgan traditional gan also shows practical promise speeding convergence addressing mode collapse
gans information geometric nutshell nowozin textit showed last year scale gans textit principle divergences approach elegant falls short full description supervised game says nothing key player generator example generator actually fit solving gan game means convergence space parameters hint generator design compare flourishing essentially experimental literature subject paper unveil broad class densities convergence happens show tight connections key gan parameters loss game model particular show current deep architectures able factor potentially large number densities hence displaying power deep architectures adequation gan game result holds provided sufficient condition textit activation functions satisfied turns satisfied popular choices key results variational generalization old theorem relates divergence regular exponential families divergences natural parameters complete picture additional results experimental insights results used ground improvements gan architectures
numerics gans paper analyze numerics common algorithms training generative adversarial networks gans using formalism smooth player games analyze associated gradient vector field gan training objectives findings suggest convergence current algorithms suffers due factors presence eigenvalues jacobian gradient vector field real part eigenvalues big imaginary part using findings design new algorithm overcomes limitations better convergence properties experimentally demonstrate superiority training common gan architectures show convergence gan architectures known notoriously hard train
generalizing gans turing perspective recently new class machine learning algorithms emerged models discriminators generated competitive setting prominent example generative adversarial networks gans paper examine algorithms relate famous turing test derive turing perspective considered defining features based features outline directions generalizing gans resulting family algorithms referred turing learning direction allow discriminators interact processes data samples obtained making interrogators turing test validate idea using case studies first case study computer infers behavior agent controlling environment second case study robot infers sensor configuration controlling movements results confirm allowing discriminators interrogate accuracy models improved
dualing gans generative adversarial nets gans promising technique modeling distribution samples however well known gan training suffers instability due nature maximin formulation paper explore ways tackle instability problem dualizing discriminator start linear discriminators case conjugate duality provides mechanism reformulate maximin objective maximization problem generator discriminator dualing gan act concert demonstrate extend intuition non linear formulations gans linear discriminators approach able remove instability training gans nonlinear discriminators approach provides alternative commonly used gan training algorithm
fisher gan generative adversarial networks gans powerful models learning complex distributions stable training gans addressed many recent works explore different metrics distributions paper introduce fisher gan fits within integral probability metrics ipm framework training gans fisher gan defines data dependent constraint second order moments critic show paper fisher gan allows stable time efficient training compromise capacity critic need data independent constraints weight clipping analyze fisher ipm theoretically provide algorithm based augmented lagrangian fisher gan validate claims image sample generation semi supervised classification using fisher gan
learning pivot adversarial networks several techniques domain adaptation proposed account differences distribution data used training testing majority work focuses binary domain label similar problems occur scientific context continuous family plausible data generation processes associated presence systematic uncertainties robust inference possible based pivot quantity whose distribution depend unknown values nuisance parameters parametrize family data generation processes work introduce derive theoretical results training procedure based adversarial networks enforcing pivotal property equivalently fairness respect continuous attributes predictive model method includes hyperparameter control trade accuracy robustness demonstrate effectiveness approach toy example examples particle physics
improved training wasserstein gans generative adversarial networks gans powerful generative models suffer training instability recently proposed wasserstein gan wgan makes significant progress toward stable training gans still generate low quality samples fail converge settings find training failures often due use weight clipping wgan enforce lipschitz constraint critic lead pathological behavior propose alternative method enforcing lipschitz constraint instead clipping weights penalize norm gradient critic respect input proposed method converges faster generates higher quality samples wgan weight clipping finally method enables stable gan training first time train wide variety gan architectures almost hyperparameter tuning including 101 layer resnets language models discrete data demonstrate state art inception scores cifar provide samples higher resolution datasets
mmd gan towards deeper understanding moment matching network generative moment matching network gmmn deep generative model differs generative adversarial network gan replacing discriminator gan sample test based kernel maximum mean discrepancy mmd although theoretical guarantees mmd studied empirical performance gmmn still competitive gan challenging large benchmark datasets computational efficiency gmmn also less desirable comparison gan partially due requirement rather large batch size training paper propose improve model expressiveness gmmn computational efficiency introducing adversarial kernel learning techniques replacement fixed gaussian kernel original gmmn new approach combines key ideas gmmn gan hence name mmd gan new distance measure mmd gan meaningful loss enjoys advantage weak topology optimized via gradient descent relatively small batch sizes evaluation multiple benchmark datasets including mnist cifar celeba lsun performance mmd gan significantly outperforms gmmn competitive representative gan works
time scale update rule generative adversarial nets generative adversarial networks gans excel generating images complex generative models maximum likelihood infeasible however training gans still proved converge propose time scale update rule ttur training gans different learning rates discriminator generator gans trained ttur proved converge mild assumptions ttur convergence carries adam stochastic optimiza tion described second order differential equation experiments show ttur improves learning original gans wasserstein gans deep convolutional gans boundary equilibrium gans ttur compared conventional gan training mnist celeba billion word benchmark lsun bedrooms ttur outperforms conventional gan training learning time performance time performance
veegan reducing mode collapse gans using implicit variational learning deep generative models provide powerful tools distributions complicated manifolds natural images many methods including generative adversarial networks gans difficult train part prone mode collapse means characterize modes true distribution address introduce veegan features reconstructor network reversing action generator mapping data noise training objective retains original asymptotic consistency guarantee gans interpreted novel autoencoder loss noise sharp contrast traditional autoencoder data points veegan require specifying loss function data rather representations standard normal assumption extensive set synthetic real world image datasets veegan indeed resists mode collapsing far greater extent recent gan variants produces realistic samples
improved semi supervised learning gans using manifold invariances semi supervised learning methods using generative adversarial networks gans shown promising empirical success recently methods use shared discriminator classifier discriminates real examples fake also predicts class label motivated ability gans capture data manifold well propose estimate tangent space data manifold using gans use inject invariances classifier process propose improvements existing methods learning inverse mapping encoder cite donahue2016adversarial greatly improve terms semantic similarity reconstructed sample input sample experiment svhn cifar semi supervised learning obtaining significant improvements baselines particularly cases number labeled examples low also provide insights fake examples influence semi supervised learning procedure
good semi supervised learning requires bad gan semi supervised learning methods based generative adversarial networks gans obtained strong empirical results clear discriminator benefits joint training generator good semi supervised classification performance good generator cannot obtained time theoretically show given discriminator objective good semi supervised learning indeed requires bad generator propose definition preferred generator empirically derive novel formulation based analysis substantially improves feature matching gans obtaining state art results multiple benchmark datasets
bayesian gans generative adversarial networks gans implicitly learn rich distributions images audio data hard model explicit likelihood present practical bayesian formulation unsupervised semi supervised learning gans use stochastic gradient hamiltonian monte carlo marginalize weights generator discriminator networks resulting approach straightforward obtains good performance without standard interventions feature matching mini batch discrimination exploring expressive posterior parameters generator bayesian gan avoids mode collapse produces interpretable candidate samples notable variability particular provides state art quantitative results semi supervised learning benchmarks including svhn celeba cifar outperforming dcgan wasserstein gans dcgan ensembles
dual discriminator generative adversarial nets propose paper novel approach tackle problem mode collapse encountered generative adversarial network gan idea intuitive proven effective especially addressing key limitations gan essence combines kullback leibler reverse divergences unified objective function thus exploits complementary statistical properties divergences effectively diversify estimated density capturing multi modes term method dual discriminator generative adversarial nets d2gan unlike gan discriminators together generator also analogy minimax game wherein discriminator rewards high scores samples data distribution whilst another discriminator conversely favoring data generator generator produces data fool discriminators develop theoretical analysis show given maximal discriminators optimizing generator d2gan reduces minimizing reverse divergences data distribution distribution induced data generated generator hence effectively avoiding mode collapsing problem conduct extensive experiments synthetic real world large scale datasets mnist cifar stl imagenet made best effort compare d2gan latest state art gan variants comprehensive qualitative quantitative evaluations experimental results demonstrate competitive superior performance approach generating good quality diverse samples baselines capability method scale imagenet database
towards understanding adversarial learning joint distribution matching investigate non identifiability issues associated bidirectional adversarial training joint distribution matching within framework conditional entropy propose adversarial non adversarial approaches learn desirable matched joint distributions unsupervised supervised tasks unify broad family adversarial models joint distribution matching problems approach stabilizes learning unsupervised bidirectional adversarial learning methods introduce extension semi supervised learning tasks theoretical results validated synthetic data real world applications
triple generative adversarial nets generative adversarial nets gans shown promise image generation semi supervised learning ssl however existing gans ssl problems generator discriminator compete learning generator cannot generate images specific class problems essentially arise player formulation single discriminator shares incompatible roles identifying fake samples predicting labels estimates data without considering labels address problems presenting triple generative adversarial net triple gan flexible game theoretical framework classification class conditional generation ssl triple gan consists players generator discriminator classifier generator classifier characterize conditional distributions images labels discriminator solely focuses identifying fake image label pairs design compatible utilities ensure distributions characterized classifier generator concentrate data distribution results various datasets demonstrate triple gan unified model simultaneously achieve state art classification results among deep generative models disentangle classes styles transfer smoothly data level via interpolation latent space class conditionally
triangle generative adversarial networks triangle generative adversarial network delta gan developed semi supervised cross domain joint distribution matching training data consists samples domain supervision domain correspondence provided paired samples delta gan consists neural networks generators discriminators generators designed learn way conditional distributions domains discriminators implicitly define ternary discriminative function trained distinguish real data pairs kinds fake data pairs generators discriminators trained together using adversarial learning mild assumptions theory joint distributions characterized generators concentrate data distribution experiments different kinds domain pairs considered image label image image image attribute pairs experiments semi supervised image classification image image translation attribute based image generation demonstrate superiority proposed approach
structured generative adversarial networks study problem conditional generative modeling based designated semantics structures existing models build conditional generators either require massive labeled instances supervision unable accurately control semantics generated samples propose structured generative adversarial networks sgans semi supervised conditional generative modeling sgan assumes data generated conditioned independent latent variables encodes designated semantic contains factors variation ensure disentangled semantics sgan builds inference networks map back latent space enforces generate samples mapped back hidden space using inferred latent code generator condition always matched regardless variations variable training sgan involves solving adversarial games equilibrium concentrating true joint data distributions avoiding distributing probability mass diffusely data space mle based methods suffer assess sgan evaluating trained networks performance downstream tasks show sgan delivers highly controllable generator disentangled representations also establishes start art results across multiple dataset applied semi supervised image classification errors mnist svhn cifar10 using 1000 4000 labels respectively benefiting separate modeling sgan generate images high visual quality strictly follows designated semantics extended wide spectrum applications style transfer
pixelgan autoencoders paper describe pixelgan autoencoder generative autoencoder generative path convolutional autoregressive neural network pixels pixelcnn conditioned latent code recognition path uses generative adversarial network gan impose prior distribution latent code show different priors result different decompositions information latent code autoregressive decoder example imposing gaussian distribution prior achieve global local decomposition imposing categorical distribution prior disentangle style content information images unsupervised fashion show pixelgan autoencoder categorical prior directly used semi supervised settings achieve competitive semi supervised classification results mnist svhn norb datasets
learning compose domain specific transformations data augmentation data augmentation ubiquitous technique increasing size labeled training sets leveraging task specific data transformations preserve class labels often easy domain experts specify individual transformations constructing tuning sophisticated compositions typically needed achieve state art results time consuming manual task practice propose method automating process learning generative sequence model user specified transformation functions using generative adversarial approach method make use arbitrary non deterministic transformation functions robust misspecified user input trained unlabeled data learned transformation model used perform data augmentation end discriminative model experiments show efficacy approach image text datasets achieving improvements accuracy points cifar points ace relation extraction task accuracy points using domain specific transformation operations medical imaging dataset compared standard heuristic augmentation approaches
unsupervised image image translation networks existing image image translation frameworks mapping image domain corresponding image another based supervised learning pairs corresponding images domains required learning translation function largely limits applications capturing corresponding images different domains often difficult task address issue propose unsupervised image image translation unit framework proposed framework based variational autoencoders generative adversarial networks learn translation function without corresponding images show learning capability enabled combining weight sharing constraint adversarial objective verify effectiveness proposed framework extensive experiment results
adversarial invariant feature learning learning meaningful representations maintain content necessary particular task filtering away detrimental variations problem great interest machine learning paper tackle problem learning representations invariant specific factor trait data leading better generalization representation learning process formulated adversarial minimax game analyze optimal equilibrium game benchmark tasks namely fair classifications bias free language independent generation lighting independent image classification show proposed framework induces invariant representation leads better generalization evidenced improved test performance
adversarial ranking language generation generative adversarial networks gans great successes synthesizing data however existing gans restrict discriminator binary classifier thus limit learning capacity tasks need synthesize output rich structures natural language descriptions paper propose novel generative adversarial network rankgan generating high quality language descriptions rather train discriminator learn assign absolute binary predicate individual data sample proposed rankgan able analyze rank collection human written machine written sentences giving reference group viewing set data samples collectively evaluating quality relative ranking scores discriminator able make better assessment turn helps learn better generator proposed rankgan optimized policy gradient technique experimental results multiple public datasets clearly demonstrate effectiveness proposed approach
efficient computation moments sum product networks bayesian online algorithms sum product networks spns need update posterior distribution seeing single additional instance must compute moments model parameters distribution best existing method computing moments scales quadratically size spn although scales linearly trees unfortunate scaling makes bayesian online algorithms prohibitively expensive except small tree structured spns propose linear time algorithm works even spn general directed acyclic graph dag algorithm significantly broadens applicability bayesian online algorithms spns achieve goal reducing moment computation problem joint inference problem spns taking advantage special structure updated posterior distribution multilinear polynomial exponentially many positive monomials evaluate moments differentiation demonstrate usefulness linear time moment computation algorithm applying develop linear time assume density filter adf spns
attention need dominant sequence transduction models based complex recurrent orconvolutional neural networks encoder decoder configuration best performing models also connect encoder decoder attentionm echanisms propose novel simple network architecture based solely onan attention mechanism dispensing recurrence convolutions entirely experiments machine translation tasks show models superiorin quality parallelizable requiring significantly less timeto train single model 165 million parameters achieves bleu onenglish german translation improving existing best ensemble result bleu english french translation outperform previoussingle state art model bleu achieving bleu score
masked autoregressive flow density estimation autoregressive models among best performing neural density estimators describe approach increasing flexibility autoregressive model based modelling random numbers model uses internally generating data constructing stack autoregressive models modelling random numbers next model stack obtain type normalizing flow suitable density estimation call masked autoregressive flow type flow closely related inverse autoregressive flow generalization real nvp masked autoregressive flow achieves state art performance range general purpose density estimation tasks
variational walkback learning transition operator stochastic recurrent net propose novel method directly learn stochastic transition operator whose repeated application provides generated samples traditional undirected graphical models approach problem indirectly learning markov chain model whose stationary distribution obeys detailed balance respect parameterized energy function energy function modified model data distributions match guarantee number steps required markov chain converge moreover detailed balance condition highly restrictive energy based models corresponding neural networks must symmetric weights unlike biological neural circuits contrast develop method directly learning arbitrarily parameterized transition operators capable expressing non equilibrium stationary distributions violate detailed balance thereby enabling learn biologically plausible asymmetric neural networks general non energy based dynamical systems training objective derive via principled variational methods encourages transition operator walk back prefer revert steps multi step trajectories start data points quickly possible back original data points present series experimental results illustrating soundness proposed approach variational walkback mnist cifar svhn celeba datasets demonstrating superior samples compared earlier attempts learn transition operator also show although rapid training trajectory limited finite variable number steps transition operator continues generate good samples well past length trajectories thereby demonstrating match non equilibrium stationary distribution data distribution
terngrad ternary gradients reduce communication distributed deep learning high network communication cost synchronizing gradients parameters well known bottleneck distributed training work propose terngrad uses ternary gradients accelerate distributed deep learning data parallelism approach requires numerical levels aggressively reduce communication time mathematically prove convergence terngrad assumption bound gradients guided bound propose layer wise ternarizing gradient clipping improve convergence experiments show applying terngrad alexnet incur accuracy loss even improve accuracy accuracy loss googlenet induced terngrad less average finally performance model proposed study scalability terngrad experiments show significant speed gains various deep neural networks
end end differentiable proving introduce deep neural networks end end differentiable theorem proving operate dense vector representations symbols neural networks recursively constructed following backward chaining algorithm used prolog specifically replace symbolic unification differentiable computation vector representations symbols using radial basis function kernel thereby combining symbolic reasoning learning subsymbolic vector representations resulting neural network trained infer facts given incomplete knowledge base using gradient descent learns place representations similar symbols close proximity vector space make use similarities prove facts iii induce logical rules use provided induced logical rules complex multi hop reasoning benchmark knowledge bases demonstrate architecture outperforms complex state art neural link prediction model time inducing interpretable function free first order logic rules
simple neural network module relational reasoning relational reasoning central component generally intelligent behavior proven difficult neural networks learn paper describe use relation networks rns simple plug play module solve problems fundamentally hinge relational reasoning tested augmented networks tasks visual question answering using challenging dataset called clevr achieve state art super human performance text based question answering using babi suite tasks complex reasoning dynamical physical systems using curated dataset called sort clevr show powerful convolutional networks general capacity solve relational questions gain capacity augmented rns thus simply augmenting convolutions lstms mlps rns remove computational burden network components well suited handle relational reasoning reduce overall network complexity gain general ability reason relations entities properties
dual path networks work present simple highly efficient modularized dual path network dpn image classification presents new topology connection paths internally revealing equivalence state art residual network resnet densely convolutional network densenet within hornn framework find resnet enables feature usage densenet enables new features exploration important learning good representations enjoy benefits path topologies proposed dual path network shares common features maintaining flexibility explore new features dual path architectures extensive experiments benchmark datasets imagnet places365 pascal voc clearly demonstrate superior performance proposed dpn state arts particular imagnet dataset shallow dpn surpasses best resnext 101 64x4d smaller model size less computational cost lower memory consumption deeper dpn achieves state art single model performance times faster training speed experiments places365 large scale scene dataset pascal voc detection dataset pascal voc segmentation dataset also demonstrate consistently better performance densenet resnet latest resnext model various applications
spherical convolutions application molecular modelling convolutional neural networks increasingly used outside domain image analysis particular various areas natural sciences concerned spatial data networks often work box cases entire model architectures image analysis carried problem domains almost unaltered unfortunately convenience trivially extend data non euclidean spaces spherical data paper address challenges arise setting particular lack translational equivariance associated using grid based uniform spacing spherical coordinates present definition spherical convolution overcomes issues extend discussion include scenarios spherical volumes several strategies parameterizing radial dimension proof concept conclude assessment performance spherical convolutions context molecular modelling considering structural environments within proteins show model capable learning non trivial functions molecular environments despite lack domain specific feature engineering demonstrate performance comparable state art methods field build decades domain specific knowledge
deep sets study problem designing objective models machine learning tasks defined finite emph sets contrast traditional approach operating fixed dimensional vectors consider objective functions defined sets invariant permutations problems widespread ranging estimation population statistics anomaly detection piezometer data embankment dams cosmology main theorem characterizes permutation invariant objective functions provides family functions permutation invariant objective function must belong family functions special structure enables design deep network architecture operate sets deployed variety scenarios including unsupervised supervised learning tasks demonstrate applicability method population statistic estimation point cloud classification set expansion outlier detection
simple scalable predictive uncertainty estimation using deep ensembles deep neural networks powerful black box predictors recently achieved impressive performance wide spectrum tasks quantifying predictive uncertainty neural networks challenging yet unsolved problem bayesian neural networks learn distribution weights currently state art estimating predictive uncertainty however require significant modifications training procedure computationally expensive compared standard non bayesian neural networks propose alternative bayesian neural networks simple implement readily parallelisable yields high quality predictive uncertainty estimates series experiments classification regression benchmarks demonstrate method produces well calibrated uncertainty estimates good better approximate bayesian neural networks assess robustness dataset shift evaluate predictive uncertainty test examples known unknown distributions show method able express higher uncertainty unseen data demonstrate scalability method evaluating predictive uncertainty estimates imagenet
self normalizing neural networks deep learning revolutionized vision via convolutional neural networks cnns natural language processing via recurrent neural networks rnns however success stories deep learning standard feed forward neural networks fnns rare fnns perform well typically shallow therefore cannot exploit many levels abstract representations introduce self normalizing neural networks snns enable high level abstract representations batch normalization requires explicit normalization neuron activations snns automatically converge towards mean unit variance activation function snns scaled exponential linear units selus induce self normalizing properties using banach fixed point theorem prove activations close mean unit variance propagated many network layers converge towards mean unit variance even presence noise perturbations convergence property snns allows train deep networks many layers employ strong regularization make learning highly robust furthermore activations close unit variance prove upper lower bound variance thus vanishing exploding gradients impossible compared snns 121 tasks uci machine learning repository drug discovery benchmarks astronomy tasks standard fnns machine learning methods random forests support vector machines fnns considered relu networks without normalization batch normalization iii layer normalization weight normalization highway networks residual networks snns significantly outperformed competing fnn methods 121 uci tasks outperformed competing methods tox21 dataset set new record astronomy data set winning snn architectures often deep
batch renormalization towards reducing minibatch dependence batch normalized models batch normalization quite effective accelerating improving training deep models however effectiveness diminishes training minibatches small consist independent samples hypothesize due dependence model layer inputs examples minibatch different activations produced training inference propose batch renormalization simple effective extension ensure training inference models generate outputs depend individual examples rather entire minibatch models trained batch renormalization perform substantially better batchnorm training small non minibatches time batch renormalization retains benefits batchnorm insensitivity initialization training efficiency
train longer generalize better closing generalization gap large batch training neural networks background deep learning models typically trained using stochastic gradient descent variants methods update weights using gradient estimated small fraction training data observed using large batch sizes persistent degradation generalization performance known generalization gap phenomena identifying origin gap closing remained open problem contributions examine initial high learning rate training phase find weight distance initialization grows logarithmicaly number weight updates therefore propose random walk random landscape statistical model known exhibit similar ultra slow diffusion behavior following hypothesis conducted experiments show empirically generalization gap stems relatively small number updates rather batch size completely eliminated adapting training regime used investigate different techniques train models large batch regime present novel algorithm named ghost batch normalization enables significant decrease generalization gap without increasing number updates validate findings conduct several additional experiments mnist cifar cifar 100 imagenet finally reassess common practices beliefs concerning training deep models suggest optimal achieve good generalization
nonlinear random matrix theory deep learning neural network configurations random weights play important role analysis deep learning define initial loss landscape closely related kernel random feature methods despite fact networks built random matrices vast powerful machinery random matrix theory far found limited success studying main obstacle direction neural networks nonlinear prevents straightforward utilization many existing mathematical results work open door direct applications random matrix theory deep learning demonstrating pointwise nonlinearities typically applied neural networks incorporated standard method proof random matrix theory known moments method test case study gram matrix random weight matrix random data matrix pointwise nonlinear activation function derive explicit representation trace resolvent matrix defines limiting spectral distribution apply results computation asymptotic performance single layer random feature methods memorization task analysis eigenvalues data covariance matrix propagates neural network byproduct analysis identify intriguing new class activation functions favorable properties
distral robust multitask reinforcement learning deep reinforcement learning algorithms data inefficient complex rich environments limiting applicability many scenarios direction improving data efficiency multitask learning shared neural network parameters efficiency improved transfer across related tasks practice however usually observed gradients different tasks interfere negatively making learning unstable sometimes even less data efficient another issue different reward schemes tasks easily lead task dominating learning shared model propose new approach joint training multiple tasks refer distral distill transfer learning instead sharing parameters different workers propose share distilled policy captures common behaviour across tasks worker trained solve task constrained stay close shared policy shared policy trained distillation centroid task policies aspects learning process derived optimizing joint objective function show approach supports efficient transfer complex environments outperforming several related methods moreover proposed learning process robust stable attributes critical deep reinforcement learning
imagination augmented agents deep reinforcement learning introduce imagination augmented agents i2as novel architecture deep reinforcement learning combining model free model based aspects contrast existing model based reinforcement learning planning methods prescribe model used arrive policy i2as learn interpret predictions trained environment model construct implicit plans arbitrary ways using predictions additional context deep policy networks i2as show improved data efficiency performance robustness model misspecification compared several strong baselines
second order optimization deep reinforcement learning using kronecker factored approximation work propose apply second order optimization deep reinforcement learning using recently proposed kronecker factored approximation curvature extend framework natural policy gradient propose optimize actor critic using kronecker factored approximate curvature fac trust region hence naming method actor critic using kronecker factored trust region method acktr tested approach across discrete domains atari games well continuous domains mujoco environment proposed methods able achieve higher rewards fold improvement sample efficiency best knowledge also first succeed training several nontrivial tasks mujoco environment directly image rather state space observations
learning combinatorial optimization algorithms graphs design good heuristics approximation algorithms hard combinatorial optimization problems often requires significant specialized knowledge trial error automate challenging tedious process learn algorithms instead many real world applications typically case optimization problem solved regular basis maintaining problem structure differing data provides opportunity learning heuristic algorithms exploit structure recurring problems paper propose unique combination reinforcement learning graph embedding address challenge learned greedy policy behaves like meta algorithm incrementally constructs solution action determined output graph embedding network capturing current state solution show framework applied diverse range optimization problems graphs learns effective algorithms minimum vertex cover maximum cut traveling salesman problems
targeting eeg lfp synchrony neural nets consider analysis electroencephalography eeg local field potential lfp datasets big terms size recorded data rarely sufficient labels required train complex models conventional deep learning methods furthermore many scientific applications goal able understand underlying features related classification prohibits blind application deep networks motivates development new model based parameterized convolutional filters guided previous neuroscience research filters learn relevant frequency bands targeting synchrony frequency specific power phase correlations electrodes results highly expressive convolutional neural network hundred parameters applicable smaller datasets proposed approach demonstrated yield competitive often state art predictive performance empirical tests yielding interpretable features gaussian process adapter developed combine analysis distinct electrode layouts allowing joint processing multiple datasets address overfitting improve generalizability finally demonstrated proposed framework effectively tracks neural dynamics children clinical trial autism spectrum disorder
toward goal driven neural network models rodent whisker trigeminal system large part rodents see world whiskers powerful tactile sense enabled series brain areas form whisker trigeminal system raw sensory data arrives form mechanical input exquisitely sensitive actively controllable whisker array processed sequence neural circuits eventually arriving cortical regions communicate decision making memory areas although long history experimental studies characterized many aspects processing stages computational operations whisker trigeminal system remain largely unknown present work take goal driven deep neural network dnn approach modeling computations first construct biophysically realistic model rat whisker array generate large dataset whisker sweeps across wide variety objects highly varying poses angles speeds next train dnns several distinct architectural families solve shape recognition task dataset architectural family represents structurally distinct hypothesis processing whisker trigeminal system corresponding different ways spatial temporal information integrated find networks perform poorly challenging shape recognition task specific architectures several families achieve reasonable performance levels finally show representational dissimilarity matrices rdms tool comparing population codes neural systems separate higher performing networks data type could plausibly collected neurophysiological imaging experiment results proof concept dnn models whisker trigeminal system potentially within reach
fast amortized inference neural activity calcium imaging data variational autoencoders calcium imaging permits optical measurement neural activity since intracellular calcium concentration indirect measurement neural activity computational tools necessary infer true underlying spiking activity fluorescence measurements bayesian model inversion used solve problem typically requires either computationally expensive mcmc sampling faster approximate maximum posteriori optimization introduce flexible algorithmic framework fast efficient accurate extraction neural spikes imaging data using framework variational autoencoders propose amortize inference training deep neural network perform model inversion efficiently recognition network trained produce samples posterior distribution spike trains trained performing inference amounts fast single forward pass network without need iterative optimization sampling show amortization applied flexibly wide range nonlinear generative models significantly improves upon state art computation time achieving competitive accuracy framework also able represent posterior distributions spike trains demonstrate generality method proposing first probabilistic approach separating backpropagating action potentials putative synaptic inputs calcium imaging dendritic spines
scene physics acquisition via visual animation introduce new paradigm fast rich physical scene understanding without human annotations core system physical world representation recovered perception module utilized physics graphics engines training perception module generative models learn visual animation interpreting reconstructing visual information stream testing system first recovers physical world state uses generative models reasoning future prediction unlike forward simulation inverting physics graphics engine computationally hard problem overcome challenge use convolutional inversion network system quickly recognizes physical world state appearance motion cues flexibility incorporate differentiable non differentiable physics graphics engines evaluate system synthetic real datasets involving multiple physical scenes demonstrate system performs well physical state estimation reasoning problems show knowledge learned synthetic dataset generalizes constrained real images
shape material sound infer hearing object falling onto ground based knowledge physical world humans able infer rich information limited data rough shape object material height falling etc paper aim approximate competency first mimic human knowledge physical world using fast physics based generative model present analysis synthesis approach infer properties falling object approximate human past experience directly mapping audio object properties using deep learning self supervision evaluate method behavioral studies compare human predictions inferring object shape material initial height falling results show method achieves near human performance without annotations
deep networks decoding natural images retinal signals decoding sensory stimuli neural signals used reveal sense physical environment critical design brain machine interfaces however existing linear techniques neural decoding fully reveal exploit fidelity neural signal develop new approximate bayesian method decoding natural images spiking activity populations retinal ganglion cells rgcs sidestep known computational challenges bayesian inference exploiting amortized inference via artificial neural networks developed computer vision enables nonlinear decoding incorporates natural scene statistics implicitly use decoder architecture first linearly reconstructs image rgc spikes applies convolutional autoencoder enhance image resulting decoder trained natural images significantly outperforms state art linear decoding well simple point wise nonlinear decoding additionally decoder trained natural images performs nearly accurately subset natural stimuli faces decoder trained specifically subset feature observed linear decoder results provide tool assessment optimization retinal prosthesis technologies reveal neural output retina provide accurate representation visual scene previously appreciated
quantifying much sensory information neural code relevant behavior determining much sensory information carried neural code contributes behavioral performance key understand sensory function neural information flow however yet analytical tools compute information lies intersection sensory coding behavioral readout develop novel measure termed information theoretic intersection information iii quantifies much sensory information carried neural response also used behavior perceptual discrimination tasks building partial information decomposition framework define iii mutual information presented stimulus consequent behavioral choice extracted compute iii analysis experimental cortical datasets show measure used compare quantitatively contributions spike timing spike rates task performance identify brain areas neural populations specifically transform sensory information choice
model based bayesian inference neural activity connectivity optical interrogation neural circuit population activity measurement calcium imaging combined cellular resolution optogenetic activity perturbations enable mapping neural connectivity vivo requires accurate inference perturbed unperturbed neural activity calcium imaging measurements noisy indirect also contaminated photostimulation artifacts developed new fully bayesian approach jointly inferring spiking activity neural connectivity vivo optical perturbation experiments contrast standard approaches perform spike inference analysis separate maximum likelihood phases joint model able propagate uncertainty spike inference inference connectivity vice versa use framework variational autoencoders model spiking activity using discrete latent variables low dimensional latent common input sparse spike slab generalized linear coupling neurons additionally model properties optogenetic perturbation target photostimulation photostimulation transients joint model includes least sets discrete random variables avoid dramatic slowdown typically caused unable differentiate variables introduce strategies knowledge used variational autoencoders using model able fit models minutes data minutes performed optical circuit mapping experiment primary visual cortex awake mouse use approach predict neural connectivity excitatory neurons layer predicted connectivity sparse consistent known correlations stimulus tuning spontaneous correlation distance
deep hyperalignment paper proposes deep hyperalignment dha regularized deep extension scalable hyperalignment method well suited applying functional alignment fmri datasets nonlinearity high dimensionality broad roi large number subjects unlink previous methods dha limited restricted fixed kernel function uses parametric approach rank singular value decomposition svd stochastic gradient descent optimization consequently time complexity dha fairly scales data size training data referenced dha computes functional alignment new subject experimental studies multi subject fmri analysis confirm dha method achieves superior performance state art algorithms
tensor encoding decomposition brain connectomes application tractography evaluation recently linear formulations convex optimization methods proposed predict diffusion weighted magnetic resonance imaging dmri data given estimates brain connections generated using tractography algorithms size linear models comprising methods grows dmri data connectome resolution become large applied modern data paper introduce method encode dmri signals large connectomes range hundreds thousands millions fascicles bundles neuronal axons using sparse tensor decomposition show tensor decomposition accurately approximates linear fascicle evaluation life model recently developed linear models provide theoretical analysis accuracy sparse decomposed model life_sd demonstrate reduce size model significantly also develop algorithms implement optimization solver using tensor representation efficient way
online dynamic programming consider problem repeatedly solving variant dynamic programming problem successive trials instance type problems consider find optimal binary search tree beginning trial learner probabilistically chooses tree keys internal nodes gaps keys leaves told frequencies keys gaps charged average search cost chosen tree problem online frequencies change trials goal develop algorithms property total average search cost loss trials close total loss best tree chosen hind sight trials challenge course algorithm deal exponential number trees develop methodology tackling problems wide class dynamic programming algorithms framework allows extend online learning algorithms like hedge component hedge significantly wider class combinatorial objects possible
unsupervised learning disentangled representations video present new model drnet learns disentangled image representations video approach leverages temporal coherence video novel adversarial loss learn representation factorizes frame stationary part temporally varying component disentangled representation used range tasks example applying standard lstm time vary components enables prediction future frames evaluating approach range synthetic real videos latter demonstrate ability coherently generate several hundred steps future
interactive submodular bandit many machine learning applications submodular functions used model evaluating utility payoff set news items recommend sensors deploy terrain nodes influence social network name heart applications assumption underlying utility payoff function known priori hence maximizing principle possi ble many real life situations however utility function fully known advance estimated via interactions instance whether user likes movie reliably evaluated shown range influence user social network estimated selected advertise product model problems interactive submodular bandit optimization round receive context previously selected movies choose action propose new movie receive noisy feedback utility action ratings model submodular function context action space develop ucb efficiently trades exploration collecting data exploration proposing good action given gathered data achieves sqrt regret bound rounds interaction specifically given bounded rkhs norm kernel context action payoff space governs smoothness utility function ucb keeps upper confidence bound payoff function allows asymptotically achieve regret finally evaluate results concrete applications including movie recommenda tion movielense data set news recommendation yahoo webscope dataset interactive influence maximization subset facebook network personalized data summarization reuters corpus observe ucb consistently outperforms prior art
streaming robust submodular maximization partitioned thresholding approach study classical problem maximizing monotone submodular function subject cardinality constraint additional twists elements arrive streaming fashion items algorithm memory might removed stream finished develop first robust submodular algorithm star based novel partitioning structure exponentially decreasing thresholding rule star makes pass data retains short robust summary show removal elements obtained summary simple greedy algorithm star greedy runs remaining elements achieves constant factor approximation guarantee different data summarization tasks demonstrate matches outperforms existing greedy streaming methods even allowed benefit knowing removed subset advance
minimizing submodular function samples paper consider problem minimizing submodular function training data submodular functions efficiently minimized conse quently heavily applied machine learning many cases however know function aim optimize rather access training data used learn function paper consider question whether submodular functions minimized cases show even learnable submodular functions cannot minimized within non trivial approximation given access polynomially many samples specifically show class submodular functions range despite pac learnable minimizable polynomial time algorithm obtain approximation strictly better using polynomially many samples drawn distribution furthermore show bound tight using trivial algorithm obtains approximation
process constrained batch bayesian optimisation abstract prevailing batch bayesian optimisation methods allow control variables freely altered iteration real world experiments however physical limitations making time consuming alter settings recommendation batch gives rise unique problem recommended batch set variables expensive experimentally change need constrained remaining control variables varied formulate process constrained batch bayesian optimisation problem propose algorithms pebo show regret sublinear demonstrate performance pebo optimising benchmark test functions tuning hyper parameters svm classifier optimising heat treatment process alloy achieve target hardness optimising short nano fiber production process
marginal value adaptive gradient methods machine learning adaptive optimization methods perform local optimization metric constructed history iterates becoming increasingly popular training deep neural networks examples include adagrad rmsprop adam show simple parameterized problems adaptive methods often find drastically different solutions vanilla stochastic gradient descent sgd construct illustrative binary classification problem data linearly separable sgd achieves test error adagrad adam attain test errors arbitrarily close additionally study empirical generalization capability adaptive methods several state art deep learning models observe solutions found adaptive methods generalize worse often significantly worse sgd even solutions better training performance results suggest practitioners reconsider use adaptive methods train neural networks
breaking nonsmooth barrier scalable parallel method composite optimization due simplicity excellent performance parallel asynchronous variants stochastic gradient descent become popular methods solve wide range large scale optimization problems multi core architectures yet despite practical success support nonsmooth objectives still lacking making unsuitable many problems interest machine learning lasso group lasso empirical risk minimization box constraints key technical issues explain paucity design algorithms asynchronous analysis work propose analyze proxasaga fully asynchronous sparse method inspired saga variance reduced incremental gradient algorithm proposed method easy implement significantly outperforms state art several nonsmooth large scale problems prove method achieves theoretical linear speedup respect sequential version assumptions sparsity gradients block separability proximal term empirical benchmarks multi core architecture illustrate practical speedups 13x core machine
beyond worst case probabilistic analysis affine policies dynamic optimization affine policies control widely used solution approach dynamic optimization computing optimal adjustable solution usually intractable worst case performance affine policies significantly bad empirical performance observed near optimal large class problem instances instance stage dynamic robust optimization problem linear covering constraints uncertain right hand side worst case approximation bound affine policies sqrt also tight see bertsimas goyal 2012 whereas observed empirical performance near optimal paper aim address stark contrast worst case empirical performance affine policies particular show affine policies give good approximation stage adjustable robust optimization problem high probability random instances constraint coefficients generated large class distributions thereby providing theoretical justification observed empirical performance hand also present distribution performance bound affine policies instances generated according distribution omega sqrt high probability however constraint coefficients demonstrates empirical performance affine policies depend generative model instances
approximate supermodularity bounds experimental design work provides performance guarantees greedy solution experimental design problems particular focuses optimal designs typical guarantees apply since mean square error maximum eigenvalue estimation error covariance matrix supermodular leverages concept approximate supermodularity derive non asymptotic worst case suboptimality bounds greedy solutions bounds reveal snr experiments decreases cost functions behave increasingly supermodular functions greedy optimal designs approach optimality results reconcile empirical success greedy experimental design non supermodularity optimality criteria
blackbox backpropagation jacobian sensing small number calls given blackbox random input perturbations show efficiently recover unknown jacobian estimate left action jacobian given vector methods based novel combination compressed sensing graph coloring techniques provably exploit structural prior knowledge jacobian sparsity symmetry noise robust demonstrate efficient backpropagation noisy blackbox layers deep neural net improved data efficiency task linearizing dynamics rigid body system generic ability handle rich class input output dependency structures jacobian estimation problems
asynchronous coordinate descent realistic assumptions asynchronous parallel algorithms potential vastly speed algorithms eliminating costly synchronization however understanding algorithms limited current convergence asynchronous block coordinate descent algorithms based somewhat unrealistic assumptions particular age shared optimization variables used update block assumed independent block updated also assumed updates applied randomly chosen blocks paper argue assumptions either fail hold imply less efficient implementations prove convergence asynchronous parallel block coordinate descent realistic assumptions particular always without independence assumption analysis permits deterministic essentially cyclic random rules block choices bound asynchronous delays available establish convergence bounded delays unbounded delays analysis also covers nonconvex weakly convex strongly convex functions construct lyapunov functions directly model objective progress delays delays treated errors noise continuous time ode provided explain construction high level
clustering noisy queries paper initiate rigorous theoretical study clustering noisy queries given set elements goal recover true clustering asking minimum number pairwise queries oracle oracle answer queries form elements belong cluster queries asked interactively adaptive queries non adaptively front answer erroneous probability paper provide first information theoretic lower bound number queries clustering noisy oracle situations design novel algorithms closely match query complexity lower bound even number clusters unknown moreover design computationally efficient algorithms adaptive non adaptive settings problem captures generalizes multiple application scenarios directly motivated growing body work use crowdsourcing entity resolution fundamental challenging data mining task aimed identify records database referring entity crowd represents noisy oracle number queries directly relates cost crowdsourcing another application comes problem sign edge prediction social network social interactions positive negative must identify sign pair wise interactions querying pairs furthermore clustering noisy oracle intimately connected correlation clustering leading improvement therein finally introduces new direction study popular stochastic block model incomplete stochastic block model matrix recover clusters
approximation algorithms ell_0 low rank approximation study ell_0 low rank approximation problem goal given times matrix output rank matrix small possible matrix denotes number non entries hard variant low rank approximation natural problems underlying metric goal capture many positions data possible provide approximation algorithms significantly improve running time approximation factor previous work show find poly time every rank log matrix leq poly log cdot opt best knowledge first algorithm provable guarantees ell_0 low rank approximation problem even bicriteria algorithms well studied case give epsilon approximation sublinear time impossible variants low rank approximation frobenius norm strengthen well studied case binary matrices obtain psi approximation sublinear time psi opt nnz opt minimum rank matrices nnz number non entries small psi approximation factor
convergence analysis layer neural networks relu activation recent years stochastic gradient descent sgd based techniques become standard tools training neural networks however formal theoretical understanding sgd train neural networks practice largely missing paper make progress understanding mystery providing convergence analysis sgd rich subset layer feedforward networks relu activations subset characterized special structure called identity mapping prove input follows gaussian distribution standard sqrt initialization weights sgd converges global minimum polynomial number steps unlike normal vanilla networks identity mapping makes network asymmetric thus global minimum unique complement theory also able show experimentally multi layer networks mapping better performance compared normal vanilla networks convergence theorem differs traditional non convex optimization techniques show sgd converges optimal phases phase gradient points wrong direction however potential function gradually decreases phase sgd enters nice point convex region converges also show identity mapping necessary convergence moves initial point better place optimization experiment verifies claims
decentralized algorithms outperform centralized algorithms case study decentralized parallel stochastic gradient descent distributed machine learning systems nowadays including tensorflow cntk built centralized fashion bottleneck centralized algorithms lies high communication cost central node motivated ask decentralized algorithms faster centralized counterpart although decentralized psgd psgd algorithms studied control community existing analysis theory show advantage centralized psgd psgd algorithms simply assuming application scenario decentralized network available paper study psgd algorithm provide first theoretical analysis indicates regime decentralized algorithms might outperform centralized algorithms distributed stochastic gradient descent psgd comparable total computational complexities psgd requires much less communication cost busiest node conduct empirical study validate theoretical analysis across multiple frameworks cntk torch different network configurations computation platforms 112 gpus network configurations low bandwidth high latency psgd order magnitude faster well optimized centralized counterparts
decomposition invariant conditional gradient general polytopes line search frank wolfe algorithms linear convergence rates recently achieved great efficiency many applications garber meshi 2016 designed new decomposition invariant pairwise variant favorable dependency domain geometry unfortunately applies restricted class polytopes cannot achieve theoretical practical efficiency time paper show employing away step update similar rates generalized arbitrary polytopes strong empirical performance new condition number domain introduced allows leveraging sparsity solution applied method reformulation svm linear convergence rate depends first time number support vectors
straggler mitigation distributed optimization data encoding slow running straggler tasks significantly reduce computation speed distributed computation recently coding theory inspired approaches applied mitigate effect straggling embedding redundancy certain linear computational steps optimization algorithm thus completing computation without waiting stragglers paper propose alternate approach embed redundancy data instead computation allow nodes operate completely oblivious encoding propose several encoding schemes demonstrate popular batch algorithms gradient descent bfgs applied coding oblivious manner deterministically achieve sample path linear convergence approximate solution original problem using arbitrarily varying subset nodes iteration moreover approximation controlled choice encoding matrix number nodes used iteration provide experimental results demonstrating advantage approach uncoded replication strategies
fixed penalty parameter admm faster convergence new adaptive penalization alternating direction method multipliers admm received tremendous interests solving numerous problems machine learning statistics signal processing however well known performance admm many variants sensitive penalty parameter quadratic term equality constraint although useful heuristic approaches proposed dynamically changing penalty parameter course optimization none shown yield explicit improvement convergence rate appropriate stochastic admm remains open problem establish explicitly faster convergence admms leveraging adaptive scheme penalty parameters paper develop new theory linearized admms new adaptive scheme penalty parameter deterministic stochastic optimization problems non smooth structured regularizers novelty proposed adaptive penalty scheme lies adaptive local sharpness property objective function also marks key difference previous work focus self adaptivity deterministic optimization adjusting penalty parameter per iteration based iterate message theoretical side local sharpness characterized constant theta show proposed deterministic admm enjoys improved iteration complexity widetilde epsilon theta footnote widetilde suppresses logarithmic factor proposed stochastic admm enjoys iteration complexity widetilde epsilon theta without smoothness strong convexity assumptions improve standard counterparts constant penalty parameter practical side demonstrate proposed algorithms converge comparably faster admm fine tuned fixed penalty parameter
accelerated stochastic greedy coordinate descent soft thresholding projection onto simplex paper study well known greedy coordinate descent gcd algorithm solve ell_1 regularized problems improve gcd popular strategies nesterov acceleration stochastic optimization firstly propose new rule greedy selection based ell_1 norm square approximation nontrivial solve convex efficient algorithm called soft thresholding projection sotopo proposed exactly solve ell_1 regularized ell_1 norm square approximation problem induced new rule based new rule sotopo algorithm nesterov acceleration stochastic optimization strategies successfully applied gcd algorithm resulted algorithm called accelerated stochastic greedy coordinate descent asgcd optimal convergence rate sqrt epsilon meanwhile reduces iteration complexity greedy selection factor sample size theoretically empirically show asgcd better performance high dimensional dense problems sparse solution
safe adaptive importance sampling importance sampling become indispensable strategy speed optimization algorithms large scale applications improved adaptive variants using importance values defined complete gradient information changes optimization enjoy favorable theoretical properties typically computationally infeasible paper propose efficient approximation gradient based sampling based safe bounds gradient proposed sampling distribution provably emph best sampling respect given bounds always better uniform sampling fixed importance sampling iii efficiently computed many applications negligible extra cost proposed sampling scheme generic easily integrated existing algorithms particular show coordinate descent stochastic gradient descent sgd enjoy significant speed novel scheme proven efficiency proposed sampling verified extensive numerical testing
sharpness restart acceleration ojasievicz inequality shows sharpness bounds minimum convex optimization problems hold almost generically show sharpness directly controls performance restart schemes constants quantifying sharpness course unobservable show optimal restart strategies robust searching best scheme increases complexity logarithmic factor compared optimal bound overall restart schemes generically accelerate accelerated methods
stochastic optimization variance reduction infinite datasets finite sum structure stochastic optimization algorithms variance reduction proven successful minimizing large finite sums functions unfortunately techniques unable deal stochastic perturbations input data induced example data augmentation cases objective longer finite sum main candidate optimization stochastic gradient descent method sgd paper introduce variance reduction approach settings objective composite strongly convex convergence rate outperforms sgd typically much smaller constant factor depends variance gradient estimates due perturbations single example
min max propagation study application min max propagation variant belief propagation approximate min max inference factor graphs show high order functions minimized mathcal omega min max message update obtained using efficient mathcal omega log procedure number variables demonstrate combination efficient updates family high order constraints enables application min max propagation efficiently approximate hard problem makespan minimization seeks distribute set tasks machines worst case load minimized
disentangled recognition nonlinear dynamics model unsupervised learning paper takes step towards temporal reasoning dynamically changing video pixel space constitutes frames latent space describes non linear dynamics objects world introduce kalman variational auto encoder framework unsupervised learning sequential data disentangles latent representations object representation coming recognition model latent state describing dynamics result evolution world imagined missing data imputed without need generate high dimensional frames time step model trained end end videos variety simulated physical systems outperforms competing methods generative missing data imputation tasks
concrete dropout dropout used practical tool obtain uncertainty estimates large vision models reinforcement learning tasks obtain well calibrated uncertainty estimates grid search dropout probabilities necessary prohibitive operation large models impossible propose new dropout variant gives improved performance better calibrated uncertainties relying recent developments bayesian deep learning use continuous relaxation dropout discrete masks together principled optimisation objective allows automatic tuning dropout probability large models result faster experimentation cycles allows agent adapt uncertainty dynamically data observed analyse proposed variant extensively range tasks give insights common practice field larger dropout probabilities often used deeper model layers
rebar low variance unbiased gradient estimates discrete latent variable models learning models discrete latent variables challenging due high variance gradient estimators generally approaches relied control variates reduce variance reinforce estimator recent work citep jang2016categorical maddison2016concrete taken different approach introducing continuous relaxation discrete variables produce low variance biased gradient estimates work combine approaches novel control variate produces low variance emph unbiased gradient estimates introduce novel continuous relaxation show tightness relaxation adapted online removing hyperparameter show state art variance reduction several benchmark generative modeling tasks generally leading faster convergence better final log likelihood
hierarchical implicit models likelihood free variational inference implicit probabilistic models flexible class models defined simulation process data form basis models encompass understanding physical word despite fundamental nature use implicit models remains limited due challenge positing complex latent structure ability inference models large data sets paper first introduce hierarchical implicit models hims hims combine idea implicit densities hierarchical bayesian modeling thereby defining models via simulators data rich hidden structure next develop likelihood free variational inference lfvi scalable variational inference algorithm hims key lfvi specifying variational family also implicit matches model flexibility allows accurate approximation posterior demonstrate diverse applications large scale physical simulator predator prey populations ecology bayesian generative adversarial network discrete data deep implicit model symbol generation
sticking landing simple lower variance gradient estimators variational inference propose simple general variant standard reparameterized gradient estimator variational evidence lower bound specifically remove part total derivative respect variational parameters corresponds score function removing term produces unbiased gradient estimator whose variance approaches approximate posterior approaches exact posterior analyze behavior gradient estimator theoretically empirically generalize complex variational distributions mixtures importance weighted posteriors
perturbative black box variational inference black box variational inference bbvi reparametrization gradients inspired exploration generalized divergence measures popular class divergences alpha divergences tuned resulting optimal variational distribution covers posterior mass preventing overfitting complex models paper analyze bbvi generalized divergences form biased importance sampling choice divergences corresponds bias variance tradeoff tight variational bound low bias low variance stochastic gradients drawing variational perturbation theory statistical physics use insights construct new variational bound tight easy optimize using reparameterization gradients show several experiments gaussian processes variational autoencoders resulting posterior covariances confident true posterior leading less overfitting therefore higher likelihoods
fast black box variational inference stochastic trust region optimization introduce trustvi fast second order algorithm black box variational inference based trust region optimization reparameterization trick iteration trustvi proposes assesses step based minibatches draws variational distribution algorithm provably converges stationary point implement trustvi stan framework compare advi trustvi typically converges tens iterations solution least good advi reaches thousands iterations trustvi iterations computationally expensive total computation typically order magnitude less experiments
excess risk bounds bayes risk using variational inference latent gaussian models bayesian models established main successful paradigms complex problems machine learning handle intractable inference research area developed new approximation methods fast effective however theoretical analysis performance approximations well developed paper furthers analysis providing bounds excess risk variational inference algorithms large class latent variable models gaussian latent variables strengthen previous results variational algorithms showing competitive point estimate predictor unlike previous work also provide bounds risk emph bayesian predictor risk gibbs predictor approximate posterior bounds applied complex models including sparse gaussian processes correlated topic models theoretical results complemented identifying novel approximations bayesian objective attempt minimize risk directly empirical evaluation compares variational new algorithms shedding light performance
learning causal graphs latent variables causality central notions allows decompose reality meaningful components construct cohesive stories explanations complex aspects reality understand world around make principled decisions intricate situations challenge learning cause effect relationships non experimental data studied different settings literature causal structural learning includes delineating boundary conditions causal relations learned passive data pearl 2000 sgs 2001 paper consider problem learning causal structures presence latent variables aided use interventions known naive approach problem would require interventions causal graph observed variables due costs technical ethical considerations rarely feasible perform practice propose efficient randomized algorithm try ameliorate problem learn causal graph including existence location latent variables using log2 interventions graphs constant degree propose efficient deterministic variant procedure useful different classes graphs including common cases bipartite time series relational type systems
permutation based causal inference algorithms interventions learning bayesian networks using observational interventional data fundamentally important problem due recent technological developments genomics generate single cell gene expression data large scale order utilize data learning gene regulatory networks efficient reliable causal inference algorithms needed make use observational interventional data paper present algorithms type prove consistent faithfulness assumption algorithms interventional adaptations greedy algorithm first algorithms using observational interventional data consistency guarantees moreover algorithms advantage non parametric makes useful analyzing inherently non gaussian gene expression data paper present algorithms consistency guarantees analyze performance simulated data protein signaling data single cell gene expression data
learning causal structures using regression invariance study causal inference multi environment setting functional relations producing variables direct causes remain across environments distribution exogenous noises vary introduce idea using invariance functional relations variables causes across set environments define notion completeness causal inference algorithm setting prove existence algorithm proposing baseline algorithm additionally present alternate algorithm significantly improved computational sample complexity compared baseline algorithm experiment results show proposed algorithm outperforms existing algorithms
counterfactual fairness machine learning impact people legal ethical consequences used automate decisions areas insurance lending hiring predictive policing many scenarios previous decisions made unfairly biased certain subpopulations example particular race gender sexual orientation since past data biased machine learning predictors must account avoid perpetuating creating discriminatory practices paper develop framework modeling fairness using tools causal inference definition counterfactual fairness captures intuition decision fair towards individual actual world counterfactual world individual belonged different demographic group demonstrate framework real world problem fair prediction success law school
causal effect inference deep latent variable models learning individual level causal effects observational data inferring effective medication specific patient problem growing importance policy makers important aspect inferring causal effects observational data handling confounders factors affect intervention outcome carefully designed observational study attempts measure important confounders however even direct access confounders exist noisy uncertain measurement proxies confounders build recent advances latent variable modeling simultaneously estimate unknown latent space summarizing confounders causal effect method based variational autoencoders vae follow causal structure inference proxies show method significantly robust existing methods matches state art previous benchmarks focused individual treatment effects
conic scan coverage algorithm nonparametric topic modeling paper propose new algorithms topic modeling number topics known approach relies analysis concentration mass angular geometry topic simplex convex polytope constructed taking convex hull topics resulting algorithm shown practice accuracy comparable gibbs sampler terms topic estimation requires number topics given moreover algorithm fastest among variety state art parametric techniques consistency estimates produced method established conditions
tractability structured probability spaces recently probabilistic sentential decision diagram psdd proposed framework systematically inducing learning distributions structured objects including combinatorial objects permutations rankings paths matchings graph etc paper study scalability models context representing learning distributions routes map particular introduce notion hierarchical route distribution show leveraged construct tractable psdds route distributions allowing scale larger maps illustrate utility model empirically route prediction task showing accuracy increased significantly compared markov models
pass glm polynomial approximate sufficient statistics scalable bayesian glm inference generalized linear models glms logistic regression poisson regression robust regression provide interpretable models diverse data types probabilistic approaches particularly bayesian ones allow coherent estimates uncertainty incorporation prior information sharing power across experiments via hierarchical models practice however approximate bayesian methods necessary inference either failed scale large data sets failed provide theoretical guarantees quality inference propose new approach based constructing polynomial approximate sufficient statistics glms pass glm demonstrate method admits simple algorithm well trivial streaming distributed extensions compound error across computations provide theoretical guarantees quality point map estimates approximate posterior posterior mean uncertainty estimates validate approach empirically case logistic regression using quadratic approximation show competitive performance terms speed accuracy including advertising data set million data points 000 covariates
adaptive bayesian sampling monte carlo present novel technique learning mass matrices samplers obtained discretized dynamics preserve energy function existing adaptive samplers use riemannian preconditioning techniques mass matrices functions parameters sampled leads significant complexities energy reformulations resultant dynamics often leading implicit systems equations requiring inversion high dimensional matrices leapfrog steps approach provides simpler alternative using existing dynamics sampling step monte carlo framework learning mass matrices step novel online technique also propose way adaptively set number samples gathered step using sampling error estimates leapfrog dynamics along novel stochastic sampler based nos poincar dynamics use framework standard hamiltonian monte carlo hmc well newer stochastic algorithms sghmc sgnht show strong performance synthetic real high dimensional sampling scenarios achieve sampling accuracies comparable riemannian samplers significantly faster
reasoning using counterfactual gaussian processes answering questions important many domains example would patient disease progression slow give dose drug ideally answer question using experiment always possible unethical alternative use non experimental data learn models make counterfactual predictions would observe run experiment paper propose counterfactual counterfactual model continuous time trajectories time series sequences actions taken continuous time develop model within potential outcomes framework neyman rubin counterfactual trained using joint maximum likelihood objective adjusts dependencies observed actions outcomes training data report sets experimental results using counterfactual first shows used learn natural progression untreated progression biomarker trajectories observational data second show cgp used medical decision support learning counterfactual models renal health different types dialysis
multi information source optimization consider bayesian methods multi information source optimization miso seek optimize expensive evaluate black box objective function also accessing cheaper biased noisy approximations information sources present novel algorithm outperforms state art problem using joint statistical model information sources better suited miso used previous approaches novel acquisition function based step optimality analysis supported efficient parallelization provide guarantee asymptotic quality solution provided algorithm experimental evaluations demonstrate algorithm consistently finds designs higher value less cost previous approaches
doubly stochastic variational inference deep gaussian processes deep gaussian processes dgps multi layer generalizations gps inference models proved challenging existing approaches inference dgp models assume approximate posteriors force independence layers work well practice present doubly stochastic variational inference algorithm force independence layers method inference demonstrate dgp model used effectively data ranging size hundreds billion points provide strong empirical evidence inference scheme dgps works well practice classification regression
convolutional gaussian processes introduce practical way introducing convolutional structure gaussian processes makes better suited high dimensional inputs like images existing kernels main contribution work construction inter domain inducing point approximation well tailored convolutional kernel allows gain generalisation benefit convolutional kernel together fast accurate posterior inference investigate several variations convolutional kernel apply mnist cifar known challenging gaussian processes also show marginal likelihood used find optimal weighting convolutional rbf kernels improve performance hope illustration usefulness marginal likelihood help automate discovering architectures larger models
multiresolution kernel approximation gaussian process regression gaussian process regression generally scale beyond thousands data points without applying sort kernel approximation method approximations focus high eigenvalue part spectrum kernel matrix leads bad performance length scale kernel small paper introduce multiresolution kernel approximation mka first true broad bandwidth kernel approximation algorithm important points mka memory efficient direct method means also makes easy approximate mathop textrm det
unifying pac regret uniform pac bounds episodic reinforcement learning statistical performance bounds reinforcement learning algorithms critical high stakes applications like healthcare paper introduces new framework theoretically measuring performance algorithms called uniform pac strengthening classical probably approximately correct pac framework contrast pac framework uniform version used derive high probability regret guarantees forms bridge setups missing literature demonstrate benefits new framework finite state episodic mdps new algorithm uniform pac simultaneously achieves optimal regret pac guarantees except factor horizon
repeated inverse reinforcement learning introduce novel repeated inverse reinforcement learning problem agent act behalf human sequence tasks wishes minimize number tasks surprises human acting suboptimally respect human would acted time human surprised agent provided demonstration desired behavior human formalize problem including sequence tasks chosen different ways provide foundational results
inverse reward design autonomous agents optimize reward function give know hard design reward function actually captures want designing reward might think specific scenarios driving clean roads make sure reward lead right behavior emph scenarios inevitably agents encounter emph new scenarios snowy roads optimizing reward lead undesired behavior driving fast insight work reward functions merely emph observations designer emph actually wants interpreted context designed introduce emph inverse reward design ird problem inferring true reward based designed reward training mdp introduce approximate methods solving ird problems use solution plan risk averse behavior test mdps empirical results suggest approach takes step towards alleviating negative side effects preventing reward hacking
utile context tree weighting reinforcement learning partially observable settings challenging agent immediate observations markov recently proposed methods learn variable order markov models underlying process steep memory requirements sensitive aliasing observation histories due sensor noise paper proposes utile context tree weighting uctw model learning method addresses limitations uctw dynamically expands suffix tree ensuring total size model depth remains bounded show uctw approximately matches performance state art alternatives stochastic time series prediction using least order magnitude less memory also apply uctw model based showing tasks require memory past observations uctw learn without prior knowledge good state representation even length history upon representation depend
policy gradient value function approximation collective multiagent planning decentralized mdps provide expressive framework sequential decision making multiagent system given computational complexity recent research focused tractable yet practical subclasses dec pomdps address subclass called cdec pomdp collective behavior population agents affects joint reward environment dynamics main contribution actor critic reinforcement learning method optimizing cdec pomdp policies vanilla slow convergence larger problems address show particular decomposition approximate action value function agents leads effective updates also derive new way train critic based local reward signals comparisons synthetic benchmark real world taxi fleet optimization problem show new approach provides better quality solutions previous best approaches
unified game theoretic approach multiagent reinforcement learning resurgence interest multiagent reinforcement learning marl due partly recent success deep neural networks simplest form marl independent reinforcement learning inrl agent treats experience part non stationary environment paper first observe policies learned using inrl overfit agents policies training failing sufficiently generalize execution introduce new metric joint policy correlation quantify effect describe meta algorithm general marl based approximate best responses mixtures policies generated using deep reinforcement learning empirical game theoretic analysis compute meta strategies policy selection meta algorithm generalizes previous algorithms inrl iterated best response double oracle fictitious play propose scalable implementation reduces memory requirement using decoupled meta solvers finally demonstrate generality resulting policies partially observable settings gridworld coordination problems emergent language games poker
dynamic safe interruptibility decentralized multi agent reinforcement learning reinforcement learning agents learn taking actions observing outcomes sometimes desirable human operator textit interrupt agent order prevent dangerous situations happening yet part learning process agents link interruptions impact reward specific states deliberately avoid situation particularly challenging multi agent context agents might learn past interruptions also agents orseau armstrong cite orseau2016safely defined emph safe interruptibility learner work naturally extend multi agent systems paper introduces textit dynamic safe interruptibility alternative definition suited decentralized learning problems studies notion learning frameworks textit joint action learners textit independent learners give realistic sufficient conditions learning algorithm enable dynamic safe interruptibility case joint action learners yet show conditions sufficient independent learners show however agents detect interruptions possible prune observations ensure dynamic safe interruptibility even independent learners
multi agent actor critic mixed cooperative competitive environments explore deep reinforcement learning methods multi agent domains begin analyzing difficulty traditional algorithms multi agent case learning challenged inherent non stationarity environment policy gradient suffers variance increases number agents grows present adaptation actor critic methods considers action policies agents able successfully learn policies require complex multi agent coordination additionally introduce training regimen utilizing ensemble policies agent leads robust multi agent policies show strength approach compared existing methods cooperative well competitive scenarios agent populations able discover various physical informational coordination strategies
spectrally normalized margin bounds neural networks show margin distribution normalized spectral complexity parameter strongly predictive neural network generalization performance namely use margin distribution correctly predict whether deep neural networks generalize changes label distribution randomization margin distribution accurately predicts difficulty deep learning tasks show normalizing margin network spectral complexity critical obtaining predictive power finally use margin distribution compare generalization performance multiple networks across different datasets even terms corresponding generalization bound places results rigorous theoretical footing
structured prediction theory calibrated convex surrogate losses provide novel theoretical insights structured prediction context efficient convex surrogate loss minimization consistency guarantees task loss construct convex surrogate optimized via stochastic gradient descent prove tight bounds called calibration function relating excess surrogate risk actual risk contrast prior related work carefully monitor effect exponential number classes learning guarantees well optimization complexity interesting consequence formalize intuition task losses make learning harder others classical loss ill suited structured prediction
collaborative pac learning introduce collaborative pac learning model players attempt learn underlying concept ask much information required learn accurate classifier players simultaneously refer ratio sample complexity collaborative pac learning non collaborative single player counterpart overhead design learning algorithms overhead personalized centralized variants model gives exponential improvement upon naive algorithm share information among players complement upper bounds omega overhead lower bound showing results tight logarithmic factor
submultiplicative glivenko cantelli uniform convergence revenues work derive variant classic glivenko cantelli theorem asserts uniform convergence empirical cumulative distribution function cdf cdf underlying distribution variant allows tighter convergence bounds extreme values cdf apply bound context revenue learning well studied problem economics algorithmic game theory derive sample complexity bounds uniform convergence rate empirical revenues true revenues assuming bound moment valuations possibly fractional uniform convergence limit give complete characterization law first moment valuations finite uniform convergence almost surely occurs conversely first moment infinite uniform convergence almost never occurs
discriminative state space models paper introduce analyze discriminative state space models forecasting non stationary time series provide data dependent generalization guarantees learning models based recently introduced notion discrepancy provide depth analysis complexity models finally also study generalization guarantees several structural risk minimization approaches problem provide efficient implementation based convex objective
delayed mirror descent continuous games paper consider model game theoretic learning based online mirror descent omd asynchronous delayed information instead focusing specific class games sum potential games introduce general equilibrium stability notion games continuous action spaces cal variational stability first contribution last iterate induced sequence play omd converges variationally stable equilibria provided feedback delays faced players synchronous bounded subsequently tackle fully decentralized asynchronous environments unbounded feedback delays propose variant omd call delayed mirror descent dmd relies repeated leveraging past information modification algorithm converges variationally stable equilibria feedback synchronicity assumptions even delays grow superlinearly relative game horizon
variance based regularization convex objectives develop approach risk minimization stochastic optimization provides convex surrogate variance allowing near optimal computationally efficient trading approximation estimation error approach builds techniques distributionally robust optimization owen empirical likelihood provide number finite sample asymptotic results characterizing theoretical performance estimator particular show procedure comes certificates optimality achieving scenarios faster rates convergence empirical risk minimization virtue automatically balancing bias variance give corroborating empirical evidence showing practice estimator indeed trades variance absolute performance training sample improving sample test performance standard empirical risk minimization number classification problems
learning mixture gaussians streaming data paper study problem learning mixture gaussians streaming data given stream points dimensions generated unknown mixture spherical gaussians goal estimate model parameters using single pass data stream analyze streaming version popular lloyd heuristic show algorithm estimates unknown centers component gaussians accurately sufficiently separated assuming pair centers sigma distant omega log sigma sigma maximum variance gaussian component show asymptotically algorithm estimates centers optimally certain constants center separation requirement matches best known result spherical gaussians citep vempalawang finite samples show bias term based initial estimate decreases poly rate variance decreases nearly optimal rate sigma analysis requires seeding algorithm good initial estimate true cluster centers provide online pca based clustering algorithm indeed asymptotic per step time complexity algorithm optimal cdot space complexity algorithm log addition bias variance terms tend hard thresholding based updates streaming lloyd algorithm agnostic data distribution hence incurs emph approximation error cannot avoided however using streaming version classical emph soft thresholding based method exploits gaussian distribution explicitly show mixture gaussians true means estimated consistently estimation error decreasing nearly optimal rate tending rightarrow infty
consistency quick shift quick shift popular mode seeking clustering algorithm however statistical properties yet understood show surprising finite sample statistical consistency guarantees mode cluster estimation mild regularity assumptions underlying density mathbb apply results construct consistent modal regression algorithm
early stopping kernel boosting algorithms general analysis localized complexities early stopping iterative algorithms widely used form regularization statistical learning commonly used conjunction boosting related gradient type algorithms although consistency results established settings estimators less well understood analogues based penalized regularization paper relatively broad class loss functions boosting algorithms including boost logitboost adaboost among others connect performance stopped iterate localized rademacher gaussian complexity associated function class connection allows show local fixed point analysis standard analysis penalized estimators used derive optimal stopping rules derive stopping rules detail various kernel classes illustrate correspondence theory practice sobolev kernel classes
sharp error analysis fused lasso implications broader settings approximate screening dimensional multiple changepoint detection problem existing theorems ell_2 error rate focus primarily model true parameters represent piecewise constant mean sequence however model suitable many applications practice bridge gap prove new ell_2 error rate fused lasso squared error loss parameterized number changepoints samples drawn sub gaussian errors centered around piecewise constant mean function achieve develop novel proof technique revolving around emph lower interpolant rate log log away minimax optimal rate setting equally important proof technique enables extend sharp error rates general models particular misspecified models broad range exponential family models best knowledge analyses extend ell_2 error rates settings results consequences sharper rates approximate screening changepoint locations
scaling limit high dimensional online independent component analysis analyze dynamics online algorithm independent component analysis high dimensional scaling limit ambient dimension tends infinity proper time scaling show time varying joint empirical measure target feature vector estimates provided algorithm converge weakly deterministic measured valued process characterized unique solution nonlinear pde numerical solutions pde involves spatial variables time variable efficiently obtained solutions provide detailed information performance ica algorithm many practical performance metrics functionals joint empirical measures numerical simulations show asymptotic analysis accurate even moderate dimensions addition providing tool understanding performance algorithm pde analysis also provides useful insight particular high dimensional limit original coupled dynamics associated algorithm asymptotically decoupled coordinate independently solving effective minimization problem via stochastic gradient descent exploiting insight design new algorithms achieving optimal trade offs computational statistical efficiency prove interesting line future research
universal analysis large scale regularized least squares solutions problem recent interest statistical inference machine learning signal processing understanding asymptotic behavior regularized least squares solutions random measurement matrices dictionaries least absolute shrinkage selection operator lasso least squares ell_1 regularization perhaps interesting examples precise expressions asymptotic performance lasso obtained number different cases particular elements dictionary matrix sampled independently gaussian distribution also empirically observed resulting expressions remain valid entries dictionary matrix independently sampled certain non gaussian distributions paper confirm observations theoretically distribution sub gaussian generalize previous expressions broader family regularization functions milder conditions underlying random possibly non gaussian dictionary matrix particular establish universality asymptotic statistics average quadratic risk lasso non gaussian dictionaries
statistical convergence analysis gradient general gaussian mixture models paper study convergence properties gradient expectation maximization algorithm cite lange1995gradient gaussian mixture models general number clusters mixing coefficients derive convergence rate depending mixing coefficients minimum maximum pairwise distances true centers dimensionality number components obtain near optimal local contraction radius recent notable works derive local convergence rates equal mixture symmetric gmm general case derivations need structurally different non trivial arguments use recent tools learning theory empirical processes achieve theoretical results
online control false discovery rate decaying memory online multiple testing problem values corresponding different null hypotheses presented decision whether reject hypothesis must made immediately next value presented alpha investing algorithms control false discovery rate first formulated foster stine since generalized applied various settings varying quality preserving databases science multiple tests internet commerce paper improves class generalized alpha investing algorithms gai ways show uniformly improve power entire class gai procedures independence awarding alpha wealth rejection giving near win win resolution dilemma raised javanmard montanari demonstrate incorporate prior weights indicate domain knowledge hypotheses likely null non null allow differing penalties false discoveries indicate hypotheses meaningful important others define new quantity called emph decaying memory false discovery rate memfdr meaningful applications explicit time component using discount factor incrementally forget past decisions alleviate potential problems describe name piggybacking alpha death gai algorithms incorporate generalizations simulatenously reduce powerful variants earlier algorithms weights decay set unity
learning bandit feedback potential games paper examines equilibrium convergence properties regret learning exponential weights potential games establish convergence minimal information requirements players side focus low information frameworks semi bandit case players access noisy estimate payoff vector including strategies play bandit case players able observe game realized payoffs semi bandit case show induced sequence play converges almost surely nash equilibrium quasi exponential rate bandit case result holds epsilon approximations nash equilibria introduce mixing factor epsilon guarantees action choice probabilities never fall epsilon particular algorithm run suitably decreasing mixing factor sequence play converges bona fide nash equilibrium probability
fully decentralized policies multi agent systems information theoretic approach learning cooperative policies multi agent systems often challenged partial observability lack coordination settings structure problem allows distributed solution limited communication central subarea node agent agent consider scenario communication available instead learn local policies agents collectively mimic solution centralized multi agent optimization problem present information theoretic framework based rate distortion theory facilitates analysis well fully decentralized policies able reconstruct optimal solution moreover framework provides natural extension addresses nodes agent communicate improve performance individual policy
revenue optimization approximate bid predictions context advertising auctions finding good reserve prices notoriously challenging learning problem due heterogeneity opportunity types non convexity objective function work show reduce reserve price optimization standard setting prediction squared loss well understood problem learning community bound gap expected bid revenue terms average loss predictor first result formally relates revenue gained quality standard machine learned model
decomposition forecast error prediction markets analyze sources error prediction market forecasts order bound difference security price ground truth estimates consider cost function based prediction markets automated market maker adjusts security prices according history trade decompose forecasting error components sampling error arising traders possess noisy estimates ground truth market maker bias resulting use particular market maker cost function facilitate trade convergence error arising point time market prices still flux goal make explicit tradeoffs error components inherent design decisions functional form cost function amount liquidity market consider specific model traders exponential utility exponential family beliefs representing noisy estimates ground truth setting sampling error vanishes number traders grows tradeoff components provide upper lower bounds market maker bias convergence error demonstrate via numerical simulations bounds tight results yield new insights question set market liquidity parameter forecasting benefits enforcing coherent prices across securities
dynamic revenue sharing many online platforms act intermediaries seller set buyers examples settings include online retailers ebay selling items behalf sellers buyers advertising exchanges adx selling pageviews behalf publishers advertisers settings revenue sharing central part running marketplace intermediary fixed percentage revenue sharing schemes often used split revenue among platform sellers particular revenue sharing schemes require platform take constant fraction alpha revenue auctions pay seller least seller declared opportunity cost item sold straightforward way satisfy constraints set reserve price alpha item optimal solution maximizing profit intermediary previous studies mirrokni gomes niazadeh focused revenue sharing schemes static double auctions paper take advantage repeated nature auctions particular introduce dynamic revenue sharing schemes balance constraints different auctions achieve higher profit seller revenue directly motivated practice advertising exchanges fixed percentage revenue share met across auctions auction paper characterize optimal revenue sharing scheme satisfies constraints expectation finally empirically evaluate revenue sharing scheme real data
multi view decision processes consider operative player sequential game agents disagree transition probabilities underlying markovian model world committing play specific policy agent correct model steer behavior agent hence achieve significant improvement utility model setting multi view decision process use formally analyze positive effect steering policies furthermore develop algorithm computing agents achievable joint policy experimentally show lead significant utility increase agents models diverge
unified semantic embedding relating taxonomies attributes propose method learns discriminative yet semantic space object categorization also embed auxiliary semantic entities supercategories attributes contrary prior work utilized side information explicitly embed semantic entities space embed categories enables represent category linear combination exploiting unified model semantics enforce category generated sparse combination supercategory attributes additional exclusive regularization learn discriminative composition proposed reconstructive regularization guides discriminative learning process learn better generalizing model well generates compact semantic description category enables humans analyze learned
distributed estimation information loss exponential families distributed learning probabilistic models multiple data repositories minimum communication increasingly important study simple communication efficient learning framework first calculates local maximum likelihood estimates mle based data subsets combines local mles achieve best possible approximation global mle based whole dataset jointly study statistical properties framework showing loss efficiency compared global setting relates much underlying distribution families deviate full exponential families drawing connection theory information loss fisher rao efron show full exponential family ness represents lower bound error rate arbitrary combinations local mles achieved divergence based combination method common linear combination method also study empirical properties linear combination methods showing method significantly outperforms linear combination practical settings issues model misspecification non convexity heterogeneous data partitions
near optimal density estimation near linear time using variable width histograms let unknown arbitrary probability distribution consider problem emph density estimation learning algorithm given draws must high probability output hypothesis distribution close main contribution paper highly efficient density estimation algorithm learning using variable width histogram hypothesis distribution piecewise constant probability density function detail eps give algorithm makes tilde eps draws runs tilde eps time outputs hypothesis distribution piecewise constant log eps pieces high probability hypothesis satisfies dtv leq cdot opt_k eps dtv denotes total variation distance statistical distance universal constant opt_k smallest total variation distance piecewise constant distribution sample size running time algorithm optimal logarithmic factors approximation factor present result inherent problem prove algorithm sample size bounded terms eps achieve regardless kind hypothesis distribution uses
bayesian case model generative approach case based reasoning prototype classification present bayesian case model bcm general framework bayesian case based reasoning cbr prototype classification clustering bcm brings intuitive power cbr bayesian generative framework bcm learns prototypes quintessential observations best represent clusters dataset performing joint inference cluster labels prototypes important features simultaneously bcm pursues sparsity learning subspaces sets features play important roles characterization prototypes prototype subspace representation provides quantitative benefits interpretability preserving classification accuracy human subject experiments verify statistically significant improvements participants understanding using explanations produced bcm compared given prior art
encoding high dimensional local features sparse coding based fisher vectors deriving gradient vector generative model local features fisher vector coding fvc identified effective coding method image classification fvc implementations employ gaussian mixture model gmm characterize generation process local features choice shown sufficient traditional low dimensional local features sift typically good performance achieved hundred gaussian distributions however number gaussians insufficient model feature space spanned higher dimensional local features become popular recently order improve modeling capacity high dimensional features turns inefficient computationally impractical simply increase number gaussians paper propose model local feature drawn gaussian distribution whose mean vector sampled subspace certain approximation model converted sparse coding procedure learning inference problems readily solved standard sparse coding methods calculating gradient vector proposed model derive new fisher vector encoding strategy termed sparse coding based fisher vector coding scfvc moreover adopt recently developed deep convolutional neural network cnn descriptor high dimensional local feature implement image classification proposed scfvc experimental evaluations demonstrate method significantly outperforms traditional gmm based fisher vector encoding also achieves state art performance generic object recognition indoor scene fine grained image classification problems
serialrank spectral ranking using seriation describe seriation algorithm ranking set items given pairwise comparisons items intuitively algorithm assigns similar rankings items compare similarly others constructing similarity matrix pairwise comparisons using seriation methods reorder matrix construct ranking first show spectral seriation algorithm recovers true ranking pairwise comparisons observed consistent total order show ranking reconstruction still exact even pairwise comparisons corrupted missing seriation based spectral ranking robust noise scoring methods additional benefit seriation formulation allows solve semi supervised ranking problems experiments synthetic real datasets demonstrate seriation based spectral ranking achieves competitive cases superior performance compared classical ranking methods
improved multimodal deep learning variation information deep learning successfully applied multimodal representation learning problems common strategy learning joint representations shared across multiple modalities top layers modality specific networks nonetheless still remains question learn good association data modalities particular good generative model multimodal data able reason missing data modality given rest data modalities paper propose novel multimodal representation learning framework explicitly aims goal rather learning maximum likelihood train model minimize variation information provide theoretical insight proposed learning objective sufficient estimate data generating joint distribution multimodal data apply method restricted boltzmann machines introduce learning methods based contrastive divergence multi prediction training addition extend deep networks recurrent encoding structure finetune whole network experiments demonstrate state art visual recognition performance mir flickr database pascal voc 2007 database without text features
provable svd based algorithm learning topics dominant admixture corpus topic models latent dirichlet allocation lda posit documents drawn admixtures distributions words known topics inference problem recovering topics collection documents drawn admixtures hard making strong assumption called separability gave first provable algorithm inference widely used lda model gave provable algorithm using clever tensor methods learn topic vectors bounded l_1 error natural measure probability vectors aim develop model makes intuitive empirically supported assumptions design algorithm natural simple components svd provably solves inference problem model bounded l_1 error topic lda models essentially characterized group occurring words motivated introduce topic specific catchwords group words occur strictly greater frequency topic topic individually required high frequency together rather individually major contribution paper show realistic assumption empirically verified real corpora singular value decomposition svd based algorithm crucial pre processing step thresholding provably recover topics collection documents drawn dominant admixtures dominant admixtures convex combination distributions distribution significantly higher contribution others apart simplicity algorithm sample complexity near optimal dependence w_0 lowest probability topic dominant better empirical evidence shows several real world corpora catchwords dominant admixture assumptions hold proposed algorithm substantially outperforms state art
robust kernel density estimation scaling projection hilbert space robust parameter estimation well studied parametric density estimation little investigation robust density estimation nonparametric setting present robust version popular kernel density estimator kde estimators robust version kde useful since sample contamination common issue datasets robustness means nonparametric density estimate straightforward topic explore paper construct robust kde scale traditional kde project nearest weighted kde norm squared norm penalizes point wise errors superlinearly causes weighted kde allocate weight high density regions demonstrate robustness spkde numerical experiments consistency result shows asymptotically spkde recovers uncontaminated density sufficient conditions contamination
delay tolerant algorithms asynchronous distributed online learning analyze new online gradient descent algorithms distributed systems large delays gradient computations corresponding updates using insights adaptive gradient methods develop algorithms adapt sequence gradients also precise update delays occur first give impractical algorithm achieves regret bound precisely quantifies impact delays analyze adaptiverevision algorithm efficiently implementable achieves comparable guarantees key algorithmic technique appropriately efficiently revising learning rate used previous gradient steps experimental results show delays grow large 1000 updates new algorithms perform significantly better standard adaptive gradient methods
sparse multi task reinforcement learning multi task reinforcement learning mtrl objective simultaneously learn multiple tasks exploit similarity improve performance single task learning paper investigate case tasks accurately represented linear approximation space using small subset original large set features equivalent assuming weight vectors task value functions textit jointly sparse set non components small shared across tasks building existing results multi task regression develop multi task extensions fitted iteration algorithm first algorithm assumes tasks jointly sparse given representation second learns transformation features attempt finding sparse representation algorithms provide sample complexity analysis numerical simulations
projecting markov random field parameters fast mixing markov chain monte carlo mcmc algorithms simple extremely powerful techniques sample almost arbitrary distributions flaw practice take large unknown amount time converge stationary distribution paper gives sufficient conditions guarantee univariate gibbs sampling markov random fields mrfs fast mixing precise sense algorithm given project onto set fast mixing parameters euclidean norm following recent work give example use project various divergence measures comparing univariate marginals obtained sampling projection common variational methods gibbs sampling original parameters
algorithms cvar optimization mdps many sequential decision making problems want manage risk minimizing measure variability costs addition minimizing standard criterion conditional value risk cvar relatively new risk measure addresses shortcomings well known variance related risk measures computational efficiencies gained popularity finance operations research paper consider mean cvar optimization problem mdps first derive formula computing gradient risk sensitive objective function devise policy gradient actor critic algorithms uses specific method estimate gradient updates policy parameters descent direction establish convergence algorithms locally risk sensitive optimal policies finally demonstrate usefulness algorithms optimal stopping problem
learning concept hierarchy multi labeled documents topic models discover patterns word usage large corpora difficult meld unsupervised structure noisy human provided labels especially label space large paper present model label hierarchy l2h induce hierarchy user generated labels topics associated labels set multi labeled documents model robust enough account missing labels untrained disparate annotators provide interpretable summary otherwise unwieldy label set show empirically effectiveness l2h predicting held words labels unseen documents
global belief recursive neural networks recursive neural networks recently obtained state art performance several natural language processing tasks however feedforward architecture cannot correctly predict phrase word labels determined context problem tasks aspect specific sentiment classification tries instance predict word android positive sentence android beats ios introduce global belief recursive neural networks rnns based idea extending purely feedforward neural networks include feedbackward step inference allows phrase level predictions representations give feedback words show effectiveness model task contextual sentiment analysis also show dropout improve rnn training combination unsupervised supervised word vector representations performs better either alone feedbackward step improves performance standard rnn task obtains state art performance semeval 2013 challenge accurately predict sentiment specific entities
subspace embeddings polynomial kernel sketching powerful dimensionality reduction tool accelerating statistical learning algorithms however applicability limited certain extent since crucial ingredient called oblivious subspace embedding applied data spaces explicit representation column span row span matrix many settings learning done high dimensional space implicitly defined data matrix via kernel transformation propose first fast oblivious subspace embeddings able embed space induced non linear kernel without explicitly mapping data high dimensional space particular propose embedding mappings induced polynomial kernel using subspace embeddings obtain fastest known algorithms computing implicit low rank approximation higher dimension mapping data matrix computing approximate kernel pca data well approximate kernel principal component regression
self adaptable templates feature coding hierarchical feed forward networks successfully applied object recognition level hierarchy features extracted encoded followed pooling step within processing pipeline common trend learn feature coding templates often referred codebook entries filters complete basis recently approach apparently use templates shown obtain promising results second order pooling o2p paper analyze o2p coding pooling scheme find testing phase o2p automatically adapts feature coding templates input features rather using templates learned training phase finding able bring common concepts coding pooling schemes o2p feature quantization allows significant accuracy improvements o2p standard benchmarks image classification namely caltech101 voc07
saga fast incremental gradient method support non strongly convex composite objectives work introduce new fast incremental gradient method saga spirit sag sdca miso svrg saga improves theory behind sag svrg better theoretical convergence rates support composite objectives proximal operator used regulariser unlike sdca saga supports non strongly convex problems directly adaptive inherent strong convexity problem give experimental results showing effectiveness method
efficient minimax strategies square loss games consider online prediction problems loss prediction outcome measured squared euclidean distance generalization squared mahalanobis distance derive minimax solutions case prediction action spaces simplex setup sometimes called brier game ell_2 ball setup related gaussian density estimation show cases value sub game quadratic function simple statistic state coefficients efficiently computed using explicit recurrence relation resulting deterministic minimax strategy randomized maximin strategy linear functions statistic
quantized estimation gaussian sequence models euclidean balls central result statistical theory pinsker theorem characterizes minimax rate normal means model nonparametric estimation paper present extension pinsker theorem estimation carried storage communication constraints particular place limits number bits used encode estimator analyze excess risk terms constraint signal size noise level give sharp upper lower bounds case euclidean ball establishes pareto optimal minimax tradeoff storage risk setting
constant nullspace strong convexity fast convergence proximal methods high dimensional settings state art statistical estimators high dimensional problems take form regularized hence non smooth convex programs key facet thesestatistical estimation problems typically strongly convex high dimensional sampling regime hessian matrix becomes rank deficient vanilla convexity however proximal optimization methods attain sublinear rate paper investigate novel variant strong convexity call constant nullspace strong convexity cnsc require objective function strongly convex constant subspace show cnsc condition naturally satisfied high dimensional statistical estimators analyze behavior proximal methods cnsc condition show global linear convergence proximal gradient local quadratic convergence proximal newton method regularization function comprising statistical estimator decomposable corroborate theory via numerical experiments show qualitative difference convergence rates proximal algorithms loss function satisfy cnsc condition
weakly supervised discovery visual pattern configurations prominence weakly labeled data gives rise growing demand object detection methods cope minimal supervision propose approach automatically identifies discriminative configurations visual patterns characteristic given object class formulate problem constrained submodular optimization problem demonstrate benefits discovered configurations remedying mislocalizations finding informative positive negative training examples together lead state art weakly supervised detection results challenging pascal voc dataset
online decision making general combinatorial spaces study online combinatorial decision problems must make sequential decisions combinatorial space without knowing advance cost decisions trial goal minimize total regret sequence trials relative best fixed decision hindsight problems studied mostly settings decisions represented boolean vectors costs linear representation study general setting costs linear suitable low dimensional vector representation elements decision space give general algorithm problems call low dimensional online mirror descent ldomd algorithm generalizes component hedge algorithm koolen 2010 recent algorithm suehiro 2012 study offers unification generalization previous work emphasizes role convex polytope arising vector representation decision space boolean representations lead polytopes general vector representations lead general polytopes study several examples types polytopes finally demonstrate benefit general framework problems via application online transportation problem associated transportation polytopes generalize birkhoff polytope doubly stochastic matrices resulting algorithm generalizes permelearn algorithm helmbold warmuth 2009
learning mixed multinomial logit model ordinal data motivated generating personalized recommendations using ordinal preference data study question learning mixture multinomial logit mnl model parameterized class distributions permutations partial ordinal preference data pair wise comparisons despite long standing importance across disciplines including social choice operations research revenue management little known question case single mnl models mixture computationally statistically tractable learning pair wise comparisons feasible however even learning mixture mnl model infeasible general given state affairs seek conditions feasible learn mixture model computationally statistically efficient manner end present sufficient condition well efficient algorithm learning mixed mnl models partial preferences comparisons data particular mixture mnl components objects learnt using samples whose size scales polynomially concretely log model parameters sufficiently incoherent algorithm phases first learn pair wise marginals component using tensor decomposition second learn model parameters component using rankcentrality introduced negahban process proving results obtain generalization existing analysis tensor decomposition realistic regime partial information sample available
probabilistic differential dynamic programming present data driven probabilistic trajectory optimization framework systems unknown dynamics called probabilistic differential dynamic programming pddp pddp takes account uncertainty explicitly dynamics models using gaussian processes gps based second order local approximation value function pddp performs dynamic programming around nominal trajectory gaussian belief spaces different typical gradient based policy search methods pddp require policy parameterization learns locally optimal time varying control policy demonstrate effectiveness efficiency proposed algorithm using nontrivial tasks compared classical ddp state art based policy search method pddp offers superior combination data efficiency learning speed applicability
learning learning rate prediction expert advice standard algorithms prediction expert advice depend parameter called learning rate learning rate needs large enough fit data well small enough prevent overfitting exponential weights algorithm sequence prior work established theoretical guarantees higher higher data dependent tunings learning rate allow increasingly aggressive learning practice theoretical tunings often still perform worse measured regret hoc tuning even higher learning rate close gap theory practice introduce approach learn learning rate factor poly logarithmic number experts inverse learning rate method performs well would know empirically best learning rate large range includes conservative small values values much higher formal guarantees previously available method employs grid learning rates yet runs linear time regardless size grid
robust bayesian max margin clustering present max margin bayesian clustering bmc general robust framework incorporates max margin criterion bayesian clustering models well concrete models bmc demonstrate flexibility effectiveness dealing different clustering tasks dirichlet process max margin gaussian mixture nonparametric bayesian clustering model relaxes underlying gaussian assumption dirichlet process gaussian mixtures incorporating max margin posterior constraints able infer number clusters data extend ideas present max margin clustering topic model learn latent topic representation document time cluster documents max margin fashion extensive experiments performed number real datasets results indicate superior clustering performance methods compared related baselines
convergence rate decomposable submodular function minimization submodular functions describe variety discrete problems machine learning signal processing computer vision however minimizing submodular functions poses number algorithmic challenges recent work introduced easy use parallelizable algorithm minimizing submodular functions decompose sum simple submodular functions empirically algorithm performs extremely well theoretical analysis given paper show algorithm converges linearly provide upper lower bounds rate convergence proof relies geometry submodular polyhedra draws results spectral graph theory
general table completion using bayesian nonparametric model even though heterogeneous databases found broad variety applications exists lack tools estimating missing data databases paper provide efficient robust table completion tool based bayesian nonparametric latent feature model particular propose general observation model indian buffet process ibp adapted mixed continuous real valued positive real valued discrete categorical ordinal count observations propose inference algorithm scales linearly number observations finally experiments real databases show proposed approach provides robust accurate estimates standard ibp bayesian probabilistic matrix factorization gaussian observations
iterative neural autoregressive distribution estimator nade training neural autoregressive density estimator nade viewed step probabilistic inference missing values data propose new model extends inference scheme multiple steps arguing easier learn improve reconstruction steps rather learn reconstruct single inference step proposed model unsupervised building block deep learning combines desirable properties nade multi predictive training test likelihood computed analytically easy generate independent samples uses inference engine superset variational inference boltzmann machines proposed nade competitive state art density estimation datasets tested
expectation backpropagation parameter free training multilayer neural networks continuous discrete weights multilayer neural networks mnns commonly trained using gradient descent based methods backpropagation inference probabilistic graphical models often done using variational bayes methods expectation propagation show based approach also used train deterministic mnns specifically approximate posterior weights given data using mean field factorized distribution online setting using online central limit theorem find analytical approximation bayes update posterior well resulting bayes estimates weights outputs despite different origin resulting algorithm expectation backpropagation ebp similar form efficiency however several additional advantages training parameter free given initial conditions prior mnn architecture useful large scale problems parameter tuning major challenge weights restricted discrete values especially useful implementing trained mnns precision limited hardware chips thus improving speed energy efficiency several orders magnitude test ebp algorithm numerically binary text classification tasks tasks ebp outperforms standard optimal constant learning rate previously reported state art interestingly ebp trained mnns binary weights usually perform better mnns continuous real weights average mnn output using inferred posterior
autoencoder approach learning bilingual word representations cross language learning allows use training data language build models different language many approaches bilingual learning require word level alignment sentences parallel corpora work explore use autoencoder based methods cross language learning vectorial word representations aligned languages relying word level alignments show simply learning reconstruct bag words representations aligned sentences within languages fact learn high quality representations without word alignments empirically investigate success approach problem cross language text classification classifier trained given language english must learn generalize different language german experiments language pairs show approach achieves state art performance outperforming method exploiting word alignments strong machine translation baseline
framework studying synaptic plasticity neural spike train data learning memory brain implemented complex time varying changes neural circuitry computational rules according synaptic weights change time subject much research precisely understood recently limitations experimental methods made challenging test hypotheses synaptic plasticity large scale however data become available barriers lifted becomes necessary develop analysis techniques validate plasticity models present highly extensible framework modeling arbitrary synaptic plasticity rules spike train data populations interconnected neurons treat synaptic weights potentially nonlinear dynamical system embedded fully bayesian generalized linear model glm addition provide algorithm inferring synaptic weight trajectories alongside parameters glm learning rules using method perform model comparison proposed variants well known spike timing dependent plasticity stdp rule nonlinear effects play substantial role synthetic data generated biophysical simulator neuron show recover weight trajectories pattern connectivity underlying learning rules
inferring sparse representations continuous signals continuous orthogonal matching pursuit many signals spike trains recorded multi channel electrophysiological recordings represented sparse sum translated scaled copies waveforms whose timing amplitudes interest aggregate signal seek estimate identities amplitudes translations waveforms compose signal present fast method recovering identities amplitudes translations method involves greedily selecting component waveforms refining estimates amplitudes translations moving iteratively steps process analogous well known orthogonal matching pursuit omp algorithm approach modeling translations borrows continuous basis pursuit cbp extend several ways selecting subspace optimally captures translated copies waveforms replacing convex optimization problem greedy approach moving fourier domain precisely estimate time shifts test resulting method call continuous orthogonal matching pursuit comp simulated neural data shows gains cbp speed accuracy
magnitude sensitive preference formation understanding neural computations underlie ability animals choose among options advanced synthesis computational modeling brain imaging behavioral choice experiments yet remains gulf theories preference learning accounts real economic choices humans face daily life choices usually amount money item paper develop theory magnitude sensitive preference learning permits agent rationally infer preferences items compared money options different magnitudes show theory yields classical anomalous supply demand curves predicts choices large panel risky lotteries accurate replications phenomena without recourse utility functions suggest theory proposed psychologically realistic econometrically viable
spectral support norm regularization support norm successfully applied sparse vector prediction problems observe belongs wider class norms call box norms within framework derive efficient algorithm compute proximity operator squared norm improving upon original method support norm extend norms vector matrix setting introduce spectral support norm study properties show closely related multitask learning cluster norm apply norms real synthetic matrix completion datasets findings indicate spectral support norm regularization gives state art performance consistently improving trace norm regularization matrix elastic net
parallel direction method multipliers consider problem minimizing block separable convex functions subject linear constraints alternating direction method multipliers admm block linear constraints intensively studied theoretically empirically spite preliminary work effective generalizations admm multiple blocks still unclear paper propose parallel randomized block coordinate method named parallel direction method multipliers pdmm solve optimization problems multi block linear constraints pdmm randomly updates blocks parallel behaving like parallel randomized block coordinate descent establish global convergence iteration complexity pdmm constant step size also show pdmm randomized block coordinate descent overlapping blocks experimental results show pdmm performs better state arts methods applications robust principal component analysis overlapping group lasso
scalable kernel methods via doubly stochastic gradients general perception kernel methods scalable neural nets become choice large scale nonlinear learning problems tried hard enough kernel methods paper propose approach scales kernel methods using novel concept called doubly stochastic functional gradients based fact many kernel methods expressed convex optimization problems approach solves optimization problems making unbiased stochastic approximations functional gradient using random training points another using random features associated kernel performing descent steps noisy functional gradient algorithm simple need commit preset number random features allows flexibility function class grow see incoming data streaming setting demonstrate function learned procedure iterations converges optimal function reproducing kernel hilbert space rate achieves generalization bound sqrt approach readily scale kernel methods regimes dominated neural nets show competitive performances approach compared neural nets datasets million energy materials molecularspace million handwritten digits mnist million photos imagenet using convolution features
pre training recurrent neural networks via linear autoencoders propose pre training technique recurrent neural networks based linear autoencoder networks sequences linear dynamical systems modelling target sequences start giving closed form solution definition optimal weights linear autoencoder given training set sequences solution however computationally demanding suggest procedure get approximate solution given number hidden units weights obtained linear autoencoder used initial weights input hidden connections recurrent neural network trained desired task using well known datasets sequences polyphonic music show proposed pre training approach highly effective since allows largely improve state art results considered datasets
predicting useful neighborhoods lazy local learning lazy local learning methods train classifier fly test time using subset training instances relevant novel test example goal tailor classifier properties data surrounding test example existing methods assume instances useful building local model strictly closest test example however fails account fact success resulting classifier depends full distribution selected training instances rather simply gather test example nearest neighbors propose predict subset training data jointly relevant training local model develop approach discover patterns queries good neighborhoods using large scale multi label classification compressed sensing given novel test point estimate composition size training subset likely yield accurate local model demonstrate approach image classification tasks sun apascal show outperforms traditional global local approaches
structure learning antiferromagnetic ising models paper investigate computational complexity learning graph structure underlying discrete undirected graphical model samples first result unconditional computational lower bound omega learning general graphical models nodes maximum degree class statistical algorithms recently introduced feldman construction related notoriously difficult learning parities noise problem computational learning theory lower bound shows widetilde runtime required bresler mossel sly exhaustive search algorithm cannot significantly improved without restricting class models aside structural assumptions graph tree hypertree tree like etc recent papers structure learning assume model correlation decay property indeed focusing ferromagnetic ising models bento montanari showed known low complexity algorithms fail learn simple graphs interaction strength exceeds number related correlation decay threshold second set results gives class repelling antiferromagnetic models emph opposite behavior strong repelling allows efficient learning time widetilde provide algorithm whose performance interpolates widetilde widetilde depending strength repulsion
deep fragment embeddings bidirectional image sentence mapping introduce model bidirectional retrieval images sentences deep multi modal embedding visual natural language data unlike previous models directly map images sentences common embedding space model works finer level embeds fragments images objects fragments sentences typed dependency tree relations common space introduce structured max margin objective allows model explicitly associate fragments across modalities extensive experimental evaluation shows reasoning global level images sentences finer level respective fragments improves performance image sentence retrieval tasks additionally model provides interpretable predictions image sentence retrieval task since inferred inter modal alignment fragments explicit
sparse bayesian structure learning dependent relevance determination prior many problem settings parameter vectors merely sparse dependent way non coefficients tend cluster together refer form dependency region sparsity classical sparse regression methods lasso automatic relevance determination ard model parameters independent priori therefore exploit dependencies introduce hierarchical model smooth region sparse weight vectors tensors linear regression setting approach represents hierarchical extension relevance determination framework add transformed gaussian process model dependencies prior variances regression weights combine structured model prior variances fourier coefficients eliminates unnecessary high frequencies resulting prior encourages weights region sparse different bases simultaneously develop efficient approximate inference methods show substantial improvements comparable methods group lasso smooth rvm simulated real datasets brain imaging
greedy subspace clustering consider problem subspace clustering given points lie near union many low dimensional linear subspaces recover subspaces end first identifies sets points close subspace uses sets estimate subspaces geometric structure clusters linear subspaces forbids proper performance general distance based approaches means many model specific methods proposed paper provide new simple efficient algorithms problem statistical analysis shows algorithms guaranteed exact perfect clustering performance certain conditions number points affinity tween subspaces conditions weaker considered standard statistical literature experimental results synthetic data generated standard unions subspaces model demonstrate theory also show algorithm performs competitively state art algorithms real world applications motion segmentation face clustering much simpler implementation lower computational cost
simple map inference via low rank relaxations focus problem maximum posteriori map inference markov random fields binary variables pairwise interactions common subclass inference tasks consider low rank relaxations interpolate discrete problem full rank semidefinite relaxation followed randomized rounding develop new theoretical bounds studying effect rank showing rank grows relaxed objective increases saturates fraction objective value retained rounded discrete solution decreases practice show algorithms optimizing low rank objectives simple implement enjoy ties underlying theory outperform existing approaches benchmark map inference tasks
fast sampling based inference balanced neuronal networks multiple lines evidence support notion brain performs probabilistic inference multiple cognitive domains including perception decision making also evidence probabilistic inference implemented brain quasi stochastic activity neural circuits producing samples appropriate posterior distributions effectively implementing markov chain monte carlo algorithm however time becomes fundamental bottleneck sampling based probabilistic representations quality inferences depends fast neural circuit generates new uncorrelated samples stationary distribution posterior explore bottleneck simple linear gaussian latent variable model posterior sampling achieved stochastic neural networks linear dynamics well known langevin sampling recipe far sampling algorithm continuous variables neural implementation suggested naturally fits dynamical framework however first show analytically simulations symmetry synaptic weight matrix implied yields critically slow mixing posterior high dimensional next using methods control theory construct inspect networks optimally fast hence orders magnitude faster far biologically plausible networks strong transient selective amplification external noise generates spatially correlated activity fluctuations prescribed posterior intriguingly although detailed balance excitation inhibition dynamically maintained detailed balance markov chain steps resulting sampler violated consistent recent findings statistical irreversibility overcome speed limitation random walks domains
elementary estimators graphical models propose class closed form estimators sparsity structured graphical models expressed exponential family distributions high dimensional settings approach builds observing precise manner classical graphical model mle breaks high dimensional settings estimator uses carefully constructed well defined closed form backward map performs thresholding operations ensure desired sparsity structure provide rigorous statistical analysis shows surprisingly simple class estimators recovers asymptotic convergence rates ell_1 regularized mles much difficult compute corroborate statistical performance well significant computational advantages via simulations discrete gaussian graphical models
computational efficiency training neural networks well known neural networks computationally hard train hand practice modern day neural networks trained efficiently using sgd variety tricks include different activation functions relu specification train networks larger needed regularization paper revisit computational complexity training neural networks modern perspective provide positive negative results yield new provably efficient practical algorithms training neural networks
weighted importance sampling policy learning linear function approximation importance sampling essential component policy model free reinforcement learning algorithms however effective variant emph weighted importance sampling carry easily function approximation utilized existing policy learning algorithms paper take steps toward bridging gap first show weighted importance sampling viewed special case weighting error individual training samples weighting theoretical empirical benefits similar weighted importance sampling second show benefits extend new weighted importance sampling version policy lstd lambda show empirically new wis lstd lambda algorithm result much rapid reliable convergence conventional policy lstd lambda 2010 bertsekas 2009
iterative hard thresholding methods high dimensional estimation use estimators generalized linear regression models high dimensional settings requires risk minimization hard l_0 constraints known methods class projected gradient descent also known iterative hard thresholding iht methods known offer fastest scalable solutions however current state art able analyze methods extremely restrictive settings hold high dimensional statistical models work bridge gap providing first analysis iht style methods high dimensional statistical setting bounds tight match known minimax lower bounds results rely general analysis framework enables analyze several popular hard thresholding style algorithms htp cosamp high dimensional regression setting finally extend analysis problem low rank matrix recovery
dependent nonparametric trees dynamic hierarchical clustering hierarchical clustering methods offer intuitive powerful way model wide variety data sets however assumption fixed hierarchy often overly restrictive working data generated period time expect structure hierarchy parameters clusters evolve time paper present distribution collections time dependent infinite dimensional trees used model evolving hierarchies present efficient scalable algorithm performing approximate inference model demonstrate efficacy model inference algorithm synthetic data real world document corpora
articulated pose estimation graphical model image dependent pairwise relations present method estimating articulated human pose single static image based graphical model novel pairwise relations make adaptive use local image measurements precisely specify graphical model human pose exploits fact local image measurements used detect parts joints also predict spatial relationships image dependent pairwise relations spatial relationships represented mixture model use deep convolutional neural networks dcnns learn conditional probabilities presence parts spatial relationships within image patches hence model combines representational flexibility graphical models efficiency statistical power dcnns method significantly outperforms state art methods lsp flic datasets also performs well buffy dataset without training
learning fredholm kernels paper propose framework supervised semi supervised learning based reformulating learning problem regularized fredholm integral equation approach fits naturally kernel framework interpreted constructing new data dependent kernels call fredholm kernels proceed discuss noise assumption semi supervised learning provide evidence evidence theoretical experimental fredholm kernels effectively utilize unlabeled data noise assumption demonstrate methods based fredholm learning show competitive performance standard semi supervised learning setting
localized data fusion kernel means clustering application cancer biology many modern applications example bioinformatics computer vision samples multiple feature representations coming different data sources multiview learning algorithms try exploit available information obtain better learner scenarios paper propose novel multiple kernel learning algorithm extends kernel means clustering multiview setting combines kernels calculated views localized way better capture sample specific characteristics data demonstrate better performance localized data fusion approach human colon rectal cancer data set clustering patients method finds relevant prognostic patient groups global data fusion methods evaluate results respect commonly used clinical biomarkers
dimensionality reduction subspace structure preservation modeling data sampled union independent subspaces widely applied number real world applications however dimensionality reduction approaches theoretically preserve independence assumption well studied key contribution show projection vectors sufficient independence preservation class data sampled union independent subspaces non trivial observation use designing dimensionality reduction technique paper propose novel dimensionality reduction algorithm theoretically preserves structure given dataset support theoretical analysis empirical results synthetic real world data achieving textit state art results compared popular dimensionality reduction techniques
blossom tree graphical models combine ideas behind trees gaussian graphical models form new nonparametric family graphical models approach attach nonparanormal blossoms arbitrary graphs collection nonparametric trees tree edges chosen connect variables violate joint gaussianity non tree edges partitioned disjoint groups assigned tree nodes using nonparametric partial correlation statistic nonparanormal blossom grown group using established methods based graphical lasso result factorization respect union tree branches blossoms defining high dimensional joint density efficiently estimated evaluated test points theoretical properties experiments simulated real data demonstrate effectiveness blossom trees
restricted boltzmann machines modeling human choice extend multinomial logit model represent empirical phenomena frequently observed choices made humans phenomena include similarity effect attraction effect compromise effect formally quantify strength phenomena represented choice model illuminates flexibility choice model show choice model represented restricted boltzmann machine parameters learned effectively data numerical experiments real data human choices suggest train choice model way represents typical phenomena choice
representation theory ranking functions paper presents representation theory permutation valued functions general form also called listwise ranking functions pointwise ranking functions assign score object independently without taking account objects consideration whereas listwise loss functions evaluate set scores assigned objects whole many supervised learning rank tasks might interest use listwise ranking functions instead particular bayes optimal ranking functions might listwise especially loss function listwise key caveat using listwise ranking functions lack appropriate representation theory functions show natural symmetricity assumption call exchangeability allows explicitly characterize set exchangeable listwise ranking functions analysis draws theories tensor analysis functional analysis finetti theorems also present experiments using novel reranking method motivated representation theory
parallel feature selection inspired group testing paper presents parallel feature selection method classification scales high dimensions large data sizes original method inspired group testing theory feature selection procedure consists collection randomized tests performed parallel test corresponds subset features scoring function applied measure relevance features classification task develop general theory providing sufficient conditions true features guaranteed correctly identified superior performance method demonstrated challenging relation extraction task large data set redundant features sample size order millions present comprehensive comparisons state art feature selection methods range data sets method exhibits competitive performance terms running time accuracy moreover also yields substantial speedup used pre processing step existing methods
dynamic rank factor model text streams propose semi parametric dynamic rank factor model topic modeling capable discovering topic prevalence time learning contemporary multi scale dependence structures providing topic word correlations byproduct high dimensional time evolving ordinal rank observations word counts arbitrary monotone transformation well accommodated underlying dynamic sparse factor model framework naturally admits heavy tailed innovations capable inferring abrupt temporal jumps importance topics posterior inference performed straightforward gibbs sampling based forward filtering backward sampling algorithm moreover efficient data subsampling scheme leveraged speed inference massive datasets modeling framework illustrated real datasets state union address jstor collection science
kernel mean estimation via spectral filtering problem estimating kernel mean reproducing kernel hilbert space rkhs central kernel methods used classical approaches centering kernel pca matrix also forms core inference step modern kernel methods kernel based non parametric tests rely embedding probability distributions rkhss previous work shown shrinkage help constructing better estimators kernel mean empirical estimator present paper studies consistency admissibility estimators proposes wider class shrinkage estimators improve upon empirical estimator considering appropriate basis functions using kernel pca basis show estimators constructed using spectral filtering algorithms shown consistent technical assumptions theoretical analysis also reveals fundamental connection kernel based supervised learning framework proposed estimators simple implement perform well practice
consistency spectral partitioning uniform hypergraphs planted partition model spectral graph partitioning methods received significant attention practitioners theorists computer science notable studies carried regarding behavior methods infinitely large sample size von luxburg 2008 rohe 2011 provide sufficient confidence practitioners effectiveness methods hand recent developments computer vision led plethora applications model deals multi way affinity relations posed uniform hyper graphs paper view models random uniform hypergraphs establish consistency spectral algorithm general setting develop planted partition model stochastic blockmodel problems using higher order tensors present spectral technique suited purpose study large sample behavior analysis reveals algorithm consistent uniform hypergraphs larger values also rate convergence improves increasing result provides first theoretical evidence establishes importance way affinities
safe screening rule sparse logistic regression regularized logistic regression sparse logistic regression widely used method simultaneous classification feature selection although many recent efforts devoted efficient implementation application high dimensional data still poses significant challenges paper present fast effective sparse logistic regression screening rule slores identify components solution vector lead substantial reduction number features entered optimization appealing feature slores data set needs scanned run screening computational cost negligible compared solving sparse logistic regression problem moreover slores independent solvers sparse logistic regression thus slores integrated existing solver improve efficiency evaluated slores using high dimensional data sets different applications extensive experimental results demonstrate slores outperforms existing state art screening rules efficiency solving sparse logistic regression improved magnitude general
distributed parameter estimation probabilistic graphical models paper presents foundational theoretical results distributed parameter estimation undirected probabilistic graphical models introduces general condition composite likelihood decompositions models guarantees global consistency distributed estimators provided local estimators consistent
multiscale fields patterns describe framework defining high order image models used variety applications approach involves modeling local patterns multiscale representation image local properties coarsened image reflect non local properties original image case binary images local properties defined binary patterns observed small neighborhoods around pixel multiscale representation capture frequency patterns observed different scales resolution framework leads expressive priors depend relatively small number parameters inference learning use mcmc method block sampling large blocks evaluate approach example applications involves contour detection involves binary segmentation
theory nonparametric pairwise similarity clustering connecting clustering classification pairwise clustering methods partition data space clusters pairwise similarity data points success pairwise clustering largely depends pairwise similarity function defined data points kernel similarity broadly used paper present novel pairwise clustering framework bridging gap clustering multi class classification pairwise clustering framework learns unsupervised nonparametric classifier data partition search optimal partition data minimizing generalization error learned classifiers associated data partitions consider nonparametric classifiers framework nearest neighbor classifier plug classifier modeling underlying data distribution nonparametric kernel density estimation generalization error bounds unsupervised nonparametric classifiers sum nonparametric pairwise similarity terms data points purpose clustering uniform distribution nonparametric similarity terms induced unsupervised classifiers exhibit well known form kernel similarity also prove generalization error bound unsupervised plug classifier asymptotically equal weighted volume cluster boundary low density separation widely used criteria semi supervised learning clustering based derived nonparametric pairwise similarity using plug classifier propose new nonparametric exemplar based clustering method enhanced discriminative capability whose superiority evidenced experimental results
top rank optimization linear time bipartite ranking aims learn real valued ranking function orders positive instances negative instances recent efforts bipartite ranking focused optimizing ranking accuracy top ranked list existing approaches either optimize task specific metrics extend rank loss emphasizing error associated top ranked instances leading high computational cost super linear number training instances propose highly efficient approach titled toppush optimizing accuracy top computational complexity linear number training instances present novel analysis bounds generalization error top ranked instances proposed approach empirical study shows proposed approach highly competitive state art approaches 100 times faster
zeta hull pursuits learning nonconvex data hulls selecting small informative subset given dataset also called column sampling drawn much attention machine learning incorporating structured data information column sampling research efforts devoted cases data points fitted clusters simplices general convex hulls paper aims study nonconvex hull learning rarely investigated literature order learn data adaptive nonconvex hulls novel approach proposed based graph theoretic measure leverages graph cycles characterize structural complexities input data points employing measure present greedy algorithmic framework dubbed zeta hulls perform structured column sampling process pursuing zeta hull involves computation matrix inverse accelerate matrix inversion computation reduce space complexity well exploit low rank approximation graph adjacency matrix using efficient anchor graph technique extensive experimental results show data representation learned zeta hulls achieve state art accuracy text image classification tasks
time data tradeoffs aggressive smoothing paper proposes tradeoff sample complexity computation time applies statistical estimators based convex optimization amount data increases smooth optimization problems aggressively achieve accurate estimates quickly work provides theoretical experimental evidence tradeoff class regularized linear inverse problems
shot recognition unreliable attributes principle shot learning makes possible train object recognition model simply specifying category attributes example classifiers generic attributes like striped legged construct classifier zebra category enumerating properties possesses even without providing zebra training images practice however standard shot paradigm suffers attribute predictions novel images hard get right propose novel random forest approach train shot models explicitly accounts unreliability attribute predictions leveraging statistics attribute error tendencies method obtains robust discriminative models unseen classes devise extensions handle shot scenario unreliable attribute descriptions datasets demonstrate benefit visual category learning training examples critical domain rare categories categories defined fly
consistency weighted majority votes revisit statistical learning perspective classical decision theoretic problem weighted expert voting particular examine consistency asymptotic finitary optimal nitzan paroush weighted majority related rules case known expert competence levels give sharp error estimates optimal rule competence levels unknown must empirically estimated provide frequentist bayesian analyses situation proof techniques non standard independent interest bounds derive nearly optimal several challenging open problems posed experimental results provided illustrate theory
inferring synaptic conductances spike trains biophysically inspired point process model popular approach neural characterization describes neural responses terms cascade linear nonlinear stages linear filter describe stimulus integration followed nonlinear function convert filter output spike rate however real neurons respond stimuli manner depends nonlinear integration excitatory inhibitory synaptic inputs introduce biophysically inspired point process model explicitly incorporates stimulus induced changes synaptic conductance dynamical model neuronal membrane potential work makes important contributions first theoretical level offers novel interpretation popular generalized linear model glm neural spike trains show classic glm special case conductance based model stimulus linearly modulates excitatory inhibitory conductances equal opposite push pull fashion model therefore viewed direct extension glm relax constraints resulting model exhibit shunting well hyperpolarizing inhibition time varying changes gain membrane time constant second practical level show model provides tractable model spike responses early sensory neurons accurate interpretable glm importantly show accurately infer intracellular synaptic conductances extracellularly recorded spike trains validate estimates using direct intracellular measurements excitatory inhibitory conductances parasol retinal ganglion cells show model fit extracellular spike trains predict excitatory inhibitory conductances elicited novel stimuli nearly accuracy model trained directly intracellular conductances
information based learning agents unbounded state spaces idea animals might use information driven planning explore unknown environment build internal model proposed quite time recent work demonstrated agents using principle efficiently learn models probabilistic environments discrete bounded state spaces however animals robots commonly confronted unbounded environments address challenging situation study information based learning strategies agents unbounded state spaces using non parametric bayesian models specifically demonstrate chinese restaurant process crp model able solve problem empirical bayes version able efficiently explore bounded unbounded worlds relying little prior information
local linear convergence forward backward partial smoothness paper consider forward backward proximal splitting algorithm minimize sum proper closed convex functions lipschitz continuous gradient partly smooth relatively active manifold mathcal propose generic framework show forward backward correctly identifies active manifold mathcal finite number iterations enters local linear convergence regime characterize precisely gives grounded unified explanation typical behaviour observed numerically many problems encompassed framework including lasso group lasso fused lasso nuclear norm regularization name results numerous applications including signal image processing processing sparse recovery machine learning
beta negative binomial process exchangeable random partitions mixed membership modeling beta negative binomial process bnbp integer valued stochastic process employed partition count vector latent random count matrix marginal probability distribution bnbp governs exchangeable random partitions grouped data yet developed current inference bnbp truncate number atoms beta process paper introduces exchangeable partition probability function explicitly describe bnbp clusters data points group random number exchangeable partitions shared across groups fully collapsed gibbs sampler developed bnbp leading novel nonparametric bayesian topic model distinct existing ones simple implementation fast convergence good mixing state art predictive performance
mondrian forests efficient online random forests ensembles randomized decision trees usually referred random forests widely used classification regression tasks machine learning statistics random forests achieve competitive predictive performance computationally efficient train test making excellent candidates real world prediction tasks popular random forest variants breiman random forest extremely randomized trees operate batches training data online methods greater demand existing online random forests however require training data batch counterpart achieve comparable predictive performance work use mondrian processes roy teh 2009 construct ensembles random decision trees call mondrian forests mondrian forests grown incremental online fashion remarkably distribution online mondrian forests batch mondrian forests mondrian forests achieve competitive predictive performance comparable existing online random forests periodically trained batch random forests order magnitude faster thus representing better computation accuracy tradeoff
integrated clustering outlier detection model joint clustering outlier detection problem using extension facility location formulation advantages combining clustering outlier selection include resulting clusters tend compact semantically coherent clusters robust data perturbations iii outliers contextualised clusters interpretable provide practical subgradient based algorithm problem also study theoretical properties algorithm terms approximation convergence extensive evaluation synthetic real data sets attest quality scalability proposed method
sensory integration density estimation integration partially redundant information multiple sensors standard computational problem agents interacting world man primates integration shown psychophysically nearly optimal sense error minimization influential generalization notion optimality populations multisensory neurons retain information unisensory afferents underlying common stimulus recently shown empirically neural network trained perform latent variable density estimation activities unisensory neurons observed data satisfies information preservation criterion even though model architecture designed match true generative process data prove analytical connection seemingly different tasks density estimation sensory integration former implies latter model used appear true models
neurons monte carlo samplers bayesian inference learning spiking networks propose layer spiking network capable performing approximate inference learning hidden markov model lower layer sensory neurons detect noisy measurements hidden world states higher layer neurons recurrent connections infer posterior distribution world states spike trains generated sensory neurons show neuronal network synaptic plasticity implement form bayesian inference similar monte carlo methods particle filtering spike population inference neurons represents sample particular hidden world state spiking activity across neural population approximates posterior distribution hidden state model provides functional explanation poisson like noise commonly observed cortical responses uncertainties spike times provide necessary variability sampling inference unlike previous models hidden world state observed sensory neurons temporal dynamics hidden state unknown demonstrate network sequentially learn hidden markov model using spike timing dependent hebbian learning rule achieve power law convergence rates
factoring variations natural images deep gaussian mixture models generative models seen swiss army knives machine learning many problems written probabilistically terms distribution data including prediction reconstruction imputation simulation promising directions unsupervised learning lie deep learning methods given success supervised learning however current problems deep unsupervised learning methods often harder scale result easier scalable shallow methods gaussian mixture model student mixture model remain surprisingly competitive paper propose new scalable deep generative model images called deep gaussian mixture model straightforward powerful generalization gmms multiple layers parametrization deep gmm allows efficiently capture products variations natural images propose new based algorithm scales well large datasets show expectation maximization steps easily distributed multiple machines density estimation experiments show deeper gmm architectures generalize better shallow ones results ballpark state art
general stochastic networks classification extend generative stochastic networks supervised learning representations particular introduce hybrid training objective considering generative discriminative cost function governed trade parameter lambda use new variant network training involving noise injection walkback training jointly optimize multiple network layers neither additional regularization constraints norms dropout variants pooling convolutional layers added nevertheless able obtain state art performance mnist dataset without using permutation invariant digits outperform baseline models sub variants mnist rectangles dataset significantly
exact post model selection inference marginal screening develop framework post model selection inference via marginal screening linear regression core framework result characterizes exact distribution linear functions response conditional model selected condition selection framework allows construct valid confidence intervals hypothesis tests regression coefficients account selection procedure contrast recent work high dimensional statistics results exact non asymptotic require eigenvalue like assumptions design matrix furthermore computational cost marginal regression constructing confidence intervals hypothesis testing negligible compared cost linear regression thus making methods particularly suitable extremely large datasets although focus marginal screening illustrate applicability condition selection framework framework much broadly applicable show apply proposed framework several selection procedures including orthogonal matching pursuit marginal screening lasso
low dimensional models neural population activity sensory cortical circuits neural responses visual cortex influenced visual stimuli ongoing spiking activity local circuits important challenge computational neuroscience develop models account features large multi neuron recordings reveal stimulus representations interact depend cortical dynamics introduce statistical model neural population activity integrates nonlinear receptive field model latent dynamical model ongoing cortical activity model captures temporal dynamics effective network connectivity large population recordings correlations due shared stimulus drive well common noise moreover nonlinear stimulus inputs mixed ongoing dynamics model account relatively large number idiosyncratic receptive field shapes small number nonlinear inputs low dimensional latent dynamical model introduce fast estimation method using online expectation maximization laplace approximations inference scales linearly population size recording duration apply model multi channel recordings primary visual cortex show accounts large number individual neural receptive fields using small number nonlinear inputs low dimensional dynamical model
capturing semantically meaningful word dependencies admixture poisson mrfs develop fast algorithm admixture poisson mrfs apm topic model propose novel metric directly evaluate model apm topic model recently introduced inouye 2014 first topic model allows word dependencies within topic unlike previous topic models like lda assume independence words within topic research semantic coherence topic models mimno 2011 newman 2010 measures model fitness mimno blei 2011 provide strong support explicitly modeling word dependencies apm could semantically meaningful essential appropriately modeling real text data though apm shows significant promise providing better topic model apm high computational complexity parameters must estimated number words inouye could provide results datasets 200 light develop parallel alternating newton like algorithm training apm model handle important step towards scaling large datasets addition inouye provided tentative inconclusive results utility apm thus motivated simple intuitions previous evaluations topic models propose novel evaluation metric based human evocation scores word pairs much word brings mind another word boyd graber 2006 provide compelling quantitative qualitative results bnc corpus demonstrate superiority apm previous topic models identifying semantically meaningful word dependencies matlab code available http bigdata ices utexas edu software apm
near optimal sample estimators spherical gaussian mixtures many important distributions high dimensional often modeled gaussian mixtures derive first sample efficient polynomial time estimator high dimensional spherical gaussian mixtures based intuitive spectral reasoning approximates mixtures spherical gaussians dimensions within ell_1 distance epsilon using mathcal log epsilon samples mathcal epsilon log computation time conversely show estimator requires omega bigl epsilon bigr samples hence algorithm sample complexity nearly optimal dimension implied time complexity factor mathcal epsilon exponential much smaller previously known also construct simple estimator dimensional gaussian mixtures uses tilde mathcal epsilon samples tilde mathcal epsilon computation time
parallel sampling hdps using sub cluster splits develop sampling technique hierarchical dirichlet process models parallel algorithm builds upon chang fisher 2013 proposing large split merge moves based learned sub clusters additional global split merge moves drastically improve convergence experimental results furthermore discover cross validation techniques adequately determine convergence previous sampling methods converge slower previously expected
attentional neural network feature selection using cognitive feedback attentional neural network new framework integrates top cognitive bias bottom feature extraction coherent architecture top influence especially effective dealing high noise difficult segmentation problems system modular extensible also easy train cheap run yet accommodate complex behaviors obtain classification accuracy better competitive state art results mnist variation dataset successfully disentangle overlaid digits high success rates view general purpose framework essential foundation larger system emulating cognitive abilities whole brain
generalized dantzig selector application support norm propose generalized dantzig selector gds linear models norm encoding parameter structure leveraged estimation investigate computational statistical aspects gds based conjugate proximal operator flexible inexact admm framework designed solving gds thereafter non asymptotic high probability bounds established estimation error rely gaussian widths unit norm ball error set consider non trivial example gds using support norm derive efficient method compute proximal operator support norm since existing methods inapplicable setting statistical analysis provide upper bounds gaussian widths needed gds analysis yielding first statistical recovery guarantee estimation support norm experimental results confirm theoretical analysis
tighten relax minimax optimal sparse pca polynomial time provide statistical computational analysis sparse principal component analysis pca high dimensions sparse pca problem highly nonconvex nature consequently though global solution attains optimal statistical rate convergence solution computationally intractable obtain meanwhile although convex relaxations tractable compute yield estimators suboptimal statistical rates convergence hand existing nonconvex optimization procedures greedy methods lack statistical guarantees paper propose stage sparse pca procedure attains optimal principal subspace estimator polynomial time main stage employs novel algorithm named sparse orthogonal iteration pursuit iteratively solves underlying nonconvex problem however analysis shows algorithm desired computational statistical guarantees within restricted region namely basin attraction obtain desired initial estimator falls region solve convex formulation sparse pca early stopping integrated analytic framework simultaneously characterize computational statistical performance stage procedure computationally procedure converges rate sqrt within initialization stage geometric rate within main stage statistically final principal subspace estimator achieves minimax optimal statistical rate convergence respect sparsity level dimension sample size procedure motivates general paradigm tackling nonconvex statistical learning problems provable statistical guarantees
sparse gaussian chain graph models paper address problem learning structure gaussian chain graph models high dimensional space chain graph models generalizations undirected directed graphical models contain mixed set directed undirected edges problem sparse structure learning studied extensively gaussian graphical models recently conditional gaussian graphical models cggms little previous work structure recovery gaussian chain graph models consider linear regression models parameterization linear regression models using cggms building blocks chain graph models argue goal recover model structures many advantages using cggms chain component models linear regression models including convexity optimization problem computational efficiency recovery structured sparsity ability leverage model structure semi supervised learning demonstrate approach simulated genomic datasets
using convolutional neural networks recognize rhythm stimuli electroencephalography recordings electroencephalography eeg recordings rhythm perception might contain enough information distinguish different rhythm types genres even identify rhythms apply convolutional neural networks cnns analyze classify eeg data recorded within rhythm perception study kigali rwanda comprises east african western rhythmic stimuli presented loop seconds participants investigate impact data representation pre processing steps classification tasks compare different network structures using cnns able recognize individual rhythms eeg mean classification accuracy chance level subjects looking less seconds single channel aggregating predictions multiple channels mean accuracy achieved individual subjects
graph clustering missing data convex algorithms analysis consider problem finding clusters unweighted graph graph partially observed analyze programs works dense graphs works sparse dense graphs requires priori knowledge total cluster size based convex optimization approach low rank matrix recovery using nuclear norm minimization commonly used stochastic block model obtain emph explicit bounds parameters problem size sparsity clusters amount observed data regularization parameter characterize success failure programs corroborate theoretical findings extensive simulations also run algorithm real data set obtained crowdsourcing image classification task amazon mechanical turk observe significant performance improvement traditional methods means
convex optimization procedure clustering theoretical revisit paper present theoretical analysis son convex optimization procedure clustering using sum norms son regularization recently proposed cite icml2011hocking_419 son lindsten650707 pelckmans2005convex particular show samples drawn cubes cluster son provably identify cluster membership provided distance cubes larger threshold linearly depends size cube ratio numbers samples cluster best knowledge paper first provide rigorous analysis understand son works believe provide important insights develop novel convex optimization based algorithms clustering
recursive context propagation network semantic scene labeling propose deep feed forward neural network architecture pixel wise semantic scene labeling uses novel recursive neural network architecture context propagation referred rcpn first maps local visual features semantic space followed bottom aggregation local information global representation entire image top propagation aggregated information takes place enhances contextual information local feature therefore information every location image propagated every location experimental results stanford background sift flow datasets show proposed method outperforms previous approaches also orders magnitude faster previous methods takes seconds gpu pixel wise labeling 256x256 image starting raw rgb pixel values given super pixel mask takes additional seconds using shelf implementation
infinite mixture infinite gaussian mixtures dirichlet process mixture gaussians dpmg used literature clustering density estimation problems however many real world data exhibit cluster distributions cannot captured single gaussian modeling data sets dpmg creates several extraneous clusters even clusters relatively well defined herein present infinite mixture infinite gaussian mixtures i2gmm flexible modeling data sets skewed multi modal cluster distributions instead using single gaussian cluster standard dpmg model generative model i2gmm uses single dpmg cluster individual dpmgs linked together centering base distributions atoms higher level prior inference performed collapsed gibbs sampler also enables partial parallelization experimental results several artificial real world data sets suggest proposed i2gmm model predict clusters accurately existing variational bayes gibbs sampler versions dpmg
deep networks internal selective attention feedback connections traditional convolutional neural networks cnn stationary feedforward neither change parameters evaluation use feedback higher lower layers real brains however deep attention selective network dasnet architecture dasnet feedback structure dynamically alter convolutional filter sensitivities classification harnesses power sequential processing improve classification performance allowing network iteratively focus internal attention convolutional filters feedback trained direct policy search huge million dimensional parameter space scalable natural evolution strategies snes cifar cifar 100 datasets dasnet outperforms previous state art model unaugmented datasets
model based reinforcement learning eluder dimension consider problem learning optimize unknown markov decision process mdp show mdp parameterized within known function class obtain regret bounds scale dimensionality rather cardinality system characterize dependence explicitly tilde sqrt d_k d_e time elapsed d_k kolmogorov dimension d_e emph eluder dimension represent first unified regret bounds model based reinforcement learning provide state art guarantees several important settings moreover present simple computationally efficient algorithm emph posterior sampling reinforcement learning psrl satisfies bounds
tight convex relaxations sparse matrix factorization based new atomic norm propose new convex formulation sparse matrix factorization problems number nonzero elements factors assumed fixed known formulation counts sparse pca multiple factors subspace clustering low rank sparse bilinear regression potential applications compute slow rates upper bound statistical dimension suggested norm rank matrices showing statistical dimension order magnitude smaller usual l_1 norm trace norm combinations even though convex formulation theory hard lead provably polynomial time algorithmic schemes propose active set algorithm leveraging structure convex problem solve show promising numerical results
compressive sensing signals gmm sparse precision matrices paper concerned compressive sensing signals drawn gaussian mixture model gmm sparse precision matrices previous work shown signal drawn given gmm perfectly reconstructed noise free measurements dominant rank covariance matrix less sparse gaussian graphical model efficiently estimated fully observed training signals using graphical lasso paper addresses problem challenging assuming gmm unknown signal partially observed incomplete linear measurements challenging assumptions develop hierarchical bayesian method simultaneously estimate gmm recover signals using solely incomplete measurements bayesian shrinkage prior promotes sparsity gaussian precision matrices addition provide theoretical performance bounds relate reconstruction error number signals measurements available sparsity level precision matrices incompleteness measurements proposed method demonstrated extensively compressive sensing imagery video results simulated hardware acquired real measurements show significant performance improvement state art methods
optimal prior dependent neural population codes shared input noise brain uses population codes form distributed noise tolerant representations sensory motor variables recent work examined theoretical optimality codes order gain insight principles governing population codes found brain however majority population coding literature considers either conditionally independent neurons neurons noise governed stimulus independent covariance matrix analyze population coding simple alternative model latent input noise corrupts stimulus encoded population provides convenient tractable description irreducible uncertainty cannot overcome adding neurons induces stimulus dependent correlations mimic certain aspects correlations observed real populations examine prior dependent bayesian optimal coding populations using exact analyses cases posterior approximately gaussian analyses extend previous results independent poisson population codes yield analytic expression squared loss tight upper bound mutual information show homogeneous populations tile input domain optimal tuning curve width depends prior loss function resource constraint amount input noise framework provides practical testbed examining issues optimality noise correlation coding fidelity realistic neural populations
bandit convex optimization towards tight bounds bandit convex optimization bco fundamental framework decision making uncertainty generalizes many problems realm online statistical learning special case linear cost functions well understood gap attainable regret bco nonlinear losses remains important open question paper take step towards understanding best attainable regret bounds bco give efficient near optimal regret algorithm bco strongly convex smooth loss functions contrast previous works bco use time invariant exploration schemes method employs exploration scheme shrinks time
layer feature reduction sparse group lasso via decomposition convex sets sparse group lasso sgl shown powerful regression technique simultaneously discovering group within group sparse patterns using combination norms however large scale applications complexity regularizers entails great computational challenges paper propose novel layer feature reduction method tlfre sgl via decomposition dual feasible set layer reduction able quickly identify inactive groups inactive features respectively guaranteed absent sparse representation removed optimization existing feature reduction methods applicable sparse models sparsity inducing regularizer best knowledge tlfre first capable dealing multiple sparsity inducing regularizers moreover tlfre low computational cost integrated existing solvers experiments synthetic real data sets show tlfre improves efficiency sgl orders magnitude
optimistic planning markov decision processes using generative model consider problem online planning markov decision process discounted rewards given initial state consider pac sample complexity problem computing probability delta epsilon optimal action using smallest possible number calls generative model provides reward next state samples design algorithm called stop stochastic optimistic planning based optimism face uncertainty principle stop used general setting requires generative model enjoys complexity bound depends local structure mdp
multilabel structured output learning random spanning trees max margin markov networks show usual score function conditional markov networks written expectation scores spanning trees also show small random sample output trees attain significant fraction margin obtained complete graph provide conditions perform tractable inference experimental results confirm practical learning scalable realistic datasets using approach
generative adversarial nets propose new framework estimating generative models via adversarial nets simultaneously train models generative model captures data distribution discriminative model estimates probability sample came training data rather training procedure maximize probability making mistake framework corresponds minimax player game space arbitrary functions unique solution exists recovering training data distribution equal everywhere case defined multilayer perceptrons entire system trained backpropagation need markov chains unrolled approximate inference networks either training generation samples experiments demonstrate potential framework qualitative quantitatively evaluation generated samples
searching higgs boson decay modes deep learning particle colliders enable probe fundamental nature matter observing exotic particles produced high energy collisions experimental measurements collisions necessarily incomplete imprecise machine learning algorithms play major role analysis experimental data high energy physics community typically relies standardized machine learning software packages analysis devotes substantial effort towards improving statistical power hand crafting high level features derived raw collider measurements paper train artificial neural networks detect decay higgs boson tau leptons dataset million simulated collision events demonstrate deep neural network architectures particularly well suited task ability automatically discover high level features data increase discovery significance
shaping social activity incentivizing users events online social network categorized roughly endogenous events users respond actions neighbors within network exogenous events users take actions due drives external network much external drive provided user network activity steered towards target state paper model social events using multivariate hawkes processes capture endogenous exogenous event intensities derive time dependent linear relation intensity exogenous events overall network activity exploiting connection develop convex optimization framework determining required level external drive order network reach desired activity level experimented event data gathered twitter show method steer activity network accurately alternatives
online optimization max norm regularization max norm regularizer extensively studied last decade promotes effective low rank estimation underlying data however max norm regularized problems typically formulated solved batch manner prevents processing big data due possible memory bottleneck paper propose online algorithm solving max norm regularized problems scalable large problems particularly consider matrix decomposition problem example although analysis also applied problems matrix completion key technique algorithm reformulate max norm matrix factorization form consisting basis component coefficients way solve optimal basis coefficients alternatively prove basis produced algorithm converges stationary point asymptotically experiments demonstrate encouraging results effectiveness robustness algorithm see full paper arxiv 1406 3190
accelerated proximal coordinate gradient method develop accelerated randomized proximal coordinate gradient apcg method solving broad class composite convex optimization problems particular method achieves faster linear convergence rates minimizing strongly convex functions existing randomized proximal coordinate gradient methods show apply apcg method solve dual regularized empirical risk minimization erm problem devise efficient implementations avoid full dimensional vector operations ill conditioned erm problems method obtains improved convergence rates state art stochastic dual coordinate ascent sdca method
scalable non linear learning adaptive polynomial expansions effectively learn nonlinear representation time comparable linear learning describe new algorithm explicitly adaptively expands higher order interaction features base linear representations algorithm designed extreme computational efficiency extensive experimental study shows computation prediction tradeoff ability compares favorably strong baselines
distributed power law graph computing theoretical empirical analysis emergence big graphs variety real applications like social networks machine learning based distributed graph computing dgc frameworks attracted much attention big data machine learning community dgc frameworks graph partitioning strategy plays key role affect performance including workload balance communication cost typically degree distributions natural graphs real applications follow skewed power laws makes challenging task recently many methods proposed solve problem however existing methods cannot achieve satisfactory performance applications power law graphs paper propose novel vertex cut method called emph degree based hashing dbh dbh makes effective use skewed degree distributions theoretically prove dbh achieve lower communication cost existing methods simultaneously guarantee good workload balance furthermore empirical results several large power law graphs also show dbh outperform state art
stream convolutional networks action recognition videos investigate architectures discriminatively trained deep convolutional networks convnets action recognition video challenge capture complementary information appearance still frames motion frames also aim generalise best performing hand crafted features within data driven learning framework contribution fold first propose stream convnet architecture incorporates spatial temporal networks second demonstrate convnet trained multi frame dense optical flow able achieve good performance spite limited training data finally show multi task learning applied different action classification datasets used increase amount training data improve performance architecture trained evaluated standard video actions benchmarks ucf 101 hmdb competitive state art also exceeds large margin previous attempts use deep nets video classification
scalable methods nonnegative matrix factorizations near separable tall skinny matrices numerous algorithms used nonnegative matrix factorization assumption matrix nearly separable paper show make algorithms scalable data matrices many rows columns called tall skinny matrices key component improved methods orthogonal matrix transformation preserves separability nmf problem final methods need read data matrix suitable streaming multi core mapreduce architectures demonstrate efficacy algorithms terabyte sized matrices scientific computing bioinformatics
optimal rates density mode estimation present related contributions independent interest high probability finite sample rates density estimation practical mode estimators based attain minimax optimal rates surprisingly general distributional conditions
asymmetric lsh alsh sublinear time maximum inner product search mips present first provably sublinear time hashing algorithm approximate emph maximum inner product search mips searching normalized inner product underlying similarity measure known difficult problem finding hashing schemes mips considered hard existing locality sensitive hashing lsh framework insufficient solving mips paper extend lsh framework allow asymmetric hashing schemes proposal based key observation problem finding maximum inner products independent asymmetric transformations converted problem approximate near neighbor search classical settings key observation makes efficient sublinear hashing scheme mips possible extended asymmetric lsh alsh framework paper provides example explicit construction provably fast hashing scheme mips proposed algorithm simple easy implement proposed hashing scheme leads significant computational savings popular conventional lsh schemes sign random projection srp hashing based stable distributions l_2 norm l2lsh collaborative filtering task item recommendations netflix movielens 10m datasets
sparse space time deconvolution calcium image analysis describe unified formulation algorithm find extremely sparse representation calcium image sequences terms cell locations cell shapes spike timings impulse responses solution single optimization problem yields cell segmentations activity estimates par state art without need heuristic pre postprocessing experiments real synthetic data demonstrate viability proposed method
local decorrelation improved pedestrian detection even advent sophisticated data hungry methods boosted decision trees remain extraordinarily successful fast rigid object detection achieving top accuracy numerous datasets effective boosted detectors use decision trees orthogonal single feature splits topology resulting decision boundary well matched natural topology data given highly correlated data decision trees oblique multiple feature splits effective use oblique splits however comes considerable computational expense inspired recent work discriminative decorrelation hog features instead propose efficient feature transform removes correlations local neighborhoods result overcomplete locally decorrelated representation ideally suited use orthogonal decision trees fact orthogonal trees locally decorrelated features outperform oblique trees trained original features fraction computational cost overall improvement accuracy dramatic caltech pedestrian dataset reduce false positives nearly tenfold previous state art
clustering labels time varying graphs present general framework graph clustering label observed pair nodes allows rich encoding various types pairwise interactions nodes propose new tractable approach problem based maximum likelihood estimator convex optimization analyze algorithm general generative model provide necessary sufficient conditions successful recovery underlying clusters theoretical results cover subsume wide range existing graph clustering results including planted partition weighted clustering partially observed graphs furthermore result applicable novel settings including time varying graphs new insights gained solving problems theoretical findings supported empirical results synthetic real data
deep learning face representation joint identification verification key challenge face recognition develop effective feature representations reducing intra personal variations enlarging inter personal differences paper show well solved deep learning using face identification verification signals supervision deep identification verification features deepid2 learned carefully designed deep convolutional networks face identification task increases inter personal variations drawing deepid2 features extracted different identities apart face verification task reduces intra personal variations pulling deepid2 features extracted identity together essential face recognition learned deepid2 features well generalized new identities unseen training data challenging lfw dataset face verification accuracy achieved compared best previous deep learning result lfw error rate significantly reduced
inference learning speeding graphical model optimization via coarse fine cascade pruning classifiers propose general versatile framework significantly speeds graphical model optimization maintaining excellent solution accuracy proposed approach refereed inference learning ibyl relies multi scale pruning scheme progressively reduces solution space use coarse fine cascade learnt classifiers thoroughly experiment classic computer vision related mrf problems novel framework constantly yields significant time speed respect efficient inference methods obtains accurate solution directly optimizing mrf make code available line
efficient partial monitoring prior information partial monitoring general model online learning limited feedback learner chooses actions sequential manner opponent chooses outcomes every round learner suffers loss receives feedback based action outcome goal learner minimize cumulative loss applications range dynamic pricing label efficient prediction dueling bandits paper assume given prior information distribution based opponent generates outcomes propose bpm family new efficient algorithms whose core track outcome distribution ellipsoid centered around estimated distribution show algorithm provably enjoys near optimal regret rate locally observable partial monitoring problems stochastic opponents demonstrated experiments synthetic well real world data algorithm outperforms previous approaches even uninformed priors order magnitude smaller regret lower running time
dual algorithm olfactory computation locust brain study early locust olfactory system attempt explain well characterized structure dynamics first propose computational function recovery high dimensional sparse olfactory signals small number measurements detailed experimental knowledge system rules standard algorithmic solutions problem instead show solving dual formulation corresponding optimisation problem yields structure dynamics good agreement biological data biological constraints lead reduced form dual formulation system uses independent component analysis continuously adapt olfactory environment allow accurate sparse recovery work demonstrates challenges rewards attempting detailed understanding experimentally well characterized systems
learning deep features scene recognition using places database scene recognition hallmark tasks computer vision allowing definition context object recognition whereas tremendous recent progress object recognition tasks due availability large datasets like imagenet rise convolutional neural networks cnns learning high level features performance scene recognition attained level success current deep features trained imagenet competitive enough tasks introduce new scene centric database called places million labeled pictures scenes propose new methods compare density diversity image datasets show places dense scene datasets diversity using cnn learn deep features scene recognition tasks establish new state art results several scene centric datasets visualization cnn layers responses allows show differences internal representations object centric scene centric networks
clustered factor analysis multineuronal spike data high dimensional simultaneous recordings neural spiking activity often explored analyzed visualized help latent variable factor models models however ill equipped extract structure beyond shared distributed aspects firing activity across multiple cells extend unstructured factor models proposing model discovers subpopulations groups cells pool recorded neurons model combines aspects mixture factor analyzer models capturing clustering structure aspects latent dynamical system models capturing temporal dependencies resulting model infer subpopulations latent factors data using variational inference model parameters estimated expectation maximization also address crucial problem initializing parameters extending sparse subspace clustering algorithm integer valued spike count observations illustrate merits proposed model applying calcium imaging data spinal cord neurons show uncovers meaningful clustering structure data
rounding based moves metric labeling metric labeling special case energy minimization pairwise markov random fields energy function consists arbitrary unary potentials pairwise potentials proportional given metric distance function label set popular methods solving metric labeling include move making algorithms iteratively solve minimum cut problem linear programming relaxation based approach order convert fractional solution relaxation integer solution several randomized rounding procedures developed literature consider large class parallel rounding procedures design move making algorithms closely mimic prove multiplicative bound move making algorithm exactly matches approximation factor corresponding rounding procedure arbitrary distance function analysis includes known results move making algorithms special cases
sequence sequence learning neural networks deep neural networks dnns powerful models achieved excellent performance difficult learning tasks although dnns work well whenever large labeled training sets available cannot used map sequences sequences paper present general end end approach sequence learning makes minimal assumptions sequence structure method uses multilayered long short term memory lstm map input sequence vector fixed dimensionality another deep lstm decode target sequence vector main result english french translation task wmt dataset translations produced lstm achieve bleu score entire test set lstm bleu score penalized vocabulary words additionally lstm difficulty long sentences comparison phrase based smt system achieves bleu score dataset used lstm rerank 1000 hypotheses produced aforementioned smt system bleu score increases close previous state art lstm also learned sensible phrase sentence representations sensitive word order relatively invariant active passive voice finally found reversing order words source sentences target sentences improved lstm performance markedly introduced many short term dependencies source target sentence made optimization problem easier
quantized kernel learning feature matching matching local visual features crucial problem computer vision accuracy greatly depends choice similarity measure generally difficult design hand similarity kernel perfectly adapted data interest learning automatically assumptions possible preferable however available techniques kernel learning suffer several limitations restrictive parametrization scalability paper introduce simple flexible family non linear kernels refer quantized kernels qks arbitrary kernels index space data quantizer piecewise constant similarities original feature space quantization allows compress features keep learning tractable result obtain state art matching performance standard benchmark dataset bits represent feature dimension qks also explicit non linear low dimensional feature mappings grant access euclidean geometry uncompressed features
reputation based worker filtering crowdsourcing paper study problem aggregating noisy labels crowd workers infer underlying true labels binary tasks unlike prior work examined problem random worker paradigm consider much broader class adversarial workers specific assumptions labeling strategy key contribution design computationally efficient reputation algorithm identify filter adversarial workers crowdsourcing systems algorithm uses concept optimal semi matchings conjunction worker penalties based label disagreements assign reputation score every worker provide strong theoretical guarantees deterministic adversarial strategies well extreme case sophisticated adversaries analyze worst case behavior algorithm finally show reputation algorithm significantly improve accuracy existing label aggregation algorithms real world crowdsourcing datasets
deep symmetry networks chief difficulty object recognition objects classes obscured large number extraneous sources variability pose part deformation sources variation represented symmetry groups sets composable transformations preserve object identity convolutional neural networks convnets achieve degree translational invariance computing feature maps translation group cannot handle groups result groups effects approximated small translations often requires augmenting datasets leads high sample complexity paper introduce deep symmetry networks symnets generalization convnets forms feature maps arbitrary symmetry groups symnets use kernel based interpolation tractably tie parameters pool symmetry spaces dimension like convnets trained backpropagation composition feature transformations layers symnet provides new approach deep learning experiments norb mnist rot show symnets affine group greatly reduce sample complexity relative convnets better capturing symmetries data
synaptical story persistent activity graded lifetime neural system persistent activity refers phenomenon cortical neurons keep firing even stimulus triggering initial neuronal responses moved persistent activity widely believed substrate neural system retaining memory trace stimulus information conventional view persistent activity regarded attractor network dynamics faces challenge closed properly contrast view attractor consider stimulus information encoded marginally unstable state network decays slowly exhibits persistent firing prolonged duration propose simple yet effective mechanism achieve goal utilizes property short term plasticity stp neuronal synapses stp forms short term depression std short term facilitation stf opposite effects retaining neuronal responses find properly combining stf std neural system hold persistent activity graded lifetime persistent activity fades away naturally without relying external drive implications results neural information representation discussed
relations lfps neural spike trains goals neuroscience identify neural networks correlate important behaviors environments genotypes work proposes strategy identifying neural networks characterized time frequency dependent connectivity patterns using convolutional dictionary learning links spike train data local field potentials lfps across multiple areas brain analytical contributions modeling dynamic relationships lfps spikes describing relationships spikes lfps analyzing ability predict lfp data region based spiking information across brain iii development clustering methodology allows inference similarities neurons multiple regions results based data sets spike lfp data recorded simultaneously brain regions mouse
large margin mechanism differentially private maximization basic problem design privacy preserving algorithms emph private maximization problem goal pick item universe approximately maximizes data dependent function constraint differential privacy problem used sub routine many privacy preserving algorithms statistics machine learning previous algorithms problem either range dependent utility diminishes size universe apply restricted function classes work provides first general purpose range independent algorithm private maximization guarantees approximate differential privacy applicability demonstrated fundamental tasks data mining machine learning
analog memories balanced rate based network neurons persistent graded activity often observed cortical circuits sometimes seen signature autoassociative retrieval memories stored earlier synaptic efficacies however despite decades theoretical work subject mechanisms support storage retrieval memories remain unclear previous proposals concerning dynamics memory networks fallen short incorporating key physiological constraints unified way specifically models violate dale law allow neurons excitatory inhibitory others restrict representation memories binary format induce recall states neurons fire rates close saturation propose novel control theoretic framework build functioning attractor networks satisfy set relevant physiological constraints directly optimize networks excitatory inhibitory neurons force sets arbitrary analog patterns become stable fixed points dynamics resulting networks operate balanced regime robust corruptions memory cue well ongoing noise incidentally explain reduction trial trial variability following stimulus onset ubiquitously observed sensory motor cortices results constitute step forward understanding neural substrate memory
design principles hippocampal cognitive map hippocampal place fields shown reflect behaviorally relevant aspects space instance place fields tend skewed along commonly traveled directions cluster around rewarded locations constrained geometric structure environment hypothesize set design principles hippocampal cognitive map explain place fields represent space way facilitates navigation reinforcement learning particular suggest place fields encode information current location also predictions future locations current transition distribution model variety place field phenomena arise naturally structure rewards barriers directional biases reflected transition policy furthermore demonstrate representation space support efficient reinforcement learning also propose grid cells compute eigendecomposition place fields part useful segmenting enclosure along natural boundaries applied recursively segmentation used discover hierarchical decomposition space thus grid cells might involved computing subgoals hierarchical reinforcement learning
fundamental limits online distributed algorithms statistical learning estimation many machine learning approaches characterized information constraints interact training data include memory sequential access constraints fast first order methods solve stochastic optimization problems communication constraints distributed learning partial access underlying data missing features multi armed bandits however currently little understanding information constraints fundamentally affect performance independent learning problem semantics example learning problems algorithm small memory footprint use bounded number bits example certain communication constraints perform worse possible without constraints paper describe single set results implies positive answers several different settings
content based recommendations poisson factorization develop collaborative topic poisson factorization ctpf generative model articles reader preferences ctpf used build recommender systems learning reader histories content recommend personalized articles interest detail ctpf models reader behavior article texts poisson distributions connecting latent topics represent texts latent preferences represent readers provides better recommendations competing methods gives interpretable latent space understanding patterns readership exploit stochastic variational inference model massive real world datasets example fit cptf full arxiv usage dataset contains million ratings million word counts within day demonstrate empirically model outperforms several baselines including previous state art approach
semi supervised learning deep generative models ever increasing size modern data sets combined difficulty obtaining label information made semi supervised learning problems significant practical importance modern data analysis revisit approach semi supervised learning generative models develop new models allow effective generalisation small labelled data sets large unlabelled ones generative approaches thus far either inflexible inefficient non scalable show deep generative models approximate bayesian inference exploiting recent advances variational methods used provide significant improvements making generative approaches highly competitive semi supervised learning
efficient optimization average precision svm accuracy information retrieval systems often measured using average precision given set positive relevant negative non relevant samples parameters retrieval system estimated using svm framework minimizes regularized convex upper bound empirical loss however high computational complexity loss augmented inference required learning svm prohibits use large training datasets alleviate deficiency propose complementary approaches first approach guarantees asymptotic decrease computational complexity loss augmented inference exploiting problem structure second approach takes advantage fact require full ranking loss augmented inference helps avoid expensive step sorting negative samples according individual scores third approach approximates loss samples loss difficult samples example incorrectly classified binary svm ensuring correct classification remaining samples using pascal voc action classification object detection datasets show approaches provide significant speed ups training without degrading test accuracy svm
feedforward learning mixture models develop biologically plausible learning rule provably converges class means general mixture models rule generalizes classical bcm neural rule within tensor framework substantially increasing generality learning problem solves achieves incorporating triplets samples mixtures provides novel information processing interpretation spike timing dependent plasticity provide proofs convergence close fit experimental data stdp
diverse sequential subset selection supervised video summarization video summarization challenging problem great application potential whereas prior approaches largely unsupervised nature focus sampling useful frames assembling summaries consider video summarization supervised subset selection problem idea teach system learn human created summaries select informative diverse subsets best meet evaluation metrics derived human perceived quality end propose sequential determinantal point process seqdpp probabilistic model diverse sequential subset selection novel seqdpp heeds inherent sequential structures video data thus overcoming deficiency standard dpp treats video frames randomly permutable items meanwhile seqdpp retains power modeling diverse subsets essential summarization extensive results summarizing videos datasets demonstrate superior performance method compared existing unsupervised methods also naive applications standard dpp model
conditional random field autoencoders unsupervised structured prediction introduce framework unsupervised learning structured predictors overlapping global features input latent representation predicted conditional observed data using feature rich conditional random field crf reconstruction input generated conditional latent structure using generative model factorizes similarly crf autoencoder formulation enables efficient exact inference without resorting unrealistic independence assumptions restricting kinds features used illustrate insightful connections traditional autoencoders posterior regularization multi view learning finally show competitive results instantiations framework canonical tasks natural language processing part speech induction bitext word alignment show training model substantially efficient comparable feature rich baselines
predictive entropy search efficient global optimization black box functions propose novel information theoretic approach bayesian optimization called predictive entropy search pes iteration pes selects next evaluation point maximizes expected information gained respect global maximum pes codifies intractable acquisition function terms expected reduction differential entropy predictive distribution reformulation allows pes obtain approximations accurate efficient alternatives entropy search furthermore pes easily perform fully bayesian treatment model hyperparameters cannot evaluate pes synthetic real world applications including optimization problems machine learning finance biotechnology robotics show increased accuracy pes leads significant gains optimization performance
learning distributed representations structured output prediction recent years distributed representations inputs led performance gains many applications allowing statistical information shared across inputs however predicted outputs labels generally structures still treated discrete objects even though outputs often discrete units meaning paper present new formulation structured prediction represent individual labels structure dense vectors allow semantically similar labels share parameters extend representation larger structures defining compositionality using tensor products give natural generalization standard structured prediction approaches define learning objective jointly learning model parameters label vectors propose alternating minimization algorithm learning show formulation outperforms structural svm baselines tasks multiclass document classification part speech tagging
covariance shrinkage autocorrelated data accurate estimation covariance matrices essential many signal processing machine learning algorithms high dimensional settings sample covariance known perform poorly hence regularization strategies analytic shrinkage ledoit wolf applied standard setting data assumed however practice time series typically exhibit strong autocorrelation structure introduces pronounced estimation bias recent work sancetta extended shrinkage framework beyond data contribute work showing sancetta estimator consistent high dimensional limit suffers high bias finite sample sizes propose alternative estimator unbiased less sensitive hyperparameter choice yields superior performance simulations toy data real world data set eeg based brain computer interfacing experiment
proximal quasi newton computationally intensive regularized estimators consider class optimization problems arising computationally intensive regularized estimators function gradient values expensive compute particular instance interest regularized mle learning conditional random fields crfs popular class statistical models varied structured prediction problems sequence labeling alignment classification label taxonomy regularized mles crfs particularly expensive optimize since computing gradient values requires expensive inference step work propose use carefully constructed proximal quasi newton algorithm computationally intensive estimation problems employ aggressive active set selection technique key contribution paper show proximal quasi newton algorithm provably super linearly convergent even absence strong convexity leveraging restricted variant strong convexity experiments proposed algorithm converges considerably faster current state art problems sequence labeling hierarchical classification
submodular meets structured finding diverse subsets exponentially large structured item sets cope high level ambiguity faced domains computer vision natural language processing robust prediction methods often search diverse set high quality candidate solutions proposals structured prediction problems becomes daunting task solution space image labelings sentence parses etc exponentially large study greedy algorithms finding diverse subset solutions structured output spaces drawing new connections submodular functions combinatorial item sets high order potentials hops studied graphical models specifically show via examples marginal gains submodular diversity functions allow structured representations enables efficient sub linear time approximate maximization reducing greedy augmentation step inference factor graph appropriately constructed hops discuss benefits tradeoffs show constructions lead significantly better proposals
nonparametric bayesian inference multivariate exponential families develop model choosing maximum entropy distribution set models satisfying certain smoothness independence criteria show inference model generalizes local kernel estimation context bayesian inference stochastic processes model enables bayesian inference contexts standard techniques like gaussian process inference expensive apply exact inference model possible likelihood function exponential family inference highly efficient requiring log time space run time demonstrate algorithm several problems show quantifiable improvement speed performance relative models based gaussian process
large scale bfgs using mapreduce bfgs applied effective parameter estimation method various machine learning algorithms since 1980s increasing demand deal massive instances variables important scale parallelize bfgs effectively distributed system paper study problem parallelizing bfgs algorithm large clusters tens thousands shared nothing commodity machines first show naive implementation bfgs using map reduce requires either significant amount memory large number map reduce steps negative performance impact second propose new bfgs algorithm called vector free bfgs avoids expensive dot product operations loop recursion greatly improves computation efficiency great degree parallelism algorithm scales well enables variety machine learning algorithms handle massive number variables large datasets prove mathematical equivalence new vector free bfgs demonstrate excellent performance scalability using real world machine learning problems billions variables production clusters
discovering learning exploiting relevance paper consider problem learning online information consider making sequential decisions formalize contextual multi armed bandit problem high dimensional dimensional context vector arrives learner needs select action maximize expected reward time step dimension context vector called type assume exists unknown relation actions types called relevance relation reward action depends contexts relevant types relation function reward action depends context single type expected reward action lipschitz continuous context relevant type propose algorithm achieves tilde gamma regret high probability gamma sqrt algorithm achieves learning unknown relevance relation whereas prior contextual bandit algorithms exploit existence relevance relation tilde regret algorithm alternates exploring exploiting require reward observations exploitations guarantees high probability actions suboptimality greater epsilon never selected exploitations proposed method applied variety learning applications including medical diagnosis recommender systems popularity prediction social networks network security etc instance time vast amounts different types information available decision maker effect action depends single type
learning graphs using orthonormal representation statistically consistent existing research cite reg suggests embedding graphs unit sphere beneficial learning labels vertices graph however choice optimal embedding remains open issue emph orthonormal representation graphs class embeddings unit sphere introduced lov asz cite lovasz_shannon paper show exists orthonormal representations statistically consistent large class graphs including power law random graphs result achieved extending notion consistency designed inductive setting graph transduction part analysis explicitly derive relationships rademacher complexity measure structural properties graphs chromatic number show fraction vertices graph nodes need labelled learning algorithm consistent also known labelled sample complexity omega left frac vartheta right frac vartheta famous lov asz vartheta function graph first time relates labelled sample complexity graph connectivity properties density graphs multiview setting whenever individual views expressed graph well known heuristic convex combination laplacians cite lap_mv1 tend improve accuracy analysis presented easily extends multiple graph transduction helps develop sound statistical understanding heuristic previously unavailable
tight bounds influence diffusion networks application bond percolation epidemiology paper derive theoretical bounds long term influence node independent cascade model icm relate bounds spectral radius particular matrix show behavior sub critical spectral radius lower specifically point general networks sub critical regime behaves sqrt size network upper bound met star shaped networks apply results epidemiology percolation arbitrary networks derive bound critical value beyond giant connected component arises finally show empirically tightness bounds large family networks
extreme bandits many areas medicine security life sciences want allocate limited resources different sources order detect extreme values paper study efficient way allocate resources sequentially limited feedback sequential design experiments well studied bandit theory commonly optimized property regret respect maximum mean reward however problems network intrusion detection interested detecting extreme value output sources therefore work study extreme regret measures efficiency algorithm compared oracle policy selecting source heaviest tail propose extremehunter algorithm provide analysis evaluate empirically synthetic real world experiments
latent source model online collaborative filtering despite prevalence collaborative filtering recommendation systems little theoretical development well works especially online setting items recommended users time address theoretical gap introducing model online recommendation systems cast item recommendation model learning problem analyze performance cosine similarity collaborative filtering method model users either likes dislikes items assume types users users given type share common string probabilities determining chance liking item time step recommend item user key distinction related bandit literature user consumes item watches movie item cannot recommended user goal maximize number likable items recommended users time main result establishes nearly log initial learning time steps simple collaborative filtering algorithm achieves essentially optimal performance without knowing algorithm exploitation step uses cosine similarity types exploration steps explore space items standard literature explore similarity users novel work
scalable inference neuronal connectivity calcium imaging fluorescent calcium imaging provides potentially powerful tool inferring connectivity neural circuits thousands neurons however key challenge using calcium imaging connectivity detection current systems often temporal response frame rate orders magnitude slower underlying neural spiking process bayesian inference based expectation maximization proposed overcome limitations often computationally demanding since step procedure typically involves state estimation high dimensional nonlinear dynamical system work propose computationally fast method state estimation based hybrid loopy belief propagation approximate message passing amp key insight neural system viewed calcium imaging factorized simple scalar dynamical systems neuron linear interconnections neurons using structure updates proposed hybrid amp methodology computed set dimensional state estimation procedures linear transforms connectivity matrix yields computationally scalable method inferring connectivity large neural circuits simulations method realistic neural networks demonstrate good accuracy computation times potentially significantly faster current approaches based markov chain monte carlo methods
multi world approach question answering real world scenes based uncertain input propose method automatically answering questions images bringing together recent advances natural language processing computer vision combine discrete reasoning uncertain predictions multi world approach represents uncertainty perceived world bayesian framework approach handle human questions high complexity realistic scenes replies range answer like counts object classes instances lists system directly trained question answer pairs establish first benchmark task seen modern attempt visual turing test
variational gaussian process state space models state space models successfully used fifty years different areas science engineering present procedure efficient variational bayesian learning nonlinear state space models based sparse gaussian processes result learning tractable posterior nonlinear dynamical systems comparison conventional parametric models offer possibility straightforwardly trade model capacity computational cost whilst avoiding overfitting main algorithm uses hybrid inference approach combining variational bayes sequential monte carlo also present stochastic variational inference online learning approaches fast learning long time series
probabilistic low rank matrix completion finite alphabets task reconstructing matrix given sample observed entries known emph matrix completion problem consideration arises wide variety problems including recommender systems collaborative filtering dimensionality reduction image processing quantum physics multi class classification name works focused recovering unknown real valued low rank matrix randomly sub sampling entries investigate case observations take finite numbers values corresponding examples ratings recommender systems labels multi class classification also consider general sampling scheme non necessarily uniform matrix entries performance nuclear norm penalized estimator analyzed theoretically precisely derive bounds kullback leibler divergence true estimated distributions practice also proposed efficient algorithm based lifted coordinate gradient descent order tackle potentially high dimensional settings
extremal mechanisms local differential privacy local differential privacy recently surfaced strong measure privacy contexts personal information remains private even data analysts working setting data providers data analysts want maximize utility statistical inferences performed released data study fundamental tradeoff local differential privacy information theoretic utility functions introduce family extremal privatization mechanisms call staircase mechanisms prove contains optimal privatization mechanism maximizes utility show information theoretic utility functions studied paper maximizing utility equivalent solving linear program outcome optimal staircase mechanism however solving linear program computationally expensive since number variables exponential data size account show simple staircase mechanisms binary randomized response mechanisms universally optimal high low privacy regimes respectively well approximate intermediate regime
gaussian process volatility model prediction time changing variances important task modeling financial data standard econometric models often limited assume rigid functional relationships evolution variance moreover functional parameters usually learned maximum likelihood lead overfitting address problems introduce vol novel non parametric model time changing variances based gaussian processes new model capture highly flexible functional relationships variances furthermore introduce new online algorithm fast inference vol method much faster current offline inference procedures avoids overfitting problems following fully bayesian approach experiments financial data show vol performs significantly better current standard alternatives
robust tensor decomposition gross corruption paper study statistical performance robust tensor decomposition gross corruption observations noisy realization superposition low rank tensor mathcal entrywise sparse corruption tensor mathcal unlike conventional noise bounded variance previous convex tensor decomposition analysis magnitude gross corruption arbitrary large show certain conditions true low rank tensor well sparse corruption tensor recovered simultaneously theory yields nonasymptotic frobenius norm estimation error bounds tensor separately show numerical experiments theory precisely predict scaling behavior practice
minimax optimal inference partial rankings paper studies problem rank aggregation plackett luce model goal infer global ranking related scores items based partial rankings provided multiple users multiple subsets items question particular interest optimally assign items users ranking many item assignments needed achieve target estimation error without assumptions items assigned users derive oracle lower bound cram rao lower bound estimation error prove upper bound estimation error achieved maximum likelihood estimator show upper bound cram rao lower bound inversely depend spectral gap laplacian appropriately defined comparison graph since random comparison graphs known large spectral gaps suggests use random assignments control precisely matching oracle lower bound upper bound estimation error imply maximum likelihood estimator together random assignment minimax optimal logarithmic factor analyze popular rank breaking scheme decompose partial rankings pairwise comparisons show even applies mismatched maximum likelihood estimator assumes independence pairwise comparisons dependent due rank breaking minimax optimal performance still achieved logarithmic factor
mind nuisance gaussian process classification using privileged noise learning privileged information setting recently attracted lot attention within machine learning community allows integration additional knowledge training process classifier even comes form data modality available test time show privileged information naturally treated noise latent function gaussian process classifier gpc contrast standard gpc setting latent function nuisance feature becomes natural measure confidence training data modulating slope gpc probit likelihood function extensive experiments public datasets show proposed gpc method using privileged noise called gpc improves standard gpc without privileged knowledge also current state art svm based method svm moreover show advanced neural networks deep learning methods compressed privileged information
bayesian model identifying hierarchically organised states neural population activity neural population activity cortical circuits solely driven external inputs also modulated endogenous states vary multiple time scales understand information processing cortical circuits need understand statistical structure internal states interaction sensory inputs present statistical model extracting hierarchically organised neural population states multi channel recordings neural spiking activity population states modelled using hidden markov decision tree state dependent tuning parameters generalised linear observation model present variational bayesian inference algorithm estimating posterior distribution parameters neural population recordings simulated data show identify underlying sequence population states reconstruct ground truth parameters using population recordings visual cortex find model levels population states outperforms state state generalised linear model finally find modelling state dependence also improves accuracy sensory stimuli decoded population response
feedback detection live predictors predictor deployed live production system perturb features uses make predictions feedback loop occur example model predicts certain type behavior ends causing behavior predicts thus creating self fulfilling prophecy paper analyze predictor feedback detection causal inference problem introduce local randomization scheme used detect non linear feedback real world problems conduct pilot study proposed methodology using predictive system currently deployed part search engine
discriminative metric learning neighborhood gerrymandering formulate problem metric learning nearest neighbor classification large margin structured prediction problem latent variable representing choice neighbors task loss directly corresponding classification error describe efficient algorithm exact loss augmented inference fast gradient descent algorithm learning model objective drives metric establish neighborhood boundaries benefit true class labels training points approach reminiscent gerrymandering redrawing political boundaries provide advantage certain parties direct handling optimizing classification accuracy previously proposed experiments variety data sets method shown achieve excellent results compared current state art metric learning
optimal neural codes control estimation agents acting natural world aim selecting appropriate actions based noisy partial sensory observations many behaviors leading decision making action selection closed loop setting naturally phrased within control theoretic framework within framework optimal control theory usually given cost function minimized selecting control law based observations standard control settings sensors assumed fixed biological systems often gain extra flexibility optimizing sensors however sensory adaptation geared towards control rather perception often assumed work show sensory adaptation control differs sensory adaptation perception even simple control setups implies consistently recent experimental results studying sensory adaptation essential account task performed
spatio temporal representations uncertainty spiking neural networks long argued inherent ambiguity noise brain needs represent uncertainty form probability distributions neural encoding distributions remains however highly controversial present novel circuit model representing multidimensional real valued distributions using spike based spatio temporal code model combines computational advantages currently competing models probabilistic codes exhibits realistic neural responses along variety classic measures furthermore model highlights challenges associated interpreting neural activity relation behavioral uncertainty points alternative population level approaches experimental validation distributed representations
fast kernel learning multidimensional pattern extrapolation ability automatically discover patterns perform extrapolation essential quality intelligent systems kernel methods gaussian processes great potential pattern extrapolation since kernel flexibly interpretably controls generalisation properties methods however automatically extrapolating large scale multidimensional patterns general difficult developing gaussian process models purpose involves several challenges vast majority kernels kernel learning methods currently succeed smoothing interpolation difficulty compounded fact gaussian processes typically tractable small datasets scaling expressive kernel learning approach poses different challenges scaling standard gaussian process model faces additional computational constraints need retain significant model structure expressing rich information available large dataset paper propose gaussian process approach large scale multidimensional pattern extrapolation recover sophisticated class kernels perform texture extrapolation inpainting video extrapolation long range forecasting land surface temperatures large multidimensional datasets including problem 383 400 training points proposed method significantly outperforms alternative scalable flexible gaussian process methods speed accuracy moreover show distinct combination expressive kernels fully non parametric representation scalable inference exploits existing model structure critical large scale multidimensional pattern extrapolation
noisy power method meta algorithm applications provide new robust convergence analysis well known power method computing dominant singular vectors matrix call noisy power method result characterizes convergence behavior algorithm large amount noise introduced matrix vector multiplication noisy power method seen meta algorithm recently found number important applications broad range machine learning problems including alternating minimization matrix completion streaming principal component analysis pca privacy preserving spectral analysis general analysis subsumes several existing hoc convergence bounds resolves number open problems multiple applications recent work mitliagkas nips 2013 gives space efficient algorithm pca streaming model samples drawn spiked covariance model give simpler general analysis applies arbitrary distributions moreover even spiked covariance model result gives quantitative improvements natural parameter regime second application provide algorithm differentially private principal component analysis runs nearly linear time input sparsity achieves nearly tight worst case error bounds complementing worst case bounds show error dependence algorithm matrix dimension replaced essentially tight dependence coherence matrix result resolves main problem left open hardt roth stoc 2013 leads strong average case improvements optimal worst case bound
dfacto distributed factorization tensors present technique significantly speeding alternating least squares als gradient descent widely used algorithms tensor factorization exploiting properties khatri rao product show efficiently address computationally challenging sub step algorithms algorithm dfacto requires matrix vector products easy parallelize dfacto scalable also average times faster competing algorithms variety datasets instance dfacto takes 480 seconds machines perform iteration als algorithm 143 seconds perform iteration algorithm million million million dimensional tensor billion non entries
conditional swap regret conditional correlated equilibrium introduce natural extension notion swap regret conditional swap regret allows action modifications conditioned player action history prove series new results conditional swap regret minimization present algorithms minimizing conditional swap regret bounded conditioning history extend results case conditional swaps considered subset actions also define new notion equilibrium conditional correlated equilibrium tightly connected notion conditional swap regret players follow conditional swap regret minimization strategies empirical distribution approaches equilibrium finally extend results multi armed bandit scenario
discrete graph hashing hashing emerged popular technique fast nearest neighbor search gigantic databases particular learning based hashing received considerable attention due appealing storage search efficiency however performance unsupervised learning based hashing methods deteriorates rapidly hash code length increases argue degraded performance due inferior optimization procedures used achieve discrete binary codes paper presents graph based unsupervised hashing model preserve neighborhood structure massive data discrete code space cast graph hashing problem discrete optimization framework directly learns binary codes tractable alternating maximization algorithm proposed explicitly deal discrete constraints yielding high quality codes well capture local neighborhoods extensive experiments performed large datasets million samples show discrete optimization based graph hashing method obtains superior search accuracy state art unsupervised hashing methods especially longer codes
sparse pca via covariance thresholding sparse principal component analysis given noisy observations low rank matrix dimension times seek reconstruct additional sparsity assumptions particular assume principal components bv_1 dots bv_r k_1 cdots k_q non entries respectively study high dimensional regime order influential paper johnstone cite johnstone2004sparse introduced simple algorithm estimates support principal vectors bv_1 dots bv_r largest entries diagonal empirical covariance method shown succeed high probability k_q c_1 sqrt log fail high probability k_q c_2 sqrt log constants c_1 c_2 infty despite considerable amount work last years practical algorithm exists provably better support recovery guarantees analyze covariance thresholding algorithm recently proposed krauthgamer nadler vilenchik cite krauthgamerspca confirm empirical evidence presented authors rigorously prove algorithm succeeds high probability order sqrt recent conditional lower bounds cite berthet2013computational suggest might impossible significantly better key technical component analysis develops new bounds norm kernel random matrices regimes considered
metric learning temporal sequence alignment paper propose learn mahalanobis distance perform alignment multivariate time series learning examples task time series true alignment known cast alignment problem structured prediction task propose realistic losses alignments optimization tractable provide experiments real data audio audio context show learning similarity measure leads improvements performance alignment task also propose use metric learning framework perform feature selection basic audio features build combination better alignment performance
differential equation modeling nesterov accelerated gradient method theory insights derive second order ordinary differential equation ode limit nesterov accelerated gradient method ode exhibits approximate equivalence nesterov scheme thus serve tool analysis show continuous time ode allows better understanding nesterov scheme byproduct obtain family schemes similar convergence rates ode interpretation also suggests restarting nesterov scheme leading algorithm rigorously proven converge linear rate whenever objective strongly convex
ranking via robust binary classification propose robirank ranking algorithm motivated observing close connection evaluation metrics learning rank loss functions robust classification algorithm shows competitive performance standard benchmark datasets representative algorithms literature large scale problems explicit feature vectors scores given algorithm efficiently parallelized across large number machines task requires 386 133 824 519 pairwise interactions items ranked algorithm finds solutions dramatically higher quality found state art competitor algorithm given amount wall clock time computation
deep learning real time atari game play using offline monte carlo tree search planning combination modern reinforcement learning deep learning approaches holds promise making significant progress challenging applications requiring rich perception policy selection arcade learning environment ale provides set atari games represent useful benchmark set applications recent breakthrough combining model free reinforcement learning deep learning called dqn achieves best real time agents thus far planning based approaches achieve far higher scores best model free approaches exploit information available human players orders magnitude slower needed real time play main goal work build better real time atari game playing agent dqn central idea use slow planning based agents provide training data deep learning architecture capable real time play proposed new agents based idea show outperform dqn
multiplicative model learning distributed text based attribute representations paper propose general framework learning distributed representations attributes characteristics text whose representations jointly learned word embeddings attributes correspond wide variety concepts document indicators learn sentence vectors language indicators learn distributed language representations meta data side information age gender industry blogger representations authors describe third order model word context attribute vectors interact multiplicatively predict next word sequence leads notion conditional word similarity meanings words change conditioned different attributes perform several experimental tasks including sentiment classification cross lingual document classification blog authorship attribution also qualitatively evaluate conditional word neighbours attribute conditioned text generation
convnets learn correspondence convolutional neural nets convnets trained massive labeled datasets substantially improved state art image classification object detection however visual understanding requires establishing correspondence finer level object category given large pooling regions training whole image labels clear convnets derive success accurate correspondence model could used precise localization paper study effectiveness convnet activation features tasks requiring correspondence present evidence convnet features localize much finer scale receptive field sizes used perform intraclass aligment well conventional hand engineered features outperform conventional features keypoint prediction objects pascal voc 2011
number linear regions deep neural networks study complexity functions computable deep feedforward neural networks piecewise linear activations terms symmetries number linear regions deep networks able sequentially map portions layer input space output way deep models compute functions react equally complicated patterns different inputs compositional structure functions enables use pieces computation exponentially often terms network depth paper investigates complexity compositional maps contributes new theoretical results regarding advantage depth neural networks piecewise linear activation functions particular analysis specific single family models example employ rectifier maxout networks improve complexity bounds pre existing work investigate behavior units higher layers
online stochastic gradient methods non decomposable loss functions modern applications sensitive domains biometrics medicine frequently require use non decomposable loss functions precision measure etc compared point loss functions hinge loss offer much fine grained control prediction time present novel challenges terms algorithm design analysis work initiate study online learning techniques non decomposable loss functions aim enable incremental learning well design scalable solvers batch problems end propose online learning framework loss functions model enjoys several nice properties chief amongst existence efficient online learning algorithms sublinear regret online batch conversion bounds model provable extension existing online learning models point loss functions instantiate popular losses prec pauc model prove sublinear regret bounds proofs require novel structural lemma ranked lists independent interest develop scalable stochastic gradient descent solvers non decomposable loss functions show large family loss functions satisfying certain uniform convergence property includes prec pauc measure methods provably converge empirical risk minimizer uniform convergence results known losses establish using novel proof techniques use extensive experimentation real life benchmark datasets establish method orders magnitude faster recently proposed cutting plane method
stochastic multi armed bandit problem non stationary rewards multi armed bandit mab problem gambler needs choose round play arms characterized unknown reward distribution reward realizations observed arm selected gambler objective maximize cumulative expected earnings given horizon play gambler needs acquire information arms exploration simultaneously optimizing immediate rewards exploitation price paid due trade often referred regret main question small price function horizon length problem studied extensively reward distributions change time assumption supports sharp characterization regret yet often violated practical settings paper focus mab formulation allows broad range temporal uncertainties rewards still maintaining mathematical tractability fully characterize regret complexity class mab problems establishing direct link extent allowable reward variation minimal achievable regret establishing connection adversarial stochastic mab frameworks
finding sparse vector subspace linear sparsity using alternating directions consider problem recovering sparsest vector subspace mathcal mathbb text dim mathcal problem considered homogeneous variant sparse recovery problem finds applications sparse dictionary learning sparse pca problems signal processing machine learning simple convex heuristics problem provably break fraction nonzero entries target sparse vector substantially exceeds sqrt contrast exhibit relatively simple nonconvex approach based alternating directions provably succeeds even fraction nonzero entries omega knowledge first practical algorithm achieve linear scaling result assumes planted sparse model target sparse vector embedded otherwise random subspace empirically proposed algorithm also succeeds challenging data models arising sparse dictionary learning
median selection subset aggregation parallel inference massive data sets efficient computation commonly relies distributed algorithms store process subsets data different machines minimizing communication costs focus regression classification problems involving many features variety distributed algorithms proposed context challenges arise defining algorithm low communication theoretical guarantees excellent practical performance general settings propose median selection subset aggregation estimator message algorithm attempts solve problems algorithm applies feature selection parallel subset using lasso another method calculates median feature inclusion index estimates coefficients selected features parallel subset averages estimates algorithm simple involves minimal communication scales efficiently sample feature size theoretical guarantees particular show model selection consistency coefficient estimation efficiency extensive experiments show excellent performance variable selection estimation prediction computation time relative usual competitors
stochastic gradient descent weighted sampling randomized kaczmarz algorithm improve recent gurantee bach moulines linear convergence sgd smooth strongly convex objectives reducing quadratic dependence strong convexity linear dependence furthermore show reweighting sampling distribution importance sampling necessary order improve convergence obtain linear dependence average smoothness dominating previous results broadly discus importance sampling sgd improve convergence also scenarios results based connection make sgd randomized kaczmarz algorithm allows transfer ideas separate bodies literature studying methods
learning time varying coverage functions coverage functions important class discrete functions capture laws diminishing returns paper propose new problem learning time varying coverage functions arise naturally applications social network analysis machine learning algorithmic game theory develop novel parametrization time varying coverage function illustrating connections counting processes present efficient algorithm learn parameters maximum likelihood estimation provide rigorous theoretic analysis sample complexity empirical experiments information diffusion social network analysis demonstrate assumptions underlying diffusion process method performs significantly better existing approaches synthetic real world data
orbit regularization propose general framework regularization based group majorization framework group defined act parameter space orbit fixed control complexity model parameters confined lie convex hull orbit orbitope common regularizers recovered particular cases connection revealed recent sorted norm hyperoctahedral group derive properties group must satisfy amenable optimization conditional projected gradient algorithms finally suggest continuation strategy orbit exploration presenting simulation results symmetric hyperoctahedral groups
provable submodular minimization using wolfe algorithm owing several applications large scale learning vision problems fast submodular function minimization sfm become critical problem theoretically unconstrained sfm performed polynomial time iwata orlin 2009 however algorithms practical 1976 wolfe proposed algorithm find minimum euclidean norm point polytope 1980 fujishige showed wolfe algorithm used sfm general submodular functions fujishige wolfe minimum norm algorithm seems best empirical performance despite good practical performance theoretically little known wolfe minimum norm algorithm knowledge result exponential time analysis due wolfe paper give maiden convergence analysis wolfe algorithm prove iterations wolfe algorithm returns approximate solution min norm point also prove robust version fujishige theorem shows approximate solution min norm point problem implies exact submodular minimization corollary get first pseudo polynomial time guarantee fujishige wolfe minimum norm algorithm submodular function minimization particular show min norm point algorithm solves sfm time upper bound maximum change single element cause function value
learning generative models visual attention attention long proposed psychologists important efficiently dealing massive amounts sensory stimulus neocortex inspired attention models visual neuroscience need object centered data generative models propose deep learning based generative framework using attention attentional mechanism propagates signals region interest scene aligned canonical representation generative modeling ignoring scene background clutter generative model concentrate resources object interest convolutional neural net employed provide good initializations posterior inference uses hamiltonian monte carlo upon learning images faces model robustly attend face region novel test subjects importantly model learn generative models new faces novel dataset large images face locations known
convolutional kernel networks important goal visual recognition devise image representations invariant particular transformations paper address goal new type convolutional neural network cnn whose invariance encoded reproducing kernel unlike traditional approaches neural networks learned either represent data solving classification task network learns approximate kernel feature map training data approach enjoys several benefits classical ones first teaching cnns invariant obtain simple network architectures achieve similar accuracy complex ones easy train robust overfitting second bridge gap neural network literature kernels natural tools model invariance evaluate methodology visual recognition tasks cnns proven perform well digit recognition mnist dataset challenging cifar stl datasets accuracy competitive state art
pewa patch based exponentially weighted aggregation image denoising patch based methods widely used noise reduction recent years paper propose general statistical aggregation method combines image patches denoised several commonly used algorithms show weakly denoised versions input image obtained standard methods serve compute efficient patch based aggregated estimd aggregation ewa estimator resulting approach pewa based mcmc sampling nice statistical foundation producing denoising results comparable current state art demonstrate performance denoising algorithm real images compare results several competitive methods
learning mixtures submodular functions image collection summarization address problem image collection summarization learning mixtures submodular functions argue submodularity natural problem show number previously used scoring functions submodular property explicitly mentioned publications provide classes submodular functions capturing necessary properties summaries namely coverage likelihood diversity learn mixtures submodular functions scoring functions formulate summarization supervised learning problem using large margin structured prediction furthermore introduce novel evaluation metric call rouge automatic summary scoring similar metric called rouge successfully applied document summarization metric known quantifying quality image collection summaries provide new dataset consisting real world image collections along many human generated ground truth summaries collected using mechanical turk also extensively compare method previously explored methods problem show learning approach outperforms competitors new dataset paper provides knowledge first systematic approach quantifying problem image collection summarization along new dataset image collections human summaries
learning discover efficient mathematical identities paper explore machine learning techniques applied discovery efficient mathematical identities introduce attribute grammar framework representing symbolic expressions given grammar math operators build trees combine different ways looking compositions analytically equivalent target expression lower computational complexity however space trees grows exponentially complexity target expression brute force search impractical simplest expressions consequently introduce novel learning approaches able learn simpler expressions guide tree search first simple gram model recursive neural network show approaches enable derive complex identities beyond reach brute force search human derivation
automated variational inference gaussian process models develop automated variational method approximate inference gaussian process models whose posteriors often intractable using mixture gaussians variational distribution show variational objective gradients approximated efficiently via sampling univariate gaussian distributions gradients hyperparameters obtained analytically regardless model likelihood propose instances variational distribution whose covariance matrices parametrized linearly number observations results allow gradient based optimization done efficiently black box manner approach thoroughly verified models using benchmark datasets performing well exact hard coded implementations running orders magnitude faster alternative mcmc sampling approaches method valuable tool practitioners researchers investigate new models minimal effort deriving model specific inference algorithms
lsda large scale detection adaptation major challenge scaling object detection difficulty obtaining labeled images large numbers categories recently deep convolutional neural networks cnns emerged clear winners object classification benchmarks part due training labeled classification images unfortunately small fraction labels available detection task much cheaper easier collect large quantities image level labels search engines collect detection data label precise bounding boxes paper propose large scale detection adaptation lsda algorithm learns difference tasks transfers knowledge classifiers categories without bounding box annotated data turning detectors method potential enable detection tens thousands categories lack bounding box annotations yet plenty classification data evaluation imagenet lsvrc 2013 detection challenge demonstrates efficacy approach algorithm enables produce detector using available classification data leaf nodes imagenet tree additionally demonstrate modify architecture produce fast detector running 2fps detector models software available
fast training pose detectors fourier domain many datasets samples related known image transformation rotation repeatable non rigid deformation applies datasets objects different viewpoints datasets augmented virtual samples datasets possess high degree redundancy geometrically induced transformations preserve intrinsic properties objects likewise ensembles classifiers used pose estimation also share many characteristics since related geometric transformation assuming transformation norm preserving cyclic propose closed form solution fourier domain eliminate redundancies leverage shelf solvers modification libsvm train several pose classifiers simultaneously extra cost experiments show training sliding window object detector pose estimator sped orders magnitude transformations diverse planar rotation walking motion pedestrians plane rotations cars
online combinatorial optimization stochastic decision sets adversarial losses work sequential learning assumes fixed set actions available time however practice actions consist picking subsets readings sensors break time time road segments blocked goods stock paper study learning algorithms able deal stochastic availability unreliable composite actions propose analyze algorithms based follow perturbed leader prediction method several learning settings differing feedback provided learner algorithms rely novel loss estimation technique call counting asleep times deliver regret bounds algorithms previously studied full information semi bandit settings well natural middle point call restricted information setting special consequence results significant improvement best known performance guarantees achieved efficient algorithm sleeping bandit problem stochastic availability finally evaluate algorithms empirically show improvement known approaches
fast robust least squares estimation corrupted linear models subsampling methods recently proposed speed least squares estimation large scale settings however algorithms typically robust outliers corruptions observed covariates concept influence developed regression diagnostics used detect corrupted observations shown paper property influence also develop randomized approximation motivates proposed subsampling algorithm large scale corrupted linear regression limits influence data points since highly influential points contribute residual error general model corrupted observations show theoretically empirically variety simulated real datasets algorithm improves current state art approximation schemes ordinary least squares
optimal regret minimization posted price auctions strategic buyers study revenue optimization learning algorithms posted price auctions strategic buyers analyze broad family monotone regret minimization algorithms problem includes previous best known algorithm show algorithm family admits strategic regret favorable omega sqrt introduce new algorithm achieves strategic regret differing lower bound factor log exponential improvement upon previous best algorithm new algorithm admits natural analysis simpler proofs ideas behind design general also report results empirical evaluations comparing algorithm previous best algorithm show consistent exponential improvement several different scenarios
drifting games analysis online learning applications boosting provide general mechanism design online learning algorithms based minimax analysis within drifting games framework different online learning settings hedge multi armed bandit problems online convex optimization studied converting various kinds drifting games original minimax analysis drifting games used generalized applying series relaxations starting choosing convex surrogate loss function different choices surrogates recover existing algorithms also propose new algorithms totally parameter free enjoy useful properties moreover drifting games framework naturally allows study high probability bounds without resorting concentration results also generalized notion regret measures good algorithm compared top small fraction candidates finally translate new hedge algorithm new adaptive boosting algorithm computationally faster shown experiments since ignores large number examples round
bayesian nonlinear support vector machines discriminative factor modeling new bayesian formulation developed nonlinear support vector machines svms based gaussian process svm hinge loss expressed scaled mixture normals integrate bayesian svm factor model feature learning nonlinear classifier design performed jointly almost previous work discriminative feature learning assumed linear classifier inference performed expectation conditional maximization ecm markov chain monte carlo mcmc extensive set experiments demonstrate utility using nonlinear bayesian svm within discriminative feature learning factor modeling standpoints accuracy interpretability
bounded regret finite armed structured bandits study new type armed bandit problem expected return arm depend returns arms present new algorithm general class problems show certain circumstances possible achieve finite expected cumulative regret also give problem dependent lower bounds cumulative regret showing least special cases new algorithm nearly optimal
deconvolution high dimensional mixtures via boosting application diffusion weighted mri human brain diffusion weighted magnetic resonance imaging dwi fiber tractography methods measure structure white matter living human brain diffusion signal modelled combined contribution many individual fascicles nerve fibers passing location white matter typically done via basis pursuit estimation exact directions limited due discretization difficulties inherent modeling dwi data shared many problems involving fitting non parametric mixture models ekanadaham proposed approach continuous basis pursuit overcome discretization error dimensional case spike sorting propose general algorithm fits mixture models dimensionality without discretization algorithm uses principles boost together refitting weights pruning parameters addition steps boost accelerates algorithm assures accuracy refer resulting algorithm elastic basis pursuit ebp since expands contracts active set kernels needed show contrast existing approaches fitting mixtures boosting framework enables selection optimal bias variance tradeoff along solution path scales high dimensional problems simulations dwi find ebp yields better parameter estimates non negative least squares nnls approach standard model used dwi tensor model serves basis diffusion tensor imaging dti demonstrate utility method dwi data acquired parts brain containing crossings multiple fascicles nerve fibers
difference convex functions programming reinforcement learning large markov decision processes mdps usually solved using approximate dynamic programming adp methods approximate value iteration avi approximate policy iteration api main contribution paper show alternatively optimal state action value function estimated using difference convex functions programming study minimization norm optimal bellman residual obr called optimal bellman operator controlling residual allows controlling distance optimal action value function show minimizing empirical norm obr consistant vapnik sense finally frame optimization problem program allows envisioning using large related literature programming address reinforcement leaning problem
consistent binary classification generalized performance metrics performance metrics binary classification designed capture tradeoffs fundamental population quantities true positives false positives true negatives false negatives despite significant interest theoretical applied communities little known either optimal classifiers consistent algorithms optimizing binary classification performance metrics beyond special cases consider fairly large family performance metrics given ratios linear combinations fundamental population quantities family includes many well known binary classification metrics classification accuracy measure measure jaccard similarity coefficient special cases analysis identifies optimal classifiers sign thresholded conditional probability positive class performance metric dependent threshold optimal threshold constructed using simple plug estimators performance metric linear combination population quantities alternative techniques required general case propose algorithms estimating optimal classifiers prove statistical consistency algorithms straightforward modifications standard approaches address key challenge optimal threshold selection thus simple implement practice first algorithm combines plug estimate conditional probability positive class optimal threshold selection second algorithm leverages recent work calibrated asymmetric surrogate losses construct candidate classifiers present empirical comparisons algorithms benchmark datasets
estimation norm regularization analysis estimation error associated structured statistical recovery based norm regularized regression lasso needs consider aspects norm loss function design matrix noise vector paper presents generalizations estimation error analysis aspects compared existing literature characterize restricted error set establish relations error sets constrained regularized problems present estimation error bound applicable norm precise characterizations bound presented variety noise vectors design matrices including sub gaussian anisotropic dependent samples loss functions including least squares generalized linear models gaussian widths measure size suitable sets associated tools play key role generalized analysis
efficient learning implicit exploration bandit problems side observations consider online learning problems partial observability model capturing situations information conveyed learner full information bandit feedback simplest variant assume addition loss learner also gets observe losses actions revealed losses depend learner action directed observation system chosen environment setting propose first algorithm enjoys near optimal regret guarantees without know observation system selecting actions along similar lines also define new partial information setting models online combinatorial optimization problems feedback received learner semi bandit full feedback predictions first algorithm cannot always computed efficiently setting propose another algorithm similar properties benefit always computationally efficient price slightly complicated tuning mechanism algorithms rely novel exploration strategy called implicit exploration shown efficient computationally information theoretically previously studied exploration strategies problem
spectral clustering graphs bethe hessian spectral clustering standard approach label nodes graph studying largest lowest eigenvalues symmetric real matrix adjacency laplacian recently argued using instead complicated non symmetric higher dimensional operator related non backtracking walk graph leads improved performance detecting clusters even optimal performance stochastic block model propose use instead simpler object symmetric real matrix known bethe hessian operator deformed laplacian show approach combines performances non backtracking operator thus detecting clusters way theoretical limit stochastic block model computational theoretical memory advantages real symmetric matrices
integer polynomial programming based framework lifted map inference paper present new approach lifted map inference markov logic networks mlns key idea approach compactly encode map inference problem integer polynomial program ipp schematically applying lifted inference steps mln lifted decomposition lifted conditioning partial grounding ipp encoding lifted sense integer assignment variable ipp represent truth assignment multiple indistinguishable ground atoms mln show solve ipp first converting integer linear program ilp solving latter using state art ilp techniques experiments several benchmark mlns show new algorithm substantially superior ground inference existing methods terms computational efficiency solution quality
sampling inference probabilistic models fast bayesian quadrature propose novel sampling framework inference probabilistic models active learning approach converges quickly wall clock time markov chain monte carlo mcmc benchmarks central challenge probabilistic inference numerical integration average ensembles models unknown hyper parameters example compute marginal likelihood partition function mcmc provided approaches numerical integration deliver state art inference suffer sample inefficiency poor convergence diagnostics bayesian quadrature techniques offer model based solution problems uptake hindered prohibitive computation costs introduce warped model probabilistic integrands likelihoods known non negative permitting cheap active learning scheme optimally select sample locations algorithm demonstrated offer faster convergence seconds relative simple monte carlo annealed importance sampling synthetic real world examples
extended unscented gaussian processes present new methods inference gaussian process models general nonlinear likelihoods inference based variational framework gaussian posterior assumed likelihood linearized variational posterior mean using either taylor series expansion statistical linearization show parameter updates obtained algorithms equivalent state update equations iterative extended unscented kalman filters respectively hence refer algorithms extended unscented gps unscented treats likelihood black box requiring derivative inference also applies non differentiable likelihood models evaluate performance algorithms number synthetic inversion problems binary classification dataset
sparse polynomial learning graph sketching let rightarrow mathbb polynomial non real coefficients give algorithm exactly reconstructing given random examples uniform distribution runs time polynomial succeeds function satisfies textit unique sign property output value corresponds unique set values participating parities sufficient condition satisfied every coefficient perturbed small random noise satisfied high probability parity functions chosen randomly coefficients positive learning sparse polynomials boolean domain time polynomial considered notoriously hard worst case result shows problem tractable almost sparse polynomials show application result hypergraph sketching problem learning sparse number hyperedges size hyperedges hypergraph uniformly drawn random cuts also provide experimental results real world dataset
learning pseudo ensembles formalize notion pseudo ensemble possibly infinite collection child models spawned parent model perturbing according noise process dropout hinton 2012 deep neural network trains pseudo ensemble child subnetworks generated randomly masking nodes parent network examine relationship pseudo ensembles involve perturbation model space standard ensemble methods existing notions robustness focus perturbation observation space present novel regularizer based making behavior pseudo ensemble robust respect noise process generating fully supervised setting regularizer matches performance dropout unlike dropout regularizer naturally extends semi supervised setting produces state art results provide case study transform recursive neural tensor network socher 2013 pseudo ensemble significantly improves performance real world sentiment analysis benchmark
neural word embedding implicit matrix factorization analyze skip gram negative sampling sgns word embedding method introduced mikolov show implicitly factorizing word context matrix whose cells pointwise mutual information pmi respective word context pairs shifted global constant find another embedding method nce implicitly factorizing similar matrix cell shifted log conditional probability word given context show using sparse shifted positive pmi word context matrix represent words improves results word similarity tasks analogy tasks dense low dimensional vectors preferred exact factorization svd achieve solutions least good sgns solutions word similarity tasks analogy questions sgns remains superior svd conjecture stems weighted nature sgns factorization
low rank time frequency synthesis many single channel signal decomposition techniques rely low rank factorization time frequency transform particular nonnegative matrix factorization nmf spectrogram power magnitude short time fourier transform stft considered many audio applications setting nmf itakura saito divergence shown underly generative gaussian composite model gcm stft step forward empirical approaches based hoc transform divergence specifications still gcm yet generative model raw signal stft work presented paper fills ultimate gap proposing novel signal synthesis model low rank time frequency structure particular new approach opens doors multi resolution representations possible traditional nmf setting describe expectation maximization algorithms estimation new model report audio signal processing results music decomposition speech enhancement
fast prediction large scale kernel machines kernel machines kernel svm kernel ridge regression usually construct high quality models however use real world applications remains limited due high prediction cost paper present novel insights improving prediction efficiency kernel machines first show adding pseudo landmark points classical nystr kernel approximation elegant way significantly reduce prediction error without much additional prediction cost second provide new theoretical analysis bounding error solution computed using nystr kernel approximation method show error related weighted kmeans objective function weights given model computed original kernel theoretical insight suggests new landmark point selection technique situation knowledge original model based insights provide divide conquer framework improving prediction speed first divide whole problem smaller local subproblems reduce problem size second phase develop kernel approximation based fast prediction approach within subproblem apply algorithm real world large scale classification regression datasets show proposed algorithm consistently significantly better competitors example covertype classification problem terms prediction time algorithm achieves 10000 times speedup full kernel svm fold speedup state art ldkl approach obtaining much higher prediction accuracy ldkl
generalized higher order orthogonal iteration tensor decomposition completion low rank tensor estimation frequently applied many real world problems despite successful applications existing schatten norm minimization snm methods become slow even applicable large scale problems address difficulty therefore propose efficient scalable core tensor schatten norm minimization method simultaneous tensor decomposition completion much lower computational complexity first induce equivalence relation schatten norm low rank tensor core tensor schatten norm core tensor used replace whole tensor leads much smaller scale matrix snm problem finally efficient algorithm rank increasing scheme developed solve proposed problem convergence guarantee extensive experimental results show method usually accurate state art methods orders magnitude faster
reducing rank relational factorization models including observable patterns tensor factorizations become popular methods learning multi relational data context rank factorization important parameter determines runtime well generalization ability determine conditions factorization efficient approach learning relational data derive upper lower bounds rank required recover adjacency tensors based findings propose novel additive tensor factorization model learning latent observable patterns multi relational data present scalable algorithm computing factorization experimentally show proposed approach improve predictive performance pure latent variable methods also reduces required rank therefore runtime memory complexity significantly
large margin convex polytope machine present convex polytope machine cpm novel non linear learning algorithm large scale binary classification tasks cpm finds large margin convex polytope separator encloses class develop stochastic gradient descent based algorithm amenable massive datasets augment heuristic procedure avoid sub optimal local minima experimental evaluations cpm large scale datasets distinct domains mnist handwritten digit recognition text topic web security demonstrate cpm trains models faster sometimes several orders magnitude state art similar approaches kernel svm methods achieving comparable better classification performance empirical results suggest unlike prior similar approaches need control number sub classifiers sides polytope avoid overfitting
non convex robust pca propose new provable method robust pca task recover low rank matrix corrupted sparse perturbations method consists simple alternating projections onto set low rank sparse matrices intermediate noising steps prove correct recovery low rank sparse components tight recovery conditions match state art convex relaxation techniques method extremely simple implement low computational complexity times input matrix say geq method log epsilon running time rank low rank component epsilon accuracy contrast convex relaxation methods running time epsilon scalable large problem instances running time nearly matches usual pca non robust rmn log epsilon thus achieve best worlds viz low computational complexity provable recovery robust pca analysis represents instances global convergence guarantees non convex methods
provable tensor factorization missing data study problem low rank tensor factorization presence missing data ask following question many sampled entries need efficiently exactly reconstruct tensor low rank orthogonal decomposition propose novel alternating minimization based method iteratively refines estimates singular vectors show certain standard assumptions method recover mode times times dimensional rank tensor exactly log randomly sampled entries process proving result solve challenging sub problems tensors missing data first analyzing initialization step prove generalization celebrated result szemer edie spectrum random graphs next prove global convergence alternating minimization good initialization simulations suggest dependence sample size dimensionality indeed tight
probabilistic ode solvers runge kutta means runge kutta methods classic family solvers ordinary differential equations odes basis state art like numerical methods return point estimates construct family probabilistic numerical methods instead return gauss markov process defining probability distribution ode solution contrast prior work construct family posterior means match outputs runge kutta family exactly thus inheriting proven good properties remaining degrees freedom identified match runge kutta chosen posterior probability measure fits observed structure ode results shed light structure runge kutta solvers new direction provide richer probabilistic output low computational cost raise new research questions
testing unfaithful gaussian graphical models global markov property gaussian graphical models ensures graph separation implies conditional independence specifically node set graph separates nodes x_u conditionally independent x_v given x_s opposite direction need true x_u perp x_v mid x_s need imply node separator relation x_u perp x_v mid x_s called faithful paper provide characterization faithful relations provide algorithm test faithfulness based knowledge conditional relations form x_i perp x_j mid x_s
analysis learning positive unlabeled data learning classifier positive unlabeled data important class classification problems conceivable many practical applications paper first show problem solved cost sensitive learning positive unlabeled data show convex surrogate loss functions hinge loss lead wrong classification boundary due intrinsic bias problem avoided using non convex loss functions ramp loss next analyze excess risk class prior estimated data show classification accuracy sensitive class prior estimation unlabeled data dominated positive data naturally satisfied inlier based outlier detection inliers dominant unlabeled dataset finally provide generalization error bounds show equal number labeled unlabeled samples generalization error learning positive unlabeled samples worse sqrt times fully supervised case theoretical findings also validated experiments
identifying attacking saddle point problem high dimensional non convex optimization central challenge many fields science engineering involves minimizing non convex error functions continuous high dimensional spaces gradient descent quasi newton methods almost ubiquitously used perform minimizations often thought main source difficulty local methods find global minimum proliferation local minima much higher error global minimum argue based results statistical physics random matrix theory neural network theory empirical evidence deeper profound difficulty originates proliferation saddle points local minima especially high dimensional problems practical interest saddle points surrounded high error plateaus dramatically slow learning give illusory impression existence local minimum motivated arguments propose new approach second order optimization saddle free newton method rapidly escape high dimensional saddle points unlike gradient descent quasi newton methods apply algorithm deep recurrent neural network training provide numerical evidence superior optimization performance
blinded bandit learning adaptive feedback study online learning setting player temporarily deprived feedback time switches different action model emph adaptive feedback naturally occurs scenarios environment reacts player actions requires time recover stabilize algorithm switches actions motivates variant multi armed bandit problem call emph blinded multi armed bandit feedback given algorithm whenever switches arms develop efficient online learning algorithms problem prove guarantee asymptotic regret optimal algorithms standard multi armed bandit problem result stands stark contrast another recent result states adding switching cost standard multi armed bandit makes substantially harder learn provides direct comparison feedback loss contribute difficulty online learning problem also extend results general prediction framework bandit linear optimization attaining near optimal regret bounds
wild bootstrap degenerate kernel tests wild bootstrap method nonparametric hypothesis tests based kernel distribution embeddings proposed bootstrap method used construct provably consistent tests apply random processes naive permutation based bootstrap fails applies large group kernel tests based statistics degenerate null hypothesis non degenerate elsewhere illustrate approach construct sample test instantaneous independence test multiple lag independence test time series experiments wild bootstrap gives strong performance synthetic examples audio data performance benchmarking gibbs sampler code available https github com kacperchwialkowski wildbootstrap
near optimal reinforcement learning factored mdps reinforcement learning algorithm applies markov decision processes mdps suffer omega sqrt sat regret mdp elapsed time cardinalities state action spaces implies omega time guarantee near optimal policy many settings practical interest due curse dimensionality enormous learning time unacceptable establish system known emph factored mdp possible achieve regret scales polynomially number emph parameters encoding factored mdp exponentially smaller provide algorithms satisfy near optimal regret bounds context posterior sampling reinforcement learning psrl upper confidence bound algorithm ucrl factored
parallel double greedy submodular maximization many machine learning problems reduced maximization submodular functions although well understood serial setting parallel maximization submodular functions remains open area research recent results addressing monotone functions optimal algorithm maximizing general class non monotone submodular functions introduced buchbinder follows strongly serial double greedy logic program analysis work propose methods parallelize double greedy algorithm first coordination free approach emphasizes speed cost weaker approximation guarantee second concurrency control approach guarantees tight approximation quantifiable cost additional coordination reduced parallelism consequence explore trade space guaranteed performance objective optimality implement evaluate algorithms multi core hardware billion edge graphs demonstrating scalability tradeoffs approach
probabilistic framework multimodal retrieval using integrative indian buffet process propose multimodal retrieval procedure based latent feature models procedure consists nonparametric bayesian framework learning underlying semantically meaningful abstract features multimodal dataset probabilistic retrieval model allows cross modal queries extension model relevance feedback experiments multimodal datasets pascal sentence sun attribute demonstrate effectiveness proposed retrieval procedure comparison state art algorithms learning binary codes
low rank approximation lower bounds row update streams study low rank approximation streaming model rows times matrix presented time arbitrary order end stream streaming algorithm output times matrix dagger leq eps a_k a_k best rank approximation deterministic streaming algorithm liberty kdd 2013 improved analysis ghashami phillips soda 2014 provides streaming algorithm using epsilon words space natural question smaller space possible give almost matching lower bound omega epsilon bits space even randomized algorithms succeed constant probability lower bound matches upper bound ghashami phillips word size improving simple omega space lower bound
log hilbert schmidt metric positive definite operators hilbert spaces paper introduces novel mathematical computational framework namely log hilbert schmidt metric positive definite operators hilbert space generalization log euclidean metric riemannian manifold positive definite matrices infinite dimensional setting general framework applied particular compute distances covariance operators reproducing kernel hilbert space rkhs obtain explicit formulas via corresponding gram matrices empirically apply formulation task multi category image classification image represented infinite dimensional rkhs covariance operator several challenging datasets method significantly outperforms approaches based covariance matrices computed directly original input features including using log euclidean metric stein jeffreys divergences achieving new state art results
asynchronous anytime sequential monte carlo introduce new sequential monte carlo algorithm call particle cascade particle cascade asynchronous anytime alternative traditional sequential monte carlo algorithms amenable parallel distributed implementations uses barrier synchronizations leads improved particle throughput memory efficiency anytime algorithm sense run forever emit unbounded number particles keeping within fixed memory budget prove particle cascade provides unbiased marginal likelihood estimator straightforwardly plugged existing pseudo marginal methods
exploiting easy data online optimization consider problem online optimization learner chooses decision given decision set suffers loss associated decision state environment learner objective minimize cumulative regret best fixed decision hindsight past decades numerous variants considered many algorithms designed achieve sub linear regret worst case however level robustness comes cost proposed algorithms often conservative failing adapt actual complexity loss sequence often far worst case paper introduce general algorithm provided safe learning algorithm opportunistic benchmark effectively combine good worst case guarantees much improved performance easy data derive general theoretical bounds regret proposed algorithm discuss implementation wide range applications notably problem learning shifting experts recent colt open problem finally provide numerical simulations setting prediction expert advice comparisons state art
beyond disagreement based agnostic active learning study agnostic active learning goal learn classifier pre specified hypothesis class interactively label queries possible making assumptions true function generating labels main algorithms problem disagreement based active learning high label requirement margin based active learning applies fairly restricted settings major challenge find algorithm achieves better label complexity consistent agnostic setting applies general classification problems paper provide algorithm solution based novel contributions reduction consistent active learning confidence rated prediction guaranteed error novel confidence rated predictor
feature cross substitution adversarial classification success machine learning particularly supervised settings led numerous attempts apply adversarial settings spam malware detection core challenge class applications adversaries static data generators make deliberate effort evade classifiers deployed detect investigate problem modeling objectives adversaries well algorithmic problem accounting rational objective driven adversaries particular demonstrate severe shortcomings feature reduction adversarial settings using several natural adversarial objective functions observation particularly pronounced adversary able substitute across similar features example replace words synonyms replace letters words offer simple heuristic method making learning robust feature cross substitution attacks present general approach based mixed integer linear programming constraint generation implicitly trades overfitting feature selection adversarial setting using sparse regularizer along evasion model approach first method combining adversarial classification algorithm general class models adversarial classifier evasion show algorithmic approach significantly outperforms state art alternatives
latent support measure machines bag words data classification many classification problems input represented set features bag words bow representation documents support vector machines svms widely used tools classification problems performance svms generally determined whether kernel values data points defined properly however svms bow representations major weakness occurrence different semantically similar words cannot reflected kernel calculation overcome weakness propose kernel based discriminative classifier bow data call latent support measure machine latent smm latent smm latent vector associated vocabulary term document represented distribution latent vectors words appearing document represent distributions efficiently use kernel embeddings distributions hold high order moment information distributions latent smm finds separating hyperplane maximizes margins distributions different classes estimating latent vectors words improve classification performance experiments show latent smm achieves state art accuracy bow text classification robust respect hyper parameters useful visualize words
bayesian inference structured spike slab priors sparse signal recovery addresses problem solving underdetermined linear inverse problems subject sparsity constraint propose novel prior formulation structured spike slab prior allows incorporate priori knowledge sparsity pattern imposing spatial gaussian process spike slab probabilities thus prior information structure sparsity pattern encoded using generic covariance functions furthermore provide bayesian inference scheme proposed model based expectation propagation framework using numerical experiments synthetic data demonstrate benefits model
sampling problem drawing samples discrete distribution converted discrete optimization problem work show sampling continuous distribution converted optimization problem continuous space central method stochastic process recently described mathematical statistics call gumbel process present new construction gumbel process sampling practical generic sampling algorithm searches maximum gumbel process using search analyze correctness convergence time sampling demonstrate empirically makes efficient use bound likelihood evaluations closely related adaptive rejection sampling based algorithms
block coordinate descent approach large scale sparse inverse covariance estimation sparse inverse covariance estimation problem arises many statistical applications machine learning signal processing problem inverse covariance matrix multivariate normal distribution estimated assuming sparse ell_1 regularized log determinant optimization problem typically solved approximate matrices memory limitations existing algorithms unable handle large scale instances problem paper present new block coordinate descent approach solving problem large scale data sets method treats sought matrix block block using quadratic approximations show approach advantages existing methods several aspects numerical experiments synthetic real gene expression data demonstrate approach outperforms existing state art methods especially large scale problems
boosting framework grounds online learning exploiting duality boosting online learning present boosting framework proves extremely powerful thanks employing vast knowledge available online learning area using framework develop various algorithms address multiple practically theoretically interesting questions including sparse boosting smooth distribution boosting agnostic learning product generalization double projection online learning algorithms
learning optimize via information directed sampling propose information directed sampling new algorithm online optimization problems decision maker must balance exploration exploitation learning partial feedback action sampled manner minimizes ratio square expected single period regret measure information gain mutual information optimal action next observation establish expected regret bound information directed sampling applies across general class models scales entropy optimal action distribution widely studied bernoulli linear bandit models demonstrate simulation performance surpassing popular approaches including upper confidence bound algorithms thompson sampling knowledge gradient present simple analytic examples illustrating information directed sampling dramatically outperform upper confidence bound algorithms thompson sampling due way measures information gain
spectral methods indian buffet process inference indian buffet process versatile statistical tool modeling distributions binary matrices provide efficient spectral algorithm alternative costly variational bayes sampling based algorithms derive novel tensorial characterization moments indian buffet process proper applications give computationally efficient iterative inference algorithm concentration measure bounds reconstruction guarantees algorithm provides superior accuracy cheaper computation comparable variational bayesian approach number reference problems
learning mixtures ranking models work concerns learning probabilistic models ranking data heterogeneous population specific problem study learning parameters mallows mixture model despite widely studied current heuristics problem theoretical guarantees get stuck bad local optima present first polynomial time algorithm provably learns parameters mixture mallows models key component algorithm novel use tensor decomposition techniques learn top prefix rankings work even question identifiability case mixture mallows models unresolved
residual bootstrap high dimensional regression near low rank designs study residual bootstrap method context high dimensional linear regression specifically analyze distributional approximation linear contrasts top hat beta rho beta hat beta rho ridge regression estimator regression coefficients estimated via least squares classical results show consistently approximates laws contrasts provided design matrix size times relatively little work considered additional structure linear model extend validity setting asymp setting propose version resamples residuals obtained ridge regression main structural assumption design matrix nearly low rank sense singular values decay according power law profile extra technical assumptions derive simple criterion ensuring consistently approximates law given contrast specialize result study confidence intervals mean response values x_i top beta x_i top row design precisely show conditionally gaussian design near low rank structure emph simultaneously approximates laws x_i top hat beta rho beta dots result also notable imposes sparsity assumptions beta furthermore since consistency results formulated terms mallows kantorovich metric existence limiting distribution required
efficient structured matrix rank minimization study problem finding structured low rank matrices using nuclear norm regularization structure encoded linear map contrast known approaches linearly structured rank minimization use full svd resort augmented lagrangian techniques solve linear systems per iteration instead formulate problem differently amenable generalized conditional gradient method results practical improvement low per iteration computational cost numerical results show approach significantly outperforms state art competitors terms running time effectively recovering low rank solutions stochastic system realization spectral compressed sensing problems
best arm identification linear bandits study best arm identification problem linear bandit rewards arms depend linearly unknown parameter theta objective return arm largest reward characterize complexity problem introduce sample allocation strategies pull arms identify best arm fixed confidence minimizing sample budget particular show importance exploiting global linear structure improve estimate reward near optimal arms analyze proposed strategies compare empirical performance finally product analysis point connection optimality criterion used optimal experimental design
stochastic network design bidirected trees investigate problem stochastic network design bidirected trees problem underlying phenomenon behavior rumor disease starts multiple sources tree spreads directions along edges actions taken increase probability propagation edges goal maximize total amount spread away sources main result rounded dynamic programming approach leads fully polynomial time approximation scheme fptas algorithm find optimal solutions problem instance time polynomial input size algorithm outperforms competing approaches motivating problem computational sustainability remove barriers river networks restore health aquatic ecosystems
scaling importance sampling markov logic networks markov logic networks mlns weighted first order logic templates generating large ground markov networks lifted inference algorithms bring power logical inference probabilistic inference algorithms operate much possible compact first order level grounding propositionalizing mln necessary result lifted inference algorithms much scalable propositional algorithms operate directly much larger ground network unfortunately existing lifted inference algorithms suffer interrelated problems severely affects scalability practice first real world mlns complex structure unable exploit symmetries end grounding atoms grounding problem second suffer evidence problem arises evidence breaks symmetries severely diminishing power lifted inference paper address problems presenting scalable lifted importance sampling based approach never grounds full mln specifically show scale main steps importance sampling sampling proposal distribution weight computation scalable sampling achieved using informed easy sample proposal distribution derived compressed mln representation fast weight computation achieved visiting small subset sampled groundings formula instead possible groundings show new algorithm yields asymptotically unbiased estimate experiments several mlns clearly demonstrate promise approach
active regression stratification propose new active learning algorithm parametric linear regression random design provide finite sample convergence guarantees general distributions misspecified model first active learner setting provably improve passive learning unlike learning settings classification regression passive learning rate epsilon cannot general improved upon nonetheless called constant rate convergence characterized distribution dependent risk improved many cases given distribution achieving optimal risk requires prior knowledge distribution following stratification technique advocated monte carlo function integration active learner approaches optimal risk using piecewise constant approximations
multi resolution cascades multiclass object detection algorithm learning fast multiclass object detection cascades introduced produces multi resolution mres cascades whose early stages binary target non target detectors eliminate false positives late stages multiclass classifiers finely discriminate target classes middle stages intermediate numbers classes determined data driven manner mres structure achieved new structurally biased boosting algorithm sbboost sbbost extends previous multiclass boosting approaches whose boosting mechanisms shown implement complementary data driven biases standard bias towards examples difficult classify bias towards difficult classes shown structural biases implemented generalizing class based bias encourage desired mres structure accomplished generalized definition multiclass margin includes set bias parameters sbboost boosting algorithm maximization margin also interpreted standard multiclass boosting algorithm augmented margin thresholds cost sensitive boosting algorithm costs defined bias parameters stage adaptive bias policy introduced determine bias parameters data driven manner shown produce mres cascades high detection rate computationally efficient experiments multiclass object detection show improved performance previous solutions
exponential concentration density functional estimator analyse plug estimator large class integral functionals continuous probability densities class includes important families entropy divergence mutual information conditional versions densities dimensional unit cube lie beta holder smoothness class prove estimator converges rate beta furthermore prove estimator obeys exponential concentration inequality mean whereas previous related results bounded expected error estimators finally demonstrate bounds case conditional renyi mutual information
bayes adaptive simulation based search value function approximation bayes adaptive planning offers principled solution exploration exploitation trade model uncertainty finds optimal policy belief space explicitly accounts expected effect future rewards reductions uncertainty however bayes adaptive solution typically intractable domains large continuous state spaces present tractable method approximating bayes adaptive solution combining simulation based search novel value function approximation technique generalises belief space method outperforms prior approaches discrete bandit tasks simple continuous navigation control tasks
global sensitivity analysis map inference graphical models study sensitivity map configuration discrete probabilistic graphical model respect perturbations parameters perturbations global sense simultaneous perturbations parameters chosen subset allowed main contribution exact algorithm check whether map configuration robust respect given perturbations complexity essentially obtaining map configuration promptly used minimal effort use algorithm identify largest global perturbation induce change map configuration successfully apply robustness measure practical scenarios prediction facial action units posed images classification multiple real public data sets strong correlation proposed robustness measure accuracy verified scenarios
hamming ball auxiliary sampling factorial hidden markov models introduce novel sampling algorithm markov chain monte carlo based bayesian inference factorial hidden markov models algorithm based auxiliary variable construction restricts model space allowing iterative exploration polynomial time sampling approach overcomes limitations common conditional gibbs samplers use asymmetric updates become easily trapped local modes instead method uses symmetric moves allows joint updating latent sequences improves mixing illustrate application approach simulated real data example
active learning best response dynamics consider setting low power distributed sensors making highly noisy measurements unknown target function center wants accurately learn function querying small number sensors ordinarily would impossible due high noise rate question address whether local communication among sensors together natural best response dynamics appropriately ned game denoise system without destroying true signal allow center succeed small number active queries prove positive negative results denoising power several natural dynamics also show experimentally combined recent agnostic active learning algorithms process achieve low error queries performing substantially better active passive learning without denoising dynamics well passive learning denoising
fast multivariate spatio temporal analysis via low rank tensor learning accurate efficient analysis multivariate spatio temporal data critical climatology geology sociology applications existing models usually assume simple inter dependence among variables space time computationally expensive propose unified low rank tensor learning framework multivariate spatio temporal analysis conveniently incorporate different properties spatio temporal data spatial clustering shared structure among variables demonstrate general framework applied cokriging forecasting tasks develop efficient greedy algorithm solve resulting optimization problem convergence guarantee conduct experiments synthetic datasets real application datasets demonstrate method significantly faster existing methods also achieves lower estimation error
combinatorial pure exploration multi armed bandits study combinatorial pure exploration cpe problem stochastic multi armed bandit setting learner explores set arms objective identifying optimal member emph decision class collection subsets arms certain combinatorial structures size subsets matchings spanning trees paths etc cpe problem represents rich class pure exploration tasks covers many existing models also novel cases object interest non trivial combinatorial structure paper provide series results general cpe problem present general learning algorithms work decision classes admit offline maximization oracles fixed confidence fixed budget settings prove problem dependent upper bounds algorithms analysis exploits combinatorial structures decision classes introduces new analytic tool also establish general problem dependent lower bound cpe problem results show proposed algorithms achieve optimal sample complexity within logarithmic factors many decision classes addition applying results back problems top arms identification multiple bandit best arms identification recover best available upper bounds constant factors partially resolve conjecture lower bounds
new rules domain independent lifted map inference lifted inference algorithms probabilistic first order logic frameworks markov logic networks mlns received significant attention recent years algorithms use called lifting rules identify symmetries first order representation reduce inference problem large probabilistic model inference problem much smaller model paper present new lifting rules enable fast map inference large class mlns first rule uses concept single occurrence equivalence class logical variables define paper rule states map assignment mln recovered much smaller mln logical variable single occurrence equivalence class replaced constant object domain variable second rule states safely remove subset formulas mln equivalence classes variables remaining mln single occurrence formulas subset tautology evaluate true extremes assignments identical truth value groundings predicate prove new rules sound demonstrate via detailed experimental evaluation approach superior terms scalability map solution quality state art approaches
deep nets really need deep currently deep neural networks state art problems speech recognition computer vision paper empirically demonstrate shallow feed forward nets learn complex functions previously learned deep nets achieve accuracies previously achievable deep models moreover cases shallow nets learn deep functions using number parameters original deep models timit phoneme recognition cifar image recognition tasks shallow nets trained perform similarly complex well engineered deeper convolutional models
positive curvature hamiltonian monte carlo jacobi metric introduced mathematical physics used analyze hamiltonian monte carlo hmc geometrical setting step hmc corresponds geodesic riemannian manifold jacobi metric calculation sectional curvature hmc manifold allows see positive cases sampling high dimensional multivariate gaussian show positive curvature used prove theoretical concentration results hmc markov chains
optimizing energy production using policy search predictive state representations consider challenging practical problem optimizing power production complex hydroelectric power plants involves control continuous action variables uncertainty amount water inflows variety constraints need satisfied propose policy search based approach coupled predictive modelling address problem approach key advantages compared alternatives dynamic programming policy representation search algorithm conveniently incorporate domain knowledge resulting policies easy interpret algorithm naturally parallelizable algorithm obtains policy outperforms solution found dynamic programming quantitatively qualitatively
multi scale graphical models spatio temporal processes learning dependency structure spatially distributed observations spatio temporal process important problem many fields geology geophysics atmospheric sciences oceanography etc however estimation systems complicated fact exhibit dynamics multiple scales space time arising due combination diffusion convection advection show time series graphical models based vector auto regressive processes inef cient capturing multi scale structure paper present hierarchical graphical model physically derived priors better represents multi scale character dynamical systems also propose algorithms ciently estimate interaction structure data demonstrate results general class problems arising exploration geophysics discovering graphical structure physically meaningful provide evidence advantages alternative approaches
altitude training strong bounds single layer dropout dropout training originally designed deep neural networks successful high dimensional single layer natural language tasks paper proposes theoretical explanation phenomenon show generative poisson topic model long documents dropout training improves exponent generalization bound empirical risk minimization dropout achieves gain much like marathon runner practices altitude classifier learns perform reasonably well training examples artificially corrupted dropout well uncorrupted test set also show similar conditions dropout preserves bayes decision boundary therefore induce minimal bias high dimensions
unsupervised transcription piano music present new probabilistic model transcribing piano music audio symbolic form model reflects process discrete musical events give rise acoustic signals superimposed produce observed data result inference procedure model naturally resolves source separation problem introduced piano polyphony order adapt properties new instrument acoustic environment transcribed learn recording specific spectral profiles temporal envelopes unsupervised fashion system outperforms best published approaches standard piano transcription task achieving relative gain note onset real piano audio
raam benefits robustness approximating aggregated mdps reinforcement learning describe use robust markov decision processes value function approximation state aggregation robustness serves reduce sensitivity approximation error sub optimal policies comparison classical methods fitted value iteration results reducing bounds gamma discounted infinite horizon performance loss factor gamma preserving polynomial time computational complexity experimental results show using robust representation significantly improve solution quality minimal additional computational cost
convex deep learning via normalized kernels deep learning long standing pursuit machine learning recently hampered unreliable training methods discovery improved heuristics embedded layer training complementary research strategy develop alternative modeling architectures admit efficient training methods expanding range representable structures toward deep models paper develop new architecture nested nonlinearities allows arbitrarily deep compositions trained global optimality approach admits parametric nonparametric forms use normalized kernels represent latent layer outcome fully convex formulation able capture compositions trainable nonlinear layers arbitrary depth
distance based network recovery feature correlation present inference method gaussian graphical models pairwise distances objects observed formally problem estimating covariance matrix mahalanobis distances dmh object lives latent feature space solve problem fully bayesian fashion integrating matrix normal likelihood matrix gamma prior resulting matrix posterior enables network recovery even strongly correlated features hereby generalize tiwnet assumes euclidean distances strict feature independence spite greatly increased flexibility model neither loses statistical power entails computational cost argue extension highly relevant yields significantly better results synthetic real world experiments successfully demonstrated network biological pathways cancer patients
tree structured gaussian process approximations gaussian process regression accelerated constructing small pseudo dataset summarise observed data idea sits heart many approximation schemes approach requires number pseudo datapoints scaled range input space accuracy approximation maintained presents problems time series settings spatial datasets large numbers pseudo datapoints required since computation typically scales quadratically pseudo dataset size paper devise approximation whose complexity grows linearly number pseudo datapoints achieved imposing tree chain structure pseudo datapoints calibrating approximation using kullback leibler minimisation inference learning performed efficiently using gaussian belief propagation algorithm demonstrate validity approach set challenging regression tasks including missing data imputation audio spatial datasets trace speed accuracy trade new method show frontier dominates obtained large number existing approximation techniques
state space model decoding auditory attentional modulation meg competing speaker environment humans able segregate auditory objects complex acoustic scene interplay bottom feature extraction top selective attention brain detailed mechanism underlying process largely unknown ability mimic procedure important problem artificial intelligence computational neuroscience consider problem decoding attentional state listener competing speaker environment magnetoencephalographic meg recordings human brain develop behaviorally inspired state space model account modulation meg respect attentional state listener construct decoder based maximum posteriori map estimate state parameters via expectation maximization algorithm resulting decoder able track attentional modulation listener multi second resolution using envelopes speech streams covariates present simulation studies well application real meg data human subjects results reveal proposed decoder provides substantial gains terms temporal resolution complexity decoding accuracy
optimizing measures cost sensitive classification present theoretical analysis measures binary multiclass multilabel classification performance measures non linear many scenarios pseudo linear functions per class false negative false positive rate based observation present general reduction measure maximization cost sensitive classification unknown costs propose algorithm provable guarantees obtain approximately optimal classifier measure solving series cost sensitive classification problems strength analysis valid dataset class classifiers extending existing theoretical results measures asymptotic nature present numerical experiments illustrate relative importance cost asymmetry thresholding learning linear classifiers various measure optimization tasks
map marginals variational inference bayesian submodular models submodular optimization found many applications machine learning beyond carry first systematic investigation inference probabilistic models defined submodular functions generalizing regular pairwise mrfs determinantal point processes particular present field variational approach general log submodular log supermodular distributions based sub supergradients obtain lower upper bounds log partition function enables compute probability intervals marginals conditionals marginal likelihoods also obtain fully factorized approximate posteriors computational cost ordinary submodular optimization framework results convex problems optimizing differentials submodular functions show optimally solve provide theoretical guarantees approximation quality respect curvature function establish natural relations variational approach classical mean field method lastly empirically demonstrate accuracy inference scheme several submodular models
information theoretic limits learning ising models provide general framework computing lower bounds sample complexity recovering underlying graphs ising models given samples recent results specific graph classes involve fairly extensive technical arguments specialized specific graph class contrast isolate key graph structural ingredients used specify sample complexity lower bounds presence structural properties makes graph class hard learn derive corollaries main result recover existing recent results also provide lower bounds novel graph classes considered previously also extend framework random graph setting derive corollaries erdos renyi graphs certain dense setting
transferable features deep neural networks many deep neural networks trained natural images exhibit curious phenomenon common first layer learn features similar gabor filters color blobs first layer features appear specific particular dataset task general applicable many datasets tasks features must eventually transition general specific last layer network transition studied extensively paper experimentally quantify generality versus specificity neurons layer deep convolutional neural network report surprising results transferability negatively affected distinct issues specialization higher layer neurons original task expense performance target task expected optimization difficulties related splitting networks adapted neurons expected example network trained imagenet demonstrate either issues dominate depending whether features transferred bottom middle top network also document transferability features decreases distance base task target task increases transferring features even distant tasks better using random features final surprising result initializing network transferred features almost number layers produce boost generalization lingers even fine tuning target dataset
deterministic symmetric positive semidefinite matrix completion consider problem recovering symmetric positive semidefinite spsd matrix subset entries possibly corrupted noise contrast previous matrix recovery work drop assumption random sampling entries favor deterministic sampling principal submatrices matrix develop set sufficient conditions recovery spsd matrix set principal submatrices present necessity results based set conditions develop algorithm exactly recover matrix conditions met proposed algorithm naturally generalized problem noisy matrix recovery provide worst case bound reconstruction error scenario finally demonstrate algorithm utility noiseless noisy simulated datasets
spectral methods supervised topic models supervised topic models simultaneously model latent topic structure large collections documents response variable associated document existing inference methods based either variational approximation monte carlo sampling paper presents novel spectral decomposition algorithm recover parameters supervised latent dirichlet allocation slda models spectral slda algorithm provably correct computationally efficient prove sample complexity bound subsequently derive sufficient condition identifiability slda thorough experiments diverse range synthetic real world datasets verify theory demonstrate practical effectiveness algorithm
spectral methods meet provably optimal algorithm crowdsourcing dawid skene estimator widely used inferring true labels noisy labels provided non expert crowdsourcing workers however since estimator maximizes non convex log likelihood function hard theoretically justify performance paper propose stage efficient algorithm multi class crowd labeling problems first stage uses spectral method obtain initial estimate parameters second stage refines estimation optimizing objective function dawid skene estimator via algorithm show algorithm achieves optimal convergence rate logarithmic factor conduct extensive experiments synthetic real datasets experimental results demonstrate proposed algorithm comparable accurate empirical approach outperforming several recently proposed methods
hard mdp distribution norm rescue reinforcement learning state art algorithms require large number samples per state action pair estimate transition kernel many problems good approximation needed instance state action pair transit states value learning cdot accurately irrelevant support matters paper aims capturing behavior defining novel hardness measure markov decision processes mdps call distribution norm distribution norm measure defined mean functions standard variation respect first provide concentration inequality dual distribution norm allows replace generic loose cdot concentration inequalities used previous analysis algorithms benefit new hardness measure show several common benchmarks low hardness measured using new norm distribution norm captures finer properties number states diameter used assess difficulty mdps
limits squared euclidean distance regularization simplest loss functions considered machine learning square loss logistic loss hinge loss common family algorithms including gradient descent without weight decay always predict linear combination past instances give random construction sets examples target linear weight vector trivial learn algorithm family drastically sub optimal lower bound latter algorithms holds even algorithms enhanced arbitrary kernel function type result known square loss however develop new techniques let prove hardness results loss function satisfying minimal requirements loss function including listed also show algorithms regularize squared euclidean distance easily confused random features finally conclude discussing related open problems regarding feed forward neural networks conjecture hardness results hold training algorithm based squared euclidean distance regularization back propagation weight decay heuristic
stochastic mixability fast rates empirical risk minimization erm fundamental learning rule statistical learning problems data generated according unknown distribution mathsf returns hypothesis chosen fixed class mathcal small loss ell parametric setting depending upon ell mathcal mathsf erm slow sqrt fast rates convergence excess risk function sample size exist several results give sufficient conditions fast rates terms joint properties ell mathcal mathsf margin condition bernstein condition non statistical prediction expert advice setting analogous slow fast rate phenomenon entirely characterized terms mixability loss ell role mathcal mathsf notion stochastic mixability builds bridge models learning reducing classical mixability special case present paper presents direct proof fast rates erm terms stochastic mixability ell mathcal mathsf provides new insight fast rates phenomenon proof exploits old result kemperman solution general moment problem also show partial converse suggests characterization fast rates erm terms stochastic mixability possible
signal aggregate constraints additive factorial hmms application energy disaggregation blind source separation problems difficult inherently unidentifiable yet entire goal identify meaningful sources introduce way incorporating domain knowledge problem called signal aggregate constraints sacs sacs encourage total signal unknown sources close specified value based observation total signal often varies widely across unknown sources often good idea total values expect incorporate sacs additive factorial hidden markov model afhmm formulate energy disaggregation problems mixture signal assumed observed convex quadratic program approximate inference employed recovering source signals real world energy disaggregation data set show use sacs dramatically improves original afhmm significantly improves recent state art approach
sparse random feature algorithm coordinate descent hilbert space paper propose sparse random feature algorithm learns sparse non linear predictor minimizing ell_1 regularized objective function hilbert space induced kernel function interpreting algorithm randomized coordinate descent infinite dimensional space show proposed approach converges solution comparable within eps precision exact kernel method drawing eps number random features contrasted eps type convergence achieved monte carlo analysis current random feature literature experiments sparse random feature algorithm obtains sparse solution requires less memory prediction time maintains comparable performance tasks regression classification meantime approximate solver infinite dimensional ell_1 regularized problem randomized approach converges better solution boosting approach greedy step boosting cannot performed exactly
simultaneous model selection optimization parameter free stochastic learning stochastic gradient descent algorithms training linear kernel predictors gaining importance thanks scalability various methods proposed speed convergence model selection phase often ignored fact theoretical works time assumptions made example prior knowledge norm optimal solution practical world validation methods remain viable approach paper propose new kernel based stochastic gradient descent algorithm performs model selection training parameters tune form cross validation algorithm builds recent advancement online learning theory unconstrained settings estimate time right regularization data dependent way optimal rates convergence proved standard smoothness assumptions target function well preliminary empirical results
multi scale spectral decomposition massive graphs computing dominant eigenvalues eigenvectors massive graphs key operation numerous machine learning applications however popular solvers suffer slow convergence especially reasonably large paper propose analyze novel multi scale spectral decomposition method mseigs first clusters graph smaller clusters whose spectral decomposition computed efficiently independently show theoretically well empirically union cluster subspaces significant overlap dominant subspace original graph provided graph clustered appropriately thus eigenvectors clusters serve good initializations block lanczos algorithm used compute spectral decomposition original graph use hierarchical clustering speed computation adopt fast early termination strategy compute quality approximations method outperforms widely used solvers terms convergence speed approximation quality furthermore method naturally parallelizable exhibits significant speedups shared memory parallel settings example graph million nodes billion edges mseigs takes less hours single core machine randomized svd takes hours obtain similar approximation top eigenvectors using cores reduce time less minutes
robust classification sample selection bias many important machine learning applications source distribution used estimate probabilistic classifier differs target distribution classifier used make predictions due asymptotic properties sample reweighted loss minimization commonly employed technique deal difference however given finite amounts labeled source data technique suffers significant estimation errors settings large sample selection bias develop framework robustly learning probabilistic classifier adapts different sample selection biases using minimax estimation formulation approach requires accurate estimates statistics source distribution otherwise robust possible unknown properties conditional label distribution except explicit generalization assumptions incorporated demonstrate behavior effectiveness approach synthetic uci binary classification tasks
multivariate divergence estimation confidence problem divergence estimation important fields machine learning information theory statistics several divergence estimators exist relatively known convergence properties particular even estimators whose mse convergence rates known asymptotic distributions unknown establish asymptotic normality recently proposed ensemble estimator divergence distributions finite number samples estimator mse convergence rate simple implement performs well high dimensions theory enables perform divergence based inference tasks testing equality pairs distributions based empirical samples experimentally validate theoretical results illustration use empirically bound best achievable classification error
decomposing parameter estimation problems propose technique decomposing parameter learning problem bayesian networks independent learning problems technique applies incomplete datasets exploits variables either hidden observed given dataset show empirically proposed technique lead orders magnitude savings learning time explain analytically empirically reasons behind reported savings compare proposed technique related ones sometimes used inference algorithms
permutation diffusion maps pdm application image association problem computer vision consistently matching keypoints across images related problem finding clusters nearby images critical components various tasks computer vision including structure motion sfm unfortunately occlusion large repetitive structures tend mislead currently used matching algorithms leading characteristic pathologies final output paper introduce new method permutations diffusion maps pdm solve matching problem well related new affinity measure derived using ideas harmonic analysis symmetric group show using preprocessing step existing sfm pipelines pdm greatly improve reconstruction quality difficult datasets
deep convolutional neural network image deconvolution many fundamental image related problems involve deconvolution operators real blur degradation seldom complies deal linear convolution model due camera noise saturation image compression name instead perfectly modeling outliers rather challenging generative model perspective develop deep convolutional neural network capture characteristics degradation note directly applying existing deep neural networks produce reasonable results solution establish connection traditional optimization based schemes neural network architecture novel separable structure introduced reliable support robust deconvolution artifacts network contains submodules trained supervised manner proper initialization yield decent performance non blind image deconvolution compared previous generative model based methods
communication cost distributed statistical estimation dimensionality explore connection dimensionality communication cost distributed learning problems specifically study problem estimating mean vectheta unknown dimensional gaussian distribution distributed setting problem samples unknown distribution distributed among different machines goal estimate mean vectheta optimal minimax rate communicating bits possible show setting communication cost scales linearly number dimensions needs deal different dimensions individually applying result previous lower bounds dimension interactive setting cite zdjw13 improved bounds simultaneous setting prove new lower bounds omega log omega bits communication needed achieve minimax squared loss interactive simultaneous settings respectively complement also demonstrate interactive protocol achieving minimax squared loss bits communication improves upon simple simultaneous protocol logarithmic factor given strong lower bounds general setting initiate study distributed parameter estimation problems structured parameters specifically parameter promised sparse show simple thresholding based protocol achieves squared loss saving factor communication conjecture tradeoff communication squared loss demonstrated protocol essentially optimal logarithmic factor
rates convergence nearest neighbor classification analyze behavior nearest neighbor classification metric spaces provide finite sample distribution dependent rates convergence minimal assumptions general existing bounds enable product establish universal consistency nearest neighbor broader range data spaces previously known illustrate upper lower bounds introducing new smoothness class customized nearest neighbor classification find instance tsybakov margin condition convergence rate nearest neighbor matches recently established lower bounds nonparametric classification
learning neural network policies guided policy search unknown dynamics present policy search method uses iteratively refitted local linear models optimize trajectory distributions large continuous problems trajectory distributions used within framework guided policy search learn policies arbitrary parameterization method fits time varying linear dynamics models speed learning rely learning global model difficult dynamics complex discontinuous show hybrid approach requires many fewer samples model free methods handle complex nonsmooth dynamics pose challenge model based techniques present experiments showing method used learn complex neural network policies successfully execute simulated robotic manipulation tasks partially observed environments numerous contact discontinuities underactuation
constrained convex minimization via model based excessive gap introduce model based excessive gap technique analyze first order primal dual methods constrained convex minimization result construct first order primal dual methods optimal convergence rates primal objec tive residual primal feasibility gap iterates separately dual smoothing prox center selection strategy framework subsumes augmented lagrangian alternating direction dual fast gradient methods special cases rates apply
multi class deep boosting present new ensemble learning algorithms multi class classification algorithms use base classifier set family deep decision trees rich complex families yet benefit strong generalization guarantees give new data dependent learning bounds convex ensembles multi class classification setting expressed terms rademacher complexities sub families composing base classifier set mixture weight assigned sub family bounds finer existing ones thanks improved dependency number classes crucially virtue favorable complexity term expressed average rademacher complexities based ensemble mixture weights introduce discuss several new multi class ensemble algorithms benefiting guarantees prove positive results consistency several report results experiments showing performance compares favorably multi class versions adaboost logistic regression regularized counterparts
spectral learning mixture hidden markov models paper propose learning approach mixture hidden markov models mhmm based method moments mom computational advantages mom make mhmm learning amenable large data sets possible directly learn mhmm existing learning approaches mainly due permutation ambiguity estimation process show possible resolve ambiguity using spectral properties global transition matrix even presence estimation noise demonstrate validity approach synthetic real data
efficient sampling learning sparse additive models high dimensions consider problem learning sparse additive models functions form vecx sum_ phi_ x_l vecx matr point queries unknown subset coordinate variables abs assuming phi_l smooth propose set points sample efficient randomized algorithm recovers textit uniform approximation unknown phi_l provide rigorous theoretical analysis scheme along sample complexity bounds algorithm utilizes recent results compressive sensing theory along novel convex quadratic program recovering robust uniform approximations univariate functions point queries corrupted arbitrary bounded noise lastly theoretically analyze impact noise either arbitrary bounded stochastic performance algorithm
robust logistic regression classification consider logistic regression arbitrary outliers covariate matrix propose new robust logistic regression algorithm called rolr estimates parameter simple linear programming procedure prove rolr robust constant fraction adversarial outliers best knowledge first result estimating logistic regression model covariate matrix corrupted performance guarantees besides regression apply rolr solving binary classification problems fraction training samples corrupted
almost label cry learning label proportions llp objective learn supervised classifier instead labels label proportions bags observations known setting broad practical relevance particular privacy preserving data processing first show mean operator statistic aggregates labels minimally sufficient minimization many proper scoring losses linear kernelized classifiers without using labels provide fast learning algorithm estimates mean operator via manifold regularizer guaranteed approximation bounds present iterative learning algorithm uses initialization ground algorithm rademacher style generalization bounds fit llp setting introducing generalization rademacher complexity label proportion complexity measure latter algorithm optimizes tractable bounds corresponding bag empirical risk experiments provided fourteen domains whose size ranges 300k observations display algorithms scalable tend consistently outperform state art llp moreover many cases algorithms compete percents auc away oracle learns knowing labels largest domains half dozen proportions suffice roughly 40k times less total number labels
near optimal sample compression nearest neighbors present first sample compression algorithm nearest neighbors non trivial performance guarantees complement guarantees demonstrating almost matching hardness lower bounds show bound nearly optimal result yields new insight margin based nearest neighbor classification metric spaces allows significantly sharpen simplify existing bounds encouraging empirical results also presented
learning search branch bound algorithms branch bound widely used method combinatorial optimization including mixed integer programming structured prediction map inference work focused developing problem specific techniques little known systematically design node searching strategy branch bound tree address key challenge learning adaptive node searching order class problem solvable branch bound strategies learned imitation learning apply algorithm linear programming based branch bound solving mixed integer programs mip compare method fastest open source solvers scip efficient commercial solver gurobi demonstrate approach achieves better solutions faster mip libraries
efficient minimax signal detection graphs several problems network intrusion community detection disease outbreak described observations attributed nodes edges graph applications presence intrusion community disease outbreak characterized novel observations unknown connected subgraph problems formulated terms optimization suitable objectives connected subgraphs problem generally computationally difficult overcome combinatorics connectivity embedding connected subgraphs linear matrix inequalities lmi computationally efficient tests realized optimizing convex objective functions subject lmi constraints prove means novel euclidean embedding argument tests minimax optimal exponential family distributions lattices show internal conductance connected subgraph family plays fundamental role characterizing detectability
projective dictionary pair learning pattern classification discriminative dictionary learning widely studied various pattern classification problems existing methods aim learn synthesis dictionary represent input signal enforcing representation coefficients representation residual discriminative however ell_0 ell_1 norm sparsity constraint representation coefficients adopted many methods makes training testing phases time consuming propose new discriminative framework namely projective dictionary pair learning dpl learns synthesis dictionary analysis dictionary jointly achieve goal signal representation discrimination compared conventional methods proposed dpl method greatly reduce time complexity training testing phases also lead competitive accuracies variety visual classification tasks
complete variational tracker introduce novel probabilistic tracking algorithm incorporates combinatorial data association constraints model based track management using variational bayes use bethe entropy approximation incorporate data association constraints often ignored previous probabilistic tracking algorithms noteworthy aspects method include model based mechanism replace heuristic logic typically used initiate destroy tracks assignment posterior linear computation cost window length opposed exponential scaling previous map based approaches demonstrate applicability method radar tracking computer vision problems
deep joint task learning generic object extraction paper investigates extract objects interest without relying hand craft features sliding windows approaches aims jointly solve sub tasks rapidly localizing salient objects images accurately segmenting objects based localizations present general joint task learning framework task either object localization object segmentation tackled via multi layer convolutional neural network networks work collaboratively boost performance particular propose incorporate latent variables bridging networks joint optimization manner first network directly predicts positions scales salient objects raw images latent variables adjust object localizations feed second network produces pixelwise object masks type method studied joint optimization iterating steps using networks estimates latent variables employing mcmc based sampling method optimizes parameters networks unitedly via back propagation fixed latent variables extensive experiments demonstrate joint learning framework significantly outperforms state art approaches accuracy efficiency 1000 times faster competing approaches
multi step stochastic admm high dimensions applications sparse optimization matrix decomposition paper consider multi step version stochastic admm method efficient guarantees high dimensional problems first analyze simple setting optimization problem consists loss function single regularizer sparse optimization extend multi block setting multiple regularizers multiple variables matrix decomposition sparse low rank components sparse optimization problem method achieves minimax rate log sparse problems dimensions steps thus unimprovable method constant factors matrix decomposition problem general loss function analyze multi step admm multiple blocks establish rate efficient scaling size matrix grows natural noise models independent noise convergence rate minimax optimal thus establish tight convergence guarantees multi block admm high dimensions experiments show sparse optimization matrix decomposition problems algorithm outperforms state art methods
joint training convolutional network graphical model human pose estimation paper proposes new hybrid architecture consists deep convolutional network markov random field show architecture successfully applied challenging problem articulated human pose estimation monocular images architecture exploit structural domain constraints geometric relationships body joint locations show joint training model paradigms improves performance allows significantly outperform existing state art techniques
streaming memory limited algorithms community detection paper consider sparse networks consisting finite number non overlapping communities disjoint clusters higher density within clusters across clusters intra inter cluster edge densities vanish size graph grows large making cluster reconstruction problem nosier hence difficult solve interested scenarios network size large adjacency matrix graph hard manipulate store data stream model columns adjacency matrix revealed sequentially constitutes natural framework setting model develop novel clustering algorithms extract clusters asymptotically accurately first algorithm offline needs store keep assignments nodes clusters requires memory scales linearly network size second algorithm online classify node corresponding column revealed discard information algorithm requires memory growing sub linearly network size construct efficient streaming memory limited clustering algorithms first address problem clustering partial information small proportion columns adjacency matrix observed develop setting new spectral algorithm independent interest
filtering approach stochastic variational inference stochastic variational inference svi uses stochastic optimization scale bayesian computation massive data present alternative perspective svi approximate parallel coordinate ascent svi trades bias variance step close unknown true coordinate optimum given batch variational bayes define model automate process model infers location next optimum sequence noisy realizations consequence construction update variational parameters using bayes rule rather hand crafted optimization schedule model kalman filter procedure recover original svi algorithm svi adaptive steps also encode additional assumptions model heavy tailed noise algorithm outperforms original svi schedule state art adaptive svi algorithm diverse domains
extracting certainty uncertainty transductive pairwise classification pairwise similarities work study problem transductive pairwise classification pairwise similarities footnote pairwise similarities usually derived side information instead underlying class labels goal transductive pairwise classification pairwise similarities infer pairwise class relationships refer pairwise labels examples given subset class relationships small set examples refer labeled examples propose simple yet effective algorithm consists simple steps first step complete sub matrix corresponding labeled examples second step reconstruct label matrix completed sub matrix provided similarity matrix analysis exhibits several mild preconditions recover label matrix small error top eigen space corresponds largest eigenvalues similarity matrix covers well column space label matrix subject low coherence number observed pairwise labels sufficiently enough demonstrate effectiveness proposed algorithm several experiments
recurrent models visual attention applying convolutional neural networks large images computationally expensive amount computation scales linearly number image pixels present novel recurrent neural network model capable extracting information image video adaptively selecting sequence regions locations processing selected regions high resolution like convolutional neural networks proposed model degree translation invariance built amount computation performs controlled independently input image size model non differentiable trained using reinforcement learning methods learn task specific policies evaluate model several image classification tasks significantly outperforms convolutional neural network baseline cluttered images dynamic visual control problem learns track simple object without explicit training signal
making pairwise binary graphical models attractive computing partition function normalizing constant given pairwise binary graphical model hard general result partition function typically estimated approximate inference algorithms belief propagation tree reweighted belief propagation trbp former provides reasonable estimates practice convergence issues later better convergence properties typically provides poorer estimates work propose novel scheme better convergence properties provably provides better partition function estimates many instances trbp particular given arbitrary pairwise binary graphical model construct specific attractive cover explore properties special cover show used construct algorithm desired properties
causal strategic inference networked microfinance economies performing interventions major challenge economic policy making propose emph causal strategic inference framework conducting interventions apply large networked microfinance economies basic solution platform consists modeling microfinance market networked economy learning parameters model real world microfinance data designing algorithms various computational problems question adopt nash equilibrium solution concept model special case model show equilibrium point always exists equilibrium interest rates unique general case give constructive proof existence equilibrium point empirical study based microfinance data bangladesh bolivia use first learn models show causal strategic inference assist policy makers evaluating outcomes various types interventions removing loss making bank market imposing interest rate cap subsidizing banks
beyond birkhoff polytope convex relaxations vector permutation problems birkhoff polytope convex hull set permutation matrices represented using theta variables constraints frequently invoked formulating relaxations optimization problems permutations using recent construction goemans 2010 show optimizing convex hull permutation vectors permutahedron reduce number variables constraints theta log theory theta log practice modify recent convex formulation sum problem introduced fogel 2013 use polytope demonstrate attain results similar quality significantly less computational time large knowledge first usage goemans compact formulation permutahedron convex optimization problem also introduce simpler regularization scheme convex formulation sum problem yields good empirical results
self paced learning diversity self paced learning spl recently proposed learning regime inspired learning process humans animals gradually incorporates easy complex samples training existing methods limited ignore important aspect learning diversity incorporate information propose approach called self paced learning diversity spld formalizes preference easy diverse samples general regularizer regularization term independent learning objective thus easily generalized various learning tasks albeit non convex optimization variables included spld regularization term sample selection globally solved linearithmic time demonstrate method significantly outperforms conventional spl real world datasets specifically spld achieves best map far reported literature hollywood2 olympic sports datasets
universal option models consider problem learning models options real time abstract planning setting reward functions specified time expected returns must efficiently computed introduce new model option independent reward function called universal option model uom prove uom option construct traditional option model given reward function option conditional return computed directly single dot product uom reward function extend uom linear function approximation show gives solution option returns value functions policies options provide stochastic approximation algorithm incrementally learning uoms data prove consistency demonstrate method domains first domain document recommendation user query defines new reward function document relevance expected return simulated random walk document references second domain real time strategy game controller must select best game unit accomplish dynamically specified tasks experiments show uoms substantially efficient evaluating option returns policies previously known methods
bayesian sampling using stochastic gradient thermostats dynamics based sampling methods hybrid monte carlo hmc langevin dynamics commonly used sample target distributions recently approaches combined stochastic gradient techniques increase sampling efficiency dealing large datasets outstanding problem approach stochastic gradient introduces unknown amount noise prevent proper sampling discretization remedy problem show leverage small number additional variables order stabilize momentum fluctuations induced unknown noise method inspired idea thermostat statistical physics justified general theory
multitask learning meets tensor factorization task imputation via convex optimization study multitask learning problem task parametrized weight vector indexed pair indices consumer time weight vectors collected tensor multilinear rank tensor controls amount sharing information among tasks types convex relaxations recently proposed tensor multilinear rank however argue optimal context multitask learning dimensions multilinear rank typically heterogeneous propose new norm call scaled latent trace norm analyze excess risk norms results apply various settings including matrix tensor completion multitask learning multilinear multitask learning theory experiments support advantage new norm tensor equal sized priori know mode low rank
spike frequency adaptation implements anticipative tracking continuous attractor neural networks extract motion information brain needs compensate time delays ubiquitous neural signal transmission processing propose simple yet effective mechanism implement anticipative tracking neural systems proposed mechanism utilizes property spike frequency adaptation sfa feature widely observed neuronal responses employ continuous attractor neural networks canns model describe tracking behaviors neural systems incorporating sfa cann exhibits intrinsic mobility manifested ability cann hold self sustained travelling waves tracking moving stimulus interplay external drive intrinsic mobility network determines tracking performance interestingly find regime anticipation effectively coincides regime intrinsic speed travelling wave exceeds external drive depending sfa amplitudes network achieve either perfect tracking lag input perfect anticipative tracking constant leading time input model successfully reproduces experimentally observed anticipative tracking behaviors sheds light understanding brain processes motion information timely manner
statistical model tensor pca consider principal component analysis problem large tensors arbitrary order single spike rank plus noise model hand use information theory recent results probability theory establish necessary sufficient conditions principal component estimated using unbounded computational resources turns possible soon signal noise ratio beta becomes larger sqrt log particular beta remain bounded problem dimensions increase hand analyze several polynomial time estimation algorithms based tensor unfolding power iteration message passing ideas graphical models show unless signal noise ratio diverges system dimensions none approaches succeeds possibly related fundamental limitation computationally tractable estimators problem moderate dimensions propose hybrid approach uses unfolding together power iteration show outperforms significantly baseline methods finally consider case additional side information available unknown signal characterize amount side information allow iterative algorithms converge good estimate
hardness parameter estimation graphical models consider problem learning canonical parameters specifying undirected graphical model markov random field mean parameters graphical models representing minimal exponential family canonical parameters uniquely determined mean parameters problem feasible principle goal paper investigate computational feasibility statistical task main result shows parameter estimation general intractable algorithm learn canonical parameters generic pair wise binary graphical model mean parameters time bounded polynomial number variables unless indeed result believed true see monograph wainwright jordan proof known proof gives polynomial time reduction approximating partition function hard core model known hard learning approximate parameters reduction entails showing marginal polytope boundary inherent repulsive property validates optimization procedure polytope use knowledge structure required ellipsoid method others
advances learning bayesian networks bounded treewidth work presents novel algorithms learning bayesian networks bounded treewidth exact approximate methods developed exact method combines mixed integer linear programming formulations structure learning treewidth computation approximate method consists sampling trees maximal graphs treewidth subsequently selecting exactly approximately best structure whose moral graph subgraph tree approaches empirically compared state art methods collection public data sets 100 variables
exploiting linear structure within convolutional networks efficient evaluation present techniques speeding test time evaluation large convolutional networks designed object recognition tasks models deliver impressive accuracy image evaluation requires millions floating point operations making deployment smartphones internet scale clusters problematic computation dominated convolution operations lower layers model exploit redundancy present within convolutional filters derive approximations significantly reduce required computation using large state art models demonstrate speedups convolutional layers cpu gpu factor keeping accuracy within original model
structure regularization structured prediction many studies weight regularization study structure regularization rare many existing systems structured prediction focus increasing level structural dependencies within model however trend could misdirected study suggests complex structures actually harmful generalization ability structured prediction control structure based overfitting propose structure regularization framework via emph structure decomposition decomposes training samples mini samples simpler structures deriving model better generalization power show theoretically empirically structure regularization effectively control overfitting risk lead better accuracy product proposed method also substantially accelerate training speed method theoretical results apply general graphical models arbitrary structures experiments well known tasks demonstrate method easily beat benchmark systems highly competitive tasks achieving record breaking accuracies yet substantially faster training speed
incremental local gaussian regression locally weighted regression lwr created nonparametric method approximate wide range functions computationally efficient learn continually large amounts incrementally collected data interesting feature lwr regress non stationary functions beneficial property instance control problems however provide proper generative model function values existing algorithms variety manual tuning parameters strongly influence bias variance learning speed results gaussian process regression hand provide generative model rather black box automatic parameter tuning higher computational cost especially big data sets non stationary model required paper suggest path gaussian process regression locally weighted regression retain best approaches using localizing function basis approximate inference techniques build gaussian process regression algorithm increasingly local nature similar computational complexity lwr empirical evaluations performed several synthetic real robot datasets increasing complexity big data scale demonstrate consistently achieve par superior performance compared current state art methods retaining principled approach fast incremental regression minimal manual tuning parameters
coresets segmentation streaming data life logging video streams financial time series twitter tweets examples high dimensional signals practically unbounded time consider problem computing optimal segmentation signals piecewise linear function using pass data maintaining coreset signal coreset enables fast analysis automatic summarization analysis signals coreset core set compact representation data seen far approximates data well specific task case segmentation stream show perhaps surprisingly segmentation problem admits coresets cardinality linear number segments independently dimension signal number points precisely construct representation size klog eps provides eps approximation sum squared distances given piecewise linear function moreover coresets constructed parallel streaming approach results rely novel eduction statistical estimations problems computational geometry empirically evaluate algorithms large synthetic real data sets gps video financial domains using 255 machines amazon cloud
analysis variational bayesian latent dirichlet allocation weaker sparsity map latent dirichlet allocation lda popular generative model various objects texts images object expressed mixture latent topics paper theoretically investigate variational bayesian learning lda specifically analytically derive leading term free energy asymptotic setup show exist transition thresholds dirichlet hyperparameters around sparsity inducing behavior drastically changes theoretically reveal notable phenomenon tends induce weaker sparsity map lda model opposed models experimentally demonstrate practical validity asymptotic theory real world last music data
exclusive feature learning arbitrary structures via ell_ norm group lasso widely used enforce structural sparsity achieves sparsity inter group level paper propose new formulation called exclusive group lasso brings sparsity intra group level context feature selection proposed exclusive group lasso applicable feature structures regardless overlapping non overlapping structures give analysis properties exclusive group lasso propose effective iteratively weighted algorithm solve corresponding optimization problem rigorous convergence analysis show applications exclusive group lasso uncorrelated feature selection extensive experiments synthetic real world datasets indicate good performance proposed methods
algorithm selection rational metareasoning model human strategy selection selecting right algorithm important problem computer science algorithm often exploit structure input efficient human mind faces challenge therefore solutions algorithm selection problem inspire models human strategy selection vice versa view algorithm selection problem special case metareasoning derive solution outperforms existing methods sorting algorithm selection apply theory model people choose cognitive strategies test prediction behavioral experiment find people quickly learn adaptively choose cognitive strategies people choices experiment consistent model inconsistent previous theories human strategy selection rational metareasoning appears promising framework reverse engineering people choose among cognitive strategies translating results better solutions algorithm selection problem
message passing inference large scale graphical models high order potentials keep big data challenge parallelized algorithms based dual decomposition proposed perform inference markov random fields despite parallelization current algorithms struggle energy high order terms graph densely connected paper propose partitioning strategy followed message passing algorithm able exploit pre computations updates high order factors passing messages across machines demonstrate effectiveness approach task joint layout semantic segmentation estimation single images show approach orders magnitude faster current methods
quic dirty quadratic approximation approach dirty statistical models paper develop family algorithms optimizing superposition structured dirty statistical estimators high dimensional problems involving minimization sum smooth loss function hybrid regularization current approaches first order methods including proximal gradient alternating direction method multipliers admm propose new family second order methods approximate loss function using quadratic approximation superposition structured regularizer leads subproblem efficiently solved alternating minimization propose general active subspace selection approach speed solver utilizing low dimensional structure given regularizers provide convergence guarantees algorithm empirically show approach times faster state art first order approaches latent variable graphical model selection problems multi task learning problems regularizer problems approach appears first algorithm extend active subspace ideas multiple regularizers
framework testing identifiability bayesian models perception bayesian observer models effective describing human performance perceptual tasks much trusted faithfully recover hidden mental representations priors likelihoods loss functions data however intrinsic degeneracy bayesian framework multiple combinations elements yield empirically indistinguishable results prompts question model identifiability propose novel framework systematic testing identifiability significant class bayesian observer models practical applications improving experimental design examine theoretical identifiability inferred internal representations case studies first show experimental designs work better remove underlying degeneracy time interval estimation task second find reconstructed representations speed perception task slow speed prior fairly robust
computing nash equilibria generalized interdependent security games study computational complexity computing nash equilibria generalized interdependent security ids games like traditional ids games originally introduced economists risk assessment experts heal kunreuther decade ago generalized ids games model agents voluntary investment decisions facing potential direct risk transfer risk exposure agents distinct feature generalized ids games however full investment reduce transfer risk result depending transfer risk reduction level generalized ids games exhibit strategic complementarity strategic substitutability consider variants generalized ids games players exhibit show determining whether pure strategy nash equilibrium psne type games complete computing single psne type games takes worst case polynomial time problem computing mixed strategy nash equilibria msne efficiently produce partial characterization whenever agent game indiscriminate terms transfer risk exposure agents case kearns ortiz originally studied context traditional ids games nips 2003 paper compute msne satisfy ordering constraints polynomial time game variants yet computational barrier general transfer case show computational problem hard pure nash extension problem also originally introduced kearns ortiz complete variants finally experimentally examine discuss practical impact additional protection transfer risk allowed generalized ids games msne solving several randomly generated instances type games graph structures taken several real world datasets
discovering structure high dimensional data correlation explanation introduce method learn hierarchy successively abstract representations complex data based optimizing information theoretic objective intuitively optimization searches set latent factors best explain correlations data measured multivariate mutual information method unsupervised requires model assumptions scales linearly number variables makes attractive approach high dimensional systems demonstrate correlation explanation corex automatically discovers meaningful structure data diverse sources including personality tests dna human language
flexible transfer learning support model shift transfer learning algorithms used sufficient training data supervised learning task source training domain limited training data second task target test domain similar identical first previous work transfer learning focused relatively restricted settings specific parts model considered carried tasks recent work covariate shift focuses matching marginal distributions observations across domains similarly work target conditional shift focuses matching marginal distributions labels adjusting conditional distributions matched across domains however covariate shift assumes support test contained support training training set richer test set target conditional shift makes similar assumption moreover much work transfer learning considered case labels test domain available also little work done marginal conditional distributions allowed change changes smooth paper consider general case support model change across domains transform location scale shift achieve transfer tasks since allow flexible transformations proposed method yields better results synthetic data real world data
statistical decision theoretic framework social choice paper take statistical decision theoretic viewpoint social choice putting focus decision made behalf system agents framework given statistical ranking model decision space loss function defined parameter decision pairs formulate social choice mechanisms decision rules minimize expected loss suggests general framework design analysis new social choice mechanisms compare bayesian estimators minimize bayesian expected loss mallows model condorcet model respectively kemeny rule consider various normative properties addition computational complexity asymptotic behavior particular show bayesian estimator condorcet model satisfies desired properties anonymity neutrality monotonicity computed polynomial time asymptotically different rules data generated condorcet model ground truth parameter
deep recursive neural networks compositionality language recursive neural networks comprise class architecture operate structured input previously successfully applied model compositionality natural language using parse tree based structural representations even though architectures deep structure lack capacity hierarchical representation exists conventional deep feed forward networks well recently investigated deep recurrent neural networks work introduce new architecture deep recursive neural network deep rnn constructed stacking multiple recursive layers evaluate proposed model task fine grained sentiment classification results show deep rnns outperform associated shallow counterparts employ number parameters furthermore approach outperforms previous baselines sentiment analysis task including multiplicative rnn variant well recently introduced paragraph vectors achieving new state art results provide exploratory analyses effect multiple layers show capture different aspects compositionality language
recursive inversion models permutations develop new exponential family probabilistic model permutations capture hierarchical structure well known mallows generalized mallows models subclasses describe parameter estimation propose approach structure search class models provide experimental evidence added flexibility improves predictive performance enables deeper understanding collections permutations
unsupervised learning efficient short term memory network learning recurrent neural networks topic fraught difficulties problems report substantial progress unsupervised learning recurrent networks keep track input signal specifically show networks learn efficiently represent present past inputs based local learning rules results based several key insights first develop local learning rule recurrent weights whose main aim drive network regime average feedforward signal inputs canceled recurrent inputs show learning rule minimizes cost function second develop local learning rule feedforward weights based networks recurrent inputs already predict feedforward inputs minimizes cost third show learning rules modified network directly encode non whitened inputs fourth show learning rules also applied network feeds time delayed version network output back consequence network starts efficiently represent signal inputs history develop main theory linear networks sketch learning rules could transferred balanced spiking networks
submodular attribute selection action recognition video real world action recognition problems low level features cannot adequately characterize rich spatial temporal structures action videos work encode actions based attributes describes actions high level concepts textit jump forward motion air base analysis types action attributes type action attributes generated humans second type data driven attributes learned data using dictionary learning methods attribute based representation exhibit high variance due noisy redundant attributes propose discriminative compact attribute based representation selecting subset discriminative attributes large attribute set attribute selection criteria proposed formulated submodular optimization problem greedy optimization algorithm presented guaranteed least approximation optimum experimental results olympic sports ucf101 datasets demonstrate proposed attribute based representation significantly boost performance action recognition algorithms outperform recently proposed recognition approaches
expectation maximization learning determinantal point processes determinantal point process dpp probabilistic model set diversity compactly parameterized positive semi definite kernel matrix fit dpp given task would like learn entries kernel matrix maximizing log likelihood available data however log likelihood non convex entries kernel matrix learning problem conjectured hard thus previous work instead focused restricted convex learning settings learning single weight row kernel matrix learning weights linear combination dpps fixed kernel matrices work propose novel algorithm learning full kernel matrix changing kernel parameterization matrix entries eigenvalues eigenvectors lower bounding likelihood manner expectation maximization algorithms obtain effective optimization procedure test method real world product recommendation task achieve relative gains test log likelihood compared naive approach maximizing likelihood projected gradient ascent entries kernel matrix
unsupervised deep haar scattering graphs classification high dimensional data defined graphs particularly difficult graph geometry unknown introduce haar scattering transform graphs computes invariant signal descriptors implemented deep cascade additions subtractions absolute values iteratively compute orthogonal haar wavelet transforms multiscale neighborhoods unknown graphs estimated minimizing average total variation pair matching algorithm polynomial complexity supervised classification dimension reduction tested data bases scrambled images signals sampled unknown irregular grids sphere
modeling deep temporal dependencies recurrent grammar cells propose modeling time series representing transformations take frame time frame time end show linear model transformations gated autoencoder turned recurrent network training predict future frames current inferred transformation using backprop time also show stacking multiple layers gating units recurrent pyramid makes possible represent syntax complicated time series outperform standard recurrent neural networks terms prediction accuracy variety tasks
learning multiple tasks parallel shared annotator introduce new multi task framework online learners sharing single annotator limited bandwidth round learners receives input makes prediction label input shared stochastic mechanism decides inputs annotated learner receives feedback label update prediction rule proceed next round develop online algorithm multi task binary classification learns setting bound performance worst case setting additionally show algorithm used solve bandits problems contextual bandits dueling bandits context allowed decouple exploration exploitation empirical study ocr data vowel prediction project document classification shows algorithm outperforms algorithms uses uniform allocation essentially makes accuracy labour annotator
shape illumination shading using generic viewpoint assumption generic viewpoint assumption gva states position viewer light scene special thus estimated parameters observation stable small perturbations object viewpoint light positions gva analyzed quantified previous works put practical use actual vision tasks paper show utilize gva estimate shape illumination single shading image without use priors propose novel linearized spherical harmonics shading model enables obtain computationally efficient form gva term together data term build model whose unknowns shape illumination model parameters estimated using alternating direction method multipliers embedded multi scale estimation framework prior free framework obtain competitive shape illumination estimation results variety models lighting conditions requiring fewer assumptions competing methods
object localization based structural svm using privileged information propose structured prediction algorithm object localization based support vector machines svms using privileged information privileged information provides useful high level knowledge image understanding facilitates learning reliable model even small number training examples setting assume information available training time since difficult obtain visual data accurately without human supervision goal improve performance incorporating privileged information ordinary learning framework adjusting model parameters better generalization tackle object localization problem based novel structural svm using privileged information alternating loss augmented inference procedure employed handle term objective function corresponding privileged information apply proposed algorithm caltech ucsd birds 200 2011 dataset obtain encouraging results suggesting investigation benefit privileged information structured prediction
model parallelization scheduling strategies distributed machine learning distributed machine learning typically approached data parallel perspective big data partitioned multiple workers algorithm executed concurrently different data subsets various synchronization schemes ensure speed correctness sibling problem received relatively less attention ensure efficient correct model parallel execution algorithms parameters program partitioned different workers undergone concurrent iterative updates argue model data parallelisms impose rather different challenges system design algorithmic adjustment theoretical analysis paper develop system model parallelism strads provides programming abstraction scheduling parameter updates discovering leveraging changing structural properties programs strads enables flexible tradeoff scheduling efficiency fidelity intrinsic dependencies within models improves memory efficiency distributed demonstrate efficacy model parallel algorithms implemented strads versus popular implementations topic modeling matrix factorization lasso
poisson process jumping unknown number rates application neural spike data introduce model rate inhomogeneous poisson process modified chinese restaurant process applying mcmc sampler model allows posterior bayesian inference number states poisson like data sampler shown get accurate results synthetic data apply neuron spike data find discrete firing rate states depending orientation stimulus
approximating hierarchical sets hierarchical clustering goal hierarchical clustering construct cluster tree viewed modal structure density purpose use convex optimization program efficiently estimate family hierarchical dense sets high dimensional distributions extend existing graph based methods approximate cluster tree distribution avoiding direct density estimation method able handle high dimensional data efficiently existing density based approaches present empirical results demonstrate superiority method existing ones
decoupled variational gaussian inference variational gaussian inference methods optimize lower bound marginal likelihood popular approach bayesian inference methods fast easy use reasonably accurate difficulty remains computation lower bound latent dimensionality large even though lower bound concave many models computation requires optimization variational parameters efficient reparameterization schemes reduce number parameters give inaccurate solutions destroy concavity leading slow convergence propose decoupled variational inference brings best worlds together first maximizes lagrangian lower bound reducing number parameters number data examples reparameterization obtained unique recovers maxima lower bound even bound concave second method maximizes lower bound using sequence convex problems parallellizable data examples computes gradient efficiently overall approach avoids direct computations covariance requiring linear projections theoretically method converges rate existing methods case concave lower bounds remaining convergent reasonable rate non concave case
tight continuous relaxation balanced cut problem spectral clustering relaxation normalized ratio cut become standard graph based clustering methods existing methods computation multiple clusters corresponding balanced cut graph either based greedy techniques heuristics weak connection original motivation minimizing normalized cut paper propose new tight continuous relaxation balanced cut problem show related recently proposed relaxation cases loose leading poor performance practice optimization tight continuous relaxation propose new algorithm hard sum ratios minimization problem achieves monotonic descent extensive comparisons show method beats existing approaches ratio cut balanced cut criteria
parallel successive convex approximation nonsmooth nonconvex optimization consider problem minimizing sum smooth possibly non convex convex possibly nonsmooth function involving large number variables popular approach solve problem block coordinate descent bcd method whereby iteration variable block updated remaining variables held fixed recent advances developments multi core parallel processing technology desirable parallelize bcd method allowing multiple blocks updated simultaneously iteration algorithm work propose inexact parallel bcd approach iteration subset variables updated parallel minimizing convex approximations original objective function investigate convergence parallel bcd method randomized cyclic variable selection rules analyze asymptotic non asymptotic convergence behavior algorithm convex non convex objective functions numerical experiments suggest special case lasso minimization problem cyclic block selection rule outperform randomized rule
semi separable hamiltonian monte carlo inference bayesian hierarchical models sampling hierarchical bayesian models often difficult mcmc methods strong correlations model parameters hyperparameters recent riemannian manifold hamiltonian monte carlo rmhmc methods significant potential advantages setting computationally expensive introduce new rmhmc method call semi separable hamiltonian monte carlo uses specially designed mass matrix allows joint hamiltonian model parameters hyperparameters decompose simpler hamiltonians structure exploited new integrator call alternating blockwise leapfrog algorithm resulting method mix faster simpler gibbs sampling simpler efficient previous instances rmhmc
sparse pca oracle property paper study estimation dimensional sparse principal subspace covariance matrix sigma high dimensional setting aim recover oracle principal subspace solution principal subspace estimator obtained assuming true support known priori end propose family estimators based semidefinite relaxation sparse pca novel regularizations particular weak assumption magnitude population projection matrix estimator within family exactly recovers true support high probability exact rank attains sqrt statistical rate convergence subspace sparsity level sample size compared existing support recovery results sparse pca approach hinge spiked covariance model limited correlation condition complement first estimator enjoys oracle property prove another estimator within family achieves sharper statistical rate convergence standard semidefinite relaxation sparse pca even previous assumption magnitude projection matrix violated validate theoretical results numerical experiments synthetic datasets
learning convolution filters inverse covariance estimation neural network connectivity consider problem inferring direct neural network connections calcium imaging time series inverse covariance estimation proven fast accurate method learning macro micro scale network connectivity brain recent kaggle connectomics competition inverse covariance main component several top solutions including winning team algorithm however accuracy inverse covariance estimation highly sensitive signal preprocessing calcium fluorescence time series furthermore brute force optimization methods grid search coordinate ascent signal processing parameters time intensive process learning take several days parameters optimize network generalize networks different size parameters paper show inverse covariance estimation dramatically improved using simple convolution filter prior applying sample covariance furthermore signal processing parameters learned quickly using supervised optimization algorithm particular maximize binomial log likelihood loss function respect convolution filter time series inverse covariance regularization parameter proposed algorithm relatively fast networks size competition 1000 neurons producing auc scores similar accuracy winning solution training time hours cpu prediction new networks size carried less minutes time takes read data write solution
incremental clustering case extra clusters explosion amount data available analysis often necessitates transition batch incremental clustering methods process element time typically store small subset data paper initiate formal analysis incremental clustering methods focusing types cluster structure able detect find incremental setting strictly weaker batch model proving fundamental class cluster structures readily detected batch setting impossible identify using incremental method furthermore show limitations incremental clustering overcome allowing additional clusters
augur data parallel probabilistic modeling implementing inference procedures new probabilistic model time consuming error prone probabilistic programming addresses problem allowing user specify model automatically generating inference procedure make practical important generate high performance inference code turn modern architectures high performance requires parallel execution paper present augur probabilistic modeling language compiler bayesian networks designed make effective use data parallel architectures gpus show compiler generate data parallel inference code scalable thousands gpu cores making use conditional independence relationships bayesian network
extracting latent structure multiple interacting neural populations developments neural recording technology rapidly enabling recording populations neurons multiple brain areas simultaneously well identification types neurons recorded excitatory inhibitory growing need statistical methods study interaction among multiple labeled populations neurons rather attempting identify direct interactions neurons number interactions grows number neurons squared propose extract smaller number latent variables population study latent variables interact specifically propose extensions probabilistic canonical correlation analysis pcca capture temporal structure latent variables well distinguish within population dynamics across population interactions termed group latent auto regressive analysis glara applied methods populations neurons recorded simultaneously visual areas found glara provides better description recordings pcca work provides foundation studying multiple populations neurons interact interaction supports brain function
fairness multi agent sequential decision making define fairness solution criterion multi agent decision making problems agents local interests new criterion aims maximize worst performance agents consideration overall performance develop simple linear programming approach scalable game theoretic approach computing optimal fairness policy game theoretic approach formulates fairness optimization player sum game employs iterative algorithm finding nash equilibrium corresponding optimal fairness policy scale approach exploiting problem structure value function approximation experiments resource allocation problems show fairness criterion provides favorable solution utilitarian criterion game theoretic approach significantly faster linear programming
real time decoding integrate fire encoder neuronal encoding models range detailed biophysically based hodgkin huxley model statistical linear time invariant model specifying firing rates terms extrinsic signal decoding former becomes intractable latter adequately capture nonlinearities present neuronal encoding system use practical applications wish record output neurons namely spikes decode signal fast order drive machine example prosthetic device introduce causal real time decoder biophysically based integrate fire encoding neuron model show upper bound real time reconstruction error decreases polynomially time norm error bounded constant depends density spikes well bandwidth decay input signal numerically validate effect parameters reconstruction error
multiplicative multitask feature learning investigate general framework multiplicative multitask feature learning decomposes task model parameters multiplication components components used across tasks component task specific several previous methods proposed special cases framework study theoretical properties framework different regularization conditions applied decomposed components prove framework mathematically equivalent widely used multitask feature learning methods based joint regularization model parameters general form regularizers analytical formula derived across task component related task specific component regularizers leading better understanding shrinkage effect study framework motivates new multitask learning algorithms propose new learning formulations varying parameters proposed framework empirical studies revealed relative advantages new formulations comparing state art provides instructive insights feature learning problem multiple tasks
randomized experimental design causal graph discovery examine number controlled experiments required discover causal graph hauser buhlmann showed number experiments required logarithmic cardinality maximum undirected clique essential graph lower bounds however assume experiment designer cannot use randomization selecting experiments show significant improvements possible aid randomization adversarial worst case setting designer recover causal graph using log log experiments expectation bound cannot improved show tight causal graphs show non adversarial average case setting even larger improvements possible causal graph chosen uniformly random erd nyi model expected number experiments discover causal graph constant finally present computer simulations complement theoretic results work exploits structural characterization essential graphs andersson characterization based upon set orientation forcing operations results show distinction forcing operations important worst case average case settings
scale adaptive blind deblurring presence noise small scale structures usually leads large kernel estimation errors blind image deblurring empirically total failure present scale space perspective blind deblurring algorithms introduce cascaded scale space formulation blind deblurring new formulation suggests natural approach robust noise small scale structures tying estimation across multiple scales balancing contributions different scales automatically learning data proposed formulation also allows handle non uniform blur straightforward extension experiments conducted benchmark dataset real world images validate effectiveness proposed method surprising finding based approach blur kernel estimation necessarily best finest scale
discriminative unsupervised feature learning convolutional neural networks current methods training convolutional neural networks depend large amounts labeled samples supervised training paper present approach training convolutional neural network using unlabeled data train network discriminate set surrogate classes surrogate class formed applying variety transformations randomly sampled seed image patch find simple feature learning algorithm surprisingly successful applied visual object recognition feature representation learned algorithm achieves classification results matching outperforming current state art unsupervised learning several popular datasets stl cifar caltech 101
stochastic proximal gradient descent acceleration techniques proximal gradient descent pgd stochastic proximal gradient descent spgd popular methods solving regularized risk minimization problems machine learning statistics paper propose analyze accelerated variant methods mini batch setting method incorporates acceleration techniques nesterov acceleration method variance reduction stochastic gradient accelerated proximal gradient descent apg proximal stochastic variance reduction gradient prox svrg trade relationship show method appropriate mini batch size achieves lower overall complexity apg prox svrg
diverse randomized agents vote win investigate power voting among diverse randomized software agents teams computer agents mind develop novel theoretical model stage noisy voting builds recent work machine learning model allows reason collection agents different biases determined first stage noise models furthermore apply randomized algorithms evaluate alternatives produce votes captured second stage noise models analytically demonstrate uniform team consisting multiple instances single agent must make significant number mistakes whereas diverse team converges perfection number agents grows experiments pit teams computer agents strong agents provide evidence effectiveness voting agents diverse
partition wise linear models region specific linear models widely used practical applications non linear highly interpretable model representations key challenges use non convexity simultaneous optimization regions region specific models paper proposes novel convex region specific linear models refer partition wise linear models key ideas assigning linear models regions partitions region specifiers representing region specific linear models linear combinations partition specific models optimizing regions via partition selection large number given partition candidates means convex structured regularizations addition providing initialization free globally optimal solutions convex formulation makes possible derive generalization bound use advanced optimization techniques proximal methods decomposition proximal maps sparsity inducing regularizations experimental results demonstrate partition wise linear models perform better least competitive state art region specific locally linear models
clamping variables approximate inference recently proved using graph covers ruozzi 2012 bethe partition function upper bounded true partition function binary pairwise model attractive provide new arguably simpler proof first principles make use idea clamping variable particular value attractive model show summing bethe partition functions sub model obtained clamping variable raise hence improve approximation fact derive stronger result useful implications repeatedly clamping obtain model cycles bethe approximation exact yields result also provide related lower bound broad class approximate partition functions general pairwise multi label models depends topology demonstrate clamping wisely chosen variables practical value dramatically reducing approximation error
depth map prediction single image using multi scale deep network predicting depth essential component understanding geometry scene stereo images local correspondence suffices estimation finding depth relations single image less straightforward requiring integration global local information various cues moreover task inherently ambiguous large source uncertainty coming overall scale paper present new method addresses task employing deep network stacks makes coarse global prediction based entire image another refines prediction locally also apply scale invariant error help measure depth relations rather scale leveraging raw datasets large sources training data method achieves state art results nyu depth kitti matches detailed depth boundaries without need superpixelation
cone constrained principal component analysis estimating vector noisy quadratic observations task arises naturally many contexts dimensionality reduction synchronization phase retrieval problems often case additional information available unknown vector instance sparsity sign magnitude entries many authors propose non convex quadratic optimization problems aim exploiting optimally information however solving problems typically hard consider simple model noisy quadratic observation unknown vector bvz unknown vector constrained belong cone cone bvz optimal estimation appears intractable general problems class provide evidence tractable cone convex cone efficient projection surprising since corresponding optimization problem non convex worst case perspective often hard characterize resulting minimax risk terms statistical dimension cone delta cone quantity already known control risk estimation gaussian observations random linear measurements rather surprising quantity plays role estimation risk quadratic measurements
divide conquer learning anchoring conical hull reduce broad class machine learning problems usually addressed sampling problem finding extremal rays spanning conical hull data point set anchors lead global solution interpretable model even outperform sampling generalization error find anchors propose novel divide conquer learning scheme dca distributes problem mathcal log type sub problems different low random hyperplanes solved solver sub problem present non iterative solver needs compute array cosine values max min entries dca also provides faster subroutine methods check whether point covered conical hull improves algorithm design multiple dimensions brings significant speedup learning apply method gmm hmm lda nmf subspace clustering show competitive performance scalability methods rich datasets
mode estimation high dimensional discrete tree graphical models paper studies following problem given samples high dimensional discrete distribution want estimate leading delta rho modes underlying distributions point defined delta rho mode local optimum density within delta neighborhood metric rho increase scale parameter delta neighborhood size increases total number modes monotonically decreases sequence delta rho modes reveal intrinsic topographical information underlying distributions though mode finding problem generally intractable high dimensions paper unveils distribution approximated well tree graphical model mode characterization significantly easier efficient algorithm provable theoretical guarantees proposed applied applications like data analysis multiple predictions
concavity reweighted kikuchi approximation analyze reweighted version kikuchi approximation estimating log partition function product distribution defined region graph establish sufficient conditions concavity reweighted objective function terms weight assignments kikuchi expansion show reweighted version sum product algorithm applied kikuchi region graph produce global optima kikuchi approximation whenever algorithm converges region graph layers corresponding bethe approximation show sufficient conditions concavity also necessary finally provide explicit characterization polytope concavity terms cycle structure region graph conclude simulations demonstrate advantages reweighted kikuchi approach
augmentative message passing traveling salesman problem graph partitioning cutting plane method augmentative constrained optimization procedure often used continuous domain optimization techniques linear convex programs investigate viability similar idea within message passing integral solutions context combinatorial problems traveling salesman problem tsp propose factor graph based held karp formulation exponential number constraint factors exponential sparse tabular form graph partitioning community mining using modularity optimization introduce binary variable model large number constraints enforce formation cliques cases able derive surprisingly simple message updates lead competitive solutions benchmark instances particular tsp able find near optimal solutions time empirically grows demonstrating augmentation practical efficient
distributed bayesian posterior sampling via moment sharing propose distributed markov chain monte carlo mcmc inference algorithm large scale bayesian posterior simulation assume dataset partitioned stored across nodes cluster procedure involves independent mcmc posterior sampler node based local partition data moment statistics local posteriors collected sampler propagated across cluster using expectation propagation message passing low communication costs moment sharing scheme improves posterior estimation quality enforcing agreement among samplers demonstrate speed inference quality method empirical studies bayesian logistic regression sparse linear regression spike slab prior
optimal teaching limited capacity human learners basic decisions judging person friend foe involve categorizing novel stimuli recent work finds people category judgments guided small set examples retrieved memory decision time limited stochastic retrieval places limits human performance probabilistic classification decisions light capacity limitation recent work finds idealizing training items saliency ambiguous cases reduced improves human performance novel test items shortcoming previous work idealization category distributions idealized hoc heuristic fashion contribution take first principles approach constructing idealized training sets apply machine teaching procedure cognitive model either limited capacity humans unlimited capacity machine learning systems predicted find machine teacher recommends idealized training sets also find human learners perform best training recommendations machine teacher based limited capacity model predicted extent learning model used machine teacher conforms true nature human learners recommendations machine teacher prove effective results provide normative basis given capacity constraints idealization procedures offer novel selection procedure models human learning
convolutional neural network architectures matching natural language sentences semantic matching central importance many natural language tasks cite bordes2014semantic retrievalqa successful matching algorithm needs adequately model internal structures language objects interaction step toward goal propose convolutional neural network models matching sentences adapting convolutional strategy vision speech proposed models nicely represent hierarchical structures sentences layer layer composition pooling also capture rich matching patterns different levels models rather generic requiring prior knowledge language hence applied matching tasks different nature different languages empirical study variety matching tasks demonstrates efficacy proposed model variety matching tasks superiority competitor models
transportability multiple environments limited experiments completeness results paper addresses problem transportability transferring causal knowledge collected several heterogeneous domains target domain passive observations limited experimental data collected paper first establishes necessary sufficient condition deciding feasibility transportability whether causal effects target domain estimable information available proves previously established algorithm computing transport formula fact complete failure algorithm implies non existence transport formula finally paper shows calculus complete transportability class
pac bayesian auc classification scoring develop scoring classification procedure based pac bayesian approach auc area curve criterion focus initially class linear score functions derive pac bayesian non asymptotic bounds types prior score parameters gaussian prior spike slab prior latter makes possible perform feature selection important advantage approach amenable powerful bayesian computational tools derive particular sequential monte carlo algorithm efficient method used gold standard expectation propagation algorithm much faster approximate method also extend method class non linear score functions essentially leading nonparametric procedure considering gaussian process prior
biclustering using message passing biclustering analog clustering bipartite graph existent methods infer biclusters local search strategies find cluster time common technique update row memberships based current column memberships vice versa propose biclustering algorithm maximizes global objective function using message passing objective function closely approximates general likelihood function separating cluster size penalty term row column count penalties use global optimization framework approach excels resolving overlaps biclusters important features biclusters practice moreover expectation maximization used learn model parameters unknown simulations find method outperforms best existing biclustering algorithms isa las planted clusters overlap applied gene expression datasets method finds coregulated gene clusters high quality terms cluster size density
learning chordal markov networks dynamic programming present algorithm finding chordal markov network maximizes given decomposable scoring function algorithm based recursive characterization clique trees runs time vertices vertex benchmark instance implementation turns million times faster recently proposed constraint satisfaction based algorithm corander nips 2013 within hours able solve instances vertices beyond restrict maximum clique size also study performance recent integer linear programming algorithm bartlett cussens uai 2013 results suggest unless bound clique sizes currently dynamic programming algorithm guaranteed solve instances around vertices
multi view perceptron deep model learning face identity view representations various factors identities views poses illuminations coupled face images disentangling identity view representations major challenge face recognition existing face recognition systems either use handcrafted features learn features discriminatively improve recognition accuracy different behavior human brain intriguingly even without accessing data human recognize face identity also imagine face images person different viewpoints given single image making face perception brain robust view changes sense human brain learned encoded face models images take account instinct paper proposes novel deep neural net named multi view perceptron mvp untangle identity view features infer full spectrum multi view images meanwhile given single face image identity features mvp achieve superior performance multipie dataset mvp also capable interpolate predict images viewpoints unobserved training data
learning shuffle ideals restricted distributions class shuffle ideals fundamental sub family regular languages shuffle ideal generated string set collection strings containing string necessarily contiguous subsequence spite apparent simplicity problem learning shuffle ideal given data known computationally intractable paper study pac learnability shuffle ideals present positive results learning problem element wise independent identical distributions markovian distributions statistical query model constrained generalization learning shuffle ideals product distributions also provided empirical direction propose heuristic algorithm learning shuffle ideals given labeled strings general unrestricted distributions experiments demonstrate advantage efficiency accuracy algorithm
improved distributed principal component analysis study distributed computing setting multiple servers holding set points wish compute functions union point sets key task setting principal component analysis pca servers would like compute low dimensional subspace capturing much variance union point sets possible given procedure approximate pca use approximately solve problems means clustering low rank approximation essential properties approximate distributed pca algorithm communication cost computational efficiency given desired accuracy downstream applications give new algorithms analyses distributed pca lead improved communication computational costs means clustering related problems empirical study real world data shows speedup orders magnitude preserving communication negligible degradation solution quality techniques develop input sparsity subspace embeddings high correctness probability dimension sparsity independent error probability independent interest
multivariate regression calibration propose new method named calibrated multivariate regression cmr fitting high dimensional multivariate regression models compared existing methods cmr calibrates regularization regression task respect noise level simultaneously tuning insensitive achieves improved finite sample performance computationally develop efficient smoothed proximal gradient algorithm worst case iteration complexity epsilon epsilon pre specified numerical accuracy theoretically prove cmr achieves optimal rate convergence parameter estimation illustrate usefulness cmr thorough numerical simulations show cmr consistently outperforms high dimensional multivariate regression methods also apply cmr brain activity prediction problem find cmr competitive handcrafted model created human experts
stochastic variational inference hidden markov models variational inference algorithms proven successful bayesian analysis large data settings recent advances using stochastic variational inference svi however methods largely studied independent exchangeable data settings develop svi algorithm learn parameters hidden markov models hmms time dependent data setting challenge applying stochastic optimization setting arises dependencies chain must broken consider minibatches observations propose algorithm harnesses memory decay chain adaptively bound errors arising edge effects demonstrate effectiveness algorithm synthetic experiments large genomics dataset batch algorithm computationally infeasible
prior distributions approximate inference structured variables present general framework constructing prior distributions structured variables prior defined information projection base distribution onto distributions supported constraint set interest cases projection intractable propose family parameterized approximations indexed subsets domain analyze special case sparse structure optimal prior intractable general show approximate inference using convex subsets tractable equivalent maximizing submodular function subject cardinality constraints result inference using greedy forward selection provably achieves within factor optimal objective value work motivated predictive modeling high dimensional functional neuroimaging data task employ gaussian base distribution induced local partial correlations consider design priors capture domain knowledge sparse support experimental results simulated data high dimensional neuroimaging data show effectiveness approach terms support recovery predictive accuracy
efficient inference continuous markov random fields polynomial potentials paper prove every multivariate polynomial even degree decomposed sum convex concave polynomials motivated property exploit concave convex procedure perform inference continuous markov random fields polynomial potentials particular show concave convex decomposition polynomials expressed sum squares optimization efficiently solved via semidefinite programming demonstrate effectiveness approach context reconstruction shape shading image denoising show approach significantly outperforms existing approaches terms efficiency well quality retrieved solution
generalized unsupervised manifold alignment paper propose generalized unsupervised manifold alignment guma method build connections different correlated datasets without known correspondences based assumption datasets theme usually similar manifold structures guma formulated explicit integer optimization problem considering structure matching preserving criteria well feature comparability corresponding points mutual embedding space main benefits model include simultaneous discovery alignment manifold structures fully unsupervised matching without pre specified correspondences efficient iterative alignment without computations permutation cases experimental results dataset matching real world applications demonstrate effectiveness practicability manifold alignment method
distributed variational inference sparse gaussian process regression latent variable models gaussian processes gps powerful tool probabilistic inference functions applied regression non linear dimensionality reduction offer desirable properties uncertainty estimates robustness fitting principled ways tuning hyper parameters however scalability models big datasets remains active topic research introduce novel parametrisation variational inference sparse regression latent variable models allows efficient distributed algorithm done exploiting decoupling data given inducing points formulate evidence lower bound map reduce setting show inference scales well data computational resources preserving balanced distribution load among nodes demonstrate utility scaling gaussian processes big data show performance improves increasing amounts data regression flight data million records latent variable modelling mnist results show gps perform better many common models often used big data
optimization methods sparse pseudo likelihood graphical model selection sparse high dimensional graphical model selection popular topic contemporary machine learning end various useful approaches proposed context ell_1 penalized estimation gaussian framework though many approaches demonstrably scalable leveraged recent advances convex optimization still depend gaussian functional form address gap convex pseudo likelihood based partial correlation graph estimation method concord recently proposed method uses cyclic coordinate wise minimization regression based pseudo likelihood shown robust model selection properties comparison gaussian approach direct contrast parallel work gaussian setting however new convex pseudo likelihood framework leveraged extensive array methods proposed machine learning literature convex optimization paper address crucial gap proposing proximal gradient methods concord ista concord fista performing ell_1 regularized inverse covariance matrix estimation pseudo likelihood framework present timing comparisons coordinate wise minimization demonstrate approach yields tremendous pay offs ell_1 penalized partial correlation graph estimation outside gaussian setting thus yielding fastest scalable approach problems undertake theoretical analysis approach rigorously demonstrate convergence also derive rates thereof
optimal decision making time varying evidence reliability previous theoretical experimental work optimal decision making restricted artificial setting reliability momentary sensory evidence remained constant within single trials work presented describes computation characterization optimal decision making realistic case evidence reliability varies across time even within trial shows case optimal behavior determined bound decision maker belief depends current past reliability furthermore demonstrate simpler heuristics fail match optimal performance certain characteristics process determines time course reliability causing drop reward rate
learning optimal commitment overcome insecurity game theoretic algorithms physical security made impressive real world impact algorithms compute optimal strategy defender commit stackelberg game attacker observes defender strategy best responds order build game model though payoffs potential attackers various outcomes must estimated inaccurate estimates lead significant inefficiencies design algorithm optimizes defender strategy prior information observing attacker responses randomized deployments resources learning priorities contrast previous work algorithm requires number queries polynomial representation game
accelerated mini batch randomized block coordinate descent method consider regularized empirical risk minimization problems particular minimize sum smooth empirical risk function nonsmooth regularization function regularization function block separable solve minimization problems randomized block coordinate descent rbcd manner existing rbcd methods usually decrease objective value exploiting partial gradient randomly selected block coordinates iteration thus need data accessible partial gradient block gradient exactly obtained however batch setting computationally expensive practice paper propose mini batch randomized block coordinate descent mrbcd method estimates partial gradient selected block based mini batch randomly sampled data iteration accelerate mrbcd method exploiting semi stochastic optimization scheme effectively reduces variance partial gradient estimators theoretically show strongly convex functions mrbcd method attains lower overall iteration complexity existing rbcd methods application trim mrbcd method solve regularized sparse learning problems numerical experiments shows mrbcd method naturally exploits sparsity structure achieves better computational performance existing methods
bregman alternating direction method multipliers mirror descent algorithm mda generalizes gradient descent using bregman divergence replace squared euclidean distance paper similarly generalize alternating direction method multipliers admm bregman admm badmm allows choice different bregman divergences exploit structure problems badmm provides unified framework admm variants including generalized admm inexact admm bethe admm establish global convergence iteration complexity badmm cases badmm faster admm factor dimensionality solving linear program mass transportation problem badmm leads massive parallelism easily run gpu badmm several times faster highly optimized commercial software gurobi
analysis brain states multi region lfp time series local field potential lfp source information broad patterns brain activity frequencies present time series measurements often highly correlated regions believed regions jointly constitute brain state relating cognition behavior infinite hidden markov model ihmm proposed model evolution brain states based electrophysiological lfp data measured multiple brain regions brain state influences spectral content region measured lfp new state dependent tensor factorization employed across brain regions spectral properties lfps characterized terms gaussian processes gps lfps modeled mixture gps state region dependent mixture weights spectral content data encoded spectral mixture covariance kernels model able estimate number brain states number mixture components mixture gps new variational bayesian split merge algorithm employed inference model infers state changes function external covariates novel electrophysiological datasets using lfp data recorded simultaneously multiple brain regions mice results validated interpreted subject matter experts
large scale canonical correlation analysis iterative least squares canonical correlation analysis cca widely used statistical tool well established theory favorable performance wide range machine learning problems however computing cca huge datasets slow since involves implementing decomposition singular value decomposition huge matrices paper introduce cca iterative algorithm compute cca fast huge sparse datasets theory asymptotic convergence finite time accuracy cca established experiments also show cca outperform fast cca approximation schemes real datasets
graphical models recovering probabilistic causal queries missing data address problem deciding whether causal probabilistic query estimable data corrupted missing entries given model missingness process extend results mohan 2013 presenting general conditions recovering probabilistic queries form well causal queries form show causal queries recoverable even factors identifying estimands recoverable specifically derive graphical conditions recovering causal effects form missingness mechanism separable finally apply results problems attrition characterize recovery causal effects data corrupted attrition
time learning fast flexible inference much research machine learning centered around search inference algorithms general purpose efficient problem extremely challenging general inference remains computationally expensive seek address problem observing specific applications model typically need perform small subset possible inference computations motivated introduce time learning framework fast flexible inference learns speed inference run time series experiments show framework allow combine flexibility sampling efficiency deterministic message passing
communication efficient distributed machine learning parameter server paper describes third generation parameter server framework distributed machine learning framework offers relaxations balance system performance algorithm efficiency propose new algorithm takes advantage framework solve non convex non smooth problems convergence guarantees present depth analysis large scale machine learning problems ranging ell_1 regularized logistic regression cpus reconstruction ica gpus using 636tb real data hundreds billions samples dimensions demonstrate using examples parameter server framework effective straightforward way scale machine learning larger problems systems previously achieved
repeated contextual auctions strategic buyers motivated real time advertising exchanges analyze problem pricing inventory repeated posted price auction consider cases truthful surplus maximizing buyer former makes decisions myopically every round latter strategically react algorithm forgoing short term surplus order trick algorithm setting better prices future assume buyer valuation good function context vector describes good sold give first algorithm attaining sublinear regret contextual setting surplus maximizing buyer also extend result repeated second price auctions multiple buyers
communication efficient distributed dual coordinate ascent communication remains significant bottleneck performance distributed optimization algorithms large scale machine learning paper propose communication efficient framework cocoa uses local computation primal dual setting dramatically reduce amount necessary communication provide strong convergence rate analysis class algorithms well experiments real world distributed datasets implementations spark experiments find compared state art mini batch versions sgd sdca algorithms cocoa converges 001 accurate solution quality average quickly
smoothed gradients stochastic variational inference stochastic variational inference svi lets scale bayesian computation massive data uses stochastic optimization fit variational distribution following easy compute noisy natural gradients traditional stochastic optimization methods svi takes precautions use unbiased stochastic gradients whose expectations equal true gradients paper explore idea following biased stochastic gradients svi method replaces natural gradient similarly constructed vector uses fixed window moving average previous terms demonstrate many advantages technique first computational cost svi storage requirements multiply constant factor second enjoys significant variance reduction unbiased estimates smaller bias averaged gradients leads smaller mean squared error full gradient test method latent dirichlet allocation large corpora
sequential monte carlo graphical models propose new framework use sequential monte carlo smc algorithms inference probabilistic graphical models pgm via sequential decomposition pgm find sequence auxiliary distributions defined monotonically increasing sequence probability spaces targeting auxiliary distributions using smc able approximate full joint distribution defined pgm key merits smc sampler provides unbiased estimate partition function model also show used within particle markov chain monte carlo framework order construct high dimensional block sampling algorithms general pgms
automatic discovery cognitive skills improve prediction student learning master discipline algebra physics students must acquire set cognitive skills traditionally educators domain experts manually determine skills select practice exercises hone particular skill propose technique uses student performance data automatically discover skills needed discipline technique assigns latent skill exercise student expected accuracy sequence skill exercises improves monotonically practice rather discarding skills identified experts technique incorporates nonparametric prior exercise skill assignments based expert provided skills weighted chinese restaurant process test technique datasets different intelligent tutoring systems designed students ranging age middle school college obtain surprising results first datasets skills inferred technique support significantly improved predictions student performance expert provided skills second expert provided skills little value technique predicts student performance nearly well ignores domain expertise attempts leverage discuss explanations surprising results also relationship skill discovery technique alternative approaches
clustering nonnegative matrix factorization using graph random walk nonnegative matrix factorization nmf promising relaxation technique clustering analysis however conventional nmf methods directly approximate pairwise similarities using least square error often yield mediocre performance data curved manifolds capture immediate similarities data samples propose new nmf clustering method replaces approximated matrix smoothed version using random walk method thus accommodate farther relationships data samples furthermore introduce novel regularization proposed objective function order improve spectral clustering new learning objective optimized multiplicative majorization minimization algorithm scalable implementation learning factorizing matrix extensive experimental results real world datasets show method strong performance terms cluster purity
bayesian estimation discrete entropy mixtures stick breaking priors consider problem estimating shannon entropy sampled regime number possible symbols unknown countably infinite pitman yor processes generalization dirichlet processes provide tractable prior distributions space countably infinite discrete distributions found major applications bayesian non parametric statistics machine learning show also provide natural priors bayesian entropy estimation due remarkable fact moments induced posterior distribution computed analytically derive formulas posterior mean bayes least squares estimate variance priors moreover show fixed dirichlet pitman yor process prior implies narrow prior meaning prior strongly determines entropy estimate sampled regime derive family continuous mixing measures resulting mixture pitman yor processes produces approximately flat improper prior explore theoretical properties resulting estimator show performs well data sampled exponential power law tailed distributions
sample complexity robust pca estimate sample complexity recent robust estimator generalized version inverse covariance matrix estimator used convex algorithm robust subspace recovery robust pca model assumes sub gaussian underlying distribution sample main result shows high probability norm difference generalized inverse covariance underlying distribution estimator sample size order eps arbitrarily small eps affecting probabilistic estimate rate convergence close direct covariance inverse covariance estimation precise probabilistic estimate implies natural settings sample complexity generalized inverse covariance estimation using frobenius norm delta arbitrarily small delta whereas sample complexity direct covariance estimation frobenius norm results provide similar rates convergence sample complexity corresponding robust subspace recovery algorithm close pca best knowledge work analyzing sample complexity robust pca algorithm
projection retrieval classification many applications classification systems often require loop human intervention cases decision process must transparent comprehensible simultaneously requiring minimal assumptions underlying data distribution tackle problem formulate axis alligned subspacefinding task assumption query specific information dictates complementary use subspaces develop regression based approach called recip efficiently solves problem finding projections minimize nonparametric conditional entropy estimator experiments show method accurate identifying informative projections dataset picking correct ones classify query points facilitates visual evaluation users
homeostatic plasticity bayesian spiking networks expectation maximization posterior constraints recent spiking network models bayesian inference unsupervised learning frequently assume either inputs arrive special format employ complex computations neuronal activation functions synaptic plasticity rules show rigorous mathematical treatment homeostatic processes previously received little attention context overcome common theoretical limitations facilitate neural implementation performance existing models particular show homeostatic plasticity understood enforcement balancing posterior constraint probabilistic inference learning expectation maximization link homeostatic dynamics theory variational inference show nontrivial terms typically appear probabilistic inference large class models drop demonstrate feasibility approach spiking winner take architecture bayesian inference learning finally sketch mathematical framework extended richer recurrent network architectures altogether theory provides novel perspective interplay homeostatic processes synaptic plasticity cortical microcircuits points essential role homeostasis inference learning spiking networks
efficient sampling bipartite matching problems bipartite matching problems characterize many situations ranging ranking information retrieval correspondence vision exact inference real world applications problems intractable making efficient approximation methods essential learning inference paper propose novel sequential matching sampler based generalization plackett luce model effectively make large moves space matchings allows sampler match difficult target distributions common problems highly multimodal distributions well separated modes present experimental results bipartite matching problems ranking image correspondence show sequential matching sampler efficiently approximates target distribution significantly outperforming sampling approaches
perceptron learning sat boolean satisfiability sat canonical complete decision problem important problems computer science practice real world sat sentences drawn distribution result efficient algorithms solution sat instances likely shared characteristics substructures work approaches exploration family sat solvers learning problem particular relate polynomial time solvability sat subset notion margin sentences mapped feature function hilbert space provided mapping based polynomial time computable statistics sentence show existance margin data points implies existance polynomial time solver sat subset based davis putnam logemann loveland algorithm furthermore show simple perceptron style learning rule find optimal sat solver bounded number training updates derive linear time computable set features show analytically margins exist important polynomial special cases sat empirical results show order magnitude improvement state art sat solver hardware verification task
convergence energy landscape cheeger cut clustering unsupervised clustering scattered noisy high dimensional data points important difficult problem continuous relaxations balanced cut problems yield excellent clustering results paper provides rigorous convergence results algorithms solve relaxed cheeger cut minimization first algorithm new steepest descent algorithm second slight modification inverse power method algorithm cite pro heinbuhler10onespec steepest descent algorithm better theoretical convergence properties practice algorithm perform equally also completely characterize local minima relaxed problem terms original balanced cut problem relate characterization convergence algorithms
lovasz theta function svms finding large dense subgraphs lovasz theta function graph fundamental tool combinatorial optimization approximation algorithms computing theta involves solving sdp extremely expensive even moderately sized graphs paper establish lovasz theta function equivalent kernel learning problem related class svm interesting connection opens many opportunities bridging graph theoretic algorithms machine learning show exist graphs call svm theta graphs lovasz theta function approximated well class svm leads novel use svm techniques solve algorithmic problems large graphs identifying planted clique size theta sqrt random graph frac classic approach problem involves computing theta function however scalable due sdp computation show random graph planted clique example svm theta graph consequence svm based approach easily identifies clique large graphs competitive state art introduce notion common orthogonal labeling extends notion orthogonal labelling single graph used defining theta function multiple graphs problem finding optimal common orthogonal labelling cast multiple kernel learning problem used identify large common dense region multiple graphs proposed algorithm achieves order magnitude scalability compared state art
learning multiple tasks using shared hypotheses work consider setting large number related tasks examples individual task rather either learning task individually large generalization error learning tasks together using single hypothesis suffering potentially large inherent error consider learning small pool shared hypotheses task mapped single hypothesis pool hard association derive dimension generalization bounds model based number tasks shared hypothesis dimension hypotheses class conducted experiments synthetic problems sentiment reviews strongly support approach
scalable imputation genetic data discrete fragmentation coagulation process present bayesian nonparametric model genetic sequence data set genetic sequences modelled using markov model partitions partitions consecutive locations genome related clusters first splitting merging model thought discrete time analogue continuous time fragmentation coagulation processes teh 2011 preserving important properties projectivity exchangeability reversibility scalable apply model problem genotype imputation showing improved computational efficiency maintaining accuracies teh 2011
tractable objectives robust policy optimization robust policy optimization acknowledges risk aversion plays vital role real world decision making faced uncertainty effects actions policy maximizes expected utility unknown parameters system also carry risk intolerably poor performance might prefer accept lower utility expectation order avoid reduce likelihood unacceptable levels utility harmful parameter realizations paper take bayesian approach parameter uncertainty unlike methods avoid making distributional assumptions form uncertainty instead focus identifying optimization objectives solutions efficiently approximated introduce percentile measures general class objectives robust policy optimization encompasses existing approaches including ones known intractable introduce broad subclass family robust policies approximated efficiently finally frame objectives context player sum extensive form game employ regret algorithm approximate optimal policy computation polynomial number states actions mdp
recognizing activities attribute dynamics work consider problem modeling dynamic structure human activities attributes space video sequence first represented semantic feature space feature encodes probability occurrence activity attribute given time generative model denoted binary dynamic system bds proposed learn distribution dynamics different activities space bds non linear dynamic system extends binary principal component analysis pca classical linear dynamic systems lds combining binary observation variables hidden gauss markov state process way integrates representation power semantic modeling ability dynamic systems capture temporal structure time varying processes algorithm learning bds parameters inspired popular lds learning method dynamic textures proposed similarity measure bdss generalizes binet cauchy kernel lds introduced used design activity classifiers proposed method shown outperform similar classifiers derived kernel dynamic system kds state art approaches dynamics based attribute based action recognition
submodular bregman divergences applications introduce class discrete divergences sets equivalently binary vectors call submodular bregman divergences consider kinds defined either tight modular upper tight modular lower bounds submodular function show properties divergences analogous standard continuous bregman divergence demonstrate generalize many useful divergences including weighted hamming distance squared weighted hamming weighted precision recall conditional mutual information generalized divergence sets also show lower bound submodular bregman actually special case generalized bregman divergence lovasz extension submodular function call lovasz bregman divergence point number applications submodular bregman divergences particular show proximal algorithm defined submodular bregman divergences provides framework many mirror descent style algorithms related submodular function optimization also show generalization means algorithm using lovasz bregman divergence natural clustering scenarios ordering important unique property algorithm computing mean ordering extremely efficient unlike order based distance measures extendedv finally provide clustering framework submodular bregman derive fast algorithms clustering sets binary vectors equivalently sets sets
multi scale hyper time hardware emulation human motor nervous system based spiking neurons using fpga central goal quantify long term progression pediatric neurological diseases typical years progression child dystonia purpose quantitative models convincing provide multi scale details ranging neuron spikes limb biomechanics models also need evaluated hyper time significantly faster real time producing useful predictions designed platform digital vlsi hardware multi scale hyper time emulations human motor nervous systems platform constructed scalable distributed array field programmable gate array fpga devices devices operate asynchronously millisecond time granularity overall system accelerated 365x real time physiological component implemented using models well documented studies flexibly modified thus validity emulation easily advised neurophysiologists clinicians maximizing speed emulation calculations implemented combinational logic instead clocked iterative circuits paper presents methodology building fpga modules correspondence components monosynaptic spinal loop results emulated activities shown paper also discusses rationale approximating neural circuitry organizing neurons sparse interconnections conclusion platform allows introducing various abnormalities neural emulation emerging motor symptoms analyzed compels test origins childhood motor disorders predict long term progressions
slice sampling normalized kernel weighted completely random measure mixture models number dependent nonparametric processes proposed model non stationary data unknown latent dimensionality however inference algorithms often slow unwieldy general highly specific given model formulation paper describe wide class nonparametric processes including several existing models present slice sampler allows efficient inference across class models
iterative thresholding algorithm sparse inverse covariance estimation sparse graphical modelling inverse covariance selection important problem machine learning seen significant advances recent years major focus methods perform model selection high dimensions end numerous convex ell_1 regularization approaches proposed literature however clear methods optimal well defined sense major gap regard pertains rate convergence proposed optimization methods address iterative thresholding algorithm numerically solving ell_1 penalized maximum likelihood problem sparse inverse covariance estimation presented proximal gradient method considered paper shown converge linear rate result first kind numerically solving sparse inverse covariance estimation problem convergence rate provided closed form related condition number optimal point numerical results demonstrating proven rate convergence presented
synchronization control regularization neural systems via correlated noise processes learn reliable rules generalize novel situations brain must capable imposing form regularization suggest theoretical computational arguments combination noise synchronization provides plausible mechanism regularization nervous system functional role regularization considered general context coupled computational systems receive inputs corrupted correlated noise noise inputs shown impose regularization synchronization upstream induces time varying correlations across noise variables degree regularization calibrated time resulting qualitative behavior matches experimental data visual cortex
optimal kernel choice large scale sample tests abstract given samples distributions sample test determines whether reject null hypothesis based value test statistic measuring distance samples choice test statistic maximum mean discrepancy mmd distance embeddings probability distributions reproducing kernel hilbert space kernel used obtaining embeddings thus critical ensuring test high power correctly distinguishes unlike distributions high probability means parameter selection sample test based mmd proposed given test level upper bound probability making type error kernel chosen maximize test power minimize probability making type error test statistic test threshold optimization kernel parameters obtained cost linear sample size properties make kernel selection test procedures suited data streams observations cannot stored memory experiments new kernel selection approach yields powerful test earlier kernel selection heuristics
cardinality restricted boltzmann machines restricted boltzmann machine rbm popular density model also good extracting features main source tractability rbm models model assumption given input hidden units activate independently another sparsity competition hidden representation believed beneficial rbm competition among hidden units would acquire attractive properties sparse coding constraints added due widespread belief resulting model would become intractable work show dynamic programming algorithm developed 1981 used implement exact sparsity rbm hidden units expand show pass derivatives layer exact sparsity makes possible fine tune deep belief network dbn consisting rbms sparse hidden layers show sparsity rbm hidden layer improves performance pre trained representations fine tuned model
spiking saturating dendrites differentially expand single neuron computation capacity integration excitatory inputs dendrites non linear multiple excitatory inputs produce local depolarization departing arithmetic sum input response taken separately depolarization bigger arithmetic sum dendrite spiking depolarization smaller dendrite saturating decomposing dendritic tree independent dendritic spiking units greatly extends computational capacity neuron maps onto layer neural network enabling compute linearly non separable boolean functions lnbfs lnbfs implemented dendritic architectures practise saturating dendrites equally expand computational capacity adress questions use binary neuron model boolean algebra first confirm spiking dendrites enable neuron compute lnbfs using architecture based disjunctive normal form dnf second prove saturating dendrites well spiking dendrites also enable neuron compute lnbfs using architecture based conjunctive normal form cnf contrary dnf based architecture cnf based architecture leads dendritic unit tuning imply neuron tuning observed experimentally third show cannot use dnf based architecture saturating dendrites consequently show important family lnbfs implemented cnf architecture require exponential number saturating dendritic units whereas family implemented either dnf architecture cnf architecture always require linear number spiking dendritic unit minimization could explain neuron spends energetic resources make dendrites spike
persistent homology learning densities bounded support present novel method learning densities bounded support enables incorporate hard topological constraints particular show emerging techniques computational algebraic topology notion persistent homology combined kernel based methods machine learning purpose density estimation proposed formalism facilitates learning models bounded support principled way incorporating persistent homology techniques approach able encode algebraic topological constraints addressed current state art probabilistic models study behaviour method synthetic examples various sample sizes exemplify benefits proposed approach real world data set learning motion model racecar show learn model respects underlying topological structure racetrack constraining trajectories car
map inference chains using column generation linear chains trees basic building blocks many applications graphical models although exact inference models performed dynamic programming computation still prohibitively expensive non trivial target variable domain sizes due quadratic dependence size standard message passing algorithms problems inefficient compute scores hypotheses strong negative local evidence reason significant previous interest beam search variants however methods provide approximate inference paper presents new efficient exact inference algorithms based combination column generation pre computed bounds model cost structure improving worst case performance impossible however method substantially speeds real world typical case inference chains trees experiments show method twice fast exact viterbi wall street journal part speech tagging thirteen times faster joint part speed named entity recognition task algorithm also extendable new techniques approximate inference faster best inference new opportunities connections inference learning
isotropic hashing existing hashing methods adopt projection functions project original data several dimensions real values projected dimensions quantized bit thresholding typically variances different projected dimensions different existing projection functions principal component analysis pca using number bits different projected dimensions unreasonable larger variance dimensions carry information although viewpoint widely accepted many researchers still verified either theory experiment methods proposed find projection equal variances different dimensions paper propose novel method called isotropic hashing isohash learn projection functions produce projected dimensions isotropic variances equal variances experimental results real data sets show isohash outperform counterpart different variances different dimensions verifies viewpoint projections isotropic variances better anisotropic variances
generalization bounds domain adaptation paper provide new framework study generalization bound learning process domain adaptation without loss generality consider kinds representative domain adaptation settings domain adaptation multiple sources domain adaptation combining source target data particular introduce quantities capture inherent characteristics domains either kind domain adaptation based quantities develop specific hoeffding type deviation inequality symmetrization inequality achieve corresponding generalization bound based uniform entropy number using resultant generalization bound analyze asymptotic convergence rate convergence learning process kind domain adaptation meanwhile discuss factors affect asymptotic behavior learning process numerical experiments support results
bayesian probabilistic subspace addition modeling data matrices paper introduces probabilistic subspace addition pcsa model simultaneously capturing dependent structures among rows columns briefly pcsa assumes entry matrix generated additive combination linear mappings features distribute row wise column wise latent subspaces consequently captures dependencies among entries intricately able model non gaussian heteroscedastic density variational inference proposed pcsa approximate bayesian learning updating posteriors formulated problem solving sylvester equations furthermore pcsa extended tackling filling missing values adapting sparseness modelling tensor data comparison several state art approaches experiments demonstrate effectiveness efficiency bayesian sparse pcsa modeling matrix tensor data filling missing values
sparse approximate manifolds differential geometric mcmc enduring challenges markov chain monte carlo methodology development proposal mechanisms make moves distant current point accepted high probability low computational cost recent introduction locally adaptive mcmc methods based natural underlying riemannian geometry models goes way alleviating problems certain classes models metric tensor analytically tractable however computational efficiency assured due necessity potentially high dimensional matrix operations iteration paper firstly investigate sampling based approach approximating metric tensor suggest valid mcmc algorithm extends applicability riemannian manifold mcmc methods statistical models admit analytically computable metric tensor secondly show approximation scheme consider naturally motivates use regularisation improve estimates obtain sparse approximate inverse metric enables stable sparse approximations local geometry made demonstrate application algorithm inferring parameters realistic system ordinary differential equations using biologically motivated robust student error model expected fisher information analytically intractable
symbolic dynamic programming continuous state observation pomdps partially observable markov decision processes pomdps provide powerful model real world sequential decision making problems recent years point based value iteration methods proven extremely effective techniques nding approximately optimal dynamic programming solutions pomdps initial set belief states known however point based work provided exact point based backups continuous state observation spaces tackle paper key insight nite number possible observations nite number observation partitionings relevant optimal decision making nite xed set reachable belief states known end make important contributions show previous exact symbolic dynamic pro gramming solutions continuous state mdps generalized continu ous state pomdps discrete observations show solution extended via recently developed symbolic methods continuous state observations derive minimal relevant observation partitioning potentially correlated multivariate observation spaces demonstrate proof concept results uni multi variate state observation steam plant control
learning visual motion recurrent neural networks present dynamic nonlinear generative model visual motion based latent representation binary gated gaussian variables trained sequences images model learns represent different movement directions different variables use online approximate inference scheme mapped dynamics networks neurons probed drifting grating stimuli moving bars light neurons model show patterns responses analogous direction selective simple cells primary visual cortex model neurons also show speed tuning respond equally well range motion directions speeds aligned constraint line respective preferred speed show computations enabled specific pattern recurrent connections learned model
exponential concentration mutual information estimation application forests prove new exponential concentration inequality plug estimator shannon mutual information previous results mutual information estimation bounded expected error advantage exponential inequality combined union bound guarantee accurate estimators mutual information many pairs random variables simultaneously application show use result optimally estimate density function graph distribution markov forest graph
tight bounds redundancy distinguishability label invariant distributions minimax divergence distribution distributions given collection several practical implications compression least additional number bits entropy needed worst case encode output distribution collection online estimation learning lowest expected log loss regret guessing sequence random values hypothesis testing upper bounds largest number distinguishable distributions collection motivated problems ranging population estimation text classification speech recognition several machine learning information theory researchers recently considered label invariant distributions properties iid drawn samples using techniques reveal exploit structure distributions improve sequence previous works show minimax divergence collection label invariant distributions length iid sequences cdot log
divide conquer method sparse inverse covariance estimation paper consider ell_1 regularized sparse inverse covariance matrix estimation problem large number variables even face high dimensionality limited number samples recent work shown estimator strong statistical guarantees recovering true structure sparse inverse covariance matrix alternatively underlying graph structure corresponding gaussian markov random field proposed algorithm divides problem smaller sub problems uses solutions sub problems build good approximation original problem derive bound distance approximate solution true solution based bound propose clustering algorithm attempts minimize bound practice able find effective partitions variables use approximate solution solution resulting solving sub problems initial point solve original problem achieve much faster computational procedure example recent state art method quic requires hours solve problem 000 nodes arises climate application proposed algorithm divide conquer quic quic requires hour solve problem
algorithms learning markov field policies present new graph based approach incorporating domain knowledge reinforcement learning applications domain knowledge given weighted graph kernel matrix loosely indicates states similar optimal actions first introduce bias policy search process deriving distribution policies policies disagree provided graph low probabilities distribution corresponds markov random field present reinforcement apprenticeship learning algorithms finding policy distributions also illustrate advantage proposed approach problems swing cart balancing nonuniform smooth frictions gridworlds teaching robot grasp new objects
practical bayesian optimization machine learning algorithms use machine learning algorithms frequently involves careful tuning learning parameters model hyperparameters unfortunately tuning often black art requiring expert experience rules thumb sometimes brute force search therefore great appeal automatic approaches optimize performance given learning algorithm problem hand work consider problem framework bayesian optimization learning algorithm generalization performance modeled sample gaussian process show certain choices nature type kernel treatment hyperparameters play crucial role obtaining good optimizer achieve expert level performance describe new algorithms take account variable cost duration learning algorithm experiments leverage presence multiple cores parallel experimentation show proposed algorithms improve previous automatic procedures reach surpass human expert level optimization many algorithms including latent dirichlet allocation structured svms convolutional neural networks
feature clustering accelerating parallel coordinate descent large scale ell_1 regularized loss minimization problems arise numerous applications compressed sensing high dimensional supervised learning including classification regression problems high performance algorithms implementations critical efficiently solving problems building upon previous work coordinate descent algorithms ell_1 regularized problems introduce novel family algorithms called block greedy coordinate descent includes special cases several existing algorithms scd greedy shotgun thread greedy give unified convergence analysis family block greedy algorithms analysis suggests block greedy coordinate descent better exploit parallelism features clustered maximum inner product features different blocks small theoretical convergence analysis supported experimental results using data diverse real world applications hope algorithmic approaches convergence analysis provide advance field also encourage researchers systematically explore design space algorithms solving large scale ell_1 regularization problems
efficient monte carlo counterfactual regret minimization games many player actions counterfactual regret minimization cfr popular iterative algorithm computing strategies extensive form games monte carlo cfr mccfr variants reduce per iteration time cost cfr traversing sampled portion tree previous effective instances mccfr still slow games many player actions since sample every action given player paper present new mccfr algorithm average strategy sampling samples subset player actions according player average strategy new algorithm inspired new tighter bound number iterations required cfr converge given solution quality addition prove similar tighter bound popular mccfr variants finally validate work demonstrating converges faster previous mccfr algorithms limit poker bluff
ensemble weighted kernel estimators multivariate entropy estimation problem estimation entropy functionals probability densities received much attention information theory machine learning statistics communities kernel density plug estimators simple easy implement widely used estimation entropy however kernel plug estimators suffer curse dimensionality wherein mse rate convergence glacially slow order gamma number samples gamma rate parameter paper shown sufficiently smooth densities ensemble kernel plug estimators combined via weighted convex combination resulting weighted estimator superior parametric mse rate convergence order furthermore shown optimal weights determined solving convex optimization problem require training data knowledge underlying density therefore performed offline novel result remarkable individual kernel plug estimators belonging ensemble suffer curse dimensionality appropriate ensemble averaging achieve parametric convergence rates
mca nonlinear spike slab sparse coding neurally plausible image encoding modelling natural images sparse coding faced main challenges flexibly representing varying pixel intensities realistically representing low level components edges paper proposes novel multiple cause generative model low level image statistics generalizes standard model crucial points uses spike slab prior distribution realistic representation component absence intensity model uses highly nonlinear combination rule maximal causes analysis mca major challenge parameter optimization model either results strongly multimodal posterior show first time model combining improvements trained efficiently retaining rich structure posterior design exact piecewise gibbs sampling method combine variational method based preselection latent dimensions combined training scheme tackles analytical computational intractability enables application model large number observed hidden dimensions applying model image patches study optimal encoding images simple cells compare model predictions vivo neural recordings contrast standard find optimal prior favors asymmetric bimodal sparse activity simple cells testing model consistency find average posterior approximately equal prior furthermore due nonlinearity model predicts large number globular receptive fields rfs another significant difference standard inferred prior high proportion predicted globular fields make model consistent neural data previous models suggesting closer tuning simple cells visual stimuli predicted
multiresolution analysis symmetric group generally accepted way define wavelets permutations address issue introducing notion coset based multiresolution analysis cmra symmetric group find corresponding wavelet functions describe fast wavelet transform complexity small sparse signals contrast complexity typical ffts discuss potential applications ranking sparse approximation multi object tracking
deep neural networks segment neuronal membranes electron microscopy images address central problem neuroanatomy namely automatic segmentation neuronal structures depicted stacks electron microscopy images necessary efficiently map brain structure connectivity segment biological neuron membranes use special type deep artificial neural network pixel classifier label pixel membrane non membrane predicted raw pixel values square window centered input layer maps window pixel neuron followed succession convolutional max pooling layers preserve information extract features increasing levels abstraction output layer produces calibrated probability class classifier trained plain gradient descent 512 times 512 times stack known ground truth tested stack size ground truth unknown authors organizers isbi 2012 segmentation challenge even without problem specific post processing approach outperforms competing techniques large margin considered metrics emph rand error emph warping error emph pixel error pixel error approach outperforming second human observer
action model based multi agent plan recognition multi agent plan recognition mapr aims recognize dynamic team structures team behaviors observed team traces activity sequences set intelligent agents previous mapr approaches required library team activity sequences team plans given input however collecting library team plans ensure adequate coverage often difficult costly paper relax constraint team plans required provided beforehand assume instead set action models available models often already created describe domain physics preconditions effects effects actions propose novel approach recognizing multi agent team plans based action models rather libraries team plans encode resulting mapr problem emph satisfiability problem solve problem using state art weighted max sat solver approach also allows incompleteness observed plan traces empirical studies demonstrate algorithm effective efficient comparison state art mapr methods based plan libraries
meta gaussian information bottleneck present reformulation information bottleneck problem terms copula using equivalence mutual information negative copula entropy focusing gaussian copula extend analytical solution available multivariate gaussian case distributions gaussian dependence structure arbitrary marginal densities also called meta gaussian distributions opens new possibles applications continuous data provides solution robust outliers
coding efficiency detectability rate fluctuations non poisson neuronal firing statistical features neuronal spike trains known non poisson investigate extent non poissonian feature affects efficiency transmitting information fluctuating firing rates purpose introduce kullbuck leibler divergence measure efficiency information encoding assume spike trains generated time rescaled renewal processes show divergence determines lower bound degree rate fluctuations temporal variation firing rates undetectable sparse data also show divergence well lower bound depends variability spikes terms coefficient variation also significantly higher order moments interspike interval isi distributions examine specific models commonly used describing stochastic nature spikes gamma inverse gaussian lognormal isi distributions find time rescaled renewal process distribution achieves largest divergence followed lognormal gamma distributions
wavelet based multi scale shape features arbitrary surfaces cortical thickness discrimination hypothesis testing signals ned surfaces cortical surface fundamental component variety studies neuroscience goal identify regions exhibit changes function clinical condition study clinical questions interest move towards identifying early signs diseases corresponding statistical differences group level invariably become weaker increasingly hard identify indeed multiple comparisons correction adopted account correlated statistical tests surface points regions survive contrast hypothesis tests point wise measurements paper make case performing statistical analysis multi scale shape descriptors characterize local topological context signal around surface vertex descriptors based recent results harmonic analysis show wavelet theory extends non euclidean settings irregular weighted graphs provide strong evidence descriptors successfully pick group wise differences traditional methods either fail yield unsatisfactory results primary application show framework allows performing cortical surface smoothing native space without mappint unit sphere
nonparametric conjugate prior distribution maximizing argument noisy function propose novel bayesian approach solve stochastic optimization problems involve nding extrema noisy nonlinear functions previous work focused representing possible functions explicitly leads step procedure rst inference function space second nding extrema functions skip representation step directly model distribution extrema end devise non parametric conjugate prior natural parameter corresponds given kernel function suf cient statistic composed observed function values resulting posterior distribution directly captures uncertainty maximum unknown function
density difference estimation address problem estimating difference probability densities naive approach step procedure first estimating densities separately computing difference however step procedure necessarily work well first step performed without regard second step thus small estimation error incurred first stage cause big error second stage paper propose single shot procedure directly estimating density difference without separately estimating densities derive non parametric finite sample error bound proposed single shot density difference estimator show achieves optimal convergence rate show proposed density difference estimator utilized distance approximation finally experimentally demonstrate usefulness proposed method robust distribution comparison class prior estimation change point detection
scalable nonconvex inexact proximal splitting study large scale nonsmooth nonconconvex optimization problems particular focus nonconvex problems emph composite objectives class problems includes extensively studied convex composite objective problems special case tackle composite nonconvex problems introduce powerful new framework based asymptotically emph nonvanishing errors avoiding common convenient assumption eventually vanishing errors within framework derive batch incremental nonconvex proximal splitting algorithms knowledge framework first develop analyze incremental emph nonconvex proximal splitting algorithms even disregard ability handle nonvanishing errors illustrate theoretical framework showing applies difficult large scale nonsmooth nonconvex problems
scalable cur matrix decomposition algorithm lower time complexity tighter bound cur matrix decomposition important extension nystr approximation general matrix approximates data matrix terms small number columns rows paper propose novel randomized cur algorithm expected relative error bound proposed algorithm advantages existing relative error cur algorithms possesses tighter theoretical bound lower time complexity avoid maintaining whole data matrix main memory finally experiments several real world datasets demonstrate significant improvement existing relative error algorithms
learning dependency structure latent factors paper study latent factor models dependency structure latent space propose general learning framework induces sparsity undirected graphical model imposed vector latent factors novel latent factor model slfa proposed matrix factorization problem special regularization term encourages collaborative reconstruction main benefit novelty model simultaneously learn lower dimensional representation data model pairwise relationships latent factors explicitly line learning algorithm devised make model feasible large scale learning problems experimental results synthetic data real world data sets demonstrate pairwise relationships latent factors learned model provide structured way exploring high dimensional data learned representations achieve state art classification performance
gaze concurrences head mounted cameras gaze concurrence point gaze directions people intersect strong indicator social saliency attention participating group focused point scenes occupied large groups people multiple concurrences occur transition time paper present method locate multiple gaze concurrences occur social scene videos taken head mounted cameras model gaze cone shaped distribution emanating center eyes capturing variation eye head motion calibrate parameters distribution exploiting fixed relationship primary gaze ray head mounted camera pose resulting gaze model enables build social saliency field estimate number locations gaze concurrences via provably convergent mode seeking social saliency field algorithm applied reconstruct multiple gaze concurrences several real world scenes evaluated quantitatively motion captured ground truth
vote issue adjusted models legislative behavior develop probabilistic model legislative data uses text bills uncover lawmakers positions specific political issues model used explore lawmaker voting patterns deviate expected deviation depends voted derive approximate posterior inference algorithms based variational methods across years legislative data demonstrate improvement heldout predictive performance model utility interpreting inherently multi dimensional space
efficient reinforcement learning high dimensional linear quadratic systems study problem adaptive control high dimensional linear quadratic system previous work established asymptotic convergence optimal controller various adaptive control schemes recently asymptotic regret bound tilde sqrt shown dimension state space work consider case matrices describing dynamic system sparse dimensions large present adaptive control scheme polylog achieves regret bound tilde sqrt particular algorithm average cost eps times optimum cost polylog eps comparison previous work dense dynamics algorithm needs omega samples estimate unknown dynamic significant accuracy believe result prominent applications emerging area computational advertising particular targeted online advertising advertising social networks
angular quantization based binary codes fast similarity search paper focuses problem learning binary embeddings efficient retrieval high dimensional non negative data data typically arises large number vision text applications counts frequencies used features also cosine distance commonly used measure dissimilarity vectors work introduce novel spherical quantization scheme generate binary embedding data analyze properties number quantization landmarks scheme grows exponentially data dimensionality resulting low distortion quantization propose efficient method computing binary embedding using large number landmarks linear transformation learned minimize quantization error adapting method input data resulting improved embedding experiments image text retrieval applications show superior performance proposed method existing state art methods
efficient coding connects prior likelihood function perceptual bayesian inference common challenge bayesian approaches modeling perceptual behavior fact fundamental components bayesian model prior distribution likelihood function formally unconstrained argue neural system emulates bayesian inference naturally imposes constraints way represents sensory information populations neurons specifically propose efficient encoding principle constrains likelihood prior based low level environmental statistics resulting bayesian estimates show biases away peaks prior distribution behavior seemingly odds traditional view bayesian estimates yet indeed reported human perception visual orientation demonstrate framework correctly predicts biases show efficient encoding characteristics model neural population matches reported orientation tuning characteristics neurons primary visual cortex results suggest efficient coding promising hypothesis constraining neural implementations bayesian inference
controlled recognition bounds visual learning exploration describe tradeoff performance visual recognition problem control authority agent exercise sensing process focus problem visual search object otherwise known static scene propose measure control authority relate expected risk proxy conditional entropy posterior density show analytically well empirically simulation using simplest known model captures phenomenology image formation including scaling occlusions show passive agent given training set provide guarantees performance beyond afforded priors omnipotent agent capable infinite control authority achieve arbitrarily good performance asymptotically
online dictionary learning application novel document detection given pervasive use social media twitter become leading source breaking news key task automated identification news detection novel documents voluminous stream text documents scalable manner motivated challenge introduce problem online dictionary learning unlike traditional dictionary learning uses squared loss penalty used measuring reconstruction error present efficient online algorithm problem based alternating directions method multipliers establish sublinear regret bound algorithm empirical results news stream twitter data shows online dictionary learning algorithm novel document detection gives order magnitude speedup previously known batch algorithm without significant loss quality results algorithm online dictionary learning could independent interest
approximate message passing consistent parameter estimation applications sparse learning consider estimation vector xbf measurements ybf obtained general cascade model consisting known linear transform followed probabilistic componentwise possibly nonlinear measurement channel present method called adaptive generalized approximate message passing adaptive gamp enables joint learning statistics prior measurement channel along estimation unknown vector xbf proposed algorithm generalization recently developed method vila schniter uses expectation maximization iterations posteriors steps computed via approximate message passing techniques applied large class learning problems including learning sparse priors compressed sensing identification linear nonlinear cascade models dynamical systems neural spiking processes prove large gaussian transform matrices asymptotic componentwise behavior adaptive gamp algorithm predicted simple set scalar state evolution equations analysis shows adaptive gamp method yield asymptotically consistent parameter estimates implies algorithm achieves reconstruction quality equivalent oracle algorithm knows correct parameter values adaptive gamp methodology thus provides systematic general computationally efficient method applicable large range complex linear nonlinear models provable guarantees
learning architecture sum product networks using clustering variables sum product network spn recently proposed deep model consisting network sum product nodes shown competitive state art deep models certain difficult tasks image completion designing spn network architecture suitable task hand open question propose algorithm learning spn architecture data idea cluster variables opposed data instances order identify variable subsets strongly interact another nodes spn network allocated towards explaining interactions experimental evidence shows learning spn architecture significantly improves performance compared using previously proposed static architecture
system predicting action content line real time action onset humans intracranial study ability predict action content neural signals real time action onset long sought neuroscientific study decision making agency volition line real time ort prediction important understanding relation neural correlates decision making conscious voluntary action epilepsy patients implanted intracranial depth microelectrodes subdural grid electrodes clinical purposes participated matching pennies game either experimenter computer trial subjects given countdown raise left right hand immediately signal appeared computer screen fixed amount money raised different hand opponent lost amount otherwise working hypothesis experiment neural precursors subjects decisions precede action onset potentially also awareness decision move signals could detected intracranial local field potentials lfp found low frequency lfp signals combination channels especially bilateral anterior cingulate cortex supplementary motor area predictive intended left right hand movements onset signal ort system predicted hand patient would raise signal accuracy patients based results constructed ort system tracked channels simultaneously tested retrospective data patients average could predict correct hand choice trials rose correct let system drop trials less confident system demonstrates first time feasibility accurately predicting binary action real time patients intracranial recordings well action occurs
burn bias rationality anchoring bayesian inference provides unifying framework addressing problems machine learning artificial intelligence robotics well problems facing human mind unfortunately exact bayesian inference intractable simplest models therefore minds machines approximate bayesian inference approximate inference algorithms achieve wide range time accuracy tradeoffs optimal tradeoff investigate time accuracy tradeoffs using metropolis hastings algorithm metaphor mind inference algorithm find reasonably accurate decisions possible long markov chain converged posterior distribution period known burn therefore strategy optimal subject mind bounded processing speed opportunity costs perform iterations resulting samples biased towards initial value resulting cognitive process model provides rational basis anchoring adjustment heuristic model quantitative predictions tested published data anchoring numerical estimation tasks theoretical empirical results suggest anchoring bias consistent approximate bayesian inference
p300 bci masses prior information enables instant unsupervised spelling usability brain computer interfaces bci based p300 speller severely hindered need long training times many repetitions stimulus contribution introduce set unsupervised hierarchical probabilistic models tackle problems simultaneously incorporating prior knowledge sources information training subjects transfer learning information words spelled language models show due prior knowledge performance unsupervised models parallels cases even surpasses supervised models eliminating tedious training session
lattice filter model visual pathway early stages visual processing thought decorrelate whiten incoming temporally varying signals typical correlation time natural stimuli well extent temporal receptive fields lateral geniculate nucleus lgn neurons much greater neuronal time constants decorrelation must done stages combining contributions multiple neurons propose model temporal decorrelation visual pathway lattice filter signal processing device stage wise decorrelation temporal signals stage wise architecture lattice filter maps naturally onto visual pathway photoreceptors bipolar cells retinal ganglion cells lgn filter weights learned using hebbian rules stage wise sequential manner moreover predictions neural activity lattice filter model consistent physiological measurements lgn neurons fruit fly second order visual neurons therefore lattice filter model useful abstraction help unravel visual system function
analog readout optical reservoir computers reservoir computing new powerful flexible machine learning technique easily implemented hardware recently using time multiplexed architecture hardware reservoir computers reached performance comparable digital implementations operating speeds allowing real time information operation reached using optoelectronic systems present main performance bottleneck readout layer uses slow digital postprocessing designed analog readout suitable time multiplexed optoelectronic reservoir computers capable working real time readout built tested experimentally standard benchmark task performance better non reservoir methods ample room improvement present work thereby overcomes major limitations future development hardware reservoir computers
towards learning theoretic analysis spike timing dependent plasticity paper suggests learning theoretic perspective synaptic plasticity benefits global brain functioning introduce model selectron arises fast time constant limit leaky integrate fire neurons equipped spiking timing dependent plasticity stdp amenable theoretical analysis show selectron encodes reward estimates spikes error bound spikes controlled spiking margin sum synaptic weights moreover efficacy spikes usefulness reward maximizing selectrons also depends total synaptic strength finally based analysis propose regularized version stdp show regularization improves robustness neuronal learning faced multiple stimuli
neurally plausible reinforcement learning working memory tasks key function brains undoubtedly abstraction maintenance information environment later use neurons association cortex play important role process learning neurons become tuned relevant features represent information required later persistent elevation activity however well known neurons acquire task relevant tuning introduce biologically plausible learning scheme explains neurons become selective relevant information animals learn trial error propose action selection stage feeds back attentional signals earlier processing levels feedback signals interact feedforward signals form synaptic tags connections responsible stimulus response mapping globally released neuromodulatory signal interacts tagged synapses determine sign strength plasticity learning scheme generic train networks different tasks simply varying inputs rewards explains neurons association cortex learn temporarily store task relevant information non linear stimulus response mapping tasks learn optimally integrate probabilistic evidence perceptual decision making
bayesian nonparametric maximum margin matrix factorization collaborative prediction present probabilistic formulation max margin matrix factorization build accordingly infinite nonparametric bayesian model automatically resolve unknown number latent factors work demonstrates successful example integrates bayesian nonparametrics max margin learning conventionally separate paradigms enjoy complementary advantages develop efficient variational learning algorithm posterior inference extensive empirical studies large scale movielens eachmovie data sets appear demonstrate advantages inherited max margin matrix factorization bayesian nonparametrics
fusion diffusion robust visual tracking weighted graph used underlying structure many algorithms like semi supervised learning spectral clustering edge weights usually deter mined single similarity measure often hard impossible capture relevant aspects similarity using single similarity measure par ticular case visual object matching beneficial integrate different similarity measures focus different visual representations paper novel approach integrate multiple similarity measures pro posed first pairs similarity measures combined diffusion process tensor product graph tpg hence diffused similarity pair jects becomes function joint diffusion original similarities turn depends neighborhood structure tpg call process fusion diffusion however higher order graph like tpg usually means significant increase time complexity case proposed approach key feature approach time complexity dif fusion tpg diffusion process original graphs moreover necessary explicitly construct tpg frame work finally diffused pairs similarity measures combined weighted sum demonstrate advantages proposed approach task visual tracking different aspects appearance similarity target object frame target object candidates frame integrated obtained method tested several challenge video sequences experimental results show outperforms state art tracking methods
coloured noise expansion parameter estimation diffusion processes stochastic differential equations sde natural tool modelling systems inherently noisy contain uncertainties modelled stochastic processes crucial process using sde build mathematical models ability estimate parameters models observed data past decades significant progress made problem still far definitive solution describe novel method approximating diffusion process show useful markov chain monte carlo mcmc inference algorithms take white noise drives diffusion process decompose terms first coloured noise term deterministically controlled set auxilliary variables second term small enables form linear gaussian small noise approximation decomposition allows take diffusion process interest cast form amenable sampling mcmc methods explain many state art inference methods fail highly nonlinear inference problems demonstrate experimentally method performs well situations results show method promising new tool use inference parameter estimation problems
time marginalized coalescent prior hierarchical clustering introduce new prior use nonparametric bayesian hierarchical clustering prior constructed marginalizing time information kingman coalescent providing prior tree structures call time marginalized coalescent tmc allows models factorize tree structure times providing benefits flexible priors constructed efficient gibbs type inference used demonstrate example model density estimation show tmc achieves competitive experimental results
mixing properties conditional markov chains unbounded feature functions conditional markov chains also known linear chain conditional random fields literature versatile class discriminative models distribution sequence hidden states conditional sequence observable variables large sample properties conditional markov chains first studied sinn poupart paper extends work directions first mixing properties models unbounded feature functions established second necessary conditions model identifiability uniqueness maximum likelihood estimates given
representer theorem hilbert spaces necessary sufficient condition representer theorem property lies foundation regularization theory kernel methods class regularization functionals said admit linear representer theorem every member class admits minimizers lie finite dimensional subspace spanned representers data recent characterization states certain classes regularization functionals differentiable regularization term admit linear representer theorem choice data regularization term radial nondecreasing function paper extend result weakening assumptions regularization term particular main result paper implies sufficiently large family regularization functionals radial nondecreasing functions lower semicontinuous regularization terms guarantee existence representer theorem choice data
dip means incremental clustering method estimating number clusters learning number clusters key problem data clustering present dip means novel robust incremental method learn number data clusters used wrapper around iterative clustering algorithm means family contrast many popular methods make assumptions underlying cluster distributions dip means assumes fundamental cluster property cluster admit unimodal distribution proposed algorithm considers cluster member viewer applies univariate statistic hypothesis test unimodality dip test distribution distances viewer cluster members important advantages unimodality test applied univariate distance vectors directly applied kernel based methods since pairwise distances involved computations experimental results artificial real datasets indicate effectiveness method superiority analogous approaches
learning curves multi task gaussian process regression study average case performance multi task gaussian process regression captured learning curve average bayes error chosen task versus total number examples tasks covariances product input dependent covariance function free form inter task covariance matrix show accurate approximations learning curve obtained arbitrary number tasks use study asymptotic learning behaviour large surprisingly multi task learning asymptotically essentially useless examples tasks help degree inter task correlation rho near maximal value rho effect extreme learning smooth target functions described squared exponential kernels also demonstrate learning many tasks learning curves separate initial phase bayes error task reduced plateau value collective learning even though tasks seen examples final decay occurs number examples proportional number tasks
nonconvex penalization levy processes concave conjugates paper study sparsity inducing nonconvex penalty functions using levy processes define penalty laplace exponent subordinator accordingly propose novel approach construction sparsity inducing nonconvex penalties particularly show nonconvex logarithmic log exponential exp penalty functions laplace exponents gamma compound poisson subordinators respectively additionaly explore concave conjugate nonconvex penalties find log exp penalties concave conjugates negatives kullback leiber distance functions furthermore relationship penalties due asymmetricity distance
clustering aggregation maximum weight independent set formulate clustering aggregation special instance maximum weight independent set mwis problem given dataset attributed graph constructed union input clusterings generated different underlying clustering algorithms different parameters vertices represent distinct clusters weighted internal index measuring cohesion separation edges connect vertices whose corresponding clusters overlap intuitively optimal aggregated clustering obtained selecting optimal subset non overlapping clusters partitioning dataset together formalize intuition mwis problem attributed graph finding heaviest subset mutually non adjacent vertices mwis problem exhibits special structure since clusters input clustering form partition dataset vertices corresponding clustering form maximal independent set mis attributed graph propose variant simulated annealing method takes advantage special structure algorithm starts mis close distinct local optimum mwis problem utilizes local search heuristic explore neighborhood order find mwis extensive experiments many challenging datasets show approach clustering aggregation automatically decides optimal number clusters require parameter tuning underlying clustering algorithms combine advantages different underlying clustering algorithms achieve superior performance robust moderate even bad input clusterings
near optimal differentially private principal components principal components analysis pca standard tool identifying good low dimensional approximations data sets high dimension many current data sets interest contain private sensitive information individuals algorithms operate data sensitive privacy risks publishing outputs differential privacy framework developing tradeoffs privacy utility outputs paper investigate theory empirical performance differentially private approximations pca propose new method explicitly optimizes utility output demonstrate real data large performance gap existing methods method show sample complexity procedures differs scaling data dimension method nearly optimal terms scaling
high dimensional nonparanormal graph estimation via smooth projected neighborhood pursuit propose new smooth projected neighborhood pursuit method estimating high dimensional undirected graphs method viewed semiparametric extension popular neighborhood pursuit approach proposed meinshausen hlmann 2006 gaussian gaussian copula models nonparanormal models proposed liu 2009 terms methodology computation project possibly indefinite symmetric matrix cone positive semidefinite matrices projection formulated smoothed element wise ell_ infty norm minimization problem develop efficient fast proximal gradient algorithm provable optimal rate convergence sqrt epsilon epsilon desired accuracy objective value terms theory provide alternative view analyze trade computational efficiency statistical error give sufficient condition secure smooth projected neighborhood pursuit estimator achieves graph estimation consistency empirically conduct real data experiments stock genomic datasets illustrate usefulness proposed method
mkl matrix induced regularization multi kernel learning applications neuroimaging multiple kernel learning mkl generalizes svms setting simultaneously trains linear classifier chooses optimal combination given base kernels model complexity typically controlled using various norm regularizations vector base kernel mixing coefficients existing methods however neither regularize exploit potentially useful information pertaining kernels input set interact higher order kernel pair relationships easily obtained via unsupervised similarity geodesics supervised correlation errors domain knowledge driven mechanisms features used construct kernel show substituting norm penalty arbitrary quadratic function succeq impose desired covariance structure mixing coefficient selection use inductive bias learning concept formulation significantly generalizes widely used norm mkl objectives explore model utility via experiments challenging neuroimaging problem goal predict subject conversion alzheimer disease exploiting aggregate information several distinct imaging modalities new model outperforms state art values briefly discuss ramifications terms learning bounds rademacher complexity
bethe partition function log supermodular graphical models sudderth wainwright willsky conjectured bethe approximation corresponding fixed point belief propagation algorithm attractive pairwise binary graphical model provides lower bound true partition function work resolve conjecture affirmative demonstrating graphical model binary variables whose potential functions necessarily pairwise log supermodular bethe partition function always lower bounds true partition function proof result follows new variant functions theorem independent interest
simple practical algorithm differentially private data release present new algorithm differentially private data release based simple combination exponential mechanism multiplicative weights update rule mwem algorithm achieves best known nearly optimal theoretical guarantees time simple implement experimentally accurate actual data sets existing techniques
supervised learning similarity functions address problem general supervised learning data accessed indefinite similarity function data points existing work learning indefinite kernels concentrated solely binary multiclass classification problems propose model generic enough handle supervised learning task also subsumes model previously proposed classification give goodness criterion similarity functions given supervised learning task adapt well known landmarking technique provide efficient algorithms supervised learning using good similarity functions demonstrate effectiveness model important supervised learning problems real valued regression ordinal regression ranking show method guarantees bounded generalization error furthermore case real valued regression give natural goodness definition used conjunction recent result sparse vector recovery guarantees sparse predictor bounded generalization error finally report results learning algorithms regression ordinal regression tasks using non psd similarity functions demonstrate effectiveness algorithms especially sparse landmark selection algorithm achieves significantly higher accuracies baseline methods offering reduced computational costs
waveform driven plasticity bifeo3 memristive devices model implementation memristive devices recently proposed efficient implementations plastic synapses neuromorphic systems plasticity memristive devices resistance change defined applied waveforms behavior resembles biological synapses whose plasticity also triggered mechanisms determined local waveforms however learning memristive devices far approached mostly pragmatic technological level focus seems finding waveform achieves spike timing dependent plasticity stdp without regard biological veracity said waveforms important forms plasticity bridging gap make use plasticity model driven neuron waveforms explains large number experimental observations adapt characteristics recently introduced bifeo memristive material based approach show stdp first time material learning window replication superior previous memristor based stdp implementations also demonstrate measurements possible overlay short long term plasticity memristive device form well known triplet plasticity best knowledge first implementations triplet plasticity physical memristive device
learned prioritization trading accuracy speed users want natural language processing nlp systems fast accurate quality often comes cost speed field manually exploring various speed accuracy tradeoffs particular problems datasets aim explore space automatically focusing case agenda based syntactic parsing cite kay 1986 unfortunately shelf reinforcement learning techniques fail learn good policies state space simply large explore naively attempt counteract applying imitation learning algorithms also fails teacher far good successfully imitate inexpensive features moreover specifically tuned known reward function propose hybrid reinforcement apprenticeship learning algorithm even inexpensive features automatically learn weights achieve competitive accuracies significant improvements speed state art baselines
compressive sensing mri wavelet tree sparsity compressive sensing magnetic resonance imaging mri reconstruct image good quality small number measurements significantly reduce scanning time according structured sparsity theory measurements reduced mathcal log tree sparse data instead mathcal log standard sparse data length however existing algorithms utilized mri use total variation wavelet sparse regularization side algorithms proposed tree sparsity regularization validated benefit tree structure mri paper propose fast convex optimization algorithm improve mri wavelet sparsity gradient sparsity tree sparsity considered model real images original complex problem decomposed simpler subproblems subproblems efficiently solved iterative scheme numerous experiments conducted show proposed algorithm outperforms state art mri algorithms gain better reconstructions results real images general tree based solvers algorithms
rational inference relative preferences statistical decision theory axiomatically assumes relative desirability different options humans perceive well described assigning option specific scalar utility functions however assumption refuted observed human behavior including studies wherein preferences shown change systematically simply variation set choice options presented paper show interpreting desirability relative comparison available options particular decision instance results rational theory value inference explains heretofore intractable violations rational choice behavior human subjects complementarily also characterize conditions rational agent selecting optimal options indicated dynamic value inference framework behave identically whose preferences encoded using static ordinal utility function
connections saliency tracking model connecting visual tracking saliency recently proposed model based saliency hypothesis tracking postulates tracking achieved top tuning based target features discriminant center surround saliency mechanisms time work identify main predictions must hold hypothesis true tracking reliability larger salient non salient targets tracking reliability dependence defining variables saliency namely feature contrast distractor heterogeneity must replicate dependence saliency variables saliency tracking implemented common low level neural mechanisms confirm first predictions hold reporting results set human behavior studies connection saliency tracking also show third prediction holds constructing common neurophysiologically plausible architecture computationally solve saliency tracking architecture fully compliant standard physiological models known attentional control area lip explaining results human behavior experiments
permutation hashing minwise hashing promising large scale learning massive binary data preprocessing cost prohibitive requires applying 500 permutations data testing time also expensive new data point new document new image processed paper develop simple textbf permutation hashing scheme address important issue true preprocessing step parallelized comes cost additional hardware implementation also reducing permutations would much textbf energy efficient might important perspective minwise hashing commonly deployed search industry theoretical probability analysis interesting experiments similarity estimation svm logistic regression also confirm theoretical results
polynomial time form robust regression despite variety robust regression methods developed current regression formulations either hard allow unbounded response even single leverage point present general formulation robust regression variational estimation unifies number robust regression methods allowing tractable approximation strategy develop estimator requires polynomial time achieving certain robustness consistency guarantees experimental evaluation demonstrates effectiveness new estimation approach compared standard methods
learning halfspaces loss time accuracy tradeoffs given alpha epsilon study time complexity required improperly learn halfspace misclassification error rate alpha gamma epsilon gamma optimal gamma margin error rate alpha gamma polynomial time sample complexity achievable using hinge loss alpha cite shalevshsr11 showed poly gamma time impossible learning possible time exp tilde gamma immediate question paper tackles achievable alpha gamma derive positive results interpolating polynomial time alpha gamma exponential time alpha particular show cases alpha gamma problem still solvable polynomial time results naturally extend adversarial online learning model pac learning malicious noise model
assessing blinding clinical trials interaction patient expected outcome intervention inherent effects intervention extraordinary effects thus clinical trials effort made conceal nature administered intervention participants trial blind yet practice perfect blinding impossible ensure even verify current standard follow trial auxiliary questionnaire allows trial participants express belief concerning assigned intervention used compute measure extent blinding trial estimated extent blinding exceeds threshold trial deemed sufficiently blinded otherwise trial deemed failed paper make several important contributions firstly identify series fundamental problems aforesaid practice discuss context commonly used blinding measures secondly motivated highlighted problems formulate novel method handling imperfectly blinded trials adopt post trial feedback questionnaire interpret collected data using original approach fundamentally different previously proposed unlike previous approaches void hoc free parameters robust small changes auxiliary data predicated strong assumptions used interpret participants feedback
globally convergent dual map relaxation solvers using fenchel young margins finding exact solution map inference problem intractable many real world tasks map relaxations shown effective practice however efficient methods perform block coordinate descent get stuck sub optimal points globally convergent work propose augment algorithms epsilon descent approach present method efficiently optimize descent direction subdifferential using margin based extension fenchel young duality theorem furthermore presented approach provides methodology construct primal optimal solution dual optimal counterpart demonstrate efficiency presented approach spin glass models protein interactions problems show approach outperforms state art solvers
semantic kernel forests multiple taxonomies learning features complex visual recognition problems labeled image exemplars alone insufficient emph object taxonomy specifying categories semantic relationships could bolster learning process relationships relevant given visual classification task single taxonomy capture ties emph relevant light issues propose discriminative feature learning approach leverages emph multiple hierarchical taxonomies representing different semantic views object categories animal classes taxonomy could reflect phylogenic ties another could reflect habitats taxonomy first learn tree semantic kernels node mahalanobis kernel optimized distinguish classes children nodes using resulting emph semantic kernel forest learn class specific kernel combinations select relationships relevant recognize object class learn weights introduce novel hierarchical regularization term exploits taxonomies structure demonstrate method challenging object recognition datasets show interleaving multiple taxonomic views yields significant accuracy improvements
kernel latent svm visual recognition latent svms lsvms class powerful tools successfully applied many applications computer vision however limitation lsvms rely linear models many computer vision tasks linear models suboptimal nonlinear models learned kernels typically perform much better therefore desirable develop kernel version lsvm paper propose kernel latent svm klsvm new learning framework combines latent svms kernel methods develop iterative training algorithm learn model parameters demonstrate effectiveness klsvm using different applications visual recognition klsvm formulation general applied solve wide range applications computer vision machine learning
discriminatively trained sparse code gradients contour detection finding contours natural images fundamental problem serves basis many tasks image segmentation object recognition core contour detection technologies set hand designed gradient features used existing approaches including state art global gpb operator work show contour detection accuracy significantly improved computing sparse code gradients scg measure contrast using patch representations automatically learned sparse coding use svd orthogonal matching pursuit efficient dictionary learning encoding use multi scale pooling power transforms code oriented local neighborhoods computing gradients applying linear svm extracting rich representations pixels avoiding collapsing prematurely sparse code gradients effectively learn measure local contrasts find contours improve measure metric bsds500 benchmark gpb contours moreover learning approach easily adapt novel sensor data kinect style rgb cameras sparse code gradients depth images surface normals lead promising contour detection using depth depth color verified nyu depth dataset work combines concept oriented gradients sparse representation opens future possibilities learning contour detection segmentation
learning align scratch unsupervised joint alignment images demonstrated improve performance recognition tasks face verification alignment reduces undesired variability due factors pose requiring weak supervision form poorly aligned examples however prior work unsupervised alignment complex real world images required careful selection feature representation based hand crafted image descriptors order achieve appropriate smooth optimization landscape paper instead propose novel combination unsupervised joint alignment unsupervised feature learning specifically incorporate deep learning congealing alignment framework deep learning obtain features represent image differing resolutions based network depth tuned statistics specific data aligned addition modify learning algorithm restricted boltzmann machine incorporating group sparsity penalty leading topographic organization learned filters improving subsequent alignment results apply method labeled faces wild database lfw using aligned images produced proposed unsupervised algorithm achieve significantly higher accuracy face verification obtained using original face images prior work unsupervised alignment prior work supervised alignment also match accuracy best available unpublished method
calibrated elastic regularization matrix completion paper concerns problem matrix completion estimate matrix observations small subset indices propose calibrated spectrum elastic net method sum nuclear frobenius penalties develop iterative algorithm solve convex minimization problem iterative algorithm alternates imputing missing entries incomplete matrix current guess estimating matrix scaled soft thresholding singular value decomposition imputed matrix resulting matrix converges calibration step follows correct bias caused frobenius penalty proper coherence conditions suitable penalties levels prove proposed estimator achieves error bound nearly optimal order proportion noise level provides unified analysis noisy noiseless matrix completion problems simulation results presented compare proposal previous ones
provable ica unknown gaussian noise implications gaussian mixtures autoencoders present new algorithm independent component analysis ica provable performance guarantees particular suppose given samples form eta unknown times matrix chosen uniformly random eta dimensional gaussian random variable unknown covariance sigma give algorithm provable recovers sigma additive epsilon whose running time sample complexity polynomial epsilon accomplish introduce novel quasi whitening step useful contexts covariance gaussian noise known advance also give general framework finding local optima function given oracle approximately finding crucial step algorithm overlooked previous attempts allows control accumulation error find columns via local search
timely object recognition large visual multi class detection framework timeliness results crucial method timely multi class detection aims give best possible performance single point start time terminated deadline time toward goal formulate dynamic closed loop policy infers contents image order decide detector deploy next contrast previous work method significantly diverges predominant greedy strategies able learn take actions deferred values evaluate method novel timeliness measure computed area average precision time curve experiments conducted eminent pascal voc object detection dataset execution stopped half detectors run method obtains better random ordering better performance intelligent baseline timeliness measure method obtains least better performance code made available upon publication easily extensible treats detectors classifiers black boxes learns execution traces using reinforcement learning
random function priors exchangeable graphs arrays fundamental problem analysis relational data graphs matrices higher dimensional arrays extract summary common structure underlying relations individual entities successful approach latent variable modeling summarizes structure embedding suitable latent space results probability theory due aldous hoover kallenberg show relational data satisfying exchangeability property represented terms random measurable function bayesian model function constitutes natural model parameter discuss available latent variable models classified according implicitly approximate parameter obtain flexible yet simple model relational data representing parameter function gaussian process efficient inference draws large available arsenal gaussian process algorithms sparse approximations prove particularly useful demonstrate applications model network data clarify relation models literature several emerge special cases
stochastic gradient descent projection although many variants stochastic gradient descent proposed large scale convex optimization require projecting solution iteration ensure obtained solution stays within feasible domain complex domains positive semidefinite cone projection step computationally expensive making stochastic gradient descent unattractive large scale optimization problems address limitation developing novel stochastic gradient descent algorithm need intermediate projections instead projection last iteration needed obtain feasible solution given domain theoretical analysis shows high probability proposed algorithms achieve sqrt convergence rate general convex optimization rate strongly convex optimization mild conditions domain objective function
multi task averaging present multi task learning approach jointly estimate means multiple independent data sets proposed multi task averaging mta algorithm results convex combination single task averages derive optimal amount regularization show effectively estimated simulations real data experiments demonstrate mta maximum likelihood james stein estimators approach estimating amount regularization rivals cross validation performance computationally efficient
stochastic optimization sparse statistical recovery optimal algorithms high dimensions develop analyze stochastic optimization algorithms problems expected loss strongly convex optimum approximately sparse previous approaches able exploit structures yielding order pdim convergence rate strongly convex objectives pdim dimensions order sqrt spindex log pdim convergence rate optimum spindex sparse algorithm based successively solving series ell_1 regularized optimization problems using nesterov dual averaging algorithm establish error solution iterations order spindex log pdim natural extensions approximate sparsity results apply locally lipschitz losses including logistic exponential hinge least squares losses recourse statistical minimax results show convergence rates optimal constants effectiveness approach also confirmed numerical simulations compare several baselines least squares regression problem
perturbed variation introduce new discrepancy score distributions gives indication emph similarity much research done determine samples come exactly distribution much less research considered problem determining finite samples come similar distributions new score gives intuitive interpretation similarity optimally perturbs distributions best fit score defined distributions efficiently estimated samples provide convergence bounds estimated score develop hypothesis testing procedures test data sets come similar distributions statistical power procedures presented simulations also compare score capacity detect similarity known measures real data
graphical gaussian vector image categorization paper proposes novel image representation called graphical gaussian vector counterpart codebook local feature matching approaches method model distribution local features gaussian markov random field gmrf efficiently represent spatial relationship among local features consider parameter gmrf feature vector image using concepts information geometry proper parameters metric gmrf obtained finally define new image feature embedding metric parameters directly applied scalable linear classifiers method obtains superior performance state art methods standard object recognition datasets comparable performance scene dataset proposed method simply calculates local auto correlations local features able achieve high classification accuracy high efficiency
hierarchical optimistic region selection driven curiosity paper aims take step forwards making term intrinsic motivation reinforcement learning theoretically well founded focusing curiosity driven learning end consider setting fixed partition continuous space given process defined unknown asked sequentially decide cell partition select well sample cell order minimize loss function inspired previous work curiosity driven learning loss cell consists term measuring simple worst case quadratic sampling error penalty term proportional range variance cell corresponding problem formulation extends setting known active learning multi armed bandits case arm continuous region show adaptation recent algorithms problem hierarchical optimistic sampling algorithms optimization used order solve problem resulting procedure called hierarchical optimistic region selection driven curiosity horse provided together finite time regret analysis
stochastic gradient method exponential convergence rate finite training sets propose new stochastic gradient method optimizing sum finite set smooth functions sum strongly convex standard stochastic gradient methods converge sublinear rates problem proposed method incorporates memory previous gradient values order achieve linear convergence rate machine learning context numerical experiments indicate new algorithm dramatically outperform standard algorithms terms optimizing training error reducing test error quickly
active comparison prediction models address problem comparing risks given predictive models instance baseline model challenger confidently possible fixed labeling budget problem occurs whenever models cannot compared held training data possibly training data unavailable reflect desired test distribution case new test instances drawn labeled cost devise active comparison method selects instances according instrumental sampling distribution derive sampling distribution maximizes power statistical test applied observed empirical risks thereby minimizes likelihood choosing inferior model empirically investigate model selection problems several classification regression tasks study accuracy resulting values
interpreting prediction markets stochastic approach strengthen recent connections prediction markets learning showing natural class market makers understood performing stochastic mirror descent trader demands sequentially drawn fixed distribution provides new insights market prices price paths interpreted summary market belief distribution relating optimization problem solved particular show stationary point stochastic process prices generated market equal market walrasian equilibrium classic market analysis together results suggest traditional market making mechanisms might replaced general purpose learning algorithms still retaining guarantees behaviour
lifting gibbs sampling algorithm statistical relational learning models combine power first order logic facto tool handling relational structure probabilistic graphical models facto tool handling uncertainty lifted probabilistic inference algorithms subject much recent research main idea algorithms improve speed accuracy scalability existing graphical models inference algorithms exploiting symmetry first order representation paper consider blocked gibbs sampling advanced variation classic gibbs sampling algorithm lift first order level propose achieve partitioning first order atoms relational model set disjoint clusters exact lifted inference polynomial cluster given assignment atoms cluster propose approach constructing clusters determining complexity show used trade accuracy computational complexity principled manner experimental evaluation shows lifted gibbs sampling superior propositional algorithm terms accuracy convergence
tca high dimensional principal component analysis non gaussian data propose high dimensional semiparametric scale invariant principle component analysis named tca utilize natural connection elliptical distribution family principal component analysis elliptical distribution family includes many well known multivariate distributions like multivariate logistic extended meta elliptical fang 2002 using copula techniques paper extend meta elliptical distribution family even larger family called transelliptical prove tca obtain near optimal log estimation consistency rate transelliptical distribution family even distributions heavy tailed infinite second moments densities possess arbitrarily continuous marginal distributions feature selection result explicit rate also provided tca also implemented numerical simulations large scale stock data illustrate empirical performance theories experiments confirm tca achieve model flexibility estimation accuracy robustness almost cost
object detection viewpoint estimation deformable cuboid model paper addresses problem category level object detection given monocular image aim localize objects enclosing tight oriented bounding boxes propose novel approach extends well acclaimed deformable part based model felz reason model represents object class deformable cuboid composed faces parts allowed deform respect anchors box model appearance face fronto parallel coordinates thus effectively factoring appearance variation induced viewpoint model reasons face visibility patters called aspects train cuboid model jointly discriminatively share weights across aspects attain efficiency inference entails sliding rotating box scoring object hypotheses inference discretize search space variables continuous model demonstrate effectiveness approach indoor outdoor scenarios show approach outperforms state art felz09 object detection hedau12
generative model parts based object segmentation shape boltzmann machine sbm recently introduced state art model foreground background object shape extend sbm account foreground object parts model multinomial sbm msbm capture local global statistics part shapes accurately combine msbm appearance model form fully generative model images objects parts based image segmentations obtained simply performing probabilistic inference model apply model challenging datasets exhibit significant shape appearance variability find obtains results comparable state art
matrix reconstruction local max norm introduce new family matrix norms local max norms generalizing existing methods max norm trace norm nuclear norm weighted smoothed weighted trace norms extensively used literature regularizers matrix reconstruction problems show new family used interpolate weighted unweighted trace norm conservative max norm test interpolation simulated data large scale netflix movielens ratings data find improved accuracy relative existing matrix norms also provide theoretical results showing learning guarantees new norms
finding exemplars pairwise dissimilarities via simultaneous sparse recovery given pairwise dissimilarities data points consider problem finding subset data points called representatives exemplars efficiently describe data collection formulate problem row sparsity regularized trace minimization problem solved efficiently using convex programming solution proposed optimization program finds representatives probability data point associated representatives obtain range regularization parameter solution proposed optimization program changes selecting representative selecting data points representatives data points distributed around multiple clusters according dissimilarities show data cluster select representatives cluster unlike metric based methods algorithm require pairwise dissimilarities metrics applied dissimilarities asymmetric violate triangle inequality demonstrate effectiveness proposed algorithm synthetic data well real world datasets images text
online allocation homogeneous partitioning piecewise constant mean approximation setting active learning multi armed bandit goal learner estimate equal precision mean finite number arms recent results show possible derive strategies based finite time confidence bounds competitive best possible strategy consider extension problem case arms cells finite partition continuous sampling space subset real goal build piecewise constant approximation noisy function piece region fixed beforehand order maintain local quadratic error approximation cell equally low although extension trivial show simple algorithm based upper confidence bounds proved adaptive function near optimal way chosen minimax optimal order class alpha lder functions
risk aversion multi armed bandits stochastic multi armed bandits objective solve exploration exploitation dilemma ultimately maximize expected reward nonetheless many practical problems maximizing expected reward desirable objective paper introduce novel setting based principle risk aversion objective compete arm best risk return trade setting proves intrinsically difficult standard multi arm bandit setting due part exploration risk introduces regret associated variability algorithm using variance measure risk introduce new algorithms investigate theoretical guarantees report preliminary empirical results
scaled gradients grassmann manifolds matrix completion paper describes gradient methods based scaled metric grassmann manifold low rank matrix completion proposed methods significantly improve canonical gradient methods especially ill conditioned matrices maintaining established global convegence exact recovery guarantees connection form subspace iteration matrix completion scaled gradient descent procedure also established proposed conjugate gradient method based scaled gradient outperforms several existing algorithms matrix completion competitive recently proposed methods
bayesian active learning localized priors fast receptive field characterization active learning substantially improve yield neurophysiology experiments adaptively selecting stimuli probe neuron receptive field real time bayesian active learning methods maintain posterior distribution select stimuli maximally reduce posterior entropy time step however existing methods tend rely simple gaussian priors exploit uncertainty level hyperparameters determining optimal stimulus uncertainty play substantial role characterization particularly rfs smooth sparse local space time paper describe novel framework active learning hierarchical conditionally gaussian priors algorithm uses sequential markov chain monte carlo sampling particle filtering mcmc hyperparameters construct mixture gaussians representation posterior selects optimal stimuli using approximate infomax criterion core elements algorithm parallelizable making computationally efficient real time experiments apply algorithm simulated real neural data show provide highly accurate receptive field estimates limited data even small number hyperparameter samples
expectation propagation gaussian process dynamical systems rich complex time series data generated engineering sys tems financial markets videos neural recordings common feature modern data analysis explaining phenomena underlying diverse data sets requires flexible accurate models paper promote gaussian process dynamical systems rich model class appropriate analysis particular present message passing algorithm approximate inference gpdss based expectation propagation phrasing inference general mes sage passing problem iterate forward backward smoothing obtain accurate posterior distributions latent structures resulting improved pre dictive performance compared state art gpds smoothers spe cial cases general iterative message passing algorithm hence provide unifying approach within contextualize message passing gpdss
minimizing uncertainty pipelines paper consider problem debugging large pipelines human labeling represent execution pipeline using directed acyclic graph nodes node represents data item produced operator pipeline assume operator assigns confidence output data want reduce uncertainty output issuing queries human expert query consists checking given data item correct paper consider problem asking optimal set queries minimize resulting output uncertainty perform detailed evaluation complexity problem various classes graphs give efficient algorithms problem trees show general dag problem intractable
training sparse natural image models fast gibbs sampler extended state space present new learning strategy based efficient blocked gibbs sampler sparse overcomplete linear models particular emphasis placed statistical image modeling overcomplete models played important role discovering sparse representations gibbs sampler faster general purpose sampling schemes also requiring tuning free parameters using gibbs sampler persistent variant expectation maximization able extract highly sparse distributions latent sources data applied natural images algorithm learns source distributions resemble spike slab distributions evaluate likelihood quantitatively compare performance overcomplete linear model complete counterpart well product experts model represents another overcomplete generalization complete linear model contrast previous claims find overcomplete representations lead significant improvements overcomplete linear model still underperforms models
polylog pivot steps simplex algorithm classification present simplex algorithm linear programming linear classification formulation paramount complexity parameter linear classification problems called margin prove margin values practical interest simplex variant performs polylogarithmic number pivot steps worst case overall running time near linear contrast general linear programming sub polynomial pivot rule known
learning partially absorbing random walks propose novel stochastic process probability alpha_i absorbed current state probability alpha_i follows random edge analyze properties show potential exploring graph structures prove proper absorption rates random walk starting set mathcal low conductance mostly absorbed mathcal moreover absorption probabilities vary slowly inside mathcal dropping sharply outside mathcal thus implementing desirable cluster assumption graph based learning remarkably partially absorbing process unifies many popular models arising variety contexts provides new insights makes possible transferring findings paradigm another simulation results demonstrate promising applications graph based learning
learning probability measures respect optimal transport metrics study problem estimating sense optimal transport metrics measure assumed supported manifold embedded hilbert space establishing precise connection optimal transport metrics optimal quantization learning theory derive new probabilistic bounds performance classic algorithm unsupervised learning means used produce probability measure derived data course analysis arrive new lower bounds well probabilistic bounds convergence rate empirical law large numbers unlike existing bounds applicable wide class measures
modeling forgetting process using image regions long term human visual memory store remarkable amount visual information tends degrade time recent works shown image memorability intrinsic property image reliably estimated using state art image features machine learning algorithms however class features image information forgotten time explored yet work propose probabilistic framework models local regions image forgotten time using data driven approach combines local global images features model automatically discovers memorability maps individual images without human annotation incorporate multiple image region attributes algorithm leading improved memorability prediction images compared previous works
high dimensional transelliptical graphical models advocate use new distribution family transelliptical robust inference high dimensional graphical models transelliptical family extension nonparanormal family proposed liu 2009 nonparanormal extends normal transforming variables using univariate functions transelliptical extends elliptical family way propose nonparametric rank based regularization estimator achieves parametric rates convergence graph recovery parameter estimation result suggests extra robustness flexibility obtained semiparametric transelliptical modeling incurs almost efficiency loss thorough numerical experiments provided back theory
tensor decomposition fast parsing latent variable pcfgs describe approach speed inference latent variable pcfgs shown highly effective natural language parsing approach based tensor formulation recently introduced spectral estimation latent variable pcfgs coupled tensor decomposition algorithm well known multilinear algebra literature also describe error bound approximation bounds difference probabilities calculated algorithm true probabilities approximated model gives empirical evaluation real world natural language parsing data demonstrates significant speed minimal cost parsing performance
better way pre train deep boltzmann machines describe pre training algorithm deep boltzmann machines dbms related pre training algorithm deep belief networks show certain conditions pre training procedure improves variational lower bound hidden layer dbm based analysis develop different method pre training dbms distributes modelling work evenly hidden layers results mnist norb datasets demonstrate new pre training algorithm allows learn better generative models
phoneme classification using constrained variational gaussian process dynamical system paper describes new acoustic model based variational gaussian process dynamical system vgpds phoneme classification proposed model overcomes limitations classical hmm modeling real speech data adopting nonlinear nonparametric model model prior dynamics function enables representing complex dynamic structure speech prior emission function successfully models global dependency observations additionally introduce variance constraint original vgpds mitigating sparse approximation error kernel matrix effectiveness proposed model demonstrated extensive experimental results including parameter estimation classification performance synthetic benchmark datasets
active learning model evidence using bayesian quadrature numerical integration key component many problems scientific computing statistical modelling machine learning bayesian quadrature model based method numerical integration relative standard monte carlo methods offers increased sample efficiency robust estimate uncertainty estimated integral propose novel bayesian quadrature approach numerical integration integrand non negative case computing marginal likelihood predictive distribution normalising constant probabilistic model approach approximately marginalises quadrature model hyperparameters closed form introduces active learning scheme optimally select function evaluations opposed using monte carlo samples demonstrate method number synthetic benchmarks real scientific problem astronomy
lucid locally uniform comparison image descriptor keypoint matching pairs images using popular descriptors like sift faster variant called surf heart many computer vision algorithms including recognition mosaicing structure motion real time mobile applications fast less accurate descriptors like brief related methods use random sampling pairwise comparisons pixel intensities image patch introduce locally uniform comparison image descriptor lucid simple description method based permutation distances ordering intensities rgb values patches lucid computable linear time respect patch size require floating point computation analysis reveals underlying issue limits potential brief related approaches compared lucid experiments demonstrate lucid faster brief accuracy directly comparable surf order magnitude faster
scaling mpe inference constrained continuous markov random fields consensus optimization probabilistic graphical models powerful tools analyzing constrained continuous domains however finding probable explanations mpes models computationally expensive paper improve scalability mpe inference class graphical models piecewise linear piecewise quadratic dependencies linear constraints continuous domains derive algorithms based consensus optimization framework demonstrate superior performance state art show empirically large scale voter preference modeling problem algorithms scale linearly number dependencies constraints
putting bayes sleep consider sequential prediction algorithms given predictions set models inputs nature data changing time different models predict well different segments data adaptivity typically achieved mixing weights round bit initial prior kind like weak restart however favored models segment small subset data likely predicted well models predicted well curiously fitting sparse composite models achieved mixing bit past posteriors self referential updating method rather peculiar efficient gives superior performance many natural data sets also important introduces long term memory model done well past recovered quickly bayesian interpretations found mixing bit initial prior bayesian interpretation known mixing past posteriors build atop specialist framework online learning literature give mixing past posteriors update proper bayesian foundation apply method well studied multitask learning problem obtain new intriguing efficient update achieves significantly better bound
convex multi view subspace learning subspace learning seeks low dimensional representation data enables accurate reconstruction however many applications data obtained multiple sources rather single source object might viewed cameras different angles document might consist text images conditional independence separate sources imposes constraints shared latent representation respected improve quality learned low dimensional representation paper present convex formulation multi view subspace learning enforces conditional independence reducing dimensionality formulation develop efficient algorithm recovers optimal data reconstruction exploiting implicit convex regularizer recovers corresponding latent representation reconstruction model jointly optimally experiments illustrate proposed method produces high quality results
entangled monte carlo propose novel method scalable parallelization smc algorithms entangled monte carlo simulation emc emc avoids transmission particles nodes instead reconstructs particle genealogy particular show reduce communication particle weights machine efficiently maintaining implicit global coherence parallel simulation explain methods efficiently maintain genealogy particles particle reconstructed demonstrate using examples bayesian phylogenetic computational gain parallelization using emc significantly outweighs cost particle reconstruction timing experiments show reconstruction particles indeed much efficient compared transmission particles
slice normalized dynamic markov logic networks markov logic widely used tool statistical relational learning uses weighted first order logic knowledge base specify markov random field mrf conditional random field crf many applications markov logic network mln trained domain used different paper focuses dynamic markov logic networks domain time points typically varies training testing previously pointed marginal probabilities truth assignments ground atoms change extends reduces domains predicates mln show addition problem standard way unrolling markov logic theory mrf result time inhomogeneity underlying markov chain furthermore even representational problems significant given domain show practical problem generating samples sequential conditional random field next slice relying samples previous slice high computational cost general case due need estimate normalization factor sample propose new discriminative model slice normalized dynamic markov logic networks dmln suffers none issues supports efficient online inference directly model influences variables within time slice causal direction contrast fully directed models dbns experimental results show improvement accuracy previous approaches online inference dynamic markov logic networks
spectral learning linear dynamics generalised linear observations application neural population data latent linear dynamical systems generalised linear observation models arise variety applications example modelling spiking activity populations neurons show spectral learning methods linear systems gaussian observations usually called subspace identification context extended estimate parameters dynamical system models observed non gaussian noise models use approach obtain estimates parameters dynamical model neural population data observed spike counts poisson distributed log rates determined latent dynamical process possibly driven external inputs show extended system identification algorithm consistent accurately recovers correct parameters large simulated data sets much smaller computational cost approximate expectation maximisation due non iterative nature subspace identification even smaller data sets provides effective initialization leading robust performance faster convergence benefits shown extend real neural data
forward backward activation algorithm hierarchical hidden markov models hierarchical hidden markov models hhmms sophisticated stochastic models enable capture hierarchical context characterization sequence data however existing hhmm parameter estimation methods require large computations time complexity least model inference depth hierarchy number states level sequence length paper propose new inference method hhmms time complexity key idea algorithm application forward backward algorithm state activation probabilities notion state activation offers simple formalization hierarchical transition behavior hhmms enables conduct model inference efficiently present experiments demonstrate proposed method works efficiently estimate hhmm parameters existing methods flattening method gibbs sampling method
finite sample convergence rates order stochastic optimization methods consider derivative free algorithms stochastic optimization problems use noisy function values rather gradients analyzing finite sample convergence rates show pairs function values available algorithms use gradient estimates based random perturbations suffer factor sqrt dim convergence rate traditional stochastic gradient methods dim dimension problem complement algorithmic development information theoretic lower bounds minimax convergence rate problems show bounds sharp respect problem dependent quantities cannot improved constant factors
learning partially observable models using temporally abstract decision trees paper introduces timeline trees partial models partially observable environments timeline trees given specific predictions make learn decision tree history main idea timeline trees use temporally abstract features identify split features key events spread arbitrarily far apart past whereas previous decision tree based methods limited finite suffix history experiments demonstrate timeline trees learn make high quality predictions complex partially observable environments high dimensional observations arcade game
line reinforcement learning using incremental kernel based stochastic factorization ability learn policy sequential decision problem continuous state space using line data long standing challenge paper presents new reinforcement learning algorithm called ikbsf extends benefits kernel based learning line scenario kernel based method proposed algorithm stable good convergence properties however unlike similar algorithms ikbsf space complexity independent number sample transitions result process arbitrary amount data present theoretical results showing ikbsf approximate level accuracy value function would learned equivalent batch non parametric kernel based reinforcement learning approximator order show effectiveness proposed algorithm practice apply ikbsf challenging pole balancing task ability process large number transitions crucial achieving high success rate
active learning multi index function models consider problem actively learning textit multi index functions form vecx mata vecx sum_ g_i veca_i vecx point evaluations assume function defined ell_2 ball real twice continuously differentiable almost everywhere mata mathbb times rank matrix propose randomized active sampling scheme estimating functions uniform approximation guarantees theoretical developments leverage recent techniques low rank matrix recovery enables derive estimator function along sample complexity bounds also characterize noise robustness scheme provide empirical evidence high dimensional scaling sample complexity bounds quite accurate
emergence object selective features unsupervised feature learning recent work unsupervised feature learning focused goal discovering high level features unlabeled images much progress made direction cases still standard use large amount labeled data order construct detectors sensitive object classes complex patterns data paper aim test hypothesis unsupervised feature learning methods provided unlabeled data learn high level invariant features sensitive commonly occurring objects though handful prior results suggest possible object class accounts large fraction data many labeled datasets unclear whether something similar accomplished dealing completely unlabeled data major obstacle test however scale cannot expect succeed small datasets small numbers learned features propose large scale feature learning system enables carry experiment learning 150 000 features tens millions unlabeled images based scalable clustering algorithms means agglomerative clustering find simple system discover features sensitive commonly occurring object class human faces also combine detectors invariant significant global distortions like large translations scale
fast resampling weighted statistics paper novel computationally fast alternative algorithm com puting weighted statistics resampling univariate multivariate data proposed avoid real resampling linked problem finite group action converted problem orbit enumeration computational cost reduction efficient method developed list orbits symmetry order calculate index function orbit sums data function orbit sums recursively computational complexity analysis shows reduction computational cost level low order polynomial level
perfect dimensionality recovery variational bayesian pca variational bayesian approach best tractable approximations bayesian estimation demonstrated perform well many applications however good performance fully understood theoretically example sometimes produces sparse solution regarded practical advantage sparsity hardly observed rigorous bayesian estimation paper focus probabilistic pca give theoretical insight empirical success specifically situation noise variance unknown derive sufficient condition perfect recovery true pca dimensionality large scale limit size observed matrix goes infinity analysis obtain bounds noise variance estimator simple closed form solutions parameters actually useful better implementation pca
minimization continuous bethe approximations positive variation develop convergent minimization algorithms bethe variational approximations explicitly constrain marginal estimates families valid distributions existing message passing algorithms define fixed point iterations corresponding stationary points bethe free energy greedy dynamics distinguish local minima maxima fail converge continuous estimation problems instability linked creation invalid marginal estimates gaussians negative variance conversely approach leverages multiplier methods well understood convergence properties uses bound projection methods ensure marginal approximations valid iterations derive general algorithms discrete gaussian pairwise markov random fields showing improvements standard loopy belief propagation also apply method hybrid model discrete continuous variables showing improvements expectation propagation
continuous relaxations discrete hamiltonian monte carlo continuous relaxations play important role discrete optimization seen much use approximate probabilistic inference show general form gaussian integral trick makes possible transform wide class discrete variable undirected models fully continuous systems continuous representation allows use gradient based hamiltonian monte carlo inference results new ways estimating normalization constants partition functions general opens number new avenues inference difficult discrete systems demonstrate continuous relaxation inference algorithms number illustrative problems
adaptive stratified sampling monte carlo integration differentiable functions consider problem adaptive stratified sampling monte carlo integration differentiable function given finite number evaluations function construct sampling scheme samples often regions function oscillates allocating samples well spread domain notion shares similitude low discrepancy prove estimate returned algorithm almost accurate estimate optimal oracle strategy would know variations function everywhere would return provide finite sample analysis
spectral learning general weighted automata via constrained matrix completion many tasks text speech processing computational biology involve functions variable length strings real numbers wide class functions computed weighted automata spectral methods based singular value decompositions hankel matrices recently proposed learning probability distributions strings computed weighted automata paper show method applied problem learning general weighted automata sample string label pairs generated arbitrary distribution main obstruction approach general entries hankel matrix needs decomposed missing propose solution based solving constrained matrix completion problem combining ingredients whole new family algorithms learning general weighted automata obtained generalization bounds particular algorithm class given proofs rely stability analysis matrix completion spectral learning
learning target prior conventional approaches supervised parametric learning relations data target variables provided training sets consisting pairs corresponded data target variables work describe new learning scheme parametric learning target variables modeled prior model relations data target variables estimated set uncorresponded data training term method learning target priors ltp specifically ltp learning seeks parameter maximizes log likelihood uncorresponded training set regards compared conventional semi supervised learning approach ltp make efficient use prior knowledge target variables form probabilistic distributions thus removes reduces reliance training data learning compared bayesian approach learned parametric regressor ltp efficiently implemented deployed tasks running efficiency critical line bci signal decoding demonstrate effectiveness proposed approach parametric regression tasks bci signal decoding pose estimation video
multimodal learning deep boltzmann machines propose deep boltzmann machine learning generative model multimodal data show use model extract meaningful representation multimodal data find learned representation useful classification information retreival tasks hence conforms notion semantic similarity model defines probability density space multimodal inputs sampling conditional distributions data modality possible create representation even data modalities missing experimental results modal data consisting images text show multimodal dbm learn good generative model joint space image text inputs useful information retrieval unimodal multimodal queries demonstrate model significantly outperform svms lda discriminative tasks finally compare model deep learning methods including autoencoders deep belief networks show achieves significant gains
accelerated training matrix norm regularization boosting approach sparse learning models typically combine smooth loss nonsmooth penalty trace norm although recent developments sparse approximation offered promising solution methods current approaches either apply matrix norm constrained problems provide suboptimal convergence rates paper propose boosting method regularized learning guarantees epsilon accuracy within epsilon iterations performance accelerated interlacing boosting fixed rank local optimization exploiting simpler local objective previous work proposed method yields state art performance large scale problems also demonstrate application latent multiview learning provide first efficient weak oracle
deep representations codes image auto annotation task assigning set relevant tags image challenging due size variability tag vocabularies consequently existing algorithms focus tag assignment fix often large number hand crafted features describe image characteristics paper introduce hierarchical model learning representations full sized color images pixel level removing need engineered feature representations subsequent feature selection benchmark model stl recognition dataset achieving state art performance features combined tagprop guillaumin outperform compete existing annotation approaches use dozen distinct image descriptors furthermore using 256 bit codes hamming distance training tagprop exchange small reduction performance efficient storage fast comparisons experiments using deeper architectures always outperform shallow ones
max margin structured output regression spatio temporal action localization structured output learning successfully applied object localization mapping image object bounding box well captured extension action localization videos however much challenging needs predict locations action patterns spatially temporally identifying sequence bounding boxes track action video problem becomes intractable due exponentially large size structured video space actions could occur propose novel structured learning approach spatio temporal action localization mapping video spatio temporal action trajectory learned intractable inference learning problems addressed leveraging efficient max path search method thus makes feasible optimize model whole structured space experiments challenging benchmark datasets show proposed method outperforms state art methods
multiplicative forests continuous time processes learning temporal dependencies variables continuous time important challenging task continuous time bayesian networks effectively model processes limited number conditional intensity matrices grows exponentially number parents per variable develop partition based representation using regression trees forests whose parameter spaces grow linearly number node splits using multiplicative assumption show update forest likelihood closed form producing efficient model updates results show multiplicative forests learned temporal trajectories large gains performance scalability
latent graphical model selection efficient methods locally tree like graphs graphical model selection refers problem estimating unknown graph structure given observations nodes model consider challenging instance problem nodes latent hidden characterize conditions tractable graph estimation develop efficient methods provable guarantees consider class ising models markov locally tree like graphs regime correlation decay propose efficient method graph estimation establish structural consistency number samples scales omega theta_ min delta eta eta log theta_ min minimum edge potential delta depth distance hidden node nearest observed nodes eta parameter depends minimum maximum node edge potentials ising model proposed method practical implement provides flexibility control number latent variables cycle lengths output graph also present necessary conditions graph estimation method show method nearly matches lower bound sample requirements
bayesian hierarchical reinforcement learning describe approach incorporating bayesian priors maxq framework hierarchical reinforcement learning hrl define priors primitive environment model task pseudo rewards since models composite tasks complex use mixed model based model free learning approach find optimal hierarchical policy show empirically approach results improved convergence non bayesian baselines given sensible priors task hierarchies bayesian priors complementary sources information using sources better either alone iii taking advantage structural decomposition induced task hierarchy significantly reduces computational cost bayesian reinforcement learning framework task pseudo rewards learned instead manually specified leading automatic learning hierarchically optimal rather recursively optimal policies
causal discovery scale mixture model spatiotemporal variance dependencies conventional causal discovery structural equation models sem directly applied observed variables meaning causal effect represented function direct causes however many real world problems significant dependencies variances energies indicates causality possibly take place level variances energies paper propose probabilistic causal scale mixture model spatiotemporal variance dependencies represent specific type generating mechanism observations particular causal mechanism including contemporaneous temporal causal relations variances energies represented structural vector autoregressive model svar prove identifiability model non gaussian assumption innovation processes also propose algorithms estimate involved parameters discover contemporaneous causal structure experiments synthesis real world data conducted show applicability proposed model algorithms
learning high density regions generalized kolmogorov smirnov test high dimensional data propose efficient generalized nonparametric statistical kolmogorov smirnov test detecting distributional change high dimensional data implement test introduce novel hierarchical minimum volume sets estimator represent distributions tested work motivated need detect changes data streams test especially efficient context provide theoretical foundations test show superiority existing methods
localizing cuboids single view images paper seek detect rectangular cuboids localize corners uncalibrated single view images depicting everyday scenes contrast recent approaches rely detecting vanishing points scene grouping line segments form cuboids build discriminative parts based detector models appearance cuboid corners internal edges enforcing consistency cuboid model model invariant different viewpoints aspect ratios able detect cuboids across many different object categories introduce database images cuboid annotations spans variety indoor outdoor scenes show qualitative quantitative results collected database model performs baseline detectors use constraints alone task localizing cuboid corners
semi supervised eigenvectors locally biased learning many applications information labels provided semi supervised manner specific target region large data set wants perform machine learning data analysis tasks nearby pre specified target region locally biased problems sort particularly challenging popular eigenvector based machine learning data analysis tools root reason eigenvectors inherently global quantities paper address issue providing methodology construct semi supervised eigenvectors graph laplacian illustrate locally biased eigenvectors used perform locally biased machine learning semi supervised eigenvectors capture successively orthogonalized directions maximum variance conditioned well correlated input seed set nodes assumed provided semi supervised manner also provide several empirical examples demonstrating semi supervised eigenvectors used perform locally biased learning
analyzing objects cluttered images present approach detecting analyzing configuration objects real world images heavy occlusion clutter focus application finding analyzing cars stage model first stage reasons shape appearance variation due within class variation station wagons look different sedans changes viewpoint rather using view based model describe compositional representation models large number effective views shapes using small number local view based templates use model propose candidate detections estimates shape estimates refined second stage using explicit model shape viewpoint use morphable model capture within class variation use weak perspective camera model capture viewpoint learn model parameters annotations demonstrate state art accuracy detection viewpoint estimation shape reconstruction challenging images pascal voc 2011 dataset
fast variational inference conjugate exponential family present general method deriving collapsed variational inference algorithms probabilistic models conjugate exponential family method unifies many existing approaches collapsed variational inference collapsed variational inference leads new lower bound marginal likelihood exploit information geometry bound derive much faster optimization methods based conjugate gradients models approach general easily applied model mean field update equations derived empirically show significant speed ups probabilistic models optimized using bound
density propagation improved bounds partition function given probabilistic graphical model density states function likelihood value gives number configurations probability introduce novel message passing algorithm called density propagation estimating function show exact tree structured graphical models general strict generalization sum product max product algorithms use density states tree decomposition introduce new family upper lower bounds partition function tree decompostion new upper bound based finer grained density state information provably least tight previously known bounds based convexity log partition function strictly stronger general condition holds conclude empirical evidence improvement convex relaxations mean field based bounds
volume regularization binary classification introduce large volume box classification binary prediction maintains subset weight vectors specifically axis aligned boxes learning algorithm seeks box large volume contains simple weight vectors accurate training set versions learning process cast convex optimization problems shown solve efficiently formulation yields natural pac bayesian performance bound shown minimize quantity directly aligned algorithm outperforms svm recently proposed arow algorithm majority nlp datasets binarized usps optical character recognition datasets
approximating equilibria sequential auctions incomplete information multi unit demand many large economic markets goods sold sequential auctions domains include ebay online auctions wireless spectrum auctions dutch flower auctions bidders domains face highly complex decision making problems preferences outcomes auction often depend outcomes auctions bidders limited information factors drive outcomes bidders preferences past actions work formulate bidder problem price prediction learning optimization define concept stable price predictions show approximate equilibrium sequential auctions characterized profile strategies approximately optimize respect approximately stable price predictions show equilibria found formulation compare known theoretical equilibria simpler auction domains find new approximate equilibria complex auction domain analytical solutions heretofore unknown
high dimensional semiparametric scale invariant principal component analysis propose high dimensional semiparametric scale invariant principal component analysis named copula component analysis coca semiparametric model assumes unspecified marginally monotone transformations distributions multivariate gaussian coca accordingly estimates leading eigenvector correlation matrix latent gaussian distribution robust nonparametric rank based correlation coefficient estimator spearman rho exploited estimation prove although marginal distributions arbitrarily continuous coca estimators obtain fast estimation rates feature selection consistent setting dimension nearly exponentially large relative sample size careful numerical experiments simulated data conducted ideal noisy settings suggest coca loses little even data truely gaussian coca also implemented large scale genomic data illustrate empirical usefulness
repulsive mixtures discrete mixtures used routinely broad sweeping applications ranging unsupervised settings fully supervised multi task learning indeed finite mixtures infinite mixtures relying dirichlet processes modifications become standard tool important issue arises using discrete mixtures low separation components particular different components introduced similar hence redundant redundancy leads many clusters similar degrading performance unsupervised learning leading computational problems unnecessarily complex model supervised settings redundancy arise absence penalty components placed close together even bayesian approach used learn number components solve problem propose novel prior generates components repulsive process automatically penalizing redundant components characterize repulsive prior theoretically propose markov chain monte carlo sampling algorithm posterior computation methods illustrated using synthetic examples iris data set
approximating concavely parameterized optimization problems consider abstract class optimization problems parameterized concavely single parameter show solution path along parameter always approximated accuracy varepsilon set size sqrt varepsilon lower bound size omega sqrt varepsilon shows upper bound tight constant factor also devise algorithm calls step size oracle computes approximate path size sqrt varepsilon finally provide implementation oracle soft margin support vector machines parameterized semi definite program matrix completion
multi criteria anomaly detection using pareto depth analysis consider problem identifying patterns data set exhibit anomalous behavior often referred anomaly detection anomaly detection algorithms dissimilarity data samples calculated single criterion euclidean distance however many cases exist single dissimilarity measure captures possible anomalous patterns case multiple criteria defined test anomalies scalarizing multiple criteria taking linear combination importance different criteria known advance algorithm need executed multiple times different choices weights linear combination paper introduce novel non parametric multi criteria anomaly detection method using pareto depth analysis pda pda uses concept pareto optimality detect anomalies multiple criteria without run algorithm multiple times different choices weights proposed pda approach scales linearly number criteria provably better linear combinations criteria
unsupervised template learning fine grained object recognition fine grained recognition refers subordinate level recognition recognizing different species birds animals plants differs recognition basic categories humans tables computers global similarities shape structure shared within category differences details object parts suggest key identifying fine grained differences lies finding right alignment image regions contain object parts propose template model purpose captures common shape patterns object parts well occurence relation shape patterns image regions aligned extracted features used classification learning template model efficient recognition results achieve significantly outperform state art algorithms
systematic approach extracting semantic information functional mri data paper introduces novel classification method functional magnetic resonance imaging datasets tens classes method designed make predictions using information many brain locations possible instead resorting feature selection decomposing pattern brain activation differently informative sub regions provide results complex semantic processing dataset show method competitive state art feature selection also suggest method used perform group exploratory analyses complex class structure
semi supervised domain adaptation non parametric copulas new framework based theory copulas proposed address semi supervised domain adaptation problems presented method factorizes multivariate density product marginal distributions bivariate copula functions therefore changes factors detected corrected adapt density model across different learning domains importantly introduce novel vine copula model allows factorization non parametric manner experimental results regression problems real world data illustrate efficacy proposed approach compared state art techniques
optimal regularized dual averaging methods stochastic optimization paper considers wide spectrum regularized stochastic optimization problems loss function regularizer non smooth develop novel algorithm based regularized dual averaging rda method simultaneously achieve optimal convergence rates convex strongly convex loss particular strongly convex loss achieves optimal rate frac frac iterations improves best known rate frac log previous stochastic dual averaging algorithms addition method constructs final solution directly proximal mapping instead averaging previous iterates widely used sparsity inducing regularizers ell_1 norm advantage encouraging sparser solutions develop multi stage extension using proposed algorithm subroutine achieves uniformly optimal rate frac exp strongly convex loss
convergence rate analysis map coordinate minimization algorithms finding maximum aposteriori map assignments graphical models important task many applications since problem generally hard linear programming relaxations often used solving relaxations efficiently thus important practical problem recent years several authors proposed message passing updates corresponding coordinate descent dual however generally guaranteed converge global optimum approach remedy smooth perform coordinate descent smoothed dual however little known convergence rate procedure perform thorough rate analysis schemes derive primal dual convergence rates also provide simple dual primal mapping yields feasible primal solutions guaranteed rate convergence empirical evaluation supports theoretical claims shows method highly competitive state art approaches yield global optima
learning canonical views internet image collections although human object recognition supposedly robust viewpoint much research human perception indicates preferred canonical view objects phenomenon discovered years ago canonical view small number categories validated experimentally moreover explanation humans prefer canonical view views remains elusive paper ask use internet image collections learn canonical views start manually finding common view results returned internet search engines queried objects used psychophysical experiments results clearly show likely view search engine corresponds view preferred human subjects experiments also present simple method find likely view image collection apply hundreds categories using new data collected present strong evidence prominent formal theories canonical views provide novel constraints new theories
fast bayesian inference non conjugate gaussian process regression present new variational inference algorithm gaussian processes non conjugate likelihood functions includes binary multi class classification well ordinal regression method constructs convex lower bound optimized using efficient fixed point update method show empirically new approach much faster existing methods without degradation performance
minimizing sparse high order energies submodular vertex cover inference high order graphical models become increasingly important recent years consider energies simple sparse high order potentials previous work area uses either specialized message passing transforms high order potential pairwise case take fundamentally different approach transforming entire original problem comparatively small instance submodular vertex cover problem vertex cover instances attacked standard pairwise methods run much faster times often effective original problem evaluate approach synthetic data show algorithm useful fast hierarchical clustering model estimation framework
geometric take metric learning multi metric learning techniques learn local metric tensors different parts feature space approach even simple classifiers competitive state art distance measure locally adapts structure data learned distance measure however non metric prevented multi metric learning generalizing tasks dimensionality reduction regression principled way prove appropriate changes multi metric learning corresponds learning structure riemannian manifold show structure gives principled way perform dimensionality reduction regression according learned metrics algorithmically provide first practical algorithm computing geodesics according learned metrics well algorithms computing exponential logarithmic maps riemannian manifold together tools let many euclidean algorithms take advantage multi metric learning illustrate approach regression dimensionality reduction tasks involve predicting measurements human body shape data
deformations parts motion based segmentation objects develop method discovering parts articulated object aligned meshes capturing various dimensional poses adapt distance dependent chinese restaurant process ddcrp allow nonparametric discovery potentially unbounded number parts simultaneously guaranteeing spatially connected segmentation allow analysis datasets object instances varying shapes model part variability across poses via affine transformations placing matrix normal inverse wishart prior affine transformations develop ddcrp gibbs sampler tractably marginalizes transformation uncertainty analyzing dataset humans captured dozens poses infer parts provide quantitatively better motion predictions conventional clustering methods
diffusion decision making adaptive nearest neighbor classification paper sheds light fundamental connections diffusion decision making model neuroscience cognitive psychology nearest neighbor classification show conventional nearest neighbor classification viewed special problem diffusion decision model asymptotic situation applying optimal strategy associated diffusion decision model adaptive rule developed determining appropriate values nearest neighbor classification making use sequential probability ratio test sprt bayesian analysis propose different criteria adaptively acquiring nearest neighbors experiments synthetic real datasets demonstrate effectivness classification criteria
fully bayesian inference neural models negative binomial spiking characterizing information carried neural populations brain requires accurate statistical models neural spike responses negative binomial distribution provides convenient model dispersed spike counts responses greater poisson variability describe powerful data augmentation framework fully bayesian inference neural models negative binomial spiking approach relies recently described latent variable representation negative binomial distribution equates polya gamma mixture normals framework provides tractable conditionally gaussian representation posterior used design efficient gibbs sampling based algorithms inference regression dynamic factor models apply model neural data primate retina show substantially outperforms poisson regression held data reveals latent structure underlying spike count correlations simultaneously recorded spike trains
small variance asymptotics exponential family dirichlet process mixture models links probabilistic non probabilistic learning algorithms arise performing small variance asymptotics letting variance particular distributions graphical model instance context clustering approach yields precise connections means algorithms paper explore small variance asymptotics exponential family dirichlet process hierarchical dirichlet process hdp mixture models utilizing connections exponential family distributions bregman divergences derive novel clustering algorithms asymptotic limit hdp mixtures feature scalability existing hard clustering methods well flexibility bayesian nonparametric models focus special cases analysis discrete data problems including topic modeling demonstrate utility results applying variants algorithms problems arising vision document analysis
visual recognition using embedded feature selection curvature self similarity category level object detection crucial need informative object representations demand led feature descriptors ever increasing dimensionality like occurrence statistics self similarity paper propose new object representation based curvature self similarity goes beyond currently popular approximation objects using straight lines however like descriptors using second order statistics also exhibits high dimensionality although improving discriminability high dimensionality becomes critical issue due lack generalization ability curse dimensionality given limited amount training data even sophisticated learning algorithms popular kernel methods able suppress noisy superfluous dimensions high dimensional data consequently natural need feature selection using present day informative features particularly curvature self similarity therefore suggest embedded feature selection method svms reduces complexity improves generalization capability object models successfully integrating proposed curvature self similarity representation together embedded feature selection widely used state art object detection framework show general pertinence approach
triangular versus edge representations towards scalable modeling networks paper argue representing networks bag triangular motifs particularly important network problems current model based approaches handle poorly due computational bottlenecks incurred using edge representations approaches require edges edges missing edges provided input consequence approximate inference algorithms models usually require omega time per iteration precluding application larger real world networks contrast triangular modeling requires less computation providing equivalent better inference quality triangular motif vertex triple containing edges number motifs theta sum_ d_i degree vertex much smaller low maximum degree networks using representation develop novel mixed membership network model approximate inference algorithm suitable large networks low max degree networks high maximum degree triangular motifs naturally subsampled node centric fashion allowing much faster inference small cost accuracy empirically demonstrate approach compared edge based model faster runtime improved accuracy mixed membership community detection conclude large scale demonstration approx 280 000 node network infeasible network models omega inference cost
spectral algorithm latent dirichlet allocation topic modeling generalization clustering posits observations words document generated emph multiple latent factors topics opposed increased representational power comes cost challenging unsupervised learning problem estimating topic word distributions words observed topics hidden work provides simple efficient learning procedure guaranteed recover parameters wide class topic models including latent dirichlet allocation lda lda procedure correctly recovers topic word distributions parameters dirichlet prior topic mixtures using trigram statistics emph third order moments estimated documents containing words method called excess correlation analysis based spectral decomposition low order moments via singular value decompositions svds moreover algorithm scalable since svds carried times matrices number latent factors topics typically much smaller dimension observation word space
privacy aware learning study statistical risk minimization problems version privacy data kept confidential even learner local privacy framework show sharp upper lower bounds convergence rates statistical estimation procedures consequence exhibit precise tradeoff amount privacy data preserves utility measured convergence rate statistical estimator
near optimal map inference determinantal point processes determinantal point processes dpps recently proposed computationally efficient probabilistic models diverse sets variety applications including document summarization image search pose estimation many dpp inference operations including normalization sampling tractable however finding likely configuration map often required practice decoding hard must resort approximate inference dpp probabilities log submodular greedy algorithms used past empirical success however methods give approximation guarantees special case dpps monotone kernels paper propose new algorithm approximating map problem based continuous techniques submodular function maximization method involves novel continuous relaxation log probability function contrast multilinear extension used general submodular functions evaluated differentiated exactly efficiently obtain practical algorithm approximation guarantee general class non monotone dpps algorithm also extends map inference complex polytope constraints making possible combine dpps markov random fields weighted matchings models demonstrate approach outperforms greedy methods synthetic real world data
dynamic pruning factor graphs maximum marginal prediction study problem maximum marginal prediction mmp probabilistic graphical models task occurs example bayes optimal decision rule hamming loss mmp typically performed stage procedure estimates variable marginal probability forms prediction states maximal probability work propose simple yet effective technique accelerating mmp inference sampling based instead stage procedure directly estimate posterior probability decision variable allows identify point time sufficiently certain individual decision whenever case dynamically prune variable confident underlying factor graph consequently time samples variable whose decision still uncertain need created experiments prototypical scenarios multi label classification image inpainting shows adaptive sampling drastically accelerate mmp without sacrificing prediction accuracy
priors diversity generative latent variable models probabilistic latent variable models cornerstones machine learning offer convenient coherent way specify prior distributions unobserved structure data unknown properties inferred via posterior inference models useful exploratory analysis visualization building density models data providing features used later discriminative tasks significant limitation models however draws prior often highly redundant due assumptions internal parameters example preference prior mixture model make components non overlapping topic model ensure ocurring words appear small number topics work revisit independence assumptions probabilistic latent variable models replacing underlying prior determinantal point process dpp dpp allows specify preference diversity latent variables using positive definite kernel function using kernel probability distributions able define dpp probability measures show perform map inference dpp priors latent dirichlet allocation mixture models leading better intuition latent variable representation quantitatively improved unsupervised feature extraction without compromising generative aspects model
large scale distributed deep networks recent work unsupervised feature learning deep learning shown able train large models dramatically improve performance paper consider problem training deep network billions parameters using tens thousands cpu cores developed software framework called distbelief utilize computing clusters thousands machines train large models within framework developed algorithms large scale distributed training downpour sgd asynchronous stochastic gradient descent procedure supporting large number model replicas sandblaster framework supports variety distributed batch optimization procedures including distributed implementation bfgs downpour sgd sandblaster bfgs increase scale speed deep network training successfully used system train deep network 100x larger previously reported literature achieves state art performance imagenet visual object recognition task million images 21k categories show techniques dramatically accelerate training modestly sized deep network commercial speech recognition service although focus report performance methods applied training large neural networks underlying algorithms applicable gradient based machine learning algorithm
confusion based online learning passive aggressive scheme paper provides first best knowledge analysis online learning algorithms multiclass problems confusion matrix taken performance measure work builds upon recent elegant results noncommutative concentration inequalities concentration inequalities apply matrices precisely matrix martingales establish generalization bounds online learning algorithm show theoretical study motivate proposition new confusion friendly learning procedure learning algorithm called copa confusion passive aggressive passive aggressive learning algorithm shown update equations copa computed analytically thus allowing user recours optimization package implement
unsupervised structure discovery semantic analysis audio approaches audio classification retrieval tasks largely rely detection based discriminative models submit models make simplistic assumption mapping acoustics directly semantics whereas actual process likely complex present generative model maps acoustics hierarchical manner increasingly higher level semantics model layers first generic sound units clear semantic associations second layer attempts find patterns generic sound units evaluate model large scale retrieval task trecvid 2011 report significant improvements standard baselines
modelling reciprocating relationships hawkes processes present bayesian nonparametric model discovers implicit social structure interaction time series data social groups often formed implicitly actions among members groups yet many models social networks use explicitly declared relationships infer social structure consider particular class hawkes processes doubly stochastic point process able model reciprocity groups individuals extend infinite relational model using reciprocating hawkes processes parameterise edges making events associated edges dependent time model outperforms general unstructured hawkes processes well structured poisson process based models predicting verbal email turn taking military conflicts among nations
nonparametric variable clustering model factor analysis models effectively summarise covariance structure high dimensional data solutions typically hard interpret motivates attempting find disjoint partition clustering observed variables variables cluster highly correlated introduce bayesian non parametric approach problem demonstrate advantages heuristic methods proposed date
identifiability unmixing latent parse trees paper explores unsupervised learning parsing models along directions first models identifiable infinite data use general technique numerically checking identifiability based rank jacobian matrix apply several standard constituency dependency parsing models second identifiable models estimate parameters efficiently suffers local optima recent work using spectral methods cannot directly applied since topology parse tree varies across sentences develop strategy unmixing deals additional complexity restricted classes parsing models
scalable inference overlapping communities develop scalable algorithm posterior inference overlapping communities large networks algorithm based stochastic variational inference mixed membership stochastic blockmodel naturally interleaves subsampling network estimating community structure apply algorithm large real world networks 000 nodes converges several orders magnitude faster state art algorithm mmsb finds hundreds communities large real world networks detects true communities 280 benchmark networks equal better accuracy compared scalable algorithms
kernel hyperalignment offer regularized kernel extension multi set orthogonal procrustes problem hyperalignment new method called kernel hyperalignment expands scope hyperalignment include nonlinear measures similarity enables alignment multiple datasets large number base features direct application fmri data analysis kernel hyperalignment well suited multi subject alignment large rois including entire cortex conducted experiments using real world multi subject fmri data
high order multi task feature learning identify longitudinal phenotypic markers alzheimer disease progression prediction alzheimer disease neurodegenerative disorder characterized progressive impairment memory cognitive functions regression analysis studied relate neuroimaging measures cognitive status however whether measures predictive power infer trajectory cognitive performance time still explored important topic research propose novel high order multi task learning model address issue proposed model explores temporal correlations existing data features regression tasks structured sparsity inducing norms addition sparsity model enables selection small number mri measures maintaining high prediction accuracy empirical studies using baseline mri serial cognitive data adni cohort yielded promising results
latent factor model highly multi relational data many data social networks movie preferences knowledge bases multi relational describe multiple relationships entities large body work focused modeling data considered modeling multiple types relationships jointly existing approaches tend breakdown number types grows paper propose method modeling large multi relational datasets possibly thousands relations model based bilinear structure captures various orders interaction data also shares sparse latent factors across different relations illustrate performance approach standard tensor factorization datasets attain outperform state art results finally nlp application demonstrates scalability ability model learn efficient semantically meaningful verb representations
multi task vector field learning multi task learning mtl aims improve generalization performance learning multiple related tasks simultaneously identifying shared information among tasks existing mtl methods focus learning linear models supervised setting propose novel semi supervised nonlinear approach mtl using vector fields vector field smooth mapping manifold tangent spaces viewed directional derivative functions manifold argue vector fields provide natural way exploit geometric structure data well shared differential structure tasks crucial semi supervised multi task learning paper develop multi task vector field learning mtvfl learns prediction functions vector fields simultaneously mtvfl following key properties vector fields learned close gradient fields prediction functions within task vector field required parallel possible expected span low dimensional subspace vector fields tasks share low dimensional subspace formalize idea regularization framework also provide convex relaxation method solve original non convex problem experimental results synthetic real data demonstrate effectiveness proposed approach
fiedler random fields large scale spectral approach statistical network modeling statistical models networks typically committed strong prior assumptions concerning form modeled distributions moreover vast majority currently available models explicitly designed capturing specific graph properties power law degree distributions makes unsuitable application domains behavior target quantities known priori key contribution paper twofold first introduce fiedler delta statistic based laplacian spectrum graphs allows dispense parametric assumption concerning modeled network properties second use defined statistic develop fiedler random field model allows efficient estimation edge distributions large scale random networks analyzing dependence structure involved fiedler random fields estimate several real world networks showing achieve much higher modeling accuracy well known statistical approaches
sketch based linear value function approximation hashing common method reduce large potentially infinite feature vectors fixed size table reinforcement learning hashing often used conjunction tile coding represent states continuous spaces hashing also promising approach value function approximation large discrete domains hearts feature vectors constructed exhaustively combining set atomic features unfortunately typical use hashing value function approximation results biased value estimates due possibility collisions recent work data stream summaries led development tug war sketch unbiased estimator approximating inner products work investigates application new data structure linear value function approximation although reinforcement learning setting use tug war sketch leads biased value estimates show bias orders magnitude less standard hashing provide empirical results benchmark domains fifty atari 2600 games highlight superior learning performance tug war hashing
symmetric correspondence topic models multilingual text analysis topic modeling widely used approach analyzing large text collections small number multilingual topic models recently explored discover latent topics among parallel comparable documents wikipedia topic models originally proposed structured data also applicable multilingual documents correspondence latent dirichlet allocation corrlda model however requires pivot language specified advance propose new topic model symmetric correspondence lda symcorrlda incorporates hidden variable control pivot language extension corrlda experimented multilingual comparable datasets extracted wikipedia demonstrate symcorrlda effective existing multilingual topic models
feature aware label space dimension reduction multi label classification label space dimension reduction lsdr efficient effective paradigm multi label classification many classes existing approaches lsdr compressive sensing principal label space transformation exploit label part dataset feature part paper propose novel approach lsdr considers label feature parts approach called conditional principal label space transformation based minimizing upper bound popular hamming loss minimization step approach carried efficiently simple use singular value decomposition addition approach extended kernelized version allows use sophisticated feature combinations assist lsdr experimental results verify proposed approach effective existing ones lsdr across many real world datasets
augmented svm automatic space partitioning combining multiple non linear dynamics non linear dynamical systems used extensively building generative models human behavior applications range modeling brain dynamics encoding motor commands many schemes proposed encoding robot motions using dynamical systems single attractor placed predefined target state space although enable robots react sudden perturbations without planning motions always directed towards single target work focus combining several distinct attractors resulting multi stable show applicability reach grasp tasks attractors represent several grasping points target object exploiting multiple attractors provides flexibility recovering unseen perturbations also increases complexity underlying learning problem present augmented svm svm model inherits region partitioning ability well known svm classifier augmented novel constraints derived individual new constraints modify original svm dual whose optimal solution results new class support vectors new ensure resulting multi stable incurs minimum deviation original dynamics stable attractors within finite region attraction show via implementations simulated degrees freedom mobile robotic platform model capable real time motion generation able adapt fly perturbations
dimensionality dependent pac bayes margin bound margin important concepts machine learning previous margin bounds svm boosting dimensionality independent major advantage dimensionality independency explain excellent performance svm whose feature spaces often high infinite dimension paper address problem whether dimensionality independency intrinsic margin bounds prove dimensionality dependent pac bayes margin bound bound monotone increasing respect dimension keeping factors fixed show bound strictly sharper previously well known pac bayes margin bound feature space finite dimension bounds tend equivalent dimension goes infinity addition show bound linear classifiers recovered bound mild conditions conduct extensive experiments benchmark datasets find new bound useful model selection significantly sharper dimensionality independent pac bayes margin bound well bound linear classifiers
variational hierarchical algorithm clustering hidden markov models paper derive novel algorithm cluster hidden markov models hmms according probability distributions propose variational hierarchical algorithm clusters given collection hmms groups hmms similar terms distributions represent characterizes group cluster center novel hmm representative group illustrate benefits proposed algorithm hierarchical clustering motion capture sequences well automatic music tagging
topology constraints graphical models graphical models useful tool describe understand natural phenomena gene expression climate change social interactions topological structure graphs networks fundamental part analysis many cases main goal study however little work done incorporating prior topological knowledge onto estimation underlying graphical models sample data work propose extensions basic joint regression model network estimation explicitly incorporate graph topological constraints corresponding optimization approach first proposed extension includes eigenvector centrality constraint thereby promoting important prior topological property second developed extension promotes formation certain motifs triangle shaped ones particular known exist example genetic regulatory networks presentation underlying formulations serve examples introduction topological constraints network estimation complemented examples diverse datasets demonstrating importance incorporating critical prior knowledge
linear time active learning algorithm link classification present efficient active learning algorithms link classification signed networks algorithms motivated stochastic model edge labels obtained perturbations initial sign assignment consistent clustering nodes provide theoretical analysis within model showing achieve optimal whithin constant factor number mistakes graph least order querying order edge labels generally show algorithm achieves optimality within factor order querying order edge labels running time algorithm order log
learning mixtures tree graphical models consider unsupervised estimation mixtures discrete graphical models class variable hidden mixture component potentially different markov graph structure parameters observed variables propose novel method estimating mixture components provable guarantees output tree mixture model serves good approximation underlying graphical model mixture sample computational requirements method scale poly component mixture variate graphical models wide class models includes tree mixtures mixtures bounded degree graphs
voodoo learning discrete graphical models via inverse covariance estimation investigate relationship support inverses generalized covariance matrices structure discrete graphical model show certain graph structures support inverse covariance matrix indicator variables vertices graph reflects conditional independence structure graph work extends results previously established multivariate gaussian distributions partially answers open question meaning inverse covariance matrix non gaussian distribution propose graph selection methods general discrete graphical model bounded degree based possibly corrupted observations verify theoretical results via simulations along way also establish new results support recovery setting sparse high dimensional linear regression based corrupted missing observations
bayesian pedigree analysis using measure factorization pedigrees family trees directed graphs used identify sites genome correlated presence absence disease advent genotyping sequencing technologies explosion amount data available number individuals number sites pedigrees number thousands individuals meanwhile analysis methods remained limited pedigrees 100 individuals limits analyses many small independent pedigrees disease models used linkage analysis log odds lod estimator similarly limited linkage anlysis originally designed different task mind ordering sites genome technologies could reveal order lods difficult interpret nontrivial extend consider interactions among sites developments difficulties call creation modern methods pedigree analysis drawing recent advances graphical model inference transducer theory introduce simple yet powerful formalism expressing genetic disease models show disease models turned accurate efficient estimators technique use constructing variational approximation potential applications inference large scale graphical models method allows inference larger pedigrees previously analyzed literature improves disease site prediction
variational inference crowdsourcing crowdsourcing become popular paradigm labeling large datasets however given rise computational task aggregating crowdsourced labels provided collection unreliable annotators approach problem transforming standard inference problem graphical models applying approximate variational methods including belief propagation mean field show algorithm generalizes majority voting recent algorithm karger method closely related commonly used algorithm cases find performance algorithms critically depends choice prior distribution workers reliability choosing prior properly perform surprisingly well simulated real world datasets competitive state art algorithms based complicated modeling assumptions
optimal neural tuning curves arbitrary stimulus distributions discrimax infomax minimum l_p loss work study stimulus distribution influences optimal coding individual neuron closed form solutions optimal sigmoidal tuning curve provided neuron obeying poisson statistics given stimulus distribution consider variety optimality criteria including maximizing discriminability maximizing mutual information minimizing estimation error general l_p norm generalize cramer rao lower bound show l_p loss written functional fisher information asymptotic limit proving moment convergence certain functions poisson random variables manner show optimal tuning curve depends upon loss function equivalence maximizing mutual information minimizing l_p loss limit goes
probabilistic event cascades alzheimer disease accurate detailed models progression neurodegenerative diseases alzheimer crucially important reliable early diagnosis determination deployment effective treatments paper introduce alpaca alzheimer disease probabilistic cascades model generative model linking latent alzheimer progression dynamics observable biomarker data contrast previous works model disease progression fixed ordering events explicitly model variability orderings among patients realistic particularly highly detailed disease progression models describe efficient learning algorithms alpaca discuss promising experimental results real cohort alzheimer patients alzheimer disease neuroimaging initiative
non existence convex calibrated surrogate losses ranking study surrogate losses learning rank framework rankings induced scores task learn scoring function focus calibration surrogate losses respect ranking evaluation metric calibration equivalent guarantee near optimal values surrogate risk imply near optimal values risk defined evaluation metric prove surrogate loss convex function scores calibrated respect evaluation metrics widely used search engine evaluation namely average precision expected reciprocal rank also show convex surrogate losses cannot calibrated respect pairwise disagreement evaluation metric used learning pairwise preferences results cast lights intrinsic difficulty ranking problems well limitations learning rank algorithms based minimization convex surrogate risk
new metric manifold kernel matrices application matrix geometric means symmetric positive definite spd matrices remarkably pervasive multitude scientific disciplines including machine learning optimization consider fundamental task measuring distances spd matrices task often nontrivial whenever application demands distance function respect non euclidean geometry spd matrices unfortunately typical non euclidean distance measures riemannian metric riem frob log inv computationally demanding also complicated use allay difficulties introduce new metric spd matrices metric respects non euclidean geometry also offers faster computation riem less complicated use support claims theoretically via series theorems relate metric riem experimentally studying nonconvex problem computing matrix geometric means based squared distances
majorization crfs latent likelihoods partition function plays key role probabilistic modeling including conditional random fields graphical models maximum likelihood estimation optimize partition functions article introduces quadratic variational upper bound inequality facilitates majorization methods optimization complicated functions iterative solution simpler sub problems bounds remain efficient compute even partition function involves graphical model small tree width latent likelihood settings large scale problems low rank versions bound provided outperform lbfgs well first order methods several learning applications shown reduce fast convergent update rules experimental results show advantages state art optimization methods
augment conquer negative binomial processes developing data augmentation methods unique negative binomial distribution unite seemingly disjoint count mixture models process framework develop fundamental properties models derive efficient gibbs sampling inference show gamma process reduced hierarchical dirichlet process normalization highlighting unique theoretical structural computational advantages variety processes distinct sharing mechanisms constructed applied topic modeling connections existing algorithms showing importance inferring dispersion probability parameters
unifying perspective parametric policy search methods markov decision processes parametric policy search algorithms methods choice optimisation markov decision processes expectation maximisation natural gradient ascent considered current state art field article provide unifying perspective algorithms showing step directions parameter space closely related search direction approximate newton method analysis leads naturally consideration approximate newton method alternative gradient based method markov decision processes able show algorithm numerous desirable properties absent naive application newton method make viable alternative either expectation maximisation natural gradient ascent empirical results suggest algorithm excellent convergence robustness properties performing strongly comparison expectation maximisation natural gradient ascent
regularized hashing multimodal data hashing based methods provide promising approach large scale similarity search obtain compact hash codes recent trend seeks learn hash functions data automatically paper study hash function learning context multimodal data propose novel multimodal hash function learning method called regularized hashing crh based boosted regularization framework hash functions bit hash codes learned solving difference convex functions programs learning multiple bits proceeds via boosting procedure bias introduced hash functions sequentially minimized empirically compare crh state art multimodal hash function learning methods publicly available data sets
communication efficient algorithms statistical optimization study communication efficient algorithms distributed statistical optimization large scale data first algorithm averaging method distributes data samples evenly machines performs separate minimization subset averages estimates provide sharp analysis average mixture algorithm showing reasonable set conditions combined parameter achieves mean squared error decays order whenever sqrt guarantee matches best possible rate achievable centralized algorithm access samples second algorithm novel method based appropriate form bootstrap requiring single round communication mean squared error decays order robust amount parallelization complement theoretical results experiments large scale problems microsoft learning rank dataset
fastex fast clustering exponential families clustering key component data analysis toolbox despite importance scalable algorithms often eschew rich statistical models favor simpler descriptions means clustering paper present sampler capable estimating mixtures exponential families heart lies novel proposal distribution using random projections achieve high throughput generating proposals crucial clustering models large numbers clusters
label ranking partial abstention based thresholded probabilistic models several machine learning methods allow abstaining uncertain predictions common settings like conventional classification abstention studied much less learning rank address abstention label ranking setting allowing learner declare certain pairs labels incomparable thus predict partial instead total orders method predictions produced via thresholding probabilities pairwise preferences labels induced predicted probability distribution set rankings formally analyze approach mallows plackett luce model showing produces proper partial orders predictions characterizing expressiveness induced class partial orders theoretical results complemented experiments demonstrating practical usefulness approach
best arm identification unified approach fixed budget fixed confidence study problem identifying best arm stochastic multi armed bandit setting problem studied literature different perspectives fixed budget fixed confidence propose unifying approach leads meta algorithm called unified gap based exploration ugape common structure similar theoretical analysis settings prove performance bound versions algorithm showing problems characterized notion complexity also show ugape algorithm well theoretical analysis extended take account variance arms multiple bandits finally evaluate performance ugape compare number existing fixed budget fixed confidence algorithms
automatic feature induction stagewise collaborative filtering recent approaches collaborative filtering concentrated estimating algebraic statistical model using model predicting missing ratings paper observe different models relative advantages different regions input space motivates approach using stagewise linear combinations collaborative filtering algorithms non constant combination coefficients based kernel smoothing resulting stagewise model computationally scalable outperforms wide selection state art collaborative filtering algorithms
convex formulation learning scale free networks via submodular relaxation key problem statistics machine learning determination network structure data consider case structure graph reconstructed known scale free show cases natural formulate structured sparsity inducing priors using submodular functions use lovasz extension obtain convex relaxation tractable classes gaussian graphical models leads convex optimization problem efficiently solved show method results improvement accuracy reconstructed networks synthetic data also show prior encourages scale free reconstructions bioinfomatics dataset
multilabel classification using bayesian compressed sensing paper present bayesian framework multilabel classification using compressed sensing key idea compressed sensing multilabel classification first project label vector lower dimensional space using random transformation learn regression functions projections approach considers components single probabilistic model thereby jointly optimizing compression well learning tasks derive efficient variational inference scheme provides joint posterior distribution unobserved labels key benefits model naturally handle datasets missing labels also measure uncertainty prediction uncertainty estimate provided model naturally allows active learning paradigms oracle provides information labels promise maximally informative prediction task experiments show significant boost prior methods terms prediction performance benchmark datasets fully labeled missing labels case finally also highlight various useful active learning scenarios enabled probabilistic model
probabilistic topic coding superset label learning superset label learning problem training instance provides set candidate labels true label instance approaches learn discriminative classifier tries minimize upper bound unobserved loss work propose probabilistic model probabilistic topic coding ptc superset label learning problem ptc model derived logistic stick breaking process first maps data topics assigns topic label drawn multinomial distribution layer topics capture underlying structure data useful model weakly supervised advantage comes little cost since model introduces additional parameters experimental tests several real world problems superset labels show results competitive superior state art discovered underlying structures also provide improved explanations classification predictions
affine independent variational inference present method approximate inference broad class non conjugate probabilistic models particular family generalized linear model target densities describe rich class variational approximating densities best fit target minimizing kullback leibler divergence approach based using fourier representation show results efficient scalable inference
efficient high dimensional maximum entropy modeling via symmetric partition functions application maximum entropy principle sequence modeling popularized methods conditional random fields crfs however approaches generally limited modeling paths discrete spaces low dimensionality consider problem modeling distributions paths continuous spaces high dimensionality problem inference generally intractable main contribution show maximum entropy modeling high dimensional continuous paths tractable long constrained features possess certain kind low dimensional structure case show associated partition function symmetric symmetry exploited compute partition function efficiently compressed form empirical results given showing application method maximum entropy modeling high dimensional human motion capture data
transferring expectations model based reinforcement learning study automatically select adapt multiple abstractions representations world support model based reinforcement learning address challenges transfer learning heterogeneous environments varying tasks present efficient online framework sequence tasks learns set relevant representations used future tasks without pre defined mapping strategies introduce general approach support transfer learning across different state spaces demonstrate potential impact system improved jumpstart faster convergence near optimum policy benchmark domains
learning map inference discrete graphical models present new formulation attacking binary classification problems instead relying convex losses regularisers svms logistic regression boosting instead non convex continuous formulations encountered neural networks deep belief networks framework entails non convex emph discrete formulation estimation amounts finding map configuration graphical model whose potential functions low dimensional discrete surrogates misclassification loss argue discrete formulation naturally account number issues typically encountered either convex continuous non convex paradigms reducing learning problem map inference problem immediately translate guarantees available many inference settings learning problem empirically demonstrate number experiments approach promising dealing issues severe label noise still global optimality guarantees due discrete nature formulation also allows emph direct regularisation cardinality based penalties ell_0 pseudo norm thus providing ability perform feature selection trade interpretability predictability principled manner also outline number open problems arising formulation
discriminative learning sum product networks sum product networks new deep architecture perform fast exact inference high treewidth models generative methods training spns proposed date paper present first discriminative training algorithms spns combining high accuracy former representational power tractability latter show class tractable discriminative spns broader class tractable generative ones propose efficient backpropagation style algorithm computing gradient conditional log likelihood standard gradient descent suffers diffusion problem networks many layers learned reliably using hard gradient descent marginal inference replaced mpe inference inferring probable state non evidence variables resulting updates simple intuitive form test discriminative spns standard image classification tasks obtain best results date cifar dataset using fewer features prior methods spn architecture learns local image structure discriminatively also report highest published test accuracy stl even though use labeled portion dataset
structured learning gaussian graphical models consider estimation multiple high dimensional gaussian graphical models corresponding single set nodes several distinct conditions assume aspects networks shared structured differences specifically network differences generated node perturbations nodes perturbed across networks edges stemming nodes differ networks corresponds simple model mechanism underlying many cancers gene regulatory network disrupted due aberrant activity specific genes propose solve problem using structured joint graphical lasso convex optimization problem based upon use novel symmetric overlap norm penalty solve using alternating directions method multipliers algorithm proposal illustrated synthetic data application brain cancer gene expression data
random utility theory social choice theory algorithms random utility theory models agent preferences alternatives drawing real valued score alternative typically independently parameterized distribution ranking according scores special case received significant attention plackett luce model fast inference methods maximum likelihood estimators available paper develops conditions general random utility models enable fast inference within bayesian framework providing unimodal log likelihood functions results real world simulated data provide support scalability approach despite flexibility
simultaneously leveraging output task structures multiple output regression multiple output regression models require estimating multiple functions output improve parameter estimation models methods based structural regularization model parameters usually needed paper present multiple output regression model leverages covariance structure functions multiple functions related well conditional covariance structure outputs contrast existing methods usually take account structures importantly unlike existing methods none structures need known priori model learned data several previously proposed structural regularization based multiple output regression models turn special cases model moreover addition rich model multiple output regression model also used estimating graphical model structure set variables multivariate outputs conditioned another set variables inputs experimental results synthetic real datasets demonstrate effectiveness method
effective split merge monte carlo methods nonparametric models sequential data applications bayesian nonparametric methods require learning inference algorithms efficiently explore models unbounded complexity develop new markov chain monte carlo methods beta process hidden markov model hmm enabling discovery shared activity patterns large video motion capture databases introducing split merge moves based sequential allocation allow large global changes shared feature structure also develop data driven reversible jump moves reliably discover rare unique behaviors proposals apply choice conjugate likelihood observed data show success multinomial gaussian autoregressive emission models together innovations allow tractable analysis hundreds time series previous inference required clever initialization least thousand burn iterations sequences
distributed probabilistic learning camera networks missing data probabilistic approaches computer vision typically assume centralized setting algorithm granted access observed data points however many problems wide area surveillance benefit distributed modeling either physical computational constraints distributed models date use algebraic approaches distributed svd result cannot explicitly deal missing data work present approach estimation learning generative probabilistic models distributed context certain sensor data missing particular show traditional centralized models probabilistic pca missing data ppca learned data distributed across network sensors demonstrate utility approach problem distributed affine structure motion experiments suggest accuracy learned probabilistic structure motion models rivals traditional centralized factorization methods able handle challenging situations missing noisy observations
factorial lda sparse multi dimensional text models multi dimensional latent variable models capture many latent factors text corpus topic author perspective sentiment introduce factorial lda multi dimensional latent variable model document influenced different factors word token depends dimensional vector latent variables model incorporates structured word priors learns sparse product factors experiments research abstracts show model learn latent factors research topic scientific discipline focus methods applications modeling improvements reduce test perplexity improve human interpretability discovered factors
bayesian nonparametric models ranked data develop bayesian nonparametric extension popular plackett luce choice model handle infinite number choice items framework based theory random atomic measures prior specified gamma process derive posterior characterization simple effective gibbs sampler posterior simulation develop time varying extension model apply model new york times lists weekly bestselling books
adaptive learning smoothing functions application electricity load forecasting paper proposes efficient online learning algorithm track smoothing functions additive models key idea combine linear representation additive models recursive least squares rls filter order quickly track changes model put weight recent data rls filter uses forgetting factor exponentially weights observations order arrival tracking behaviour enhanced using adaptive forgetting factor updated based gradient priori errors using results lyapunov stability theory upper bounds learning rate analyzed proposed algorithm applied years electricity load data provided french utility company electricite france edf compared state art methods achieves superior performance terms model tracking prediction accuracy
ancestor sampling particle gibbs present novel method family particle mcmc methods refer particle gibbs ancestor sampling similarly existing backward simulation procedure use backward sampling considerably improve mixing kernel instead using separate forward backward sweeps however achieve effect single forward sweep apply framework challenging class non markovian state space models develop truncation strategy models applicable principle backward simulation based method particularly well suited framework particular show simulation study yield order magnitude improved accuracy relative due robustness truncation error several application examples discussed including rao blackwellized particle smoothing inference degenerate state space models
regularized policy learning present novel l_1 regularized policy convergent learning method termed able learn sparse representations value functions low computational complexity algorithmic framework underlying integrates key ideas policy convergent gradient methods tdc convex concave saddle point formulation non smooth convex optimization enables first order solvers feature selection using online convex regularization detailed theoretical experimental analysis presented variety experiments presented illustrate policy convergence sparse feature selection capability low computational cost algorithm
multiclass learning approaches theoretical comparison implications theoretically analyze compare following popular multiclass classification methods pairs tree based classifiers error correcting output codes ecoc randomly generated code matrices multiclass svm first methods classification based reduction binary classification consider case binary classifier comes class dimension particular class halfspaces reals analyze estimation error approximation error methods analysis reveals interesting conclusions practical relevance regarding success different approaches various conditions proof technique employs tools theory analyze emph approximation error hypothesis classes sharp contrast previous uses theory deal estimation error
truly nonparametric online variational inference hierarchical dirichlet processes variational methods provide computationally scalable alternative monte carlo methods large scale bayesian nonparametric learning practice however conventional batch online variational methods quickly become trapped local optima paper consider nonparametric topic model based hierarchical dirichlet process hdp develop novel online variational inference algorithm based split merge topic updates derive simpler faster variational approximation hdp show intelligently splitting merging components variational posterior achieve substantially better predictions test data conventional online batch variational algorithms streaming analysis large datasets batch analysis infeasible show split merge updates better capture nonparametric properties underlying model allowing continual learning new topics
hierarchical spike coding sound develop probabilistic generative model representing acoustic event structure multiple scales via stage hierarchy first stage consists spiking representation encodes sound sparse set kernels different frequencies positioned precisely time coarse time frequency statistical structure first stage spikes encoded second stage spiking representation fine scale statistical regularities encoded recurrent interactions within first stage fitted speech data model encodes acoustic features harmonic stacks sweeps frequency modulations composed represent complex acoustic events model also able synthesize sounds higher level representation provides significant improvement wavelet thresholding techniques denoising task
learning networks heterogeneous influence information disease influence diffuse networks entities natural systems human society analyzing transmission networks plays important role understanding diffusion processes predicting events future however underlying transmission networks often hidden incomplete observe time stamps cascades events happen paper attempt address challenging problem uncovering hidden network cascades structure discovery problem complicated fact influence among different entities network heterogeneous described simple parametric model therefore propose kernel based method capture diverse range different types influence without prior assumption synthetic real cascade data show model better recover underlying diffusion network drastically improve estimation influence functions networked entities
human memory search random walk semantic network human mind remarkable ability store vast amount information memory even remarkable ability retrieve experiences needed understanding representations algorithms underlie human memory search could potentially useful information retrieval settings including internet search psychological studies revealed clear regularities people search memory clusters semantically related items tending retrieved together findings recently taken evidence human memory search similar animals foraging food patchy environments people making rational decision switch away cluster related information becomes depleted demonstrate results taken evidence account also emerge random walk semantic network much like random web surfer model used internet search engines offers simpler unified account people search memory postulating single process rather process exploring cluster process switching clusters
near optimal chernoff bounds markov decision processes expected return widely used objective decision making uncertainty many algorithms value iteration proposed optimize risk aware settings however expected return often appropriate objective optimize propose new optimization objective risk aware planning show desirable theoretical properties also draw connections previously proposed objectives risk aware planing minmax exponential utility percentile mean minus variance method applies extended class markov decision processes allow costs stochastic long bounded additionally present efficient algorithm optimizing proposed objective synthetic real world experiments illustrate effectiveness method scale
learning invariant representations molecules atomization energy prediction accurate prediction molecular energetics chemical compound space crucial ingredient rational compound design inherently graph like non vectorial nature molecular data gives rise unique difficult machine learning problem paper adopt learning scratch approach quantum mechanical molecular energies predicted directly raw molecular geometry study suggests benefit setting flexible priors enforcing invariance stochastically rather structurally results improve state art factor almost bringing statistical methods step closer holy grail chemical accuracy
clustering sparse graphs develop new algorithm cluster sparse unweighted graphs partition nodes disjoint clusters higher density within clusters low across clusters sparsity mean setting cluster across cluster edge densities small possibly vanishing size graph sparsity makes problem noisier hence difficult solve clustering involves tradeoff minimizing kinds errors missing edges within clusters present edges across clusters insight sparse case must penalized differently analyze algorithm performance natural classical widely studied planted partition model also called stochastic block model show algorithm cluster sparser graphs smaller clusters previous methods seen empirically well
minimax multi task learning generalized loss compositional paradigm mtl since inception modus operandi multi task learning mtl minimize task wise mean empirical risks introduce generalized loss compositional paradigm mtl includes spectrum formulations subfamily endpoint spectrum minimax mtl new mtl formulation minimizes maximum tasks empirical risks via certain relaxation minimax mtl obtain continuum mtl formulations spanning minimax mtl classical mtl full paradigm loss compositional operating vector empirical risks incorporates minimax mtl relaxations many new mtl formulations special cases show theoretically minimax mtl tends avoid worst case outcomes newly drawn test tasks learning learn ltl test setting results several mtl formulations synthetic real problems mtl ltl test settings encouraging
graphical models via generalized linear models undirected graphical models markov networks gaussian graphical models ising models enjoy popularity variety applications many settings however data follow gaussian binomial distribution assumed models introduce new class graphical models based generalized linear models glm assuming node wise conditional distributions arise exponential families models allow estimate networks wide class exponential distributions poisson negative binomial exponential fitting penalized glms select neighborhood node major contribution paper rigorous statistical analysis showing high probability neighborhood graphical models recovered exactly provide examples high throughput genomic networks learned via glm graphical models multinomial poisson distributed data
super bit locality sensitive hashing sign random projection locality sensitive hashing srp lsh probabilistic dimension reduction method provides unbiased estimate angular similarity yet suffers large variance estimation work propose super bit locality sensitive hashing sblsh easy implement orthogonalizes random projection vectors batches theoretically guaranteed sblsh also provides unbiased estimate angular similarity yet smaller variance angle estimate within extensive experiments real data well validate given length binary code sblsh achieve significant mean squared error reduction estimating pairwise angular similarity moreover sblsh shows superiority srp lsh approximate nearest neighbor ann retrieval experiments
neural autoregressive topic model describe new model learning meaningful representations text documents unlabeled collection documents model inspired recently proposed replicated softmax undirected graphical model word counts shown learn better generative model meaningful document representations specifically take inspiration conditional mean field recursive equations replicated softmax order define neural network architecture estimates probability observing new word given document given previously observed words paradigm also allows replace expensive softmax distribution words hierarchical distribution paths binary tree words end result model whose training complexity scales logarithmically vocabulary size instead linearly replicated softmax experiments show model competitive generative model documents document representation learning algorithm
mechanistic model early sensory processing based subtracting sparse representations early stages sensory systems face challenge compressing information numerous receptors onto much smaller number projection neurons called communication bottleneck make efficient use limited bandwidth compression achieved using predictive coding whereby predictable redundant components stimulus removed case retina srinivasan 1982 suggested feedforward inhibitory connections subtracting linear prediction generated nearby receptors implement compression resulting biphasic center surround receptive fields however feedback inhibitory circuits common early sensory circuits furthermore dynamics nonlinear circuits implement predictive coding well solving transient dynamics nonlinear reciprocal feedback circuits analogy signal processing algorithm called linearized bregman iteration show nonlinear predictive coding implemented inhibitory feedback circuit response step stimulus interneuron activity time constructs progressively less sparse accurate representations stimulus temporally evolving prediction analysis provides powerful theoretical framework interpret understand dynamics early sensory processing variety physiological experiments yields novel predictions regarding relation activity stimulus statistics
truncation free online variational inference bayesian nonparametric models present truncation free online variational inference algorithm bayesian nonparametric models unlike traditional online variational inference algorithms require truncations model variational distribution method adapts model complexity fly experiments dirichlet process mixture models hierarchical dirichlet process topic models large scale data sets show better performance previous online variational inference algorithms
mcmc continuous time discrete state systems propose simple novel framework mcmc inference continuous time discrete state systems pure jump trajectories construct exact mcmc sampler systems alternately sampling random discretization time given trajectory system new trajectory given discretization first step performed efficiently using properties poisson process second step avail discrete time mcmc techniques based forward backward algorithm compare approach particle mcmc uniformization based sampler show advantages
locating changes highly dependent data unknown number change points problem multiple change point estimation considered sequences unknown number change points consistency framework suggested suitable highly dependent time series asymptotically consistent algorithm proposed order consistency established assumption required data generated stationary ergodic time series distributions modeling independence parametric assumptions made data allowed dependent dependence arbitrary form theoretical results complemented experimental evaluations
distributed non stochastic experts consider online distributed non stochastic experts problem distributed system consists coordinator node connected sites sites required communicate via coordinator time step site nodes pick expert set site receives information payoffs experts round goal distributed system minimize regret time horizon simultaneously keeping communication minimum extreme solutions problem full communication essentially simulates non distributed setting obtain optimal sqrt log regret bound cost communication communication site runs independent copy regret sqrt log communication paper shows difficulty simultaneously achieving regret asymptotically better sqrt communication better give novel algorithm oblivious adversary achieves non trivial trade regret sqrt epsilon communication epsilon value epsilon also consider variant model coordinator picks expert model show label efficient forecaster cesa bianchi 2005 already gives strategy near optimal regret communication trade
nonparametric bayesian inverse reinforcement learning multiple reward functions present nonparametric bayesian approach inverse reinforcement learning irl multiple reward functions previous irl algorithms assume behaviour data obtained agent optimizing single reward function assumption hard met practice approach based integrating dirichlet process mixture model bayesian irl provide efficient metropolis hastings sampling algorithm utilizing gradient posterior estimate underlying reward functions demonstrate approach outperforms previous ones via experiments number problem domains
multiclass learning simplex coding paper dicuss novel framework multiclass learning defined suitable coding decoding strategy namely simplex coding allows generalize multiple classes relaxation approach commonly used binary classification framework relaxation error analysis developed avoiding constraints considered hypotheses class moreover show setting possible derive first provably consistent regularized methods training tuning complexity independent number classes tools convex analysis introduced used beyond scope paper
relax randomize value algorithms show principled way deriving online learning algorithms minimax analysis various upper bounds minimax value previously thought non constructive shown yield algorithms allows seamlessly recover known methods derive new ones also capturing unorthodox methods follow perturbed leader forecaster understanding inherent complexity learning problem thus leads development algorithms illustrate approach present several new algorithms including family randomized methods use idea random play new versions follow perturbed leader algorithms presented well methods based littlestone dimension efficient methods matrix completion trace norm algorithms problems transductive learning prediction static experts
weighted likelihood policy search model selection reinforcement learning methods based direct policy search dps actively discussed achieve efficient approach complicated markov decision processes mdps although brought much progress practical applications still remains unsolved problem dps related model selection policy paper propose novel dps method weighted likelihood policy search wlps policy efficiently learned weighted likelihood estimation wlps naturally connects dps statistical inference problem thus various sophisticated techniques statistics applied dps problems directly hence following idea information criterion develop new measurement model comparison dps based weighted log likelihood
statistical consistency ranking methods rank differentiable probability space paper concerned statistical consistency ranking methods recently proven many commonly used pairwise ranking methods inconsistent weighted pairwise disagreement loss wpdl viewed true loss ranking even low noise setting result interesting also surprising given pairwise ranking methods shown effective practice paper argue aforementioned result might conclusive depending kind assumptions used give new assumption labels objects rank lie rank differentiable probability space rdps prove pairwise ranking methods become consistent wpdl assumption especially inspiring rdps actually stronger similar low noise setting studies provide theoretical justifications empirical findings pairwise ranking methods unexplained bridge gap theory applications
proper losses learning partial labels paper discusses problem calibrating posterior class probabilities partially labelled data instance assumed labelled belonging several candidate categories true generalize concept proper loss scenario establish necessary sufficient condition loss function proper show direct procedure construct proper loss partial labels conventional proper loss problem characterized mixing probability matrix relating true class data observed labels interesting result full knowledge matrix required losses constructed proper subset probability simplex
mandatory leaf node prediction hierarchical multilabel classification hierarchical classification prediction paths required always end leaf nodes called mandatory leaf node prediction mlnp particularly useful leaf nodes much stronger semantic meaning internal nodes however lot mlnp methods hierarchical multiclass classification performing mlnp hierarchical multilabel classification much difficult paper propose novel mlnp algorithm considers global hierarchy structure used hierarchies trees dags show efficiently maximize joint posterior probability node labels simple greedy algorithm moreover extended minimization expected symmetric loss experiments performed number real world data sets tree dag structured label hierarchies proposed method consistently outperforms hierarchical flat multilabel classification methods
mirror descent meets fixed share feels regret mirror descent entropic regularizer known achieve shifting regret bounds logarithmic dimension done using either carefully designed projection weight sharing technique via novel unified analysis show approaches deliver essentially equivalent bounds notion regret generalizing shifting adaptive discounted related regrets analysis also captures extends generalized weight sharing technique bousquet warmuth refined several ways including improvements small losses adaptive tuning parameters
mixability statistical learning statistical learning sequential prediction different related formalisms study quality predictions mapping relations transferring ideas active area investigation provide another piece puzzle showing important concept sequential prediction mixability loss natural counterpart statistical setting call stochastic mixability ordinary mixability characterizes fast rates worst case regret sequential prediction stochastic mixability characterizes fast rates statistical learning show special case log loss stochastic mixability reduces well known usually unnamed martingale condition used existing convergence theorems minimum description length bayesian inference case loss reduces margin condition mammen tsybakov case model consideration contains possible predictors equivalent ordinary mixability
marginalized particle gaussian process regression present novel marginalized particle gaussian process mpgp regression provides fast accurate online bayesian filtering framework model latent function using state space model established data construction procedure mpgp recursively filters estimation hidden function values gaussian mixture meanwhile provides new online method training hyperparameters number weighted particles demonstrate estimated performance mpgp simulated real large data sets results show mpgp robust estimation algorithm high computational efficiency outperforms state art sparse methods
context sensitive decision forests object detection paper introduce context sensitive decision forests new perspective exploit contextual information popular decision forest framework object detection problem tree structured classifiers ability access intermediate prediction classification regression information training inference time intermediate prediction available sample allows develop context based decision criteria used refining prediction process addition introduce novel split criterion combination priority based way constructing trees allows accurate regression mode selection hence improves current context information experiments demonstrate improved results task pedestrian detection challenging tud data set compared state art methods
local supervised learning space partitioning develop novel approach supervised learning based adaptively partitioning feature space different regions learning local region specific classifiers formulate empirical risk minimization problem incorporates partitioning classification single global objective show space partitioning equivalently reformulated supervised learning problem consequently discriminative learning method utilized conjunction approach nevertheless consider locally linear schemes learning linear partitions linear region classifiers locally linear schemes approximate complex decision boundaries ensure low training error also provide tight control fitting generalization error train locally linear classifiers using lda logistic regression perceptrons scheme scalable large data sizes high dimensions present experimental results demonstrating improved performance state art classification techniques benchmark datasets also show improved robustness label noise
learning manifolds means flats study problem estimating manifold random samples particular consider piecewise constant piecewise linear estimators induced means ats analyze performance extend previous results means separate directions first provide new results means reconstruction manifolds secondly prove reconstruction bounds higher order approximation ats known results previously available results means novel technical tools well established literature case ats results mathematical tools new
semi crowdsourced clustering generalizing crowd labeling robust distance metric learning main challenges data clustering define appropriate similarity measure objects crowdclustering addresses challenge defining pairwise similarity based manual annotations obtained crowdsourcing despite encouraging results key limitation crowdclustering cluster objects manual annotations available address limitation propose new approach clustering called textit semi crowdsourced clustering effectively combines low level features objects manual annotations subset objects obtained via crowdsourcing key idea learn appropriate similarity measure based low level features objects manual annotations small portion data clustered difficulty learning pairwise similarity measure significant amount noise inter worker variations manual annotations obtained via crowdsourcing address difficulty developing metric learning algorithm based matrix completion method empirical study real world image data sets shows proposed algorithm outperforms state art distance metric learning algorithms clustering accuracy computational efficiency
query complexity derivative free optimization derivative free optimization dfo attractive objective function derivatives available evaluations costly moreover function evaluations noisy approximating gradients finite differences difficult paper gives quantitative lower bounds performance dfo noisy function evaluations exposing fundamental unavoidable gap optimization performance based noisy evaluations versus noisy gradients challenges conventional wisdom method finite differences comparable stochastic gradient however situations dfo unavoidable situations propose new dfo algorithm proved near optimal class strongly convex objective functions distinctive feature algorithm uses boolean valued function comparisons rather evaluations makes algorithm useful even wider range applications including optimization based paired comparisons human subjects example remarkably show regardless whether dfo based noisy function evaluations boolean valued function comparisons convergence rate
learning optimal spike based representations neural networks learn represent information address question assuming neural networks seek generate optimal population representation fixed linear decoder define loss function quality population read derive dynamical equations neurons synapses requirement minimize loss dynamical equations yield network integrate fire neurons undergoing hebbian plasticity show learning initially regular highly correlated spike trains evolve towards poisson distributed independent spike trains much lower firing rates learning rule drives network asynchronous balanced regime inputs network represented optimally given decoder show network dynamics synaptic plasticity jointly balance excitation inhibition received unit tightly possible minimize prediction error inputs decoded outputs turn spikes signalled whenever prediction error exceeds certain value thereby implementing predictive coding scheme work suggests several features reported cortical networks high trial trial variability balance excitation inhibition spike timing dependent plasticity simply signatures efficient spike based code
imitation learning coaching imitation learning shown successful solving many challenging real world problems recent approaches give strong performance guarantees training policy iteratively however important note guarantees depend well policy found imitate oracle training data substantial difference oracle ability learner policy space fail find policy low error training set cases propose use coach demonstrates easy learn actions learner gradually approaches oracle reduction learning demonstration online learning prove coaching yield lower regret bound using oracle apply algorithm novel cost sensitive dynamic feature selection problem hard decision problem considers user specified accuracy cost trade experimental results uci datasets show method outperforms state art imitation learning methods dynamic features selection static feature selection methods
bayesian choose models classification ranking categorical data often structure number variables take label example total number objects image number highly relevant documents per query web search tend follow structured distribution paper study probabilistic model explicitly includes prior distribution counts along count conditional likelihood defines probabilities subsets given size labels binary prior counts poisson binomial distribution standard logistic regression model recovered count distributions priors induce global dependencies combinatorics appear complicate learning inference however demonstrate simple efficient learning procedures derived general forms model show utility formulation exploring multi object classification maximum likelihood learning ranking top classification loss sensitive learning
collaborative ranking parameters primary application collaborate filtering recommend small set items user entails ranking approaches however formulate problem rating prediction overlooking ranking perspective work present method collaborative ranking leverages strengths main approaches neighborhood model based novel method highly efficient seventeen parameters optimize single hyperparameter tune beats state art collaborative ranking methods also show parameters learned dataset yield excellent results different dataset without retraining
non linear metric learning paper introduce novel metric learning algorithms lmnn lmnn explicitly designed non linear easy use approaches achieve goal fundamentally different ways lmnn inherits computational benefits linear mapping linear metric learning uses non linear distance explicitly capture similarities within histogram data sets lmnn applies gradient boosting learn non linear mappings directly function space takes advantage approach robustness speed parallelizability insensitivity towards single additional hyper parameter various benchmark data sets demonstrate methods match current state art terms knn classification error case lmnn obtain best results learning settings
multiple choice learning learning produce multiple structured outputs paper addresses problem generating multiple hypotheses prediction tasks involve interaction users successive components cascade given set multiple hypotheses components users ability automatically rank results thus retrieve best standard approach handling scenario learn single model produce best maximum posteriori map hypotheses model contrast formulate multiple choice learning task multiple output structured output prediction problem loss function captures natural setup problem present max margin formulation minimizes upper bound loss function experimental results problems image segmentation protein side chain prediction show method outperforms conventional approaches used scenario leads substantial improvements prediction accuracy
image denoising inpainting deep neural networks present novel approach low level vision problems combines sparse coding deep networks pre trained denoising auto encoder propose alternative training scheme successfully adapts originally designed unsupervised feature learning tasks image denoising blind inpainting method achieves state art performance image denoising task importantly blind image inpainting task proposed method provides solutions complex problems tackled specifically automatically remove complex patterns like superimposed text image rather simple patterns like pixels missing random moreover proposed method need information regarding region requires inpainting given priori experimental results demonstrate effectiveness proposed method tasks image denoising blind inpainting also show new training scheme effective improve performance unsupervised feature learning
sparse prediction support norm derive novel norm corresponds tightest convex relaxation sparsity combined ell_2 penalty show new norm provides tighter relaxation elastic net thus good replacement lasso elastic net sparse prediction problems studying new norm also bound looseness elastic net thus shedding new light providing justification use
bayesian nonparametric models bipartite graphs develop novel bayesian nonparametric model random bipartite graphs model based theory completely random measures able handle potentially infinite number nodes show model appealing properties particular exhibit power law behavior derive posterior characterization indian buffet like generative process network growth simple efficient gibbs sampler posterior simulation model shown well fitted several real world social networks
bayesian models large scale hierarchical classification challenging problem hierarchical classification leverage hierarchical relations among classes improving classification performance even greater challenge manner computationally feasible large scale problems usually encountered practice paper proposes set bayesian methods model hierarchical dependencies among class labels using multivari ate logistic regression specifically parent child relationships modeled placing hierarchical prior children nodes centered around parame ters parents thereby encouraging classes nearby hierarchy share similar model parameters present new efficient variational algorithms tractable posterior inference models provide parallel implementa tion comfortably handle large scale problems hundreds thousands dimensions tens thousands classes run comparative evaluation multiple large scale benchmark datasets highlights scalability approach shows significant performance advantage state art hierarchical methods
probabilistic low rank subspace clustering paper consider problem clustering data points low dimensional subspaces presence outliers pose problem using density estimation formulation associated generative model based probability model first develop iterative expectation maximization algorithm derive global solution addition develop bayesian methods based variational bayesian approximation capable automatic dimensionality selection first method based alternating optimization scheme unknowns second method makes use recent results matrix factorization leading fast effective estimation methods extended handle sparse outliers robustness handle missing values experimental results suggest proposed methods effective clustering identifying outliers
communication computation tradeoffs consensus based distributed optimization study scalability consensus based distributed optimization algorithms considering questions many processors use given problem often communicate communication free central analysis problem specific value quantifies communication computation tradeoff show organizing communication among nodes regular expander graph cite kregexpanders yields speedups pairs nodes communicate complete graph optimal number processors depends surprisingly speedup obtained terms time reach fixed level accuracy communicating less less frequently computation progresses experiments real cluster solving metric learning non smooth convex minimization tasks demonstrate strong agreement theory practice
pointwise tracking optimal regression function paper examines possibility reject option context least squares regression shown using rejection theoretically possible learn selective regressors epsilon pointwise track best regressor hindsight hypothesis class rejecting bounded portion domain moreover rejected volume vanishes training set size certain conditions develop efficient exact implementation selective regressors case linear regression empirical evaluation suite real world datasets corroborates theoretical analysis indicates selective regressors provide substantial advantage reducing estimation error
multi stage multi task feature learning multi task sparse feature learning aims improve generalization performance exploiting shared features among tasks successfully applied many applications including computer vision biomedical informatics existing multi task sparse feature learning algorithms formulated convex sparse regularization problem usually suboptimal due looseness approximating ell_0 type regularizer paper propose non convex formulation multi task sparse feature learning based novel regularizer solve non convex optimization problem propose multi stage multi task feature learning msmtfl algorithm moreover present detailed theoretical analysis showing msmtfl achieves better parameter estimation error bound convex formulation empirical studies synthetic real world data sets demonstrate effectiveness msmtfl comparison state art multi task sparse feature learning algorithms
cost sensitive exploration bayesian reinforcement learning paper consider bayesian reinforcement learning brl actions incur costs addition rewards thus exploration constrained terms expected total cost learning maximize expected long term total reward order formalize cost sensitive exploration use constrained markov decision process cmdp model environment naturally encode exploration requirements using cost function extend beetle model based brl method learning environment cost constraints demonstrate cost sensitive exploration behaviour number simulated problems
topographic unsupervised learning natural sounds auditory cortex computational modelling primary auditory cortex less fruitful primary visual cortex due less organized properties greater disorder recently demonstrated tonotopy traditionally considered ordered retinotopy disorder appears incongruous given uniformity neocortex however hypothesized would adopt efficient coding strategy disorder reflects natural sound statistics provide computational model tonotopic disorder used model originally proposed smooth map contrast natural images natural sounds exhibit distant correlations learned reflected disordered map auditory model predicted harmonic relationships among neighbouring cells furthermore mechanism used model complex cells reproduced nonlinear responses similar pitch selectivity results contribute understanding sensory cortices different modalities novel integrated manner
exploration model based reinforcement learning empirically estimating learning progress formal exploration approaches model based reinforcement learning estimate accuracy currently learned model without consideration empirical prediction error example pac mdp approaches rmax base model certainty amount collected data bayesian approaches assume prior transition dynamics propose extensions approaches drive exploration solely based empirical estimates learner accuracy learning progress provide sanity check theoretical analysis discussing behavior extensions standard stationary finite state action case provide experimental studies demonstrating robustness exploration measures cases non stationary environments original approaches misled wrong domain assumptions
accuracy top introduce new notion classification accuracy based top tau quantile values scoring function relevant criterion number problems arising search engines define algorithm optimizing convex surrogate corresponding loss show solution obtained solving several convex optimization problems also present margin based guarantees algorithm based tau quantile functions hypothesis set finally report results several experiments evaluating performance algorithm comparison bipartite setting several algorithms seeking high precision top algorithm achieves better performance precision top
collaborative gaussian processes preference learning present new model based gaussian processes gps learning pairwise preferences expressed multiple users inference simplified using emph preference kernel gps allows combine supervised learning user preferences unsupervised dimensionality reduction multi user systems model exploits collaborative information shared structure user behavior also incorporate user features available approximate inference implemented using combination expectation propagation variational bayes finally present efficient active learning strategy querying preferences proposed technique performs favorably real world data state art multi user preference learning algorithms
multilabel classification ranking partial feedback present novel multilabel ranking algorithm working partial information settings algorithm based 2nd order descent methods relies upper confidence bounds trade exploration exploitation analyze algorithm partial adversarial setting covariates adversarial multilabel probabilities ruled generalized linear models show log regret bounds improve several ways existing results test effectiveness upper confidence scheme contrasting full information baselines real world multilabel datasets often obtaining comparable performance
efficient spike coding multiplicative adaptation spike response model neural adaptation underlies ability neurons maximize encoded information wide dynamic range input stimuli adaptation intrinsic feature neuronal models like hodgkin huxley model challenge integrate adaptation models neural computation recent computational models like adaptive spike response model implement adaptation spike based addition fixed size fast spike triggered threshold dynamics slow spike triggered currents adaptation shown accurately model neural spiking behavior limited dynamic range taking cue kinetic models adaptation propose multiplicative adaptive spike response model spike triggered adaptation dynamics scaled multiplicatively adaptation state time spiking show unlike additive adaptation model firing rate multiplicative adaptation model saturates maximum spike rate simulating variance switching experiments model also quantitatively fits experimental data wide dynamic range furthermore dynamic threshold models adaptation suggest straightforward interpretation neural activity terms dynamic signal encoding shifted weighted exponential kernels show thus encoding rectified filtered stimulus signals multiplicative adaptive spike response model achieves high coding efficiency maintains efficiency changes dynamic signal range several orders magnitude without changing model parameters
selecting diverse features via spectral regularization study problem diverse feature selection linear regression selecting small subset diverse features predict given objective diversity useful several reasons interpretability robustness noise etc propose several spectral regularizers capture notion diversity features show submodular set functions regularizers added objective function linear regression result approximately submodular functions maximized approximately efficient greedy local search algorithms provable guarantees compare algorithms traditional greedy ell_1 regularization schemes show obtain diverse set features result regression problem stable perturbations
nonparanormal belief propagation npbp empirical success belief propagation approximate inference algorithm inspired numerous theoretical algorithmic advances yet continuous non gaussian domains performing belief propagation remains challenging task recent innovations nonparametric kernel belief propagation useful come substantial computational cost offer little theoretical guarantees even tree structured models work present nonparanormal performing efficient inference distributions parameterized gaussian copulas network univariate marginals tree structured networks approach guaranteed exact powerful class non gaussian models importantly method efficient standard gaussian convergence properties depend complexity univariate marginals even nonparametric representation used
complex inference neural circuits probabilistic population codes topic models recent experiments demonstrated humans animals typically reason probabilistically environment ability requires neural code represents probability distributions neural circuits capable implementing operations probabilistic inference proposed probabilistic population coding ppc framework provides statistically efficient neural representation probability distributions broadly consistent physiological measurements capable implementing basic operations probabilistic inference biologically plausible way however experiments corresponding neural models largely focused simple tractable probabilistic computations cue combination coordinate transformations decision making result remains unclear generalize framework complex probabilistic computations address short coming showing general approximate inference algorithm known variational bayesian expectation maximization implemented within linear ppc framework apply approach generic problem faced given layer cortex namely identification latent causes complex mixtures spikes identify formal equivalent spike pattern demixing problem topic models used document classification particular latent dirichlet allocation lda construct neural network implementation variational inference learning lda utilizes linear ppc network relies critically non linear operations divisive normalization super linear facilitation ubiquitously observed neural circuits also demonstrate online learning achieved using variation hebb rule describe extesion work allows deal time varying correlated latent causes
deep learning invariant features via tracked video sequences use video sequences produced tracking training data learn invariant features features spatial instead temporal well suited extract still images temporal coherence objective multi layer neural network encodes invariance grow increasingly complex layer hierarchy without fine tuning labels achieve competitive performance non temporal image datasets state art classification accuracy stl object recognition dataset
deep spatio temporal architectures learning protein structure prediction residue residue contact prediction fundamental problem protein structure prediction hower despite considerable research efforts contact prediction methods still largely unreliable introduce novel deep machine learning architecture consists multidimensional stack learning modules contact prediction idea implemented dimensional stack neural networks index spatial coordinates contact map indexes time temporal dimension introduced capture fact protein folding instantaneous process rather progressive refinement networks level stack trained supervised fashion refine predictions produced previous level hence addressing problem vanishing gradients typical deep architectures increased accuracy generalization capabilities approach established rigorous comparison classical machine learning approaches contact prediction deep approach leads accuracy difficult long range contacts roughly state art many variations architectures training algorithms possible leaving room improvements furthermore approach applicable problems strong underlying spatial temporal components
learning wisdom crowds minimax entropy important way make large training sets gather noisy labels crowds nonexperts propose minimax entropy principle improve quality labels method assumes labels generated probability distribution workers items labels maximizing entropy distribution method naturally infers item confusability worker expertise infer ground truth minimizing entropy distribution show minimizes kullback leibler divergence probability distribution unknown truth show simple coordinate descent scheme optimize minimax entropy empirically results substantially better previously published methods problem
monte carlo methods maximum margin supervised topic models effective strategy exploit supervising side information discovering predictive topic representations impose discriminative constraints induced information posterior distributions topic model strategy adopted number supervised topic models medlda employs max margin posterior constraints however unlike likelihood based supervised topic models posterior inference carried using bayes rule max margin posterior constraints made monte carlo methods infeasible least directly applicable thereby limited choice inference algorithms based variational approximation strict mean field assumptions paper develop efficient monte carlo methods much weaker assumptions max margin supervised topic models based importance sampler collapsed gibbs sampler respectively convex dual formulation report thorough experimental results compare approach favorably existing alternatives accuracy efficiency
iterative ranking pair wise comparisons question aggregating pairwise comparisons obtain global ranking collection objects interest long time ranking online gamers msr trueskill system chess players aggregating social opinions deciding product sell based transactions settings addition obtaining ranking finding scores object player rating interest understanding intensity preferences paper propose novel iterative rank aggregation algorithm discovering scores objects pairwise comparisons algorithm natural random walk interpretation graph objects edges present objects compared scores turn stationary probability random walk algorithm model independent establish efficacy method however consider popular bradley terry luce btl model object associated score determines probabilistic outcomes pairwise comparisons objects bound finite sample error rates scores assumed btl model estimated algorithm essence leads order optimal dependence number samples required learn scores well algorithm indeed experimental evaluation shows model independent algorithm performs well maximum likelihood estimator btl model outperforms recently proposed algorithm ammar shah
imagenet classification deep convolutional neural networks trained large deep convolutional neural network classify million high resolution images lsvrc 2010 imagenet training set 1000 different classes test data achieved top top error rates considerably better previous state art results neural network million parameters 500 000 neurons consists convolutional layers followed max pooling layers globally connected layers final 1000 way softmax make training faster used non saturating neurons efficient gpu implementation convolutional nets reduce overfitting globally connected layers employed new regularization method proved effective
fused sparsity robust estimation linear models unknown variance paper develop novel approach problem learning sparse representations context fused sparsity unknown noise level propose algorithm termed scaled fused dantzig selector sfds accomplishes aforementioned learning task means second order cone program special emphasize put particular instance fused sparsity corresponding learning presence outliers establish finite sample risk bounds carry experimental evaluation synthetic real data
topic partitioned multinetwork embeddings introduce joint model network content context designed exploratory analysis email networks via visualization topic specific communication patterns model admixture model text network attributes uses multinomial distributions words mixture components explaining text latent euclidean positions actors mixture components explaining network attributes validate appropriateness model achieving state art performance link prediction task achieving semantic coherence equivalent latent dirichlet allocation demonstrate capability model descriptive explanatory exploratory analysis investigating inferred topic specific communication patterns new government email dataset new hanover county email corpus
inverse reinforcement learning structured classification paper adresses inverse reinforcement learning irl problem inferring reward demonstrated expert behavior optimal introduce new algorithm scirl whose principle use called feature expectation expert parameterization score function multi class classifier approach produces reward function expert policy provably near optimal contrary existing irl algorithms scirl require solving direct problem moreover appropriate heuristic succeed trajectories sampled according expert behavior illustrated car driving simulator
efficient bayes adaptive reinforcement learning using sample based search bayesian model based reinforcement learning formally elegant approach learning optimal behaviour model uncertainty trading exploration exploitation ideal way unfortunately finding resulting bayes optimal policies notoriously taxing since search space becomes enormous paper introduce tractable sample based method approximate bayes optimal planning exploits monte carlo tree search approach outperformed prior bayesian model based algorithms significant margin several well known benchmark problems avoids expensive applications bayes rule within search tree lazily sampling models current beliefs illustrate advantages approach showing working infinite state space domain qualitatively reach almost previous work bayesian exploration
recovery sparse probability measures via convex programming consider problem cardinality penalized optimization convex function probability simplex additional convex constraints well known classical regularizer fails promote sparsity probability simplex since norm probability simplex trivially constant propose direct relaxation minimum cardinality problem show efficiently solved using convex programming first application consider recovering sparse probability measure given moment constraints formulation becomes linear programming hence solved efficiently sufficient condition exact recovery minimum cardinality solution derived arbitrary affine constraints develop penalized version noisy setting solved using second order cone programs proposed method outperforms known heuristics based norm second application consider convex clustering using sparse gaussian mixture compare results well known soft means algorithm
entropy estimations using correlated symmetric stable random projections methods efficiently estimating shannon entropy data streams important applications learning data mining network anomaly detections ddos attacks nonnegative data streams method compressed counting based maximally skewed stable random projections provide accurate estimates shannon entropy using small storage however longer applicable entries data streams common scenario comparing streams paper propose algorithm entropy estimation general data streams allow negative entries method shannon entropy approximated finite difference correlated frequency moments estimated correlated samples symmetric stable random variables experiments confirm method able substantially better approximate shannon entropy compared prior state art
compressive neural representation sparse high dimensional probabilities paper shows sparse high dimensional probability distributions could represented neurons exponential compression representation novel application compressive sensing sparse probability distributions rather usual sparse signals compressive measurements correspond expected values nonlinear functions probabilistically distributed variables expected values estimated sampling quality compressed representation limited quality sampling since compression preserves geometric structure space sparse probability distributions probabilistic computation performed compressed domain interestingly functions satisfying requirements compressive sensing implemented simple perceptrons use perceptrons simple model feedforward computation neurons results show mean activity relatively small number neurons accurately represent high dimensional joint distribution implicitly even without accounting noise correlations comprises novel hypothesis neurons could encode probabilities brain
selective labeling via error bound minimization many practical machine learning problems acquisition labeled data often expensive time consuming motivates study problem follows given label budget select data points label learning performance optimized propose selective labeling method analyzing generalization error laplacian regularized least squares laprls particular derive deterministic generalization error bound laprls trained subsampled data propose select subset data points label minimizing upper bound since minimization combinational problem relax continuous domain solve projected gradient descent experiments benchmark datasets show proposed method outperforms state art methods
natural images gaussian mixtures dead leaves simple gaussian mixture models gmms learned pixels natural image patches recently shown surprisingly strong performers modeling statistics natural images provide depth analysis simple yet rich model show gmm model able compete even successful models natural images log likelihood scores denoising performance sample quality provide analysis model learns natural images function number mixture components including covariance structure contrast variation intricate structures textures boundaries finally show salient properties gmm learned natural images derived simplified dead leaves model explicitly models occlusion explaining surprising success relative models
nystr method random fourier features theoretical empirical comparison random fourier features nystr method successfully applied efficient kernel learning work investigate fundamental difference approaches difference could affect generalization performances unlike approaches based random fourier features basis functions cosine sine functions sampled distribution independent training data basis functions used nystr method randomly sampled training examples therefore data dependent exploring difference show large gap eigen spectrum kernel matrix approaches based nystr method yield impressively better generalization error bound random fourier features based approach empirically verify theoretical findings wide range large data sets
multiresolution gaussian processes propose multiresolution gaussian process capture long range non markovian dependencies allowing abrupt changes multiresolution hierarchically couples collection smooth gps defined element random nested partition long range dependencies captured top level partition points define abrupt changes due inherent conjugacy gps analytically marginalize gps compute conditional likelihood observations given partition tree allows efficient inference partition employ graph theoretic techniques apply multiresolution analysis magnetoencephalography meg recordings brain activity
robustness risk sensitivity markov decision processes uncover relations robust mdps risk sensitive mdps objective robust mdp minimize function expectation cumulative cost worst case parameters uncertainties objective risk sensitive mdp minimize risk measure cumulative cost parameters known show risk sensitive mdp minimizing expected exponential utility equivalent robust mdp minimizing worst case expectation penalty deviation uncertain parameters nominal values measured kullback leibler divergence also show risk sensitive mdp minimizing iterated risk measure composed certain coherent risk measures equivalent robust mdp minimizing worst case expectation possible deviations uncertain parameters nominal values characterized concave function
shifting weights adapting object detectors image video typical object detectors trained images perform poorly video clear distinction domain types data paper tackle problem adapting object detectors learned images work well videos treat problem unsupervised domain adaptation given labeled data source domain image unlabeled data target domain video approach self paced domain adaptation seeks iteratively adapt detector training detector automatically discovered target domain examples starting easiest first iteration algorithm adapts considering increased number target domain examples decreased number source domain examples discover target domain examples vast amount video data introduce simple robust approach scores trajectory tracks instead bounding boxes also show rich expressive features specific target domain incorporated framework show promising results 2011 trecvid multimedia event detection labelme video datasets illustrate benefit approach adapt object detectors video
quasi newton proximal splitting method describe efficient implementations proximity calculation useful class functions implementations exploit piece wise linear nature dual problem second part paper applies previous result acceleration convex minimization problems leads elegant quasi newton method optimization method compares favorably state art alternatives algorithm extensive applications including signal processing sparse regression recovery machine learning classification
latent coincidence analysis hidden variable model distance metric learning describe latent variable model supervised dimensionality reduction distance metric learning model discovers linear projections high dimensional data shrink distance similarly labeled inputs expand distance differently labeled ones model continuous latent variables locate pairs examples latent space lower dimensionality model differs significantly classical factor analysis posterior distribution latent variables always multivariate gaussian nevertheless show inference completely tractable derive expectation maximization algorithm parameter estimation also compare model approaches distance metric learning model main advantage simplicity iteration algorithm distance metric estimated solving unconstrained least squares problem experiments show simple updates highly effective
proximal newton type methods minimizing convex objective functions composite form consider minimizing convex objective functions emph composite form begin align minimize_ end align convex twice continuously differentiable convex necessarily differentiable function whose proximal mapping evaluated efficiently derive generalization newton type methods handle convex nonsmooth objective functions many problems relevance high dimensional statistics machine learning signal processing formulated composite form prove methods globally convergent minimizer achieve quadratic rates convergence vicinity unique minimizer also demonstrate performance methods using problems relevance machine learning high dimensional statistics
gender generic diversified ranking algorithm diversified ranking fundamental task machine learning broadly applicable many real world problems information retrieval team assembling product search etc paper consider generic setting aim diversify top ranking list based arbitrary relevance function arbitrary similarity function among examples formulate optimization problem show general hard show large volume parameter space proposed objective function enjoys diminishing returns property enables design scalable greedy algorithm find near optimal solution experimental results real data sets demonstrate effectiveness proposed algorithm
cocktail party processing via structured prediction human listeners excel selectively attending conversation cocktail party machine performance still far inferior comparison show cocktail party problem speech separation problem effectively approached via structured prediction account temporal dynamics speech employ conditional random fields crfs classify speech dominance within time frequency unit sound mixture capture complex nonlinear relationship input output state transition feature functions crfs learned deep neural networks formulation problem classification allows directly optimize measure well correlated human speech intelligibility proposed system substantially outperforms existing ones variety noises
bandit algorithms boost brain computer interfaces motor task selection brain controlled button brain computer interface bci allows users communicate computer without using muscles bci based sensori motor rhythms use imaginary motor tasks moving right left hand send control signals performances bci vary greatly across users also depend tasks used making problem appropriate task selection important issue study presents new procedure automatically select fast possible discriminant motor task brain controlled button develop purpose adaptive algorithm ucb classif based stochastic bandit theory shortens training stage thereby allowing exploration greater variety tasks wasting time inefficient tasks focusing promising ones algorithm results faster task selection efficient use bci training session comparing proposed method standard practice task selection fixed time budget ucb classif leads improve classification rate fix classification rate reduction time spent training
gradient based kernel method feature extraction variable selection propose novel kernel approach dimension reduction supervised learning feature extraction variable selection former constructs small number features predictors latter finds subset predictors first method linear feature extraction proposed using gradient regression function based recent development kernel method comparison existing methods proposed wide applicability without strong assumptions regressor type variables uses computationally simple eigendecomposition thus applicable large data sets second combination sparse penalty method extended variable selection following approach chen 2010 experimental results show proposed methods successfully find effective features variables without parametric models
hamming distance metric learning motivated large scale multimedia applications propose learn mappings high dimensional data binary codes preserve semantic similarity binary codes well suited large scale applications storage efficient permit exact sub linear knn search framework applicable broad families mappings uses flexible form triplet ranking loss overcome discontinuous optimization discrete mappings minimizing piecewise smooth upper bound empirical loss inspired latent structural svms develop new loss augmented inference algorithm quadratic code length show strong retrieval performance cifar mnist promising classification results using knn binary codes
recursive deep learning point clouds recent advances sensing technologies make possible easily record color depth images together improve object recognition current methods rely well designed features new modality introduce novel model based sparse recursive autoencoders rae learning features object categories raw point clouds well standard images model differs previous rae models fixes tree structures includes short circuit connections tree nodes final classifier allows model take consideration low level features well global features object using fully learned architecture achieve state art performance standard rgb object recognition dataset rivaling random forest classifiers hand designed features sift spin images method fast classify images second standard desktop machine matlab possible method requires matrix multiplications classify image household objects
dual space analysis sparse linear model sparse linear generalized linear models combine standard likelihood function sparse prior unknown coefficients priors conveniently expressed maximization mean gaussians different variance hyperparameters standard map estimation type involves maximizing hyperparameters coefficients empirical bayesian alternative type first marginalizes coefficients maximizes hyperparameters leading tractable posterior approximation underlying cost functions related via dual space framework wipf 2011 allows type type objectives expressed either coefficient hyperparmeter space perspective useful analyses extensions conducive development space herein consider estimation trade parameter balancing sparsity data fit parameter effectively variance natural estimators exist assessing problem hyperparameter variance space transitioning natural ideas type solve much less intuitive type contrast analyses update rules sparsity properties local global solutions well extensions general likelihood models leverage coefficient space techniques developed type apply type example allows prove type inspired techniques successful recovering sparse coefficients unfavorable restricted isometry properties rip lead failure popular reconstructions also facilitates analysis type non gaussian likelihood models lead intractable integrations
forging graphs low rank positive semidefinite graph learning approach many graph based machine learning data mining approaches quality graph critical however real world applications especially semi supervised learning unsupervised learning evaluation quality graph often expensive sometimes even impossible due cost unavailability ground truth paper proposed robust approach convex optimization forge graph input graph learn graph higher quality major concern ideal graph shall satisfy following constraints non negative symmetric low rank positive semidefinite develop graph learning algorithm solving convex optimization problem develop efficient optimization obtain global optimal solutions theoretical guarantees non sensitive parameter method shown experimental results robust achieve higher accuracy semi supervised learning clustering various settings preprocessing graphs method wide range potential applications machine learning data mining
exact stable recovery sequences signals sparse increments via differential minimization consider problem recovering sequence vectors x_k increments x_k s_k sparse s_k typically smaller s_1 based linear measurements y_k a_k x_k e_k a_k e_k denote measurement matrix noise respectively assuming a_k obeys restricted isometry property rip certain order depending s_k show absence noise convex program minimizes weighted sum ell_1 norm successive differences subject linear measurement constraints recovers sequence x_k emph exactly interesting result convex program equivalent standard compressive sensing problem highly structured aggregate measurement matrix satisfy rip requirements standard sense yet achieve exact recovery presence bounded noise propose quadratically constrained convex program recovery derive bounds reconstruction error sequence supplement theoretical analysis simulations application real video data support validity proposed approach acquisition recovery signals time varying sparsity
bayesian approach policy learning trajectory preference queries consider problem learning control policies via trajectory preference queries expert particular learning agent present expert short runs pair policies originating state expert indicates preferred trajectory agent goal elicit latent target policy expert queries possible tackle problem propose novel bayesian model querying process introduce methods exploit model actively select expert queries experimental results benchmark problems indicate model effectively learn policies trajectory preference queries active query selection substantially efficient random selection
coupling nonparametric mixtures via latent dirichlet processes mixture distributions often used model complex data paper develop new method jointly estimates mixture models multiple data sets exploiting statistical dependencies specifically introduce set latent dirichlet processes sources component models atoms data set construct nonparametric mixture model combining sub sampled versions latent dps mixture model acquire atoms different latent dps atom shared multiple mixtures multi multi association distinguishes proposed method prior constructions rely tree chain structures allowing mixture models coupled flexibly addition derive sampling algorithm jointly infers model parameters present experiments document analysis image modeling
prior probability influences decision making unifying probabilistic model brain combine prior knowledge sensory evidence making decisions uncertainty competing descriptive models proposed based experimental data first posits additive offset decision variable implying static effect prior however model inconsistent recent data motion discrimination task involving temporal integration uncertain sensory evidence explain data second model proposed assumes time varying influence prior present normative model decision making incorporates prior knowledge principled way show additive offset model time varying prior model emerge naturally decision making viewed within framework partially observable markov decision processes pomdps decision making model reduces computing beliefs given observations prior information bayesian manner selecting actions based beliefs maximize expected sum future rewards show model explain data previously explained using additive offset model well recent data time varying influence prior knowledge decision making
learning label trees probabilistic modelling implicit feedback user preferences items inferred either explicit feedback item ratings implicit feedback rental histories research collaborative filtering concentrated explicit feedback resulting development accurate scalable models however since explicit feedback often difficult collect important develop effective models take advantage widely available implicit feedback introduce probabilistic approach collaborative filtering implicit feedback based modelling user item selection process interests scalability restrict attention tree structured distributions items develop principled efficient algorithm learning item trees data also identify problem widely used protocol evaluating implicit feedback models propose way addressing using small quantity explicit feedback data
cprl extension compressive sensing phase retrieval problem compressive sensing vibrant active research fields past years development applies linear models limits application excludes many areas ideas could make difference paper presents novel extension phase retrieval problem intensity measurements linear system used recover complex sparse signal propose novel solution using lifting technique cprl relaxes hard problem nonsmooth semidefinite program analysis shows cprl inherits many desirable properties guarantees exact recovery provide scalable numerical solvers accelerate implementation source code algorithms provided public
factoring nonnegative matrices linear programs paper describes new approach computing nonnegative matrix factorizations nmfs linear programming key idea data driven model factorization salient features data used express remaining features precisely given data matrix algorithm identifies matrix satisfies linear constraints matrix selects features used compute low rank nmf theoretical analysis demonstrates approach type guarantees recent nmf algorithm arora 2012 contrast earlier work proposed method better noise tolerance extends general noise models leads efficient scalable algorithms experiments synthetic real datasets provide evidence new approach also superior practice optimized implementation new algorithm factor multi gigabyte matrix matter minutes
nonparametric reduced rank regression propose approach multivariate nonparametric regression generalizes reduced rank regression linear models additive model estimated dimension dimensional response shared dimensional predictor variable control complexity model employ functional form fan nuclear norm resulting set function estimates low rank backfitting algorithms derived justified using nonparametric form nuclear norm subdifferential oracle inequalities excess risk derived exhibit scaling behavior procedure high dimensional setting methods illustrated gene expression data
parametric local metric learning nearest neighbor classification study problem learning local metrics nearest neighbor classification previous works local metric learning learn number local unrelated metrics independence approach delivers increased flexibility downside considerable risk overfitting present new parametric local metric learning method learn smooth metric matrix function data manifold using approximation error bound metric matrix function learn local metrics linear combinations basis metrics defined anchor points different regions instance space constrain metric matrix function imposing linear combinations manifold regularization makes learned metric matrix function vary smoothly along geodesics data manifold metric learning method excellent performance terms predictive power scalability experimented several large scale classification problems tens thousands instances compared several state art metric learning methods global local well svm automatic kernel selection outperforms significant manner
non parametric approximate dynamic programming via kernel method paper presents novel non parametric approximate dynamic programming adp algorithm enjoys graceful dimension independent approximation sample complexity guarantees particular establish theoretically computationally proposal serve viable alternative state art parametric adp algorithms freeing designer carefully specifying approximation architecture accomplish developing kernel based mathematical program adp via computational study controlled queueing network show non parametric procedure competitive parametric adp approaches
neuronal spike generation mechanism oversampling noise shaping converter explore hypothesis neuronal spike generation mechanism analog digital converter rectifies low pass filtered summed synaptic currents encodes spike trains linearly decodable post synaptic neurons digitally encode analog current waveform sampling rate spike generation mechanism must exceed nyquist rate oversampling consistent experimental observation precision spike generation mechanism order magnitude greater cut frequency dendritic low pass filtering achieve additional reduction error analog digital conversion electrical engineers rely noise shaping noise shaping used neurons would introduce correlations spike timing reduce low frequency nyquist transmission error cost high frequency nyquist sampling rate using experimental data different classes neurons demonstrate biological neurons utilize noise shaping also argue rectification spike generation mechanism improve energy efficiency carry noising finally zoo ion channels neurons viewed set predictors various subsets activated depending statistics input current
bayesian nonparametric modeling suicide attempts national epidemiologic survey alcohol related conditions nesarc database contains large amount information regarding way life medical conditions depression etc representative sample population present paper interested seeking hidden causes behind suicide attempts propose model subjects using nonparametric latent model based indian buffet process ibp due nature data need adapt observation model discrete random variables propose generative model observations drawn multinomial logit distribution given ibp matrix implementation efficient gibbs sampler accomplished using laplace approximation allows integrate weighting factors multinomial logit likelihood model finally experiments nesarc database show model properly captures hidden causes model suicide attempts
dynamical graph learning object shape modeling detection paper studies novel discriminative part based model represent recognize object shapes graph define model consisting layers leaf nodes collaborative edges localizing local parts nodes specifying switch leaf nodes root node encoding global verification discriminative learning algorithm extended cccp proposed train model dynamical manner model structure configuration leaf nodes associated nodes automatically determined optimizing multi layer parameters iteration advantages method fold graph model enables handle well large intra class variance background clutters object shape detection images proposed learning algorithm able obtain graph representation without requiring elaborate supervision initialization validate proposed method several challenging databases inria horse ethz shape uiuc people outperforms state arts approaches
searching objects driven context dominant visual search paradigm object class detection sliding windows although simple effective also wasteful unnatural rigidly hardwired propose strategies search objects intelligently explore space windows making sequential observations locations decided based previous observations strategies adapt class searched content particular test image driving force exploiting context statistical relation appearance window location relative object observed training set addition elegant sliding windows demonstrate experimentally pascal voc 2010 dataset strategies evaluate orders magnitude fewer windows time achieving higher detection accuracy
delay compensation dynamical synapses time delay pervasive neural information processing achieve real time tracking critical compensate transmission processing delays neural system present study show dynamical synapses short term depression enhance mobility continuous attractor network extent system tracks time varying stimuli timely manner state network either track instantaneous position moving stimulus perfectly lag lead effectively constant time agreement experiments head direction systems rodents parameter regions delayed perfect anticipative tracking correspond network states static ready move spontaneously moving respectively demonstrating strong correlation tracking performance intrinsic dynamics network also find speed stimulus coincides natural speed network state delay becomes effectively independent stimulus amplitude
trajectory based short sighted probabilistic planning probabilistic planning captures uncertainty plan execution probabilistically modeling effects actions environment therefore probability reaching different states given state action order compute solution probabilistic planning problem planners need manage uncertainty associated different paths initial state goal state several approaches manage uncertainty proposed consider paths perform determinization actions sampling paper introduce trajectory based short sighted stochastic shortest path problems ssps novel approach manage uncertainty probabilistic planning problems states reachable low probability substituted artificial goals heuristically estimate cost reach goal state also extend theoretical results short sighted probabilistic planner ssipp ref proving ssipp always finishes asymptotically optimal sufficient conditions structure short sighted ssps empirically compare ssipp using trajectory based short sighted ssps winners previous probabilistic planning competitions state art planners triangle tireworld problems trajectory based ssipp outperforms competitors planner able scale problem number problem optimal solution contains approximately states
reducing statistical time series problems binary classification show binary classification methods developed work data used solving statistical problems seemingly unrelated classification concern highly dependent time series specifically problems time series clustering homogeneity testing sample problem addressed algorithms construct solving problems based new metric time series distributions evaluated using binary classification methods universal consistency proposed algorithms proven general assumptions theoretical results illustrated experiments synthetic real world data
learning distributions via support measure machines paper presents kernel based discriminative learning framework probability measures rather relying large collections vectorial training examples framework learns using collection probability distributions constructed meaningfully represent training data representing probability distributions mean embeddings reproducing kernel hilbert space rkhs able apply many standard kernel based learning techniques straightforward fashion accomplish construct generalization support vector machine svm called support measure machine smm analyses smms provides several insights relationship traditional svms based insights propose flexible svm flex svm places different kernel functions training example experimental results synthetic real world data demonstrate effectiveness proposed framework
gradient weights help nonparametric regressors regression problems real unknown function often varies coordinates others show weighting coordinate estimated norm derivative efficient way significantly improve performance distance based regressors kernel regressors propose simple estimator derivative norms prove consistency moreover proposed estimator efficiently learned online
joint modeling matrix associated text via latent binary features new methodology developed joint analysis matrix accompanying documents documents associated matrix rows columns documents modeled focused topic model inferring latent binary features topics document new matrix decomposition developed latent binary features associated rows columns imposition low rank constraint matrix decomposition topic model coupled sharing latent binary feature vectors associated model applied roll call data associated documents defined legislation state art results manifested prediction votes new piece legislation based observed text legislation coupling text legislation also demonstrated yield insight properties matrix decomposition roll call data
strategic impatience nogo versus forced choice decision making alternative forced choice 2afc nogo gng tasks behavioral choice paradigms commonly used study sensory cognitive processing choice behavior gng thought isolate sensory decisional component removing need response selection consistent bias towards response higher hits false alarm rates gng task suggests possible fundamental differences sensory cognitive processes engaged tasks existing mechanistic models choice tasks mostly variants drift diffusion model ddm related leaky competing accumulator models capture various aspects behavior address provenance bias postulate impatience strategic adjustment response implicit asymmetry cost structure gng nogo response requires waiting response deadline response immediately terminates current trial show bayes risk minimizing decision policy minimizes error rate average decision delay naturally exhibits experimentally observed bias optimal decision policy formally equivalent ddm time varying threshold initially rises stimulus onset collapses near response deadline initial rise due fading temporal advantage choosing response fixed delay nogo response show fitting simpler fixed threshold ddm optimal model reproduces counterintuitive result higher threshold gng 2afc decision making previously observed direct ddm fit behavioral data although approximations cannot reproduce bias thus observed discrepancies gng 2afc decision making arise rational strategic adjustments cost structure need imply additional differences underlying sensory cognitive processes
online regret bounds undiscounted continuous reinforcement learning derive sublinear regret bounds undiscounted reinforcement learning continuous state space proposed algorithm combines state aggregation use upper confidence bounds implementing optimism face uncertainty beside existence optimal policy satisfies poisson equation assumptions made hoelder continuity rewards transition probabilities
link prediction graphs autoregressive features paper consider problem link prediction time evolving graphs assume certain graph features node degree follow vector autoregressive var model propose use information improve accuracy prediction strategy involves joint optimization procedure space adjacency matrices var matrices takes account sparsity low rank properties matrices oracle inequalities derived illustrate trade offs choice smoothing parameters modeling joint effect sparsity low rank property estimate computed efficiently using proximal methods generalized forward backward agorithm
learning recursive perceptual representations linear support vector machines svms become popular vision part state art object recognition classification tasks require high dimensional feature spaces good performance deep learning methods find compact representations current methods employ multilayer perceptrons require solving difficult non convex optimization problem propose deep non linear classifier whose layers svms incorporates random projection core stacking element method learns layers linear svms recursively transforming original data manifold random projection weak prediction computed layer method scales linear svms rely kernel computations nonconvex optimization exhibits better generalization ability kernel based svms especially true number training samples smaller dimensionality data common scenario many real world applications use random projections key method show experiments section observe consistent improvement previous often complicated methods several vision speech benchmarks
learning image descriptors boosting trick paper apply boosting learn complex non linear local visual feature representations drawing inspiration successful application visual object detection main goal local feature descriptors distinctively represent salient image region remaining invariant viewpoint illumination changes representation improved using machine learning however past approaches mostly limited learning linear feature mappings either original input kernelized input feature space kernelized methods proven somewhat effective learning non linear local feature descriptors rely heavily choice appropriate kernel function whose selection often difficult non intuitive propose use boosting trick obtain non linear mapping input high dimensional feature space non linear feature mapping obtained boosting trick highly intuitive employ gradient based weak learners resulting learned descriptor closely resembles well known sift demonstrated experiments resulting descriptor learned directly intensity patches achieving state art performance
value pursuit iteration value pursuit iteration vpi approximate value iteration algorithm finds close optimal policy reinforcement learning planning problems large state spaces vpi main features first nonparametric algorithm finds good sparse approximation optimal value function given dictionary features algorithm almost insensitive number irrelevant features second iteration vpi algorithm adds set functions based currently learned value function dictionary increases representation power dictionary way directly relevant goal good approximation optimal value function theoretically study vpi provide finite sample error upper bound
identification recurrent patterns activation brain networks identifying patterns neuroimaging recordings brain activity related unobservable psychological mental state individual treated unsupervised pattern recognition problem main challenges however analysis fmri data defining physiologically meaningful feature space representing spatial patterns across time dealing high dimensionality data robustness various artifacts confounds fmri time series paper present network aware feature space represent states general network enables comparing clustering states manner meaningful terms network connectivity structure computationally efficient low dimensional relatively robust structured random noise artifacts feature space obtained spherical relaxation transportation distance metric measures cost transporting mass network transform function another theoretical empirical assessments demonstrate accuracy efficiency approximation especially large problems application presented identifying distinct brain activity patterns fmri feature space applied problem identifying recurring patterns detecting outliers measurements many different types networks including sensor control social networks
multiple operator valued kernel learning positive definite operator valued kernels generalize well known notion reproducing kernels naturally adapted multi output learning situations paper addresses problem learning finite linear combination infinite dimensional operator valued kernels suitable extending functional data analysis methods nonlinear contexts study problem case kernel ridge regression functional responses norm constraint combination coefficients resulting optimization problem involved multiple scalar valued kernel learning since operator valued kernels pose technical theoretical issues propose multiple operator valued kernel learning algorithm based solving system linear operator equations using block coordinate descent procedure experimentally validate approach functional regression task context finger movement prediction brain computer interfaces
newton like methods sparse inverse covariance estimation propose classes second order optimization methods solving sparse inverse covariance estimation problem first approach call newton lasso method minimizes piecewise quadratic model objective function every iteration generate step employ fast iterative shrinkage thresholding method fista solve subproblem second approach call orthant based newton method phase algorithm first identifies orthant face minimizes smooth quadratic approximation objective function using conjugate gradient method methods exploit structure hessian efficiently compute search direction avoid explicitly storing hessian show quasi newton methods also effective context describe limited memory bfgs variant orthant based newton method present numerical results suggest techniques described paper attractive properties constitute useful tools solving sparse inverse covariance estimation problem comparisons method implemented quic software package presented
patient risk stratification hospital associated diff time series classification task patient risk adverse events affected temporal processes including nature timing diagnostic therapeutic activities overall evolution patient pathophysiology time yet many investigators ignore temporal aspect modeling patient risk considering patient current aggregate state explore representing patient risk time series patient risk stratification becomes time series classification task task differs applications time series analysis like speech processing since time series must first extracted thus begin defining extracting approximate textit risk processes evolving approximate daily risk patient obtained use signals explore different approaches time series classification goal identifying high risk patterns apply classification specific task identifying patients risk testing positive hospital acquired colonization textit clostridium difficile achieve area receiver operating characteristic curve held set several hundred patients stage approach risk stratification outperforms classifiers consider patient current state
bayesian warped gaussian processes warped gaussian processes wgp model output observations regression tasks parametric nonlinear transformation gaussian process use nonlinear transformation included part probabilistic model shown enhance performance providing better prior model several data sets order learn parameters maximum likelihood used work show possible use non parametric nonlinear transformation wgp variationally integrate resulting bayesian wgp able work scenarios maximum likelihood wgp failed low data regime data censored values classification etc demonstrate superior performance bayesian warped gps several real data sets
use non stationary policies stationary infinite horizon markov decision processes consider infinite horizon stationary gamma discounted markov decision processes known exists stationary optimal policy using value policy iteration error epsilon iteration well known compute stationary policies frac gamma gamma epsilon optimal arguing guarantee tight develop variations value policy iteration computing non stationary policies frac gamma gamma epsilon optimal constitutes significant improvement usual situation gamma close surprisingly shows problem computing near optimal non stationary policies much simpler computing near optimal stationary policies
efficient exploration value function generalization deterministic systems consider problem reinforcement learning episodes finite horizon deterministic system solution propose optimistic constraint propagation ocp algorithm designed synthesize efficient exploration value function generalization establish true value function lies within given hypothesis class ocp selects optimal actions episodes eluder dimension given hypothesis class establish efficiency asymptotic performance guarantees apply even true value function lie given hypothesis space special case hypothesis space span pre specified indicator functions disjoint sets
local privacy minimax bounds sharp rates probability estimation provide detailed study estimation probability distributions discrete continuous stringent setting data kept private even statistician give sharp minimax rates convergence estimation locally private settings exhibiting fundamental tradeoffs privacy convergence rate well providing tools allow movement along privacy statistical efficiency continuum consequences results warner classical work randomized response optimal way perform survey sampling maintaining privacy respondents
fast determinantal point process sampling application clustering determinantal point process dpp gained much popularity modeling sets diverse items gist dpp probability choosing particular set items proportional determinant positive definite matrix defines similarity items however computing determinant requires time cubic number items hence impractical large sets paper address problem constructing rapidly mixing markov chain acquire sample given dpp sub cubic time addition show framework extended sampling cardinality constrained dpps application show sampling algorithm used provide fast heuristic determining number clusters resulting better clustering
compressive feature learning paper addresses problem unsupervised feature learning text data method grounded principle minimum description length uses dictionary based compression scheme extract succinct feature set specifically method finds set word grams minimizes cost reconstructing text losslessly formulate document compression binary optimization task show solve approximately via sequence reweighted linear programs efficient solve parallelizable method unsupervised features extracted subsequently used variety tasks demonstrate performance features range scenarios including unsupervised exploratory analysis supervised text categorization compressed feature space orders magnitude smaller full gram space matches text categorization accuracy achieved full feature space dimensionality reduction results faster training times also help elucidate structure unsupervised learning tasks reduce amount training data necessary supervised learning
stochastic convex optimization multiple objectives paper interested development efficient algorithms convex optimization problems simultaneous presence multiple objectives stochasticity first order information cast stochastic multiple objective optimization problem constrained optimization problem choosing function objective try bound objectives appropriate thresholds first examine stages exploration exploitation based algorithm first approximates stochastic objectives sampling solves constrained stochastic optimization problem projected gradient method method attains suboptimal convergence rate even strong assumption objectives second approach efficient primal dual stochastic algorithm leverages theory lagrangian method constrained optimization attains optimal convergence rate sqrt high probability general lipschitz continuous objectives
deep content based music recommendation automatic music recommendation become increasingly relevant problem recent years since lot music sold consumed digitally recommender systems rely collaborative filtering however approach suffers cold start problem fails usage data available effective recommending new unpopular songs paper propose use latent factor model recommendation predict latent factors music audio cannot obtained usage data compare traditional approach using bag words representation audio signals deep convolutional neural networks evaluate predictions quantitatively qualitatively million song dataset show using predicted latent factors produces sensible recommendations despite fact large semantic gap characteristics song affect user preference corresponding audio signal also show recent advances deep learning translate well music recommendation setting deep convolutional neural networks significantly outperforming traditional approach
adaptive submodular maximization bandit setting maximization submodular functions wide applications machine learning artificial intelligence adaptive submodular maximization traditionally studied assumption model world expected gain choosing item given previously selected items states known paper study scenario expected gain initially unknown learned interacting repeatedly optimized function propose efficient algorithm solving problem prove expected cumulative regret increases logarithmically time regret bound captures inherent property submodular maximization earlier mistakes costly later ones refer approach optimistic adaptive submodular maximization oasm trades exploration exploitation based optimism face uncertainty principle evaluate method preference elicitation problem show non trivial step policies learned hundred interactions problem
correlated random features fast semi supervised learning paper presents correlated nystrom views xnv fast semi supervised algorithm regression classification algorithm draws main ideas first generates views consisting computationally inexpensive random features second multiview regression using canonical correlation analysis cca unlabeled data biases regression towards useful features shown cca regression substantially reduce variance minimal increase bias views contains accurate estimators recent theoretical empirical work shows regression random features closely approximates kernel regression implying accuracy requirement holds random views show xnv consistently outperforms state art algorithm semi supervised learning substantially improving predictive performance reducing variability performance wide variety real world datasets whilst also reducing runtime orders magnitude
online learning nonparametric mixture models via sequential variational approximation reliance computationally expensive algorithms inference limiting use bayesian nonparametric models large scale applications tackle problem propose bayesian learning algorithm mixture models instead following conventional paradigm random initialization plus iterative update take progressive approach starting given prior method recursively transforms approximate posterior sequential variational approximation process new components incorporated fly needed algorithm reliably estimate mixture model pass making particularly suited applications massive data experiments synthetic data real datasets demonstrate remarkable improvement efficiency orders magnitude speed compared state art
projecting ising model parameters fast mixing inference general ising models difficult due high treewidth making tree based algorithms intractable moreover interactions strong gibbs sampling take exponential time converge stationary distribution present algorithm project ising model parameters onto parameter set guaranteed fast mixing several divergences find gibbs sampling using projected parameters accurate original parameters interaction strengths strong limited time available sampling
learning stochastic inverses describe class algorithms amortized inference bayesian networks setting invest computation upfront support rapid online inference wide range queries approach based learning inverse factorization model joint distribution factorization turns observations root nodes algorithms accumulate information estimate local conditional distributions constitute factorization stochastic inverses used invert computation steps leading observation sampling backwards order quickly find likely explanation show estimated inverses converge asymptotically number prior posterior training samples make use inverses convergence describe inverse mcmc algorithm uses stochastic inverses make block proposals metropolis hastings sampler explore efficiency sampler variety parameter regimes bayes nets
active learning probabilistic hypotheses using maximum gibbs error criterion introduce new objective function pool based bayesian active learning probabilistic hypotheses objective function called policy gibbs error expected error rate random classifier drawn prior distribution examples adaptively selected active learning policy exact maximization policy gibbs error hard propose greedy strategy maximizes gibbs error iteration gibbs error instance expected error random classifier selected posterior label distribution instance apply maximum gibbs error criterion active learning scenarios non adaptive adaptive batch active learning scenario prove criterion achieves near maximal policy gibbs error constrained fixed budget practical implementations provide approximations maximum gibbs error criterion bayesian conditional random fields transductive naive bayes experimental results named entity recognition task text classification task show maximum gibbs error criterion effective active learning criterion noisy models
doubt swap high dimensional sparse recovery correlated measurements consider problem accurately estimating high dimensional sparse vector using small number linear measurements contaminated noise well known standard computationally tractable sparse recovery algorithms lasso omp various extensions perform poorly measurement matrix contains highly correlated columns develop simple greedy algorithm called swap iteratively swaps variables desired loss function cannot decreased swap surprisingly effective handling measurement matrices high correlations prove swap easily used wrapper around standard sparse recovery algorithms improved performance theoretically quantify statistical guarantees swap complement analysis numerical results synthetic real data
sensor selection high dimensional gaussian trees nuisances consider sensor selection problem multivariate gaussian distributions emph subset latent variables inferential interest pairs vertices connected unique path graph show exist decompositions nonlocal mutual information local information measures computed efficiently output message passing algorithms integrate decompositions computationally efficient greedy selector computational expense quantification distributed across nodes network experimental results demonstrate comparative efficiency algorithms sensor selection high dimensional distributions additionally derive online computable performance bound based augmentations relevant latent variable set valid augmentation exists applicable emph distribution nuisances
sample complexity subspace learning large number algorithms machine learning principal component analysis pca non linear kernel extensions recent spectral embedding support estimation methods rely estimating linear subspace samples paper introduce general formulation problem derive novel learning error estimates results rely natural assumptions spectral properties covariance operator associated data distribution hold wide class metrics subspaces special cases discuss sharp error estimates reconstruction properties pca spectral support estimation key analysis operator theoretic approach broad applicability spectral learning methods
least informative dimensions present novel non parametric method finding subspace stimulus features contains information response system method generalizes similar approaches problem spike triggered average spike triggered covariance maximally informative dimensions instead maximizing mutual information features responses directly use integral probability metrics kernel hilbert spaces minimize information uninformative features combination informative features responses since estimators metrics access data via kernels easy compute exhibit good theoretical convergence properties method easily generalized populations neurons spike patterns using particular expansion mutual information show informative features must contain information make uninformative features independent rest
estimation optimization parallelism data sparse study stochastic optimization problems emph data sparse sense dual current understanding high dimensional statistical learning optimization highlight difficulties terms increased sample complexity sparse data necessitates potential benefits terms allowing parallelism asynchrony design algorithms concretely derive matching upper lower bounds minimax rate optimization learning sparse data exhibit algorithms achieving rates algorithms adaptive achieve best possible rate data observed also show leveraging sparsity leads still minimax optimal parallel asynchronous algorithms providing experimental evidence complementing theoretical results medium large scale learning tasks
auditing active learning outcome dependent query costs propose learning setting unlabeled data free cost label depends value known advance study binary classification extreme case algorithm pays negative labels motivation applications fraud detection investigating honest transaction avoided possible term setting auditing consider auditing complexity algorithm number negative points labels learn hypothesis low relative error design auditing algorithms thresholds line axis aligned rectangles show algorithms auditing complexity significantly lower active label complexity discuss general approach auditing general hypothesis class describe several interesting directions future work
optimality active learning gaussian random fields common classifier unlabeled nodes undirected graphs uses label propagation labeled nodes equivalent harmonic predictor gaussian random fields grfs active learning grfs commonly used optimality criterion queries nodes reduce regression loss optimality satisfies submodularity property showing greedy reduction produces globally optimal solution however loss characterise true nature loss classification problems thus best choice active learning consider new criterion call optimality queries node minimizes sum elements predictive covariance optimality directly optimizes risk surveying problem determine proportion nodes belonging class paper extend submodularity guarantees optimality optimality using properties specific grfs show grfs satisfy suppressor free condition addition conditional independence inherited markov random fields test optimality real world graphs synthetic real data show outperforms optimality related methods classification
improved generalized upper bounds complexity policy iteration given markov decision process mdp states actions per state study number iterations needed policy iteration algorithms converge optimal gamma discounted optimal policy consider variations howard changes actions states positive advantage simplex changes action state maximal advantage show howard terminates left frac gamma log left frac gamma right right iterations improving factor log result hansen 2013 simplex terminates left frac gamma log left frac gamma right right iterations improving factor log result 2011 structural assumptions mdp consider bounds independent discount factor gamma given measure maximal transient time tau_t maximal time tau_r revisit states recurrent classes policies show simplex terminates tilde left tau_t tau_r right iterations generalizes recent result deterministic mdps post 2012 tau_t tau_r explain similar results seem hard derive howard finally additional restrictive assumption state space partitioned sets respectively states transient recurrent policies show simplex howard terminate tilde tau_t tau_r iterations
nearly optimal algorithms private online learning full information bandit settings provide general technique making online learning algorithms differentially private full information bandit settings technique applies algorithms aim minimize emph convex loss function sum smaller convex loss terms data point modify popular emph mirror descent approach rather variant called emph follow approximate leader technique leads first nonprivate algorithms private online learning bandit setting full information setting algorithms improve regret bounds previous work many cases algorithms settings matching dependence input length emph optimal nonprivate regret bounds logarithmic factors algorithms require logarithmic space update time
sampling gibbs distribution random maximum posteriori perturbations paper describe map inference used sample efficiently gibbs distributions specifically provide means drawing either approximate unbiased samples gibbs distributions introducing low dimensional perturbations solving corresponding map assignments approach also leads new ways derive lower bounds partition functions demonstrate empirically method excels typical high signal high coupling regime setting results ragged energy landscapes challenging alternative approaches sampling lower bounds
marginals models reducibility consider number classical new computational problems regarding marginal distributions inference models specifying full joint distribution prove general efficient reductions number problems demonstrate algorithmic progress inference automatically yields progress pure data problems main technique involves formulating problems linear programs proving dual separation oracle ellipsoid method provided target problem technique independent interest probabilistic inference
pass efficient unsupervised feature selection goal unsupervised feature selection identify small number important features represent data propose new algorithm modification classical pivoted algorithm businger golub requires small number passes data improvements based ideas keeping track multiple features pass skipping calculations shown affect final selection algorithm selects exact features classical pivoted algorithm favorable numerical stability describe experiments real world datasets sometimes show improvements several orders magnitude classical algorithm results appear competitive recently proposed randomized algorithms terms pass efficiency run time hand randomized algorithms produce better features cost small probability failure
efficient algorithm privately releasing smooth queries study differentially private mechanisms answering emph smooth queries databases consisting data points mathbb smooth query specified function whose partial derivatives order bounded develop epsilon differentially private mechanism class smooth queries accuracy left frac right frac epsilon mechanism first outputs summary database obtain answer query user runs public evaluation algorithm contains information database outputting summary runs time frac evaluation algorithm answering query runs time tilde frac frac mechanism based infty approximation transformed smooth functions low degree even trigonometric polynomials small efficiently computable coefficients
streaming variational bayes present sda bayes framework treaming istributed synchronous computation bayesian posterior framework makes streaming updates estimated posterior according user specified approximation primitive function demonstrate usefulness framework variational bayes primitive fitting latent dirichlet allocation model large scale document collections demonstrate advantages algorithm stochastic variational inference svi single pass setting svi designed streaming setting svi apply
bayesian optimization explains human active search many real world problems complicated objective functions optimize functions humans utilize sophisticated sequential decision making strategies many optimization algorithms also developed purpose compare humans terms performance behavior try unravel general underlying algorithm people using searching maximum invisible function subjects click blank screen shown ordinate function clicked abscissa location task find function maximum clicks possible subjects win get close enough maximum location analysis non maths undergraduates optimizing functions different families shows humans outperform well known optimization algorithms bayesian optimization based gaussian processes exploit values tried values obtained far pick next predicts human performance searched locations better follow controlled experiments subjects covering interpolation extrapolation optimization tasks confirm gaussian processes provide general unified theoretical account explain passive active function learning search humans
online learning dynamic parameters social networks paper addresses problem online learning dynamic setting consider social network individual observes private signal underlying state world communicates neighbors time period unlike many existing approaches underlying state dynamic evolves according geometric random walk view scenario optimization problem agents aim learn true state suffering smallest possible loss based decomposition global loss function introduce update mechanisms generates estimate true state establish tight bound rate change underlying state individuals track parameter bounded variance characterize explicit expressions steady state mean square deviation msd estimates truth per individual observe estimators recovers optimal msd underscores impact objective function decomposition learning quality finally provide upper bound regret proposed methods measured average errors estimating parameter finite time
bayesian inference low rank spatiotemporal neural receptive fields receptive field sensory neuron describes neuron integrates sensory stimuli time space typical experiments naturalistic flickering spatiotemporal stimuli rfs high dimensional due large number coefficients needed specify integration profile across time space estimating coefficients small amounts data poses variety challenging statistical computational problems address challenges developing bayesian reduced rank regression methods estimation corresponds modeling sum several space time separable rank filters proves accurate even neurons strongly oriented space time rfs approach substantially reduces number parameters needed specify 100k mere 100s examples consider confers substantial benefits statistical power computational efficiency particular introduce novel prior low rank rfs using restriction matrix normal prior manifold low rank matrices use localized prior row column covariances obtain sparse smooth localized estimates spatial temporal components develop methods inference resulting hierarchical model fully bayesian method using blocked gibbs sampling fast approximate method employs alternating coordinate ascent conditional marginal likelihood develop methods gaussian poisson noise models show low rank estimates substantially outperform full rank estimates accuracy speed using neural data retina
modeling overlapping communities node popularities develop probabilistic approach accurate network modeling using node popularities within framework mixed membership stochastic blockmodel mmsb model integrates basic properties nodes social networks homophily preferential connection popular nodes develop scalable algorithm posterior inference based novel nonconjugate variant stochastic variational inference evaluate link prediction accuracy algorithm real world networks 000 nodes benchmark networks demonstrate algorithm predicts better mmsb using benchmark networks show node popularities essential achieving high accuracy presence skewed degree distribution noisy links characteristics real networks
spectral methods neural characterization using generalized quadratic models describe set fast tractable methods characterizing neural responses high dimensional sensory stimuli using model refer generalized quadratic model gqm gqm consists low rank quadratic form followed point nonlinearity exponential family noise quadratic form characterizes neuron stimulus selectivity terms set linear receptive fields followed quadratic combination rule invertible nonlinearity maps output desired response range special cases gqm include 2nd order volterra model marmarelis marmarelis 1978 koh powers 1985 elliptical linear nonlinear poisson model park pillow 2011 show canonical form gqms spectral decomposition first response weighted moments yields approximate maximum likelihood estimators via quantity called expected log likelihood resulting theory generalizes moment based estimators spike triggered covariance gaussian noise case provides closed form estimators large class non gaussian stimulus distributions show estimators fast provide highly accurate estimates far lower computational cost full maximum likelihood moreover gqm provides natural framework combining multi dimensional stimulus sensitivity spike history dependencies within single model show applications analog spiking data using intracellular recordings membrane potential extracellular recordings retinal spike trains
supervised sparse analysis synthesis operators paper propose new computationally efficient framework learning sparse models formulate unified approach contains particular cases models promoting sparse synthesis analysis type priors mixtures thereof supervised training proposed model formulated bilevel optimization problem operators optimized achieve best possible performance specific task reconstruction classification restricting operators shift invariant approach thought way learning analysis synthesis sparsity promoting convolutional operators leveraging recent ideas fast trainable regressors designed approximate exact sparse codes propose way constructing feed forward neural networks capable approximating learned models fraction computational cost exact solvers shift invariant case leads principled way constructing task specific convolutional networks illustrate proposed models several experiments music analysis image processing applications
relevance topic model unstructured social group activity recognition unstructured social group activity recognition web videos challenging task due semantic gap class labels low level visual features lack labeled training data tackle problem propose relevance topic model jointly learning meaningful mid level representations upon bag words bow video representations classifier sparse weights approach sparse bayesian learning incorporated undirected topic model replicated softmax discover topics relevant video classes suitable prediction rectified linear units utilized increase expressive power topics explain better video data containing complex contents make variational inference tractable proposed model efficient variational algorithm presented model parameter estimation inference experimental results unstructured social activity attribute dataset show model achieves state art performance outperforms supervised topic model terms classification accuracy particularly case small number labeled training videos
embed project discrete sampling universal hashing consider problem sampling probability distribution defined high dimensional discrete set specified instance graphical model propose sampling algorithm called paws based embedding set higher dimensional space randomly projected using universal hash functions lower dimensional subspace explored using combinatorial search methods scheme leverage fast combinatorial optimization tools blackbox unlike mcmc methods samples produced guaranteed within arbitrarily small constant factor true probability distribution demonstrate using state art combinatorial search tools paws efficiently sample ising grids strong interactions software verification instances mcmc variational methods fail cases
graphical transformation belief propagation maximum weight matchings odd sized cycles max product belief propagation popular distributed heuristic finding maximum posteriori map assignment joint probability distribution represented graphical model recently shown converges correct map assignment class loopy gms following common feature linear programming relaxation map problem tight integrality gap unfortunately tightness relaxation general guarantee convergence correctness algorithm failure cases motivates reverse engineering solution namely given tight design good algorithm paper design algorithm maximum weight matching mwm problem general graphs prove algorithm converges correct optimum respective relaxation include inequalities associated non intersecting odd sized cycles tight significant part approach introduction novel graph transformation designed force convergence theoretical result suggests efficient based heuristic mwm problem consists making sequential cutting plane modifications underlying experiments show heuristic performs well traditional cutting plane algorithms using solvers mwm problems
power asymmetry binary hashing approximating binary similarity using hamming distance short binary hashes shown even similarity symmetric shorter accurate hashes using distinct code maps approximating similarity hamming distance distinct binary codes rather hamming distance
hierarchical modular optimization convolutional networks achieves representations similar macaque human ventral stream humans recognize visually presented objects rapidly accurately understand ability seek construct models ventral stream series cortical areas thought subserve object recognition tool assess quality model ventral stream representation dissimilarity matrix rdm uses set visual stimuli measures distances produced either brain fmri voxel responses neural firing rates models features previous work shown known models ventral stream fail capture rdm pattern observed either cortex highest ventral area human ventral stream work construct models ventral stream using novel optimization procedure category level object recognition problems produce rdms resembling macaque human ventral stream model novel optimization procedure develops long standing functional hypothesis ventral visual stream hierarchically arranged series processing stages optimized visual object recognition
computing stationary distribution locally computing stationary distribution large finite countably infinite state space markov chain become central many problems statistical inference network analysis standard methods involve large matrix multiplications power iteration simulations long random walks sample states stationary distribution markov chain monte carlo mcmc however methods computationally costly either involve operations every state scale computation time least linearly size state space paper provide novel algorithm answers whether chosen state stationary probability larger delta estimates stationary probability algorithm uses information local neighborhood state graph induced constant size relative state space provide correctness convergence guarantees depend algorithm parameters mixing properties simulation results show mcs method gives tight estimates
reflection methods user friendly submodular optimization recently become evident submodularity naturally captures widely occurring concepts machine learning signal processing computer vision consequence need efficient optimization procedures submodular functions particular minimization problems general submodular minimization challenging propose new approach exploits existing decomposability submodular functions contrast previous approaches method neither approximate impractical need cumbersome parameter tuning moreover easy implement parallelize key component approach formulation discrete submodular minimization problem continuous best approximation problem solved sequence reflections solution automatically thresholded obtain optimal discrete solution method solves continuous discrete formulations problem therefore applications learning inference reconstruction experiments show benefits new algorithms image segmentation tasks
sparse nonnegative deconvolution compressive calcium imaging algorithms phase transitions propose compressed sensing calcium imaging framework monitoring large neuronal populations image randomized projections spatial calcium concentration timestep instead measuring concentration individual locations develop scalable nonnegative deconvolution methods extracting neuronal spike time series observations also address problem demixing spatial locations neurons using rank penalized matrix factorization methods exploiting sparsity neural spiking demonstrate number measurements needed per timestep significantly smaller total number neurons result potentially enable imaging larger populations considerably faster rates compared traditional raster scanning techniques unlike traditional setups problem involves block diagonal sensing matrix non orthogonal sparse basis spans multiple timesteps study effect distinctive features noiseless setup using recent results relating conic geometry provide tight approximations number measurements needed perfect deconvolution certain classes spiking processes show number displays phase transition similar phenomena observed standard settings however case required measurement rate depends mean sparsity level also details underlying spiking process
overcomplete topic models identifiable uniqueness tensor tucker decompositions structured sparsity overcomplete latent representations popular unsupervised feature learning recent years paper specify overcomplete models identified given observable moments certain order consider probabilistic admixture topic models overcomplete regime number latent topics greatly exceed size observed word vocabulary general overcomplete topic models identifiable establish generic identifiability constraint referred topic persistence sufficient conditions identifiability involve novel set higher order expansion conditions topic word matrix population structure model set higher order expansion conditions allow overcomplete models require existence perfect matching latent topics higher order observed words establish random structured topic models identifiable overcomplete regime identifiability results allow general non degenerate distributions modeling topic proportions thus handle arbitrarily correlated topics framework identifiability results imply uniqueness class tensor decompositions structured sparsity contained class tucker decompositions general candecomp parafac decomposition
blind calibration compressed sensing using message passing algorithms compressed sensing concept allows acquire compressible signals small number measurements attractive hardware implementations therefore correct calibration hardware central issue paper study called blind calibration training signals available perform calibration sparse unknown extend approximate message passing amp algorithm used case blind calibration calibration amp gains sensors elements signals treated unknowns algorithm also applicable settings sensors distort measurements ways multiplication gain unlike previously suggested blind calibration algorithms based convex relaxations study numerically phase diagram blind calibration problem show even cases convex relaxation possible algorithm requires smaller number measurements signals order perform well
scalable inference logistic normal topic models logistic normal topic models effectively discover correlation structures among latent topics however inference remains challenge non conjugacy logistic normal prior multinomial topic mixing proportions existing algorithms either make restricting mean field assumptions scalable large scale applications paper presents partially collapsed gibbs sampling algorithm approaches provably correct distribution exploring ideas data augmentation improve time efficiency present parallel implementation deal large scale applications learn correlation structures thousands topics millions documents extensive empirical results demonstrate promise
gang bandits multi armed bandit problems receiving great deal attention adequately formalize exploration exploitation trade offs arising several industrially relevant applications online advertisement generally recommendation systems many cases however applications strong social component whose integration bandit algorithm could lead dramatic performance increase instance want serve content group users taking advantage underlying network social relationships among paper introduce novel algorithmic approaches solution networked bandit problems specifically design analyze global strategy allocates bandit algorithm network node user allows share signals contexts payoffs neghboring nodes derive scalable variants strategy based different ways clustering graph nodes experimentally compare algorithm variants state art methods contextual bandits use relational information experiments carried synthetic real world datasets show marked increase prediction performance obtained exploiting network structure
online learning markov decision processes adversarially chosen transition probability distributions study problem online learning markov decision processes mdps transition distributions loss functions chosen adversary present algorithm mixing assumption achieves sqrt log log regret respect comparison set policies regret independent size state action spaces expectations sample paths computed efficiently comparison set polynomial size algorithm efficient also consider episodic adversarial online shortest path problem episode adversary choose weighted directed acyclic graph identified start finish node goal learning algorithm choose path minimizes loss traversing start finish node end episode loss function given weights edges revealed learning algorithm goal minimize regret respect fixed policy selecting paths problem special case online mdp problem randomly chosen graphs adversarial losses problem efficiently solved show also efficiently solved adversarial graphs randomly chosen losses graphs losses adversarially chosen present efficient algorithm whose regret scales linearly number distinct graphs finally show designing efficient algorithms adversarial online shortest path problem hence adversarial mdp problem hard learning parity noise notoriously difficult problem used design efficient cryptographic schemes
approximate efficient solver rounding many problems machine learning solved rounding solution appropriate linear program propose scheme based quadratic program relaxation allows use parallel stochastic coordinate descent approximately solve large linear programs efficiently software order magnitude faster cplex commercial linear programming solver yields similar solution quality results include novel perturbation analysis quadratic penalty formulation linear programming convergence result use derive running time quality guarantees
robust learning low dimensional dynamics large neural ensembles recordings large populations neurons make possible search hypothesized low dimensional dynamics finding dynamics requires models take account biophysical constraints fit efficiently robustly present approach dimensionality reduction neural data convex make strong assumptions dynamics require averaging many trials extensible complex statistical models combine local global influences results combined spectral methods learn dynamical systems models basic method seen extension pca exponential family using nuclear norm minimization evaluate effectiveness method using exact decomposition bregman divergence analogous variance explained pca show model data parameters latent linear dynamical systems recovered even dynamics stationary still recover true latent subspace also demonstrate extension nuclear norm minimization separate sparse local connections global latent dynamics finally demonstrate improved prediction real neural data monkey motor cortex compared fitting linear dynamical models without nuclear norm smoothing
speedup matrix completion side information application multi label learning standard matrix completion theory required least observed entries perfectly recover low rank matrix size times leading large number observations large many real tasks side information addition observed entries often available work develop novel theory matrix completion explicitly explore side information reduce requirement number observed entries show appropriate conditions assistance side information matrices number observed entries needed perfect recovery matrix dramatically reduced demonstrate effectiveness proposed approach matrix completion transductive incomplete multi label learning
buy bulk active learning many practical applications active learning cost effective request labels large batches rather time cost labeling large batch examples often sublinear number examples batch work study label complexity active learning algorithms request labels given number batches well tradeoff total number queries number rounds allowed additionally study total cost sufficient learning abstract notion cost requesting labels given number examples particular find sublinear cost functions often desirable request labels large batches buying bulk although increase total number labels requested reduces total cost required learning
optimal neural population codes high dimensional stimulus variables neural population process sensory information optimal coding theories assume neural tuning curves adapted prior distribution stimulus variable previous work discussed optimal solutions dimensional stimulus variables expand ideas present new solutions define optimal tuning curves high dimensional stimulus variables consider solutions minimal case number neurons population equal number stimulus dimensions diffeomorphic case dimensional stimulus variables analytically derive optimal solutions different optimal criteria minimal reconstruction error maximal mutual information higher dimensional case learning rule improve population code provided
better approximation faster algorithm using proximal average common practice approximate complicated functions friendly ones large scale machine learning applications nonsmooth losses regularizers entail great computational challenges usually approximated smooth functions examine powerful methodology point nonsmooth approximation simply pretends linearity proximal map new approximation justified using recent convex analysis tool proximal average yields novel proximal gradient algorithm strictly better based smoothing without incurring extra overhead numerical experiments conducted important applications overlapping group lasso graph guided fused lasso corroborate theoretical claims
convex relaxations permutation problems seriation seeks reconstruct linear order variables using unsorted similarity information direct applications archeology shotgun gene sequencing example prove equivalence seriation combinatorial sum problem quadratic minimization problem permutations class similarity matrices seriation problem solved exactly spectral algorithm noiseless case produce convex relaxation sum problem improve robustness solutions noisy setting relaxation also allows impose additional structural constraints solution solve semi supervised seriation problems present numerical experiments archeological data markov chains gene sequences
similarity component analysis measuring similarity crucial many learning tasks also richer broader notion metric learning algorithms model example similarity arise process aggregating decisions multiple latent components latent component compares data way focusing different subset features paper propose similarity component analysis sca probabilistic graphical model discovers latent components data sca latent component generates local similarity value computed metric independently components final similarity measure obtained combining local similarity values noisy gate derive based algorithm fitting model parameters similarity annotated data pairwise comparisons validate sca model synthetic datasets sca discovers ground truth latent components also apply sca multiway classification task link prediction task tasks sca attains significantly better prediction accuracies competing methods moreover show sca instrumental exploratory analysis data gain insights data examining patterns hidden latent components local similarity values
estimating lasso risk noise level study fundamental problems variance risk estimation high dimensional statistical modeling particular consider problem learning coefficient vector theta_0 noisy linear observation theta_0 popular estimation procedure solving ell_1 penalized least squares objective known lasso basis pursuit denoising bpdn context develop new estimators ell_2 estimation risk hat theta theta_0 variance noise used select regularization parameter optimally approach combines stein unbiased risk estimate stein recent results bayati montanari analysis approximate message passing risk lasso establish high dimensional consistency estimators sequences matrices increasing dimensions independent gaussian entries establish validity broader class gaussian designs conditional validity certain conjecture statistical physics approach first provides asymptotically consistent risk estimator addition demonstrate simulation variance estimation outperforms several existing methods literature
learning using language via recursive pragmatic reasoning agents language users remarkably good making inferences speakers intentions context children learning native language also display substantial skill acquiring meanings unknown words cases deeply related language users invent new terms conversation language learners learn literal meanings words based pragmatic inferences words used pragmatic inference word learning independently characterized probabilistic terms current work unifies describe model language learners assume jointly approximate shared external lexicon reason recursively goals others using lexicon model captures phenomena word learning pragmatic inference additionally leads insights emergence communicative systems conversation mechanisms pragmatic inferences become incorporated word meanings
online learning episodic markovian decision processes relative entropy policy search study problem online learning finite episodic markov decision processes loss function allowed change episodes natural performance measure learning problem regret defined difference total loss best stationary policy total loss suffered learner assume learner given access finite action space state space layered structure layers state transitions possible consecutive layers describe variant recently proposed relative entropy policy search algorithm show regret episodes sqrt log bandit setting sqrt log full information setting guarantees largely improve previously known results much milder assumptions cannot significantly improved general assumptions
manifold based similarity adaptation label propagation label propagation state art methods semi supervised learning estimates labels propagating label information graph label propagation assumes data points nodes connected graph similar labels consequently label estimation heavily depends edge weights graph represent similarity node pair propose method graph capture manifold structure input features using edge weights parameterized similarity function approach edge weights represent similarity local reconstruction weight simultaneously reasonable label propagation justification provide analytical considerations including interpretation cross validation propagation model feature space error analysis based low dimensional manifold model experimental results demonstrated effectiveness approach synthetic real datasets
memoized online variational inference dirichlet process mixture models variational inference algorithms provide effective framework large scale training bayesian nonparametric models stochastic online approaches promising sensitive chosen learning rate often converge poor local optima present new algorithm memoized online variational inference scales large yet finite datasets avoiding complexities stochastic gradient algorithm maintains finite dimensional sufficient statistics batches full dataset requiring additional memory still scaling millions examples exploiting nested families variational bounds infinite nonparametric models develop principled birth merge moves allowing non local optimization births adaptively add components model escape local optima merges remove redundancy improve speed using dirichlet process mixture models image clustering denoising demonstrate major improvements robustness accuracy
approximate gaussian process inference drift function stochastic differential equations introduce nonparametric approach estimating drift functions systems stochastic differential equations incomplete observations state vector using gaussian process prior drift function state vector develop approximate algorithm deal unobserved latent dynamics observations posterior states approximated piecewise linearized process map estimation drift facilitated sparse gaussian process regression
learning prune metric non metric spaces best knowledge work first successful attempt employ tree learned pruning algorithm non metric spaces focus approximate nearest neighbor retrieval methods experiment simple yet effective learning prune approaches density estimation sampling stretching triangle inequality methods evaluated using data sets metric euclidean non metric divergence distance functions tree learned pruner compared recently proposed state art approaches bbtree multi probe locality sensitive hashing lsh permutation methods method competitive state art methods outperformed cases wide margin experiments also showed bbtree exact search method typically slower exhaustive searching divergence evaluated efficiently precomputing logarithms index time conditions spaces tree applicable discussed
probabilistic low rank matrix completion adaptive spectral regularization algorithms propose novel class algorithms low rank matrix completion approach builds novel penalty functions singular values low rank matrix exploiting mixture model representation penalty show suitably chosen set latent variables enables derive expectation maximization algorithm obtain maximum posteriori estimate completed low rank matrix resulting algorithm iterative soft thresholded algorithm iteratively adapts shrinkage coefficients associated singular values algorithm simple implement scale large matrices provide numerical comparisons approach recent alternatives showing interest proposed approach low rank matrix completion
polar operators structured sparse estimation structured sparse estimation become important technique many areas data analysis unfortunately estimators normally create computational difficulties entail sophisticated algorithms first contribution uncover rich class structured sparse regularizers whose polar operator evaluated efficiently operator simple conditional gradient method developed combined smoothing local optimization significantly reduces training time state art also demonstrate new reduction polar proximal maps enables efficient latent fused lasso
generalized method moments rank aggregation paper propose class efficient generalized method moments gmm algorithms computing parameters plackett luce model data consists full rankings alternatives technique based breaking full rankings pairwise comparisons computing parameters satisfy set generalized moment conditions identify conditions output gmm unique identify general class consistent inconsistent breakings show theory experiments algorithms run significantly faster classical minorize maximization algorithm achieving competitive statistical efficiency
mixed optimization smooth functions well known optimal convergence rate stochastic optimization smooth functions sqrt stochastic optimization lipschitz continuous convex functions contrast optimizing smooth functions using full gradients yields convergence rate work consider new setup optimizing smooth functions termed mixed optimization allows access stochastic oracle full gradient oracle goal significantly improve convergence rate stochastic optimization smooth functions additional small number accesses full gradient oracle show calls full gradient oracle calls stochastic oracle proposed mixed optimization algorithm able achieve optimization error
stability based validation procedure differentially private machine learning differential privacy cryptographically motivated definition privacy gained considerable attention algorithms machine learning data mining communities explosion work differentially private machine learning algorithms major barrier achieving end end differential privacy practical machine learning applications lack effective procedure differentially private parameter tuning determining parameter value bin size histogram regularization parameter suitable particular application paper introduce generic validation procedure differentially private machine learning algorithms apply certain stability condition holds training algorithm validation performance metric training data size privacy budget used training procedure independent number parameter values searched apply generic procedure fundamental tasks statistics machine learning training regularized linear classifier building histogram density estimator result end end differentially private solutions problems
accelerated mini batch stochastic dual coordinate ascent stochastic dual coordinate ascent sdca effective technique solving regularized loss minimization problems machine learning paper considers extension sdca mini batch setting often used practice main contribution introduce accelerated mini batch version sdca prove fast convergence rate method discuss implementation method parallel computing system compare results vanilla stochastic dual coordinate ascent accelerated deterministic gradient descent method nesterov 2007
learning chordal markov networks constraint satisfaction investigate problem learning structure markov network data shown structure networks described terms constraints enables use existing solver technology optimization capabilities compute optimal networks starting initial scores computed data achieve efficient encodings develop novel characterization markov network structure using balancing condition separators cliques forming network resulting translations propositional satisfiability extensions maximum satisfiability satisfiability modulo theories answer set programming enable prove optimality networks previously found stochastic search
solving multi way matching problem permutation synchronization problem matching different sets objects arises variety contexts including finding correspondence feature points across multiple images computer vision present usually solved matching sets pairwise series contrast propose new method permutation synchronization finds matchings jointly shot via relaxation eigenvector decomposition resulting algorithm computationally efficient demonstrate theoretical arguments well experimental results much stable noise previous methods
cluster trees manifolds investigate problem estimating cluster tree density supported near smooth dimensional manifold isometrically embedded mathbb study nearest neighbor based algorithm recently proposed chaudhuri dasgupta mild assumptions obtain rates convergence depend ambient dimension also provide sample complexity lower bound natural class clustering algorithms use dimensional neighborhoods
minimax theory high dimensional gaussian mixtures sparse mean separation several papers investigated computationally statistically efficient methods learning gaussian mixtures precise minimax bounds statistical performance well fundamental limits high dimensional settings well understood paper provide precise information theoretic bounds clustering accuracy sample complexity learning mixture isotropic gaussians high dimensions small mean separation sparse subset relevant dimensions determine mean separation sample complexity depends number relevant dimensions mean separation achieved simple computationally efficient procedure results provide first step theoretical basis recent methods combine feature selection clustering
latent structured active learning paper present active learning algorithms context structured prediction problems reduce amount labeling necessary learn good models algorithms label subsets output end query examples using entropies local marginals good surrogate uncertainty demonstrate effectiveness approach task layout prediction single images show good models learned labeling handful random variables particular performance using full training set obtained labeling random variables
low rank matrix tensor completion via adaptive sampling study low rank matrix tensor completion propose novel algorithms employ adaptive sampling schemes obtain strong performance guarantees problems algorithms exploit adaptivity identify entries highly informative identifying column space matrix tensor consequently results hold even row space highly coherent contrast previous analysis matrix completion absence noise show exactly recover times matrix rank using log observations better best known bound random sampling also show recover order tensor using log noisy recovery show consistently estimate low rank matrix corrupted noise using textrm polylog observations complement study simulations verify theoretical guarantees demonstrate scalability algorithms
scalable approach probabilistic latent space inference large scale networks propose scalable approach making inference latent spaces large networks succinct representation networks bag triangular motifs parsimonious statistical model efficient stochastic variational inference algorithm able analyze real networks million vertices hundreds latent roles single machine matter hours setting reach many existing methods compared state art probabilistic approaches method several orders magnitude faster competitive improved accuracy latent space recovery link prediction
learning prices repeated auctions strategic buyers inspired real time exchanges online display advertising consider problem inferring buyer value distribution good buyer repeatedly interacting seller posted price mechanism model buyer strategic agent whose goal maximize long term surplus interested mechanisms maximize seller long term revenue present seller algorithms regret buyer discounts future surplus buyer prefers showing advertisements users sooner rather later also give lower bound regret increases buyer discounting weakens shows particular seller algorithm suffer linear regret discounting
convex tensor decomposition via structured schatten norm regularization propose new class structured schatten norms tensors includes recently proposed norms overlapped latent convex optimization based tensor decomposition based properties structured schatten norms mathematically analyze performance latent approach tensor decomposition empirically found perform better overlapped approach settings show theoretically indeed case particular unknown true tensor low rank specific mode approach performs well knowing mode smallest rank along way show novel duality result structures schatten norms also interesting general context structured sparsity confirm numerical simulations theory precisely predict scaling behaviour mean squared error
aggregating optimistic planning trees solving markov decision processes paper addresses problem online planning markov decision processes using generative model propose new algorithm based construction forest single successor state planning trees every explored state action tree contains exactly successor state drawn generative model trees built using planning algorithm follows optimism face uncertainty principle assuming favorable outcome absence information decision making step algorithm individual trees combined discuss approach prove proposed algorithm consistent empirically show performs better related algorithm additionally assumes knowledge transition distributions
edml learning parameters directed undirected graphical models edml recently proposed algorithm learning parameters bayesian networks originally derived terms approximate inference meta network underlies bayesian approach parameter estimation initial derivation helped discover edml first place provided concrete context identifying properties contrast formal setting somewhat tedious number concepts drew paper propose greatly simplified perspective edml casts general approach continuous optimization new perspective several advantages first makes immediate results non trivial prove initially second facilitates design edml algorithms new graphical models leading new algorithm learning parameters markov networks derive algorithm paper show empirically sometimes learn better estimates complete data several times faster commonly used optimization methods conjugate gradient bfgs
linear convergence condition number independent access full gradients smooth strongly convex optimization optimal iteration complexity gradient based algorithm sqrt kappa log epsilon kappa conditional number case optimization problem ill conditioned need evaluate larger number full gradients could computationally expensive paper propose reduce number full gradient required allowing algorithm access stochastic gradients objective function end present novel algorithm named epoch mixed gradient descent emgd able utilize kinds gradients distinctive step emgd mixed gradient descent use combination gradient stochastic gradient update intermediate solutions performing fixed number mixed gradient descents able improve sub optimality solution constant factor thus achieve linear convergence rate theoretical analysis shows emgd able find epsilon optimal solution computing log epsilon full gradients kappa log epsilon stochastic gradients
accelerating stochastic gradient descent using predictive variance reduction stochastic gradient descent popular large scale optimization slow convergence asymptotically due inherent variance remedy problem introduce explicit variance reduction method stochastic gradient descent call stochastic variance reduced gradient svrg smooth strongly convex functions prove method enjoys fast convergence rate stochastic dual coordinate ascent sdca stochastic average gradient sag however analysis significantly simpler intuitive moreover unlike sdca sag method require storage gradients thus easily applicable complex problems structured prediction problems neural network learning
multiscale dictionary learning estimating conditional distributions nonparametric estimation conditional distribution response given high dimensional features challenging problem important allow mean also variance shape response density change flexibly features massive dimensional propose multiscale dictionary learning model expresses conditional response density convex combination dictionary densities densities used weights dependent path tree decomposition feature space fast graph partitioning algorithm applied obtain tree decomposition bayesian methods used adaptively prune average different sub trees soft probabilistic manner algorithm scales efficiently approximately million features state art predictive performance demonstrated toy examples neuroscience applications including million features
data driven distributionally robust polynomial optimization consider robust optimization polynomial optimization problems uncertainty set set candidate probability density functions set ball around density function estimated data samples data driven random polynomial optimization problems inherently hard due nonconvex objectives constraints however show employing polynomial histogram density estimates introduce robustness respect distributional uncertainty sets without making problem harder show solution distributionally robust problem limit sequence tractable semidefinite programming relaxations also give finite sample consistency guarantees data driven uncertainty sets finally apply model solution method water network problem
contrastive learning using spectral methods many natural settings analysis goal characterize single data set isolation rather understand difference set observations another example given background corpus news articles together writings particular author want topic model explains word patterns themes specific author another example comes genomics biological signals collected different regions genome wants model captures differential statistics observed regions paper formalizes notion contrastive learning mixture models develops spectral algorithms inferring mixture components specific foreground data set contrasted background data set method builds recent moment based estimators tensor decompositions latent variable models intuitive feature using background data statistics appropriately modify moments estimated foreground data key advantage method background data need coarsely modeled important background complex noisy interest method demonstrated applications contrastive topic modeling genomic sequence analysis
model selection high dimensional regression generalized irrepresentability condition high dimensional regression model response variable linearly related covariates sample size smaller assume small subset covariates active corresponding coefficients non consider model selection problem identifying active covariates popular approach estimate regression coefficients lasso ell_1 regularized least squares known correctly identify active set irrelevant covariates roughly orthogonal relevant ones quantified called irrepresentability condition paper study gauss lasso selector simple stage method first solves lasso performs ordinary least squares restricted lasso active set formulate generalized irrepresentability condition gic assumption substantially weaker irrepresentability prove gic gauss lasso correctly recovers active set
linear convergence proximal gradient method trace norm regularization motivated various applications machine learning problem minimizing convex smooth loss function trace norm regularization received much attention lately currently popular method solving problem proximal gradient method pgm known sublinear rate convergence paper show large class loss functions convergence rate pgm fact linear result established without strong convexity assumption loss function key ingredient proof new lipschitzian error bound aforementioned trace norm regularized problem independent interest
generalized random utility models multiple types propose model demand estimation multi agent differentiated product settings present estimation algorithm uses reversible jump mcmc techniques classify agents types model extends popular setup berry levinsohn pakes 1995 allow data driven classification agents types using agent level data focus applications involving data agents ranking alternatives present theoretical conditions establish identifiability model uni modality likelihood posterior results real simulated data provide support scalability approach
message passing algorithm multi agent trajectory planning describe novel approach computing collision free emph global trajectories agents specified initial final configurations based improved version alternating direction method multipliers admm algorithm compared existing methods approach naturally parallelizable allows incorporating different cost functionals minor adjustments apply method classical challenging instances observe computational requirements scale well several cost functionals also show specialization algorithm used local motion planning solving problem joint optimization velocity space
curvature optimal algorithms learning minimizing submodular functions investigate related important problems connected machine learning namely approximating submodular function everywhere learning submodular function pac like setting constrained minimization submodular functions problems provide improved bounds depend curvature submodular function improve previously known best results problems function curved property true many real world submodular functions former problems obtain bounds generic black box transformation potentially work algorithm case submodular minimization propose framework algorithms depend choosing appropriate surrogate submodular function cases provide almost matching lower bounds improved curvature dependent bounds shown monotone submodular maximization existence similar improved bounds aforementioned problems open resolve question paper showing notion curvature provides improved results empirical experiments add support claims
convex layer modeling latent variable prediction models multi layer networks impose auxiliary latent variables inputs outputs allow automatic inference implicit features useful prediction unfortunately models difficult train inference latent variables must performed concurrently parameter optimization creating highly non convex problem instead proposing another local training method develop convex relaxation hidden layer conditional models admits global training approach extends current convex modeling approaches handle nested nonlinearities separated non trivial adaptive latent layer resulting methods able acquire layer models cannot represented single layer model features improving training quality local heuristics
reasoning neural tensor networks knowledge base completion common problem knowledge representation related fields reasoning large joint knowledge graph represented triples relation entities goal paper develop powerful neural network model suitable inference relationships previous models suffer weak interaction entities simple linear projection vector space address problems introducing neural tensor network ntn model allow entities relations interact multiplicatively additionally observe knowledge base models improved representing entity average vectors words entity name giving additional dimension similarity entities share statistical strength assess model considering problem predicting additional true relations entities given partial knowledge base model outperforms previous models classify unseen relationships wordnet freebase accuracy respectively
stochastic ratio matching rbms sparse high dimensional inputs sparse high dimensional data vectors common many application domains large number rarely non features devised unfortunately creates computational bottleneck unsupervised feature learning algorithms based auto encoders rbms involve reconstruction step whole input vector predicted current feature values algorithm recently developed successfully handle case auto encoders based importance sampling scheme stochastically selecting input elements actually reconstruct training particular example generalize idea rbms propose stochastic ratio matching algorithm inherits computational advantages unbiasedness importance sampling scheme show stochastic ratio matching good estimator allowing approach beat state art bag word text classification benchmarks newsgroups rcv1 keeping computational cost linear number non zeros
robust sparse principal component regression high dimensional elliptical model paper focus principal component regression application high dimension non gaussian data major contributions folds first low dimensions double asymptotic framework dimension sample size increase borrowing strength recent development minimax optimal principal component estimation first time sharply characterize potential advantage classical principal component regression least square estimation gaussian model secondly propose analyze new robust sparse principal component regression high dimensional elliptically distributed data elliptical distribution semiparametric generalization gaussian including many well known distributions multivariate gaussian rank deficient gaussian cauchy logistic allows random vector heavy tailed tail dependence extra flexibilities make suitable modeling finance biomedical imaging data elliptical model prove method estimate regression coefficients optimal parametric rate therefore good alternative gaussian based methods experiments synthetic real world data conducted illustrate empirical usefulness proposed method
shot learning cross modal transfer work introduces model recognize objects images even training data available object class necessary knowledge unseen categories comes unsupervised text corpora unlike previous shot learning models differentiate unseen classes model operate mixture objects simultaneously obtaining state art performance classes thousands training images reasonable performance unseen classes achieved seeing distributions words texts semantic space understanding objects look like deep learning model require manually defined semantic visual features either words images images mapped close semantic word vectors corresponding classes resulting image embeddings used distinguish whether image seen unseen class separate recognition model employed type demonstrate strategies first gives high accuracy unseen classes second conservative prediction novelty keeps seen classes accuracy high
neural representation action sequences far simple snippet matching model take macaque superior temporal sulcus sts brain area receives integrates inputs ventral dorsal visual processing streams thought specialize form motion processing respectively processing articulated actions prior work shown even small population sts neurons contains sufficient information decoding actor invariant action action invariant actor well specific conjunction actor action paper addresses questions first invariance properties individual neural representations rather population representation sts second neural encoding mechanisms produce individual neural representations streams pixel images find baseline model simply computes linear weighted sum ventral dorsal responses short action snippets produces surprisingly good fits neural data interestingly even using inputs single stream actor invariance action invariance produced simply different linear weights
dropout training adaptive regularization dropout feature noising schemes control overfitting artificially corrupting training data generalized linear models dropout performs form adaptive regularization using viewpoint show dropout regularizer first order equivalent lii regularizer applied scaling features estimate inverse diagonal fisher information matrix also establish connection adagrad online learner find close relative adagrad operates repeatedly solving linear dropout regularized problems casting dropout regularization develop natural semi supervised algorithm uses unlabeled data create better adaptive regularizer apply idea document classification tasks show consistently boosts performance dropout training improving state art results imdb reviews dataset
learning stochastic feedforward neural networks multilayer perceptrons mlps neural networks popular models used nonlinear regression classification tasks regressors mlps model conditional distribution predictor variables given input variables however predictive distribution assumed unimodal gaussian tasks structured prediction problems conditional distribution multimodal forming many mappings using stochastic hidden variables rather deterministic ones sigmoid belief nets sbns induce rich multimodal distribution output space however previously proposed learning algorithms sbns slow work well real valued data paper propose stochastic feedforward network hidden layers emph deterministic stochastic variables new generalized training procedure using importance sampling allows efficiently learn complicated conditional distributions demonstrate superiority model conditional restricted boltzmann machines mixture density networks synthetic datasets modeling facial expressions moreover show latent features model improves classification provide additional qualitative results color images
compete compute local competition among neighboring neurons common biological neural networks nns apply concept gradient based backprop trained artificial multilayer nns nns competing linear units tend outperform non competing nonlinear units avoid catastrophic forgetting training sets change time
restricting exchangeable nonparametric distributions distributions exchangeable matrices infinitely many columns useful constructing nonparametric latent variable models however distribution implied models number features exhibited data point poorly suited many modeling tasks paper propose class exchangeable nonparametric priors obtained restricting domain existing models models allow specify distribution number features per data point achieve better performance data sets number features well modeled original distribution
multi prediction deep boltzmann machines introduce multi prediction deep boltzmann machine dbm dbm seen single probabilistic model trained maximize variational approximation generalized pseudolikelihood family recurrent nets share parameters approximately solve different inference problems prior methods training dbms either perform well classification tasks require initial learning pass trains dbm greedily layer time dbm require greedy layerwise pretraining outperforms standard dbm classification classification missing inputs mean field prediction tasks
variational planning graph based mdps markov decision processes mdps extremely useful modeling solving sequential decision making problems graph based mdps provide compact representation mdps large numbers random variables however complexity exactly solving graph based mdp usually grows exponentially number variables limits application present new variational framework describe solve planning problem mdps derive exact approximate planning algorithms particular exploiting graph structure graph based mdps propose factored variational value iteration algorithm value function first approximated multiplication local scope value functions solved minimizing kullback leibler divergence divergence optimized using belief propagation algorithm complexity exponential cluster size graph experimental comparison different models shows algorithm outperforms existing approximation algorithms finding good policies
phase retrieval using alternating minimization phase retrieval problems involve solving linear equations missing sign phase complex numbers information last decades popular generic empirical approach many variants problem alternating minimization alternating estimating missing phase information candidate solution paper show simple alternating minimization algorithm geometrically converges solution problem finding vector denotes vector element wise magnitudes assumption gaussian empirically algorithm performs similar recently proposed convex techniques variant based lifting convex matrix problem sample complexity robustness noise however algorithm much efficient scale large problems analytically show geometric convergence solution sample complexity log factors obvious lower bounds also establish close optimal scaling case unknown vector sparse work represents known proof alternating minimization variant phase retrieval problems non convex setting
learning pass expectation propagation messages expectation propagation popular approximate posterior inference algorithm often provides fast accurate alternative sampling based methods however framework theory allows complex non gaussian factors still significant practical barrier using within requires implementation message update operators difficult require hand crafted approximations work study question whether possible automatically derive fast accurate updates learning discriminative model neural network random forest map message inputs message outputs address practical concerns arise process provide empirical analysis several challenging diverse factors indicating space factors approach appears promising
discriminative transfer learning tree based priors paper proposes way improving classification performance classes training examples key idea discover classes similar transfer knowledge among method organizes classes tree hierarchy tree structure used impose generative prior classification parameters show priors combined discriminative models deep neural networks method benefits power discriminative training deep neural networks time using tree based generative priors classification parameters also propose algorithm learning underlying tree structure gives model flexibility tune tree tree pertinent task solved show model transfer knowledge across related classes using fixed semantic trees moreover learn new meaningful trees usually leading improved performance method achieves state art classification results cifar 100 image data set mir flickr multimodal data set
non uniform camera shake removal using spatially adaptive sparse penalty typical blur camera shake often deviates standard uniform convolutional assumption part problematic rotations create greater blurring away unknown center point consequently successful blind deconvolution removing shake artifacts requires estimation spatially varying non uniform blur operator using ideas bayesian inference convex analysis paper derives non uniform blind deblurring algorithm several desirable yet previously unexplored attributes underlying objective function includes spatially adaptive penalty couples latent sharp image non uniform blur operator noise level together coupling allows penalty automatically adjust shape based estimated degree local blur image structure regions large blur prominent edges discounted remaining regions modest blur revealing edges therefore dominate overall estimation process without explicitly incorporating structure selection heuristics algorithm implemented using optimization strategy virtually parameter free simpler existing methods detailed theoretical analysis empirical validation real images serve validate proposed method
complexity approximation binary evidence lifted inference lifted inference algorithms exploit symmetries probabilistic models speed inference show impressive performance calculating unconditional probabilities relational models often resort non lifted inference computing conditional probabilities reason conditioning evidence breaks many model symmetries preempts standard lifting techniques recent theoretical results show example conditioning evidence corresponds binary relations hard suggesting lifting expected worst case paper balance grim result identifying boolean rank evidence key parameter characterizing complexity conditioning lifted inference particular show conditioning binary evidence bounded boolean rank efficient opens possibility approximating evidence low rank boolean matrix factorization investigate theoretically empirically
discovering hidden variables noisy networks using quartet tests give polynomial time algorithm provably learning structure parameters bipartite noisy bayesian networks binary variables top layer completely hidden unsupervised learning models form discrete factor analysis enabling discovery hidden variables causal relationships observed data obtain efficient learning algorithm family bayesian networks call quartet learnable meaning every latent variable children parents common show existence quartet allows uniquely identify latent variable learn parameters involving latent variable underlying algorithm new techniques structure learning quartet test determine whether set binary variables singly coupled conditional mutual information test use learn parameters also show subtract already learned latent variables model create new singly coupled quartets substantially expands class structures learn finally give proof polynomial sample complexity learning algorithm experimentally compare variational
low rank matrix reconstruction clustering via approximate message passing study problem reconstructing low rank matrices noisy observations formulate problem bayesian framework allows exploit structural properties matrices addition low rankedness sparsity propose efficient approximate message passing algorithm derived belief propagation algorithm perform bayesian inference matrix reconstruction also successfully applied proposed algorithm clustering problem formulating problem clustering low rank matrix reconstruction problem additional structural property numerical experiments show proposed algorithm outperforms lloyd means algorithm
distributed submodular maximization identifying representative elements massive data many large scale machine learning problems clustering non parametric learning kernel machines etc require selecting massive data set manageable representative subset problems often reduced maximizing submodular set function subject cardinality constraints classical approaches require centralized access full data set truly large scale problems rendering data centrally often impractical paper consider problem submodular function maximization distributed fashion develop simple stage protocol greedi easily implemented using mapreduce style computations theoretically analyze approach show certain natural conditions performance close impractical centralized approach achieved extensive experiments demonstrate effectiveness approach several applications including sparse gaussian process inference tens millions examples using hadoop
stochastic optimization pca capped msg study pca stochastic optimization problem propose novel stochastic approximation algorithm refer matrix stochastic gradient msg well practical variant capped msg study method theoretically empirically
machine teaching bayesian learners exponential family teacher knows learning goal wants design good training data machine learner propose optimal teaching framework aimed learners employ bayesian models framework expressed optimization problem teaching examples balance future loss learner effort teacher optimization problem general hard case learner employs conjugate exponential family models present approximate algorithm finding optimal teaching set algorithm optimizes aggregate sufficient statistics unpacks actual teaching examples give several examples illustrate framework
noise enhanced associative memories recent advances associative memory design structured pattern sets graph based inference algorithms allowed reliable learning recall exponential number patterns although designs correct external errors recall assume neurons compute noiselessly contrast highly variable neurons hippocampus olfactory cortex consider associative memories noisy internal computations analytically characterize performance long internal noise level specified threshold error probability recall phase made exceedingly small surprisingly show internal noise actually improves performance recall phase computational experiments lend additional support theoretical analysis work suggests functional benefit noisy neurons biological neuronal networks
recurrent networks coupled winner take oscillators solving constraint satisfaction problems present recurrent neuronal network modeled continuous time dynamical system solve constraint satisfaction problems discrete variables represented coupled winner take wta networks values encoded localized patterns oscillations learned recurrent weights networks constraints variables encoded network connectivity although sources noise network escape local optima search solutions satisfy constraints modifying effective network connectivity oscillations solution satisfies constraints network state changes pseudo random manner trajectory approximates sampling procedure selects variable assignment probability increases fraction constraints satisfied assignment external evidence input network force variables specific values new inputs applied network evaluates entire set variables search states satisfy maximum number constraints consistent external input results demonstrate proposed network architecture perform deterministic search optimal solution problems non convex cost functions network inspired canonical microcircuit models cortex suggests possible dynamical mechanisms solve constraint satisfaction problems present biological networks implemented neuromorphic electronic circuits
learning multi level sparse representations bilinear approximation matrix powerful paradigm unsupervised learning applications however natural hierarchy concepts ought reflected unsupervised analysis example neurosciences image sequence considered semantic concepts pixel rightarrow neuron rightarrow assembly find counterpart unsupervised analysis driven concrete problem propose decomposition matrix observations product sparse matrices rank decreasing lower higher levels contrast prior work allow hierarchical heterarchical relations lower level higher level concepts addition learn nature relations rather imposing finally describe optimization scheme allows optimize decomposition levels jointly rather greedy level level fashion proposed bilevel shmf sparse heterarchical matrix factorization first formalism allows simultaneously interpret calcium imaging sequence terms constituent neurons membership assemblies time courses neurons assemblies experiments show proposed model fully recovers structure difficult synthetic data designed imitate experimental data importantly bilevel shmf yields plausible interpretations real world calcium imaging data
scalable influence estimation continuous time diffusion networks piece information released media site spread month million web pages influence estimation problem challenging since time sensitive nature problem issue scalability need addressed simultaneously paper propose randomized algorithm influence estimation continuous time diffusion networks algorithm estimate influence every node network vcal nodes ecal edges accuracy epsilon using epsilon randomizations logarithmic factors ecal vcal computations used subroutine greedy influence maximization algorithm proposed method guaranteed find set nodes influence least operatorname opt epsilon operatorname opt optimal value experiments synthetic real world data show proposed method easily scale networks millions nodes significantly improves previous state arts terms accuracy estimated influence quality selected nodes maximizing influence
universal models binary spike patterns using centered dirichlet processes probabilistic models binary spike patterns provide powerful tool understanding statistical dependencies large scale neural recordings maximum entropy maxent models seek explain dependencies terms low order interactions neurons enjoyed remarkable success modeling patterns particularly small groups neurons however models computationally intractable large populations low order maxent models shown inadequate datasets overcome limitations propose family universal models binary spike patterns universality refers ability model arbitrary distributions binary patterns construct universal models using dirichlet process centered well behaved parametric base measure naturally combines flexibility histogram parsimony parametric model derive computationally efficient inference methods using bernoulli cascade logistic base measures scale tractably large populations also establish condition equivalence cascade logistic 2nd order maxent ising model making cascade logistic reasonable choice base measure universal model illustrate performance models using neural data
perfect associative learning spike timing dependent plasticity recent extensions perceptron tempotron suggest theoretical concept highly relevant also understanding networks spiking neurons brain known however computational power perceptron variants might accomplished plasticity mechanisms real synapses prove spike timing dependent plasticity anti hebbian form excitatory synapses well spike timing dependent plasticity hebbian shape inhibitory synapses sufficient realizing original perceptron learning rule respective plasticity mechanisms act concert hyperpolarisation post synaptic neurons also show simple yet biologically realistic dynamics tempotrons efficiently learned proposed mechanism might underly acquisition mappings spatio temporal activity patterns area brain onto spatio temporal spike patterns another region long term memories cortex results underline learning processes realistic networks spiking neurons depend crucially interactions synaptic plasticity mechanisms dynamics participating neurons
optimistic concurrency control distributed unsupervised learning research distributed machine learning algorithms focused primarily extremes algorithms obey strict concurrency constraints algorithms obey constraints consider intermediate alternative algorithms optimistically assume conflicts unlikely conflicts arise conflict resolution protocol invoked view optimistic concurrency control paradigm particularly appropriate large scale machine learning algorithms particularly unsupervised setting demonstrate approach problem areas clustering feature learning online facility location evaluate methods via large scale experiments cluster computing environment
real time inference gamma process model neural spiking simultaneous measurements ever increasing populations neurons growing need sophisticated tools recover signals individual neurons electrophysiology experiments classically proceeds step process threshold waveforms detect putative spikes cluster waveforms single units neurons extend previous bayesian nonparamet ric models neural spiking jointly detect cluster neurons using gamma process model importantly develop online approximate inference scheme enabling real time analysis performance exceeding previous state art via exploratory data analysis using data partial ground truth well novel data sets find several features model collectively contribute improved performance including accounting colored noise tecting overlapping spikes iii tracking waveform dynamics using mul tiple channels hope enable novel experiments simultaneously measuring many thousands neurons possibly adapting stimuli dynamically probe ever deeper mysteries brain
inferring neural population dynamics multiple partial recordings neural circuit simultaneous recordings activity large neural populations extremely valuable used infer dynamics interactions neurons local circuit shedding light computations performed possible measure activity hundreds neurons using photon calcium imaging however many computations thought involve circuits consisting thousands neurons cortical barrels rodent somatosensory cortex contribute statistical method stitching together sequentially imaged sets neurons model phrasing problem fitting latent dynamical system missing observations method allows substantially expand population sizes population dynamics characterized beyond number simultaneously imaged neurons particular demonstrate using recordings mouse somatosensory cortex method makes possible predict noise correlations non simultaneously recorded neuron pairs
translating embeddings modeling multi relational data consider problem embedding entities relationships multi relational data low dimensional vector spaces objective propose canonical model easy train contains reduced number parameters scale large databases hence propose transe method models relationships interpreting translations operating low dimensional embeddings entities despite simplicity assumption proves powerful since extensive experiments show transe significantly outperforms state art methods link prediction knowledge bases besides successfully trained large scale data set entities 25k relationships 17m training samples
provable subspace clustering lrr meets ssc sparse subspace clustering ssc low rank representation lrr considered state art methods subspace clustering methods fundamentally similar convex optimizations exploiting intuition self expressiveness main difference ssc minimizes vector ell_1 norm representation matrix induce sparsity lrr minimizes nuclear norm aka trace norm promote low rank structure representation matrix often simultaneously sparse low rank propose new algorithm termed low rank sparse subspace clustering lrssc combining ssc lrr develops theoretical guarantees algorithm succeeds results reveal interesting insights strength weakness ssc lrr demonstrate lrssc take advantages methods preserving self expressiveness property graph connectivity time
expressive power restricted boltzmann machines paper examines question kinds distributions efficiently represented restricted boltzmann machines rbms characterize rbm unnormalized log likelihood function type neural network called rbm network series simulation results relate networks types better understood show surprising result rbm networks efficiently compute function depends number input parity also provide first known example particular type distribution provably cannot efficiently represented rbm equivalently cannot efficiently computed rbm network assuming realistic exponential upper bound size weights formally demonstrating relatively simple distribution cannot represented efficiently rbm results provide new rigorous justification use potentially expressive generative models deeper ones
deep architecture matching short texts many machine learning problems interpreted learning matching types objects images captions users products queries documents matching level objects usually measured inner product certain feature space modeling effort focuses mapping objects original space feature space schema although proven successful range matching tasks insufficient capturing rich structure matching process complicated objects paper propose new deep architecture effectively model complicated matching relations objects heterogeneous domains specifically apply model matching tasks natural language finding sensible responses tweet relevant answers given question new architecture naturally combines localness hierarchy intrinsic natural language problems therefore greatly improves upon state art models
simultaneous rectification alignment via robust recovery low rank tensors work propose general method recovering low rank order tensors data deformed unknown transformation corrupted arbitrary sparse errors since unfolding matrices tensor interdependent introduce auxiliary variables relax hard equality constraints augmented lagrange multiplier method improve computational efficiency introduce proximal gradient step alternating direction minimization method provided proof convergence linearized version problem inner loop overall algorithm simulations experiments show methods efficient effective previous work proposed method easily applied simultaneously rectify align multiple images videos frames context state art algorithms rasl tilt viewed special cases work yet performs part function method
memory frontier complex synapses incredible gulf separates theoretical models synapses often described solely single scalar value denoting size postsynaptic potential immense complexity molecular signaling pathways underlying real synapses understand functional contribution molecular complexity learning memory essential expand theoretical conception synapse single scalar entire dynamical system many internal molecular functional states moreover theoretical considerations alone demand expansion network models scalar synapses assuming finite numbers distinguishable synaptic strengths strikingly limited memory capacity raises fundamental question synaptic complexity give rise memory address develop new mathematical theorems elucidating relationship structural organization memory properties complex synapses molecular networks moreover proving theorems uncover framework based first passage time theory impose order internal states complex synaptic models thereby simplifying relationship synaptic structure function
simple example dirichlet process mixture inconsistency number components data assumed come finite mixture unknown number components become common use dirichlet process mixtures dpms density estimation also inferences number components typical approach use posterior distribution number components occurring far posterior number clusters observed data however turns posterior consistent converge true number components note give elementary demonstration inconsistency perhaps simplest possible setting dpm normal components unit variance applied data mixture standard normal component find example exhibits severe inconsistency instead going posterior probability cluster goes
adaptive anonymity via matching adaptive anonymity problem formalized individual shares data along integer value indicate personal level desired privacy problem leads generalization anonymity matching setting novel algorithms theory provided implement type anonymity relaxation achieves better utility admits theoretical privacy guarantees strong importantly accommodates variable level anonymity individual empirical results confirm improved utility benchmark social data sets
predicting parameters deep learning demonstrate significant redundancy parameterization several deep learning models given weight values feature possible accurately predict remaining values moreover show parameter values predicted many need learned train several different architectures learning small number weights predicting rest best case able predict weights network without drop accuracy
distributed representations words phrases compositionality recently introduced continuous skip gram model efficient method learning high quality distributed vector representations capture large number precise syntactic semantic word relationships paper present several improvements make skip gram model expressive enable learn higher quality vectors rapidly show subsampling frequent words obtain significant speedup also learn higher quality representations measured tasks also introduce negative sampling simplified variant noise contrastive estimation nce learns accurate vectors frequent words compared hierarchical softmax inherent limitation word representations indifference word order inability represent idiomatic phrases example meanings canada air cannot easily combined obtain air canada motivated example present simple efficient method finding phrases show vector representations accurately learned skip gram model
regret based robust solutions uncertain markov decision processes paper seek robust policies uncertain markov decision processes mdps robust optimization approaches problems focussed computation maximin policies maximize value corresponding worst realization uncertainty recent work proposed minimax regret suitable alternative maximin objective robust optimization however existing algorithms handling minimax regret restricted models uncertainty rewards provide algorithms employ sampling improve across multiple dimensions handle uncertainties transition reward models dependence model uncertainties across state action pairs decision epochs scalability quality bounds finally demonstrate empirical effectiveness sampling approaches provide comparisons benchmark algorithms domains literature also provide sample average approximation saa analysis compute posteriori error bounds
nonparametric multi group membership model dynamic networks relational data like graphs networks matrices often dynamic relational structure evolves time fundamental problem analysis time varying network data extract summary common structure dynamics underlying relations entities build intuition changes network structure driven dynamics level groups nodes propose nonparametric multi group membership model dynamic networks model contains main components model birth death groups respect dynamics network structure via distance dependent indian buffet process capture evolution individual node group memberships via factorial hidden markov model explain dynamics network structure explicitly modeling connectivity structure demonstrate model capability identifying dynamics latent groups number different types network data experimental results show model achieves higher predictive performance future network forecasting missing link prediction
auxiliary variable exact hamiltonian monte carlo samplers binary distributions present new approach sample generic binary distributions based exact hamiltonian monte carlo algorithm applied piecewise continuous augmentation binary distribution interest extension idea distributions mixtures binary continuous variables allows sample posteriors linear probit regression models spike slab priors truncated parameters illustrate advantages algorithms several examples outperform metropolis gibbs samplers
new convex relaxation tensor completion study problem learning tensor set linear measurements prominent methodology problem based extension trace norm regularization used extensively learning low rank matrices tensor setting paper highlight limitations approach propose alternative convex relaxation euclidean unit ball describe technique solve associated regularization problem builds upon alternating direction method multipliers experiments synthetic dataset real datasets indicate proposed method improves significantly tensor trace norm regularization terms estimation error remaining computationally tractable
learning hidden markov models non sequence data via tensor decomposition learning dynamic models observed data central issue many scientific studies engineering tasks usual setting data collected sequentially trajectories dynamical system operation quite modern scientific modeling tasks however turns reliable sequential data rather difficult gather whereas order snapshots much easier obtain examples include modeling galaxies chronic diseases alzheimer certain biological processes existing methods learning dynamic model non sequence data mostly based expectation maximization involves non convex optimization thus hard analyze inspired recent advances spectral learning methods propose study problem different perspective moment matching spectral decomposition framework identify reasonable assumptions generative process non sequence data propose learning algorithms based tensor decomposition method cite anandkumar2012tensor textit provably recover first order markov models hidden markov models best knowledge first formal guarantee learning non sequence data preliminary simulation results confirm theoretical findings
near optimal entrywise sampling data matrices consider problem independently sampling non entries matrix order produce sparse sketch minimizes large times matrices example representing observations attributes give distributions exhibiting important properties first closed forms probability sampling item computable minimal information regarding second allow sketching matrices whose non zeros presented algorithm arbitrary order stream computation per non third resulting sketch matrices sparse non entries highly compressible lastly importantly mild assumptions distributions provably competitive optimal offline distribution note probabilities optimal offline distribution complex functions entries matrix therefore regardless computational complexity optimal distribution might impossible compute streaming model
bayesian entropy estimation binary spike train data using parametric prior knowledge shannon entropy basic quantity information theory fundamental building block analysis neural codes estimating entropy discrete distribution samples important difficult problem received considerable attention statistics theoretical neuroscience however neural responses characteristic statistical structure generic entropy estimators fail exploit example existing bayesian entropy estimators make naive assumption spike words equally likely priori makes inefficient allocation prior probability mass cases spikes sparse develop bayesian estimators entropy binary spike trains using priors designed flexibly exploit statistical structure simultaneously recorded spike responses define prior distributions spike words using mixtures dirichlet distributions centered simple parametric models parametric model captures high level statistical features data average spike count spike word allows posterior entropy concentrate rapidly standard estimators cases probability spiking differs strongly conversely dirichlet distributions assign prior mass distributions far parametric model ensuring consistent estimates arbitrary distributions devise compact representation data prior allow computationally efficient implementations bayesian least squares empirical bayes entropy estimators large numbers neurons apply estimators simulated real neural data show substantially outperform traditional methods
demixing odors fast inference olfaction olfactory system faces difficult inference problem determine odors present based distributed activation receptor neurons derive neural implementations approximate inference algorithms could used brain variational algorithm builds work beck 2012 based sampling importantly use realistic prior distribution odors used past use spike slab prior odors concentration mapping algorithms onto neural dynamics find infer correct odors less 100 although takes 500 eliminate false positives thus behavioral level algorithms make similar predictions however make different assumptions connectivity neural computations make different predictions neural activity thus distinguishable experimentally would provide insight mechanisms employed olfactory system algorithms use different coding strategies would also provide insight networks represent probabilities
flexible sampling discrete data correlations without marginal distributions learning joint dependence discrete variables fundamental problem machine learning many applications including prediction clustering dimensionality reduction recently framework copula modeling gained popularity due modular parametrization joint distributions among properties copulas provide recipe combining flexible models univariate marginal distributions parametric families suitable potentially high dimensional dependence structures radically extended rank likelihood approach hoff 2007 bypasses learning marginal models completely information ancillary learning task hand standard dimensionality reduction problems copula parameter estimation main idea represent data observable rank statistics ignoring information marginals inference typically done bayesian framework gaussian copulas complicated fact implies sampling within space number constraints increase quadratically number data points result slow mixing using shelf gibbs sampling present efficient algorithm based recent advances constrained hamiltonian markov chain monte carlo simple implement require paying quadratic cost sample size
rnade real valued neural autoregressive density estimator introduce rnade new model joint density estimation real valued vectors model calculates density datapoint product dimensional conditionals modeled using mixture density networks shared parameters rnade learns distributed representation data tractable expression calculation densities tractable likelihood allows direct comparison methods training standard gradient based optimizers compare performance rnade several datasets heterogeneous perceptual data finding outperforms mixture models case
recurrent linear models simultaneously recorded neural populations population neural recordings long range temporal structure often best understood terms shared underlying low dimensional dynamical process advances recording technology provide access ever larger fraction population standard computational approaches available identify collective dynamics scale poorly size dataset describe new scalable approach discovering low dimensional dynamics underlie simultaneously recorded spike trains neural population method based recurrent linear models rlms relates closely timeseries models based recurrent neural networks formulate rlms neural data generalising kalman filter based likelihood calculation latent linear dynamical systems lds models incorporate generalised linear observation process show rlms describe motor cortical population data better either directly coupled generalised linear models latent linear dynamical system models generalised linear observations also introduce cascaded linear model clm capture low dimensional instantaneous correlations neural populations clm describes cortical recordings better either ising gaussian models like rlm fit exactly quickly clm also seen generalization low rank gaussian model case factor analysis computational tractability rlm clm allow scale high dimensional neural data
decomposing proximal map proximal map key step gradient type algorithms become prevalent large scale high dimensional problems simple functions proximal map available closed form complicated functions become highly nontrivial motivated need combining regularizers simultaneously induce different types structures paper initiates systematic investigation proximal map sum functions decomposes composition proximal maps individual summands unify known results scattered literature also discover several new decompositions obtained almost effortlessly theory
integrated non factorized variational inference present non factorized variational method full posterior inference bayesian hierarchical models goal capturing posterior variable dependencies via efficient possibly parallel computation approach unifies integrated nested laplace approximation inla variational framework proposed method applicable challenging scenarios typically assumed inla bayesian lasso characterized non differentiability ell_ norm arising independent laplace priors derive upper bound kullback leibler divergence yields fast closed form solution via decoupled optimization method reliable analytic alternative markov chain monte carlo mcmc results tighter evidence lower bound mean field variational bayes method
correlations strike back case associative memory retrieval long recognised statistical dependencies neuronal activity need taken account decoding stimuli encoded neural population less studied though equally pernicious need take account dependencies synaptic weights decoding patterns previously encoded auto associative memory show activity dependent learning generically produces correlations failing take account dynamics memory retrieval leads catastrophically poor recall derive optimal network dynamics recall face synaptic correlations caused range synaptic plasticity rules dynamics involve well studied circuit motifs forms feedback inhibition experimentally observed dendritic nonlinearities therefore show addressing problem synaptic correlations leads novel functional account key biophysical features neural substrate
stochastic blockmodel approximation graphon theory consistent estimation given convergent sequence graphs exists limit object called graphon random graphs generated nonparametric perspective random graphs opens door study graphs beyond traditional parametric models time also poses challenging question estimate graphon underlying observed graphs paper propose computationally efficient algorithm estimate graphon set observed graphs generated show approximating graphon stochastic block models graphon consistently estimated estimation error vanishes size graph approaches infinity
determinantal point process latent variable model inhibition neural spiking data point processes popular models neural spiking behavior provide statistical distribution temporal sequences spikes help reveal complexities underlying series recorded action potentials however common neural point process models poisson process gamma renewal process capture interactions correlations critical modeling populations neurons develop novel model based determinantal point process latent embeddings neurons effectively captures helps visualize complex inhibitory competitive interaction show model natural extension popular generalized linear model sets interacting neurons model extended incorporate gain control divisive normalization modulation neural spiking based periodic phenomena applied neural spike recordings rat hippocampus see model captures inhibitory relationships dichotomy classes neurons periodic modulation theta rhythm known present data
approximate bayesian image interpretation using generative probabilistic graphics programs idea computer vision bayesian inverse problem computer graphics long history appealing elegance proved difficult directly implement instead vision tasks approached via complex bottom processing pipelines show possible write short simple probabilistic graphics programs define flexible generative models automatically invert interpret real world images generative probabilistic graphics programs consist stochastic scene generator renderer based graphics software stochastic likelihood model linking renderer output data latent variables adjust fidelity renderer tolerance likelihood model representations algorithms computer graphics originally designed produce high quality images instead used deterministic backbone highly approximate stochastic generative models formulation combines probabilistic programming computer graphics approximate bayesian computation depends general purpose automatic inference techniques describe applications reading sequences degraded adversarially obscured alphanumeric characters inferring road models vehicle mounted camera images probabilistic graphics programs present relies lines probabilistic code supports accurate approximately bayesian inferences ambiguous real world images
multisensory encoding decoding identification investigate spiking neuron model multisensory integration multiple stimuli different sensory modalities encoded single neural circuit comprised multisensory bank receptive fields cascade population biophysical spike generators demonstrate stimuli different dimensions faithfully multiplexed encoded spike domain derive tractable algorithms decoding stimulus common pool spikes also show identification multisensory processing single neuron dual recovery stimuli encoded population multisensory neurons prove projection circuit onto input stimuli identified provide example multisensory integration using natural audio video discuss performance proposed decoding identification algorithms
top regularization deep belief networks designing principled effective algorithm learning deep architectures challenging problem current approach involves training phases fully unsupervised learning followed strongly discriminative optimization suggest deep learning strategy bridges gap phases resulting phase learning procedure propose implement scheme using method regularize deep belief networks top information network constructed building blocks restricted boltzmann machines learned combining bottom top sampled signals global optimization procedure merges samples forward bottom pass top pass used experiments mnist dataset show improvements existing algorithms deep belief networks object recognition results caltech 101 dataset also yield competitive results
exact stable recovery pairwise interaction tensors tensor completion incomplete observations problem significant practical interest however unlikely exists efficient algorithm provable guarantee recover general tensor limited number observations paper study recovery algorithm pairwise interaction tensors recently gained considerable attention modeling multiple attribute data due simplicity effectiveness specifically absence noise show exactly recover pairwise interaction tensor solving constrained convex program minimizes weighted sum nuclear norms matrices log observations noisy cases also prove error bounds constrained convex program recovering tensors experiments synthetic dataset demonstrate recovery performance algorithm agrees well theory addition apply algorithm temporal collaborative filtering task obtain state art results
efficient online inference bayesian nonparametric relational models stochastic block models characterize observed network relationships via latent community memberships large social networks expect entities participate multiple communities number communities grow network size introduce new model phenomena hierarchical dirichlet process relational model allows nodes mixed membership unbounded set communities allow scalable learning derive online stochastic variational inference algorithm focusing assortative models undirected networks also propose efficient structured mean field variational bound online methods automatically pruning unused communities compared state art online learning methods parametric relational models show significantly improved perplexity link prediction accuracy sparse networks tens thousands nodes also showcase analysis littlesis large network knows heights business government
structured learning via logistic regression successful approach structured learning write learning objective joint function linear parameters inference messages iterate updates paper observes inference problem smoothed addition entropy terms fixed messages learning objective reduces traditional non structured logistic regression problem respect parameters logistic regression problems training example bias term determined current set messages based insight structured energy function extended linear factors function class oracle exists minimize logistic loss
understanding dropout dropout relatively new algorithm training neural networks relies stochastically dropping neurons training order avoid adaptation feature detectors introduce general formalism studying dropout either units connections arbitrary probability values use analyze averaging regularizing properties dropout linear non linear networks deep neural networks averaging properties dropout characterized recursive equations including approximation expectations normalized weighted geometric means provide estimates bounds approximations corroborate results simulations also show simple cases dropout performs stochastic gradient descent regularized error function
large scale distributed sparse precision estimation consider problem sparse precision matrix estimation high dimensions using clime estimator several desirable theoretical properties present inexact alternating direction method multiplier admm algorithm clime establish rates convergence objective optimality conditions develop large scale distributed framework computations scales millions dimensions trillions parameters using hundreds cores proposed framework solves clime column blocks involves elementwise operations parallel matrix multiplications evaluate algorithm shared memory distributed memory architectures use block cyclic distribution data parameters achieve load balance improve efficiency use memory hierarchies experimental results show algorithm substantially scalable state art methods scales almost linearly number cores
approximate inference latent gaussian markov models continuous time observations propose approximate inference algorithm continuous time gaussian markov process models discrete continuous time likelihoods show continuous time limit expectation propagation algorithm exists results hybrid fixed point iteration consisting expectation propagation updates discrete time terms variational updates continuous time term introduce corrections methods improve marginals approximation approach extends classical kalman bucy smoothing procedure non gaussian observations enabling continuous time inference variety models including spiking neuronal models state space models point process observations box likelihood models experimental results real simulated data demonstrate high distributional accuracy significant computational savings compared discrete time approaches neural application
matrix completion given set observations matrix completion problem aim recover unknown real matrix subset entries problem comes many application areas received great deal attention context netflix prize central approach problem output matrix lowest possible complexity rank trace norm agrees partially specified matrix performance approach assumption revealed entries sampled randomly received considerable attention practice often set revealed entries chosen random results apply therefore left guarantees performance algorithm using present means obtain performance guarantees respect set initial observations first step remains find matrix lowest possible complexity agrees partially specified matrix give new way interpret output algorithm next finding probability distribution non revealed entries respect bound generalization error proven complex set revealed entries according certain measure better bound generalization error
transportability multiple environments limited experiments paper considers problem transferring experimental findings learned multiple heterogeneous domains target environment limited experiments performed reduce questions transportability multiple domains limited scope symbolic derivations calculus thus extending treatment transportability full experiments introduced pearl bareinboim 2011 provide different graphical algorithmic conditions computing transport formula setting way fusing observational experimental information scattered throughout different domains synthesize consistent estimate desired effects
causal inference time series using restricted structural equation models causal inference uses observational data infer causal structure data generating system study class restricted structural equation models time series call time series models independent noise timino models require independent residual time series whereas traditional methods like granger causality exploit variance residuals work contains main contributions theoretical restricting model class additive noise provide general identifiability results existing ones results cover lagged instantaneous effects nonlinear unfaithful non instantaneous feedbacks time series practical feedback loops time series propose algorithm based non linear independence tests time series data causally insufficient data generating process satisfy model assumptions algorithm still give partial results mostly avoids incorrect answers structural equation model point view allows extend theoretical algorithmic part situations time series measured different time delays happen fmri data example timino outperforms existing methods artificial real data code provided
unsupervised spectral learning finite state transducers finite state transducers fst standard tool modeling paired input output sequences used numerous applications ranging computational biology natural language processing recently balle presented spectral algorithm learning fst samples aligned input output sequences paper address realistic yet challenging setting alignments unknown learning algorithm frame fst learning finding low rank hankel matrix satisfying constraints derived observable statistics formulation provide identifiability results fst distributions following previous work rank minimization propose regularized convex relaxation objective based minimizing nuclear norm penalty subject linear constraints solved efficiently
reciprocally coupled local estimators implement bayesian information integration distributively psychophysical experiments demonstrated brain integrates information multiple sensory cues near bayesian optimal manner present study proposes novel mechanism achieve consider reciprocally connected networks mimicking integration heading direction information dorsal medial superior temporal mstd ventral intraparietal vip areas network serves local estimator receives independent cue either visual vestibular direct input external stimulus find positive reciprocal interactions improve decoding accuracy individual network implements bayesian inference cues model successfully explains experimental finding mstd vip achieve bayesian multisensory integration though receives single cue direct external input result suggests brain implement optimal information integration distributively local estimator reciprocal connections cortical regions
wavelets graphs via deep learning increasing number applications require processing signals defined weighted graphs wavelets provide flexible tool signal processing classical setting regular domains existing graph wavelet constructions less flexible guided solely structure underlying graph take directly consideration particular class signals processed paper introduces machine learning framework constructing graph wavelets sparsely represent given class signals construction uses lifting scheme based observation recurrent nature lifting scheme gives rise structure resembling deep auto encoder network particular properties resulting wavelets must satisfy determine training objective structure involved neural networks training unsupervised conducted similarly greedy pre training stack auto encoders training completed obtain linear wavelet transform applied graph signal time memory linear size graph improved sparsity wavelet transform test signals confirmed via experiments synthetic real data
global solver efficient approximation variational bayesian low rank subspace clustering probabilistic model prior given bayesian learning offers inference automatic parameter tuning however bayesian learning often obstructed computational difficulty rigorous bayesian learning intractable many models variational bayesian approximation prone suffer local minima paper overcome difficulty low rank subspace clustering lrsc providing exact global solver efficient approximation lrsc extracts low dimensional structure data embedding samples union low dimensional subspaces variational bayesian variant shown good performance first prove key property lrsc model highly redundant thanks property optimization problem lrsc separated small subproblems small number unknown variables exact global solver relies another key property stationary condition subproblem written set polynomial equations solvable homotopy method computational efficiency also propose efficient approximate variant stationary condition written polynomial equation single variable experimental results show usefulness approach
robust image denoising multi column deep neural networks stacked sparse denoising auto encoders ssdas recently shown successful removing noise corrupted images however like denoising techniques ssda robust variation noise types beyond seen training present multi column stacked sparse denoising autoencoder novel technique combining multiple ssdas multi column ssda ssda combining outputs ssda eliminate need determine type noise let alone statistics test time show good denoising performance achieved single system variety different noise types including ones seen training set additionally experimentally demonstrate efficacy ssda denoising achieving mnist digit error rates denoised images close uncorrupted images
learning noisy labels paper theoretically study problem binary classification presence random classification noise learner instead seeing true labels sees labels independently flipped small probability moreover random label noise emph class conditional flip probability depends class provide approaches suitably modify given surrogate loss function first provide simple unbiased estimator loss obtain performance bounds empirical risk minimization presence iid data noisy labels loss function satisfies simple symmetry condition show method leads efficient algorithm empirical minimization second leveraging reduction risk minimization noisy labels classification weighted loss suggest use simple weighted surrogate loss able obtain strong empirical risk bounds approach remarkable consequence methods used practice biased svm weighted logistic regression provably noise tolerant synthetic non separable dataset methods achieve accuracy even labels corrupted competitive respect recently proposed methods dealing label noise several benchmark datasets
matrix factorization binary components motivated application computational biology consider constrained low rank matrix factorization problems constraints factors addition non convexity shared general matrix factorization schemes problem complicated combinatorial constraint set size cdot dimension data points rank factorization despite apparent intractability provide line recent work non negative matrix factorization arora 2012 algorithm provably recovers underlying factorization exact case operations order mnr worst case obtain result invoke theory centered around fundamental result combinatorics littlewood offord lemma
variance reduction stochastic gradient optimization stochastic gradient optimization class widely used algorithms training machine learning models optimize objective uses noisy gradient computed random data samples instead true gradient computed entire dataset however variance noisy gradient large algorithm might spend much time bouncing around leading slower convergence worse performance paper develop general approach using control variate variance reduction stochastic gradient data statistics low order moments pre computed estimated online used form control variate demonstrate construct control variate practical problems using stochastic gradient optimization convex map estimation logistic regression non convex stochastic variational inference latent dirichlet allocation problems approach shows faster convergence better performance classical approach
capacity strong attractor patterns model behavioural cognitive prototypes solve mean field equations stochastic hopfield network temperature noise presence strong multiply stored patterns use solution obtain storage capacity network result provides first time rigorous solution mean field equations standard hopfield model contrast mathematically unjustifiable replica technique hitherto used derivation show critical temperature stability strong pattern equal degree multiplicity sum cubes degrees stored patterns negligible compared network size case single strong pattern presence simple patterns ratio number stored patterns network size positive constant obtain distribution overlaps patterns mean field deduce storage capacity retrieving strong pattern exceeds retrieving simple pattern multiplicative factor equal square degree strong pattern square law property provides justification using strong patterns model attachment types behavioural prototypes psychology psychotherapy
bayesian hierarchical community discovery propose efficient bayesian nonparametric model discovering hierarchical community structure social networks model tree structured mixture potentially exponentially many stochastic blockmodels describe family greedy agglomerative model selection algorithms whose worst case scales quadratically number vertices network independent number communities algorithms orders magnitude faster infinite relational model achieving comparable better accuracy
stochastic gradient riemannian langevin dynamics probability simplex paper investigate use langevin monte carlo methods probability simplex propose new method stochastic gradient riemannian langevin dynamics simple implement applied online apply method latent dirichlet allocation online setting demonstrate achieves substantial performance improvements state art online variational bayesian methods
bayesian inference iterated random functions applications sequential inference graphical models propose general formalism iterated random functions semigroup property exact approximate bayesian posterior updates viewed specific instances convergence theory iterated random functions presented application general theory analyze convergence behaviors exact approximate message passing algorithms arise sequential change point detection problem formulated via latent variable directed graphical model sequential inference algorithm supporting theory illustrated simulated examples
adaptive dropout training deep neural networks recently shown dropping hidden activities probability deep neural networks perform well describe model binary belief network overlaid neural network used decrease information content hidden units selectively setting activities dropout network trained jointly neural network approximately computing local expectations binary dropout variables computing derivatives using back propagation using stochastic gradient descent interestingly experiments show learnt dropout network parameters recapitulate neural network parameters suggesting good dropout network regularizes activities according magnitude evaluated mnist norb datasets found method used achieve lower classification error rates feather learning methods including standard dropout denoising auto encoders restricted boltzmann machines example model achieves error norb test set better state art results obtained using convolutional architectures
generalized denoising auto encoders generative models recent work shown denoising contractive autoencoders implicitly capture structure data generating density case corruption noise gaussian reconstruction error squared error data continuous valued led various proposals sampling implicitly learned density function using langevin metropolis hastings mcmc however remained unclear connect training procedure regularized auto encoders implicit estimation underlying data generating distribution data discrete using forms corruption process reconstruction errors another issue mathematical justification valid limit small corruption noise propose different attack problem deals issues arbitrary noisy enough corruption arbitrary reconstruction loss seen log likelihood handling discrete continuous valued variables removing bias due non infinitesimal corruption noise non infinitesimal contractive penalty
graphical models inference missing data address problem deciding whether exists consistent estimator given relation data missing random employ formal representation called missingness graphs explicitly portray causal mechanisms responsible missingness encode dependencies mechanisms variables measured using representation define notion textit recoverability ensures given missingness graph given query algorithm exists limit large samples produces estimate textit data missing present conditions graph satisfy order recoverability hold devise algorithms detect presence conditions
learning efficient random maximum posteriori predictors non decomposable loss functions work develop efficient methods learning random map predictors structured label problems particular construct posterior distributions perturbations adjusted via stochastic gradient methods show every smooth posterior distribution would suffice define smooth pac bayesian risk bound suitable gradient methods addition relate posterior distributions computational properties map predictors suggest multiplicative posteriors learn super modular potential functions accompany specialized map predictors graph cuts also describe label augmented posterior models use efficient map approximations arising linear program relaxations
annealing distributions averaging moments many powerful monte carlo techniques estimating partition functions annealed importance sampling ais based sampling sequence intermediate distributions interpolate tractable initial distribution intractable target distribution near universal practice use geometric averages initial target distributions alternative paths perform substantially better present novel sequence intermediate distributions exponential families averaging moments initial target distributions derive asymptotically optimal piecewise linear schedule moments path show performs least well geometric averages linear schedule moment averaging performs well empirically estimating partition functions restricted boltzmann machines rbms form building blocks many deep learning models including deep belief networks deep boltzmann machines
memory limited streaming pca consider streaming pass principal component analysis pca high dimensional regime limited memory dimensional samples presented sequentially goal produce dimensional subspace best approximates points standard algorithms require memory meanwhile algorithm better memory since output requires memory storage complexity meaningful understood context computational sample complexity sample complexity high dimensional pca typically studied setting spiked covariance model dimensional points generated population covariance equal identity white noise plus low dimensional perturbation spike signal recovered well understood spike recovered number samples scales proportionally dimension yet algorithms provably achieve memory complexity meanwhile algorithms memory complexity provable bounds sample complexity comparable present algorithm achieves uses memory meaning storage kind able compute dimensional spike log sample complexity first algorithm kind theoretical analysis focuses spiked covariance model simulations show algorithm successful much general models data
firing rate predictions optimal balanced networks firing rates spiking network related neural input connectivity network function important problem firing rates important measures network activity study neural computation neural network dynamics however difficult problem spiking mechanism individual neurons highly non linear individual neurons interact strongly connectivity develop new technique calculating firing rates optimal balanced networks particularly interesting networks provide optimal spike based signal representation producing cortex like spiking activity dynamic balance excitation inhibition calculate firing rates treating balanced network dynamics algorithm optimizing signal representation identify algorithm calculate firing rates finding solution algorithm firing rate calculation relates network firing rates directly network input connectivity function allows explain function underlying mechanism tuning curves variety systems
reconciling priors priors without prejudice major routes address linear inverse problems whereas regularization based approaches build estimators solutions penalized regression optimization problems bayesian estimators rely posterior distribution unknown given assumed family priors seem radically different approaches recent results shown context additive white gaussian denoising bayesian conditional mean estimator always solution penalized regression problem contribution paper twofold first extend additive white gaussian denoising results general linear inverse problems colored gaussian noise second characterize conditions penalty function associated conditional mean estimator satisfy certain popular properties convexity separability smoothness sheds light tradeoff computational efficiency estimation accuracy sparse regularization draws connections bayesian estimation proximal optimization
analyzing hogwild parallel gaussian gibbs sampling sampling inference methods computationally difficult scale many models part global dependencies reduce opportunities parallel computation without strict conditional independence structure among variables standard gibbs sampling theory requires sample updates performed sequentially even dependence variables strong empirical work shown models sampled effectively going hogwild simply running gibbs updates parallel periodic global communication successes limitations strategy well understood step towards understanding study hogwild gibbs sampling strategy context gaussian distributions develop framework provides convergence conditions error bounds along simple proofs connections methods numerical linear algebra particular show gaussian precision matrix generalized diagonally dominant hogwild gibbs sampler update schedule allocation variables processors yields stable sampling process correct sample mean
hedge option adversary black scholes pricing minimax optimal consider popular problem finance option pricing lens online learning game nature investor black scholes option pricing model 1973 investor continuously hedge risk option trading underlying asset assuming asset price fluctuates according geometric brownian motion gbm consider worst case model nature chooses sequence price fluctuations cumulative quadratic volatility constraint investor make sequence hedging decisions main result show value proposed game regret hedging strategy converges black scholes option price use significantly weaker assumptions previous work instance allow large jumps asset price show black scholes hedging strategy near optimal investor even non stochastic framework
comparative framework preconditioned lasso algorithms lasso cornerstone modern multivariate data analysis yet performance suffers common situation covariates correlated limitation led growing number emph preconditioned lasso algorithms pre multiply matrices p_x p_y prior running standard lasso direct comparison similar lasso style algorithms original lasso difficult performance methods depends critically auxiliary penalty parameter lambda paper propose agnostic theoretical framework comparing preconditioned lasso algorithms lasso without choose lambda apply framework preconditioned lasso instances highlight outperform lasso additionally theory offers insights fragilities algorithms provide partial solutions
summary statistics partitionings feature allocations infinite mixture models commonly used clustering sample posterior mixture assignments monte carlo methods find maximum posteriori solution optimization however problems posterior diffuse hard interpret sampled partitionings paper introduce novel statistics based block sizes representing sample sets partitionings feature allocations develop element based definition entropy quantify segmentation among elements propose simple algorithm called entropy agglomeration summarize visualize information experiments various infinite mixture posteriors well feature allocation dataset demonstrate proposed statistics useful practice
designed measurements vector count data consider design linear projection measurements vector poisson signal model projections performed vector poisson rate mathbb observed data vector counts mathbb projection matrix designed maximizing mutual information latent class label dots associated consider mutual information respect new analytic expressions gradient presented gradient performed respect measurement matrix connections made widely studied gaussian measurement model example results presented compressive topic modeling document corpora word counting hyperspectral compressive sensing chemical classification photon counting
dirty statistical models provide unified framework high dimensional analysis superposition structured dirty statistical models model parameters superposition structurally constrained parameters allow number types structures statistical model consider general class estimators minimize sum loss function instance call hybrid regularization infimal convolution weighted regularization functions structural component provide corollaries showcasing unified framework varied statistical models linear regression multiple regression principal component analysis varied superposition structures
online pca contaminated data consider online principal component analysis pca contaminated samples containing outliers revealed sequentially principal components pcs estimator due sensitiveness outliers previous online pca algorithms fail case results arbitrarily bad propose online robust pca algorithm able improve pcs estimation upon initial steadily even faced constant fraction outliers show final result proposed online rpca acceptable degradation optimum actually mild conditions online rpca achieves maximal robustness breakdown point moreover online rpca shown efficient storage computation since need explore previous samples traditional robust pca algorithms endows online rpca scalability large scale data
eluder dimension sample complexity optimistic exploration paper considers sample complexity multi armed bandit dependencies among arms successful algorithms problem use principle optimism face uncertainty guide exploration clearest example class upper confidence bound ucb algorithms recent work shown simple posterior sampling algorithm sometimes called thompson sampling also shares close theoretical connection optimistic approaches paper develop regret bound holds classes algorithms bound applies broadly specialized many model classes depends new notion refer eluder dimension measures degree dependence among action rewards compared ucb algorithm regret bounds specific model classes general bound matches best available linear models stronger best available generalized linear models
locally adaptive bayesian multivariate time series modeling multivariate time series important allow time varying smoothness mean covariance process particular certain time intervals exhibiting rapid changes others changes slow locally adaptive smoothness accounted obtain misleading inferences predictions smoothing across erratic time intervals smoothing across times exhibiting slow variation lead miscalibration predictive intervals substantially narrow wide depending time propose continuous multivariate stochastic process time series locally varying smoothness mean covariance matrix process constructed utilizing latent dictionary functions time given nested gaussian process priors linearly related observed data sparse mapping using differential equation representation bypass usual computational bottlenecks obtaining mcmc online algorithms approximate bayesian inference performance assessed simulations illustrated financial application
multilinear dynamical systems tensor time series many scientific data occur sequences multidimensional arrays called tensors hidden evolving trends data extracted preserving tensor structure model traditionally used linear dynamical system lds treats observation time slice vector paper propose multilinear dynamical system mlds modeling tensor time series expectation maximization algorithm estimate parameters mlds models time slice tensor time series multilinear projection corresponding member sequence latent low dimensional tensors compared lds equal number parameters mlds achieves higher prediction accuracy marginal likelihood simulated real datasets
ocsvm quantile estimator high dimensional distributions paper introduce novel method efficiently estimate family hierarchical dense sets high dimensional distributions method regarded natural extension class svm ocsvm algorithm finds multiple parallel separating hyperplanes reproducing kernel hilbert space call method ocsvm used estimate quantiles high dimensional distribution purpose introduce new global convex optimization program finds estimated sets show solved efficiently prove correctness method present empirical results demonstrate superiority existing methods
latent maximum margin clustering present maximum margin framework clusters data using latent variables using latent representations enables framework model unobserved information embedded data implement idea large margin learning develop alternating descent algorithm effectively solve resultant non convex optimization problem instantiate latent maximum margin clustering framework tag based video clustering tasks video represented latent tag model describing presence absence video tags experimental results obtained standard datasets show proposed method outperforms non latent maximum margin clustering well conventional clustering approaches
kernel test variable interactions introduce kernel nonparametric tests lancaster variable interaction total independence using embeddings signed measures reproducing kernel hilbert space resulting test statistics straightforward compute used powerful variable interaction tests consistent alternatives large family reproducing kernels show lancaster test sensitive cases independent causes individually weak influence third dependent variable combined effect strong influence makes lancaster test especially suited finding structure directed graphical models outperforms competing nonparametric tests detecting structures
non strongly convex smooth stochastic approximation convergence rate consider stochastic approximation problem convex function minimized given knowledge unbiased estimates gradients certain points framework includes machine learning methods based minimization empirical risk focus problems without strong convexity previously known algorithms achieve convergence rate function values sqrt consider analyze algorithms achieve rate classical supervised learning problems least squares regression show averaged stochastic gradient descent constant step size achieves desired rate logistic regression achieved simple novel stochastic gradient algorithm constructs successive local quadratic approximations loss functions preserving running time complexity stochastic gradient descent algorithms provide non asymptotic analysis generalization error expectation also high probability least squares run extensive experiments showing often outperform existing approaches
rapid distance based outlier detection via sampling distance based approaches outlier detection popular data mining require model underlying probability distribution particularly challenging high dimensional data present empirical comparison various approaches distance based outlier detection across large number datasets report surprising observation simple sampling based scheme outperforms state art techniques terms efficiency effectiveness better understand phenomenon provide theoretical analysis sampling based approach outperforms alternative methods based nearest neighbor search
stochastic majorization minimization algorithms large scale optimization majorization minimization algorithms consist iteratively minimizing majorizing surrogate objective function simplicity wide applicability principle popular statistics signal processing paper intend make principle scalable introduce stochastic majorization minimization scheme able deal large scale possibly infinite data sets applied convex optimization problems suitable assumptions show achieves expected convergence rate sqrt iterations strongly convex functions equally important scheme almost surely converges stationary points large class non convex problems develop several efficient algorithms based framework first propose new stochastic proximal gradient method experimentally matches state art solvers large scale ell_1 logistic regression second develop online programming algorithm non convex sparse estimation finally demonstrate effectiveness technique solving large scale structured matrix factorization problems
variational inference mahalanobis distance metrics gaussian process regression introduce novel variational method allows approximately integrate kernel hyperparameters length scales gaussian process regression approach consists novel variant variational framework recently developed gaussian process latent variable model additionally makes use standardised representation gaussian process consider technique learning mahalanobis distance metrics gaussian process regression setting provide experimental evaluations comparisons existing methods considering datasets high dimensional inputs
fantope projection selection near optimal convex relaxation sparse pca propose novel convex relaxation sparse principal subspace estimation based convex hull rank projection matrices fantope convex problem solved efficiently using alternating direction method multipliers admm establish near optimal convergence rate terms sparsity ambient dimension sample size estimation principal subspace general covariance matrix without assuming spiked covariance model special case result implies near optimality dspca even solution rank also provide general theoretical framework analyzing statistical properties method arbitrary input matrices extends applicability provable guarantees wide array settings demonstrate application kendall tau correlation matrices transelliptical component analysis
relationship binary classification bipartite ranking binary class probability estimation investigate relationship fundamental problems machine learning binary classification bipartite ranking binary class probability estimation cpe known good binary cpe model used obtain good binary classification model thresholding also obtain good bipartite ranking model using cpe model directly ranking model also known binary classification model necessarily yield cpe model however much known directions formally relationships involve regret transfer bounds paper introduce notion weak regret transfer bounds mapping needed transform model problem another depends underlying probability distribution practice must estimated data show weaker sense good bipartite ranking model used construct good classification model thresholding suitable point surprisingly also construct good binary cpe model calibrating scores ranking model
regularized spectral clustering degree corrected stochastic blockmodel spectral clustering fast popular algorithm finding clusters networks recently chaudhuri amini proposed variations algorithm artificially inflate node degrees improved statistical performance current paper extends previous theoretical results canonical spectral clustering algorithm way removes assumption minimum degree provides guidance choice tuning parameter moreover results show star shape eigenvectors consistently observed empirical networks explained degree corrected stochastic blockmodel extended planted partition model statistical model allow highly heterogeneous degrees throughout paper characterizes justifies several variations spectral clustering algorithm terms models
bayesian inference learning gaussian process state space models particle mcmc state space models successfully used many areas science engineering economics model time series dynamical systems present fully bayesian approach inference learning nonlinear nonparametric state space models place gaussian process prior transition dynamics resulting flexible model able capture complex dynamical phenomena however enable efficient inference marginalize dynamics model instead infer directly joint smoothing distribution use specially tailored particle markov chain monte carlo samplers approximation smoothing distribution computed state transition predictive distribution formulated analytically make use sparse gaussian process models greatly reduce computational complexity approach
dynamic clustering via asymptotics dependent dirichlet process mixture paper presents novel algorithm based upon dependent dirichlet process mixture model ddpmm clustering batch sequential data containing unknown number evolving clusters algorithm derived via low variance asymptotic analysis gibbs sampling algorithm ddpmm provides hard clustering convergence guarantees similar means algorithm empirical results synthetic test moving gaussian clusters test real ads aircraft trajectory data demonstrate algorithm requires orders magnitude less computational time contemporary probabilistic hard clustering algorithms providing higher accuracy examined datasets
learning invariance via linear functionals reproducing kernel hilbert space incorporating invariance information important many learning problems exploit invariances existing methods resort approximations either lead expensive optimization problems semi definite programming rely separation oracles retain tractability methods limit space functions settle non convex models paper propose framework learning reproducing kernel hilbert spaces rkhs using local invariances explicitly characterize behavior target function around data instances invariances emph compactly encoded linear functionals whose value penalized loss function based representer theorem establish formulation efficiently optimized via convex program representer theorem hold linear functionals required bounded rkhs show true variety commonly used rkhs invariances experiments learning unlabeled data transform invariances show proposed method yields better similar results compared state art
flat versus hierarchical classification large scale taxonomies study paper flat hierarchical classification strategies context large scale taxonomies end first propose multiclass hierarchical data dependent bound generalization error classifiers deployed large scale taxonomies bound provides explanation several empirical results reported literature related performance flat hierarchical classifiers introduce another type bounds targeting approximation error family classifiers derive features used meta classifier decide nodes prune flatten large scale taxonomy finally illustrate theoretical developments several experiments conducted widely used taxonomies
trading computation communication distributed stochastic dual coordinate ascent present study distributed optimization algorithm employing stochastic dual coordinate ascent method stochastic dual coordinate ascent methods enjoy strong theoretical guarantees often better performances stochastic gradient descent methods optimizing regularized loss minimization problems still lacks efforts studying distributed framework make progress along line presenting distributed stochastic dual coordinate ascent algorithm star network analysis tradeoff computation communication verify analysis experiments real data sets moreover compare proposed algorithm distributed stochastic gradient descent methods distributed alternating direction methods multipliers optimizing svms distributed framework observe competitive performances
new subsampling algorithms fast least squares regression address problem fast estimation ordinary least squares ols large amounts data propose methods solve big data problem subsampling covariance matrix using either single stage estimation run order size input best method uluru gives error bound sqrt independent amount subsampling long threshold provide theoretical bounds algorithms fixed design randomized hadamard preconditioning well sub gaussian random design setting also compare performance methods synthetic real world datasets show observations sub gaussian directly subsample without expensive randomized hadamard preconditioning without loss accuracy
noise efficient multi task gaussian process inference structured residuals multi task prediction models widely used couple regressors classification models sharing information across related tasks common pitfall models assume output tasks independent conditioned inputs propose multi task gaussian process approach model relatedness regressors well task correlations residuals order accurately identify true sharing regressors resulting gaussian model covariance term sum kronecker products efficient parameter inference sample prediction feasible synthetic examples applications phenotype prediction genetics find substantial benefits modeling structured noise compared established alternatives
moment based uniform deviation bounds means friends suppose centers fit points heuristically minimizing means cost corresponding fit source distribution question resolved distributions geq bounded moments particular difference sample cost distribution cost decays min essential technical contribution mechanism uniformly control deviations face unbounded parameter sets cost functions source distributions demonstrate mechanism soft clustering variant means cost also considered namely log likelihood gaussian mixture subject constraint covariance matrices bounded spectrum lastly rate refined constants provided means instances possessing cluster structure
learning multiple models via regularized weighting consider general problem multiple model learning mml data statistical algorithmic perspectives problem includes clustering multiple regression subspace clustering special cases common approach solving new mml problems generalize lloyd algorithm clustering expectation maximization soft clustering however approach unfortunately sensitive outliers large noise single exceptional point take models propose different general formulation seeks model distribution data points weights regularized sufficiently spread enhances robustness making assumptions class balance provide generalization bounds explain new iterations computed efficiently demonstrate robustness benefits approach experimental results prove important case clustering approach non trivial breakdown point guaranteed robust fixed percentage adversarial unbounded outliers
scoring workers crowdsourcing many control questions enough study problem estimating continuous quantities prices probabilities point spreads using crowdsourcing approach challenging aspect combining crowd answers workers reliabilities biases usually unknown highly diverse control items known answers used evaluate workers performance hence improve combined results target items unknown answers raises problem many control items use total number items workers answer limited control items evaluates workers better leaves fewer resources target items direct interest vice versa give theoretical results problem different scenarios provide simple rule thumb crowdsourcing practitioners byproduct also provide theoretical analysis accuracy different consensus methods
sparse overlapping sets lasso multitask learning application fmri analysis multitask learning effective features useful task also useful tasks group lasso standard method selecting common subset features paper interested less restrictive form multitask learning wherein available features organized subsets according notion similarity features useful task similar necessarily identical features best suited tasks main contribution paper new procedure called sparse overlapping sets sos lasso convex optimization automatically selects similar features related learning tasks error bounds derived soslasso consistency established squared error loss particular soslasso motivated multi subject fmri studies functional activity classified using brain voxels features experiments real synthetic data demonstrate advantages soslasso compared lasso group lasso
prototype learning rigid structures paper study following new variant prototype learning called prototype learning problem rigid structures given set rigid structures find set rigid structures prototype cluster given rigid structures total cost dissimilarity minimized prototype learning core problem machine learning wide range applications many areas existing results problem mainly focused graph domain paper present first algorithm learning multiple prototypes rigid structures result based number new insights rigid structures alignment clustering prototype reconstruction practically efficient quality guarantee validate approach using type data sets random data biological data chromosome territories experiments suggest approach effectively learn prototypes types data
submodular optimization submodular cover submodular knapsack constraints investigate new optimization problems minimizing submodular function subject submodular lower bound constraint submodular cover maximizing submodular function subject submodular upper bound constraint submodular knapsack motivated number real world applications machine learning including sensor placement data subset selection require maximizing certain submodular function like coverage diversity simultaneously minimizing another like cooperative cost problems often posed minimizing difference submodular functions worst case inapproximable show however phrasing problems constrained optimization natural many applications achieve number bounded approximation guarantees also show problems closely related approximation algorithm solving used obtain approximation guarantee provide hardness results problems thus showing approximation factors tight log factors finally empirically demonstrate performance good scalability properties algorithms
using multiple samples learn mixture models mixture models problem assumed distributions theta_ ldots theta_ gets observe sample mixture distributions unknown coefficients goal associate instances generating distributions identify parameters hidden distributions work make assumption access several samples drawn underlying distributions different mixing weights topic modeling multiple samples often reasonable assumption instead pooling data sample prove possible use differences samples better recover underlying structure present algorithms recover underlying structure milder assumptions current state art either dimensionality separation high methods applied topic modeling allow generalization words present training data
shot learning inverting compositional causal process people learn new visual class example yet machine learning algorithms typically require hundreds thousands examples tackle problems present hierarchical bayesian model based compositionality causality learn wide range natural although simple visual concepts generalizing human like ways image evaluated performance challenging shot classification task model achieved human level error rate substantially outperforming deep learning models also used visual turing test show model produces human like performance conceptual tasks including generating new examples parsing
error minimizing estimates universal entry wise error bounds low rank matrix completion propose general framework reconstructing denoising single entries incomplete noisy entries describe effective algorithms deciding entry reconstructed reconstructing denoising priori bounds error entry individually noiseless case algorithm exact rank matrices new algorithm fast admits highly parallel implementation produces error minimizing estimate qualitatively close theoretical state art nuclear norm optspace methods
gaussian process conditional copulas applications financial time series estimation dependencies multiple variables central problem analysis financial time series common approach express dependencies terms copula function typically copula function assumed constant innacurate covariates could large influence dependence structure data account bayesian framework estimation conditional copulas proposed framework parameters copula non linearly related arbitrary conditioning variables evaluate ability method predict time varying dependencies several equities currencies observe consistent performance gains compared static copula models time varying copula methods
bayesian inference online experimental design mapping neural microcircuits develop inference optimal design procedure recovering synaptic weights neural microcircuits base procedure data experiment populations putative presynaptic neurons stimulated subthreshold recording made single postsynaptic neuron present realistic statistical model accounts main sources variability experiment allows large amounts information biological system incorporated available present simpler model facilitate online experimental design entails use efficient bayesian inference optimized approach results equal quality posterior estimates synaptic weights roughly half number experimental trials experimentally realistic conditions tested synthetic data generated full model
test non parametric low variance kernel sample test propose family maximum mean discrepancy mmd kernel sample tests low sample complexity consistent test hyperparameter allows control tradeoff sample complexity computational time family tests denote tests computationally statistically efficient combining favorable properties previously proposed mmd sample tests better leveraging samples produce low variance estimates finite sample case avoiding quadratic number kernel evaluations complex null hypothesis approximation would required tests relying sample statistics test uses smaller quadratic number kernel evaluations avoids completely computational burden complex null hypothesis approximation maintaining consistency probabilistically conservative thresholds type error finally recent results combining multiple kernels transfer seamlessly hypothesis test allowing increase discriminative power decrease sample complexity
information theoretic lower bounds distributed statistical estimation communication constraints establish minimax risk lower bounds distributed statistical estimation given budget total number bits communicated lower bounds turn reveal minimum amount communication required procedure achieve classical optimal rate statistical estimation study classes protocols machines send messages either independently interactively lower bounds established variety problems estimating mean population estimating parameters linear regression binary classification
randomized dependence coefficient introduce randomized dependence coefficient rdc measure non linear dependence random variables arbitrary dimension based hirschfeld gebelein nyi maximum correlation coefficient rdc defined terms correlation random non linear copula projections invariant respect marginal distribution transformations low computational cost easy implement lines code included end paper
efficient optimization sparse gaussian process regression propose efficient discrete optimization algorithm selecting subset training data induce sparsity gaussian process regression algorithm estimates inducing set hyperparameters using single objective either marginal likelihood variational free energy space time complexity linear training set size algorithm applied large regression problems discrete continuous domains empirical evaluation shows state art performance discrete case competitive results continuous case
learning kernels using local rademacher complexity use notion local rademacher complexity design new algorithms learning kernels algorithms thereby benefit sharper learning bounds based notion certain general conditions guarantee faster convergence rate devise new learning kernel algorithms based convex optimization problem give efficient solution using existing learning kernel techniques another formulated programming problem describe solution detail also report results experiments algorithms binary multi class classification tasks
linear decision rule aspiration simple decision heuristics many attempts understand success simple decision heuristics examined heuristics approximation linear decision rule research identified environmental structures aid heuristics dominance cumulative dominance noncompensatoriness develop ideas examine empirical relevance natural environments find structures prevalent making possible simple rules reach accuracy levels linear decision rule using less information
shot learning big data model shot learning situation scalar observations y_1 y_n available associated observation y_i high dimensional vector x_i provides context y_i enables predict subsequent observations given context salient features analysis problems studied easier dimension x_i large words prediction becomes easier context provided proposed methodology variant principal component regression pcr rigorous analysis sheds new light pcr instance show classical pcr estimators inconsistent specified setting unless multiplied scalar unless classical estimator expanded expansion phenomenon appears somewhat novel contrasts shrinkage methods far common big data analyses
space partitioning tree use search consider task nearest neighbor search class binary space partitioning trees includes trees principal axis trees random projection trees try rigorously answer question tree use nearest neighbor search end present theoretical results imply trees better vector quantization performance better search performance guarantees also explore another factor affecting search performance margins partitions trees demonstrate theoretically empirically large margin partitions improve search performance space partitioning tree
bandits experts tale domination independence consider partial observability model multi armed bandits introduced mannor shamir 2011 main result characterization regret directed observability model terms dominating independence numbers observability graph also show undirected case learner achieve optimal regret without even accessing observability graph selecting action results shown using variants exp3 algorithm operating observability graph time efficient manner
sign cauchy projections chi square kernel method cauchy random projections popular computing l_1 distance high dimension paper propose use signs projected data show probability collision signs differ accurately approximated function chi square chi similarity popular measure nonnegative data features generated histograms common text vision applications experiments confirm method sign cauchy random projections promising large scale learning applications furthermore extend idea sign alpha stable random projections derive bound collision probability
online robust pca via stochastic optimization robust pca methods typically based batch optimization load samples memory prevents efficiently processing big data paper develop online robust principal component analysis pca processes sample per time instance hence memory cost independent data size significantly enhancing computation storage efficiency proposed method based stochastic optimization equivalent reformulation batch rpca method indeed show pca provides sequence subspace estimations converging optimum batch counterpart hence provably robust sparse corruption moreover pca naturally applied tracking dynamic subspace comprehensive simulations subspace recovering tracking demonstrate robustness efficiency advantages pca online pca batch rpca methods
density estimation unweighted nearest neighbor graphs roadmap consider unweighted nearest neighbor graph points sampled unknown density prove estimate density unweighted adjacency matrix graph without knowing points distance similarity scores key insights local differences link numbers used estimate local function integrating function along shortest paths leads estimate underlying density
message passing inference chemical reaction networks recent work molecular programming explored new possibilities computational abstractions biomolecules including logic gates neural networks linear systems future abstractions might enable nanoscale devices sense control world molecular scale macroscale robotics critical devices learn environment reason uncertainty small scale systems typically modeled chemical reaction networks work develop procedure take arbitrary probabilistic graphical models represented factor graphs discrete random variables compile chemical reaction networks implement inference particular show marginalization based sum product message passing implemented terms reactions chemical species whose concentrations represent probabilities show algebraically steady state concentration species correspond marginal distributions random variables graph validate results simulations standard sum product inference procedure yields exact results tree structured graphs approximate solutions loopy graphs
adaptivity local smoothness dimension kernel regression present first result kernel regression procedure adapts locally point unknown local dimension metric unknown lder continuity regression function result holds high probability simultaneously points metric space unknown structure
multi task bayesian optimization bayesian optimization recently proposed framework automatically tuning hyperparameters machine learning models shown yield state art performance impressive ease efficiency paper explore whether possible transfer knowledge gained previous optimizations new tasks order find optimal hyperparameter settings efficiently approach based extending multi task gaussian processes framework bayesian optimization show method significantly speeds optimization process compared standard single task approach propose straightforward extension algorithm order jointly minimize average error across multiple tasks demonstrate used greatly speed fold cross validation lastly significant contribution adaptation recently proposed acquisition function entropy search cost sensitive multi task settings demonstrate utility new acquisition function utilizing small dataset order explore hyperparameter settings large dataset algorithm dynamically chooses dataset query order yield information per unit cost
target algorithms infinite armed bandits bernoulli rewards consider infinite armed bandit problem bernoulli rewards mean rewards independent uniformly distributed rewards referred success failure respectively propose novel algorithm decision exploit arm based successive targets namely total number successes first failure first failures respectively fixed parameter target algorithm achieves long term average regret sqrt large parameter known time horizon regret optimal strictly less regret achieved best known algorithms sqrt results extended mean reward distribution whose support contains unknown time horizons numerical experiments show performance algorithm finite time horizons
fast algorithms gaussian noise invariant independent component analysis performance standard algorithms independent component analysis quickly deteriorates addition gaussian noise partially due common first step typically consists whitening applying principal component analysis pca rescaling components identity covariance invariant gaussian noise paper develop first practical algorithm independent component analysis provably invariant gaussian noise main contributions work follows develop implement efficient version gaussian noise invariant decorrelation quasi orthogonalization algorithm using hessians cumulant functions propose simple efficient fixed point ica gradient iteration ica algorithm compatible quasi orthogonalization well usual pca based whitening noiseless case algorithm based special form gradient iteration different gradient descent provide analysis algorithm demonstrating fast convergence following basic properties cumulants also present number experimental comparisons existing methods showing superior results noisy data competitive performance noiseless case
latent source model nonparametric time series classification classifying time series nearest neighbor approach widely used practice performance often competitive better elaborate methods neural networks decision trees support vector machines develop theoretical justification effectiveness nearest neighbor like classification time series guiding hypothesis many applications forecasting topics become trends twitter actually many prototypical time series begin relative number time series access topics become trends twitter distinct manners whereas collect massive amounts twitter data operationalize hypothesis propose latent source model time series naturally leads weighted majority voting classification rule approximated nearest neighbor classifier establish nonasymptotic performance guarantees weighted majority voting nearest neighbor classification model accounting much time series observe model complexity experimental results synthetic data show weighted majority voting achieving misclassification rate nearest neighbor classification observing less time series use weighted majority forecast news topics twitter become trends able detect trending topics advance twitter time mean early advantage hour minutes true positive rate false positive rate
spike train entropy rate estimation using hierarchical dirichlet process priors entropy rate quantifies amount disorder stochastic process spiking neurons entropy rate places upper bound rate spike train convey stimulus information large literature focused problem estimating entropy rate spike train data present bayes least squares empirical bayesian entropy rate estimators binary spike trains using hierarchical dirichlet process hdp priors estimator leverages fact entropy rate ergodic markov chain known transition probabilities calculated analytically many stochastic processes non markovian still well approximated markov processes sufficient depth choosing appropriate depth markov model presents challenges due possibly long time dependencies short data sequences deeper model better account long time dependencies difficult infer limited data approach mitigates difficulty using hierarchical prior share statistical power across markov chains different depths present fully bayesian empirical bayes entropy rate estimator based model demonstrate performance simulated real neural spike train data
online variational approximations non exponential family change point models application radar tracking bayesian online change point detection bocpd algorithm provides efficient way exact inference parameters underlying model suddenly change time bocpd requires computation underlying model posterior predictives computed online time memory exponential family models develop variational approximations posterior change point times formulated run lengths efficient inference underlying model exponential family tractable posterior predictive distributions develop improvements online variational inference apply methodology tracking problem using radar data signal noise feature rice distributed also develop variational method inferring parameters non exponential family rice distribution
adaptive market making via online learning consider design strategies emph market making market like stock commodity currency exchange order obtain profit guarantees market maker typically requires particular stochastic assumptions sequence price fluctuations asset question propose class spread based market making strategies whose performance controlled even worst case adversarial settings prove structural properties strategies allows design master algorithm obtains low regret relative best strategy hindsight run set experiments showing favorable performance real world price data
small variance asymptotics hidden markov models small variance asymptotics provide emerging technique obtaining scalable combinatorial algorithms rich probabilistic models present small variance asymptotic analysis hidden markov model infinite state bayesian nonparametric extension starting standard hmm first derive hard inference algorithm analogous means arises particular variances model tend analysis extended bayesian nonparametric case yielding simple scalable flexible algorithm discrete state sequence data non fixed number states also derive corresponding combinatorial objective functions arising analysis involve means like term along penalties based state transitions number states key property algorithms particularly nonparametric setting standard probabilistic inference algorithms lack scalability heavily dependent good initialization number results synthetic real data sets demonstrate advantages proposed framework
probabilistic principal geodesic analysis principal geodesic analysis pga generalization principal component analysis pca dimensionality reduction data riemannian manifold currently pga defined geometric fit data rather probabilistic model inspired probabilistic pca present latent variable model pga provides probabilistic framework factor analysis manifolds compute maximum likelihood estimates parameters model develop monte carlo expectation maximization algorithm expectation approximated hamiltonian monte carlo sampling latent variables demonstrate ability method recover ground truth parameters simulated sphere data well effectiveness analyzing shape variability corpus callosum data set human brain images
regularized estimators nonconvexity statistical algorithmic theory local optima establish theoretical results concerning local optima various regularized estimators loss penalty functions allowed nonconvex results show long loss function satisfies restricted strong convexity penalty function satisfies suitable regularity conditions local optimum composite objective function lies within statistical precision true parameter vector theory covers broad class nonconvex objective functions including corrected versions lasso errors variables linear models regression generalized linear models using nonconvex regularizers scad mcp graph inverse covariance matrix estimation optimization side show simple adaptation composite gradient descent used compute global optimum statistical precision epsilon log epsilon iterations fastest possible rate first order method provide variety simulations illustrate sharpness theoretical predictions
fast convergence incremental pca prove first finite sample convergence rates incremental pca algorithm using sub quadratic time memory per iteration algorithm analyzed oja learning rule efficient well known scheme estimating top principal component analysis non convex problem yields expected high probability convergence rates tilde novel technique relate guarantees existing rates stochastic gradient descent strongly convex functions extend results also include experiments demonstrate convergence behaviors predicted analysis
robust low rank kernel embeddings multivariate distributions kernel embedding distributions led many recent advances machine learning however latent low rank structures prevalent real world distributions rarely taken account setting furthermore prior work kernel embedding literature addressed issue robust embedding latent low rank information misspecified paper propose hierarchical low rank decomposition kernels embeddings exploit low rank structures data robust model misspecification also illustrate empirical evidence estimated low rank embeddings lead improved performance density estimation
optimizing instructional policies psychologists interested developing instructional policies boost student learning instructional policy specifies manner content instruction example domain concept learning policy might specify nature exemplars chosen training sequence traditional psychological studies compare several hand selected policies contrasting policy selects difficult classify exemplars policy gradually progresses training sequence easy exemplars difficult known fading propose alternative traditional methodology define parameterized space policies search space identify optimum policy example concept learning policies might described fading function specifies exemplar difficulty time propose experimental technique searching policy spaces using gaussian process surrogate based optimization generative model student performance instead evaluating experimental conditions many human subjects traditional methodology technique evaluates many experimental conditions subjects even though individual subjects provide noisy estimate population mean optimization method allows determine shape policy space identify global optimum efficient subject budget traditional comparison evaluate method via behavioral studies suggest method broad applicability optimization problems involving humans domains beyond educational arena
statistical analysis coupled time series kernel cross spectral density operators many applications require analysis complex interactions time series interactions non linear involve vector valued well complex data structures graphs strings provide general framework statistical analysis interactions random variables sampled stationary time series arbitrary objects achieve goal analyze properties kernel cross spectral density operator induced positive definite kernels arbitrary input domains framework enables develop independence test time series well similarity measure compare different types coupling performance test compared hsic test using assumptions showing improvement terms detection errors well suitability approach testing dependency complex dynamical systems finally use approach characterize complex interactions electrophysiological neural time series
bayesian mixture modelling inference based thompson sampling monte carlo tree search monte carlo tree search drawing great interest domain planning uncertainty particularly little domain knowledge available central problems trade exploration exploitation paper present novel bayesian mixture modelling inference based thompson sampling approach addressing dilemma proposed dirichlet normalgamma mcts dng mcts algorithm represents uncertainty accumulated reward actions mcts search tree mixture normal distributions inferences bayesian settings choosing conjugate priors form combinations dirichlet normalgamma distributions thompson sampling used select best action decision node experimental results show proposed algorithm achieved state art comparing popular uct algorithm context online planning general markov decision processes
lasso screening rules via dual polytope projection lasso widely used regression technique find sparse representations dimension feature space number samples extremely large solving lasso problem remains challenging improve efficiency solving large scale lasso problems ghaoui colleagues proposed safe rules able quickly identify inactive predictors predictors components solution vector inactive predictors features removed optimization problem reduce scale transforming standard lasso dual form shown inactive predictors include set inactive constraints optimal dual solution paper propose efficient effective screening rule via dual polytope projections dpp mainly based uniqueness nonexpansiveness optimal dual solution due fact feasible set dual space convex closed polytope moreover show screening rule extended identify inactive groups group lasso best knowledge currently exact screening rule group lasso evaluated screening rule using many real data sets results show rule effective identify inactive predictors existing state art screening rules lasso
total variation hypergraphs learning hypergraphs revisited hypergraphs allow encode higher order relationships data thus flexible modeling tool current learning methods either based approximations hypergraphs via graphs tensor methods applicable special conditions paper present new learning framework hypergraphs fully uses hypergraph structure key element family regularization functionals based total variation hypergraphs
robust transfer principal component analysis rank constraints principal component analysis pca well established technique data analysis processing provides convenient form dimensionality reduction effective cleaning small gaussian noises presented data however applicability standard principal component analysis real scenarios limited sensitivity large errors paper tackle challenge problem recovering data corrupted errors high magnitude developing novel robust transfer principal component analysis method method based assumption useful information recovery corrupted data matrix gained uncorrupted related data matrix speci cally formulate data recovery problem joint robust principal component analysis problem data matrices shared common principal components across matrices individual principal components speci data matrix formulated optimization problem minimization problem convex objective function non convex rank constraints develop cient proximal projected gradient descent algorithm solve proposed optimization problem convergence guarantees empirical results image denoising tasks show proposed method effectively recover images random large errors signi cantly outperform standard pca robust pca
regression tree tuning streaming setting consider problem maintaining data structures partition based regression procedure setting training data arrives sequentially time prove possible maintain structure time log time step achieving nearly optimal regression rate tilde terms unknown metric dimension finally prove new regression lower bound independent given data size hence appropriate streaming setting
synthesizing robust plans incomplete domain models current planners assume complete domain models focus generating correct plans unfortunately domain modeling laborious error prone task thus real world agents plan incomplete domain models domain experts cannot guarantee completeness often able circumscribe incompleteness model providing annotations parts domain model incomplete cases goal synthesize plans robust respect known incompleteness domain paper first introduce annotations expressing knowledge domain incompleteness formalize notion plan robustness respect incomplete domain model show approach compiling problem finding robust plans conformant probabilistic planning problem present experimental results probabilistic planner
unsupervised structure learning stochastic grammars stochastic grammars compactly represent compositionality reconfigurability used model different types data images events present unified formalization stochastic grammars agnostic type data modeled propose unsupervised approach learning structures well parameters grammars starting trivial initial grammar approach iteratively induces compositions reconfigurations unified manner optimizes posterior probability grammar empirical evaluation applied approach learning event grammars image grammars achieved comparable better performance previous approaches
sequential transfer multi armed bandit finite set models learning prior tasks transferring experience improve future performance critical building lifelong learning agents although results supervised reinforcement learning show transfer significantly improve learning performance literature transfer focused batch learning tasks paper study problem sequential transfer online learning notably multi arm bandit framework objective minimize cumulative regret sequence tasks incrementally transferring knowledge prior tasks introduce novel bandit algorithm based method moments approach estimation possible tasks derive regret bounds
faster ridge regression via subsampled randomized hadamard transform propose fast algorithm ridge regression number features much larger number observations standard way solve ridge regression setting works dual space gives running time algorithm srht drr runs time log works preconditioning design matrix randomized walsh hadamard transform subsequent subsampling features provide risk bounds srht drr algorithm fixed design setting show experimental results synthetic real datasets
robust bloom filters large multilabel classification tasks paper presents approach multilabel classification mlc large number labels approach reduction binary classification label sets represented low dimensional binary vectors representation follows principle bloom filters space efficient data structure originally designed approximate membership testing show naive application bloom filters mlc robust individual binary classifiers errors present approach exploits specific feature real world datasets number labels large many labels almost never appear together approch provably robust sublinear training inference complexity respect number labels compares favorably state art algorithms large scale multilabel datasets
sketching structured matrices faster nonlinear regression motivated desire extend fast randomized techniques nonlinear l_p regression consider class structured regression problems problems involve vandermonde matrices arise naturally various statistical modeling settings including classical polynomial fitting problems recently developed randomized techniques scalable kernel methods show structure exploited accelerate solution regression problem achieving running times faster input sparsity present empirical results confirming practical value modeling framework well speedup benefits randomized regression
thompson sampling dimensional exponential family bandits thompson sampling demonstrated many complex bandit models however theoretical guarantees available parametric multi armed bandit still limited bernoulli case extend proving asymptotic optimality algorithm using jeffreys prior dimensional exponential family bandits proof builds previous work also makes extensive use closed forms kullback leibler divergence fisher information thus jeffreys prior available exponential family allow give finite time exponential concentration inequality posterior distributions exponential families interest right moreover analysis covers distributions optimistic algorithm yet proposed including heavy tailed exponential families
effective distributed via stale synchronous parallel parameter server propose parameter server system distributed follows stale synchronous parallel ssp model computation maximizes time computational workers spend useful work algorithms still providing correctness guarantees parameter server provides easy use shared interface read write access model values parameters variables ssp model allows distributed workers read older stale versions values local cache instead waiting get central storage significantly increases proportion time workers spend computing opposed waiting furthermore ssp model ensures algorithm correctness limiting maximum age stale values provide proof correctness ssp well empirical results demonstrating ssp model achieves faster algorithm convergence several different problems compared fully synchronous asynchronous schemes
data speeds training time learning halfspaces sparse vectors increased availability data recent years led several authors ask whether possible use data computational resource data available beyond sample complexity limit possible use extra examples speed computation time required perform learning task give first positive answer question natural supervised learning problem consider agnostic pac learning halfspaces sparse vectors class inefficiently learnable using left epsilon right examples main contribution novel non cryptographic methodology establishing computational statistical gaps allows show widely believed assumption refuting random mathrm 3cnf formulas hard efficiently learning class using left epsilon right examples impossible show stronger hardness assumptions even left 499 epsilon right examples suffice hand show new algorithm learns class efficiently using tilde omega left epsilon right examples formally establishes tradeoff sample computational complexity natural supervised learning problem
predictive pac learning process decompositions informally call stochastic process learnable admits generalization error approaching probability concept class finite dimension iid processes simplest example mixture learnable processes need learnable certainly generalization error need decay rate paper argue natural predictive pac condition past observations mixture component sample path definition matches realistic learner might demand also allows sidestep several otherwise grave problems learning dependent data particular give novel pac generalization bound mixtures learnable processes generalization error worse mixture component also provide characterization mixtures absolutely regular beta mixing processes independent interest
distributed means median clustering general communication topologies paper provides new algorithms distributed clustering popular center based objectives median means algorithms provable guarantees improve communication complexity existing approaches following classic approach clustering cite har2004coresets reduce problem finding clustering low cost problem finding coreset small size provide distributed method constructing global coreset improves previous methods reducing communication complexity works general communication topologies provide experimental evidence approach synthetic real data sets
convex calibrated surrogates low rank loss matrices applications subset ranking losses design convex calibrated surrogate losses whose minimization entails consistency respect desired target loss important concept emerged theory machine learning recent years give explicit construction convex least squares type surrogate loss designed calibrated multiclass learning problem target loss matrix low rank structure surrogate loss operates surrogate target space dimension rank target loss use result design convex calibrated surrogates variety subset ranking problems target losses including precision expected rank utility mean average precision pairwise disagreement
statistical active learning algorithms describe framework designing efficient active learning algorithms tolerant random classification noise framework based active learning algorithms statistical sense rely estimates expectations functions filtered random examples builds powerful statistical query framework kearns 1993 show efficient active statistical learning algorithm automatically converted efficient active learning algorithm tolerant random classification noise well forms uncorrelated noise complexity resulting algorithms information theoretically optimal quadratic dependence eta eta noise rate demonstrate power framework showing commonly studied concept classes including thresholds rectangles linear separators efficiently actively learned framework results combined generic conversion lead first known computationally efficient algorithms actively learning concept classes presence random classification noise provide exponential improvement dependence error epsilon passive counterparts addition show algorithms automatically converted efficient active differentially private algorithms leads first differentially private active learning algorithms exponential label savings passive case
multiclass total variation clustering ideas image processing literature recently motivated new set clustering algorithms rely concept total variation algorithms perform well partitioning tasks recursive extensions yield unimpressive results multiclass clustering tasks paper presents general framework multiclass total variation clustering rely recursion results greatly outperform previous total variation algorithms compare well state art nmf approaches
prior free prior dependent regret bounds thompson sampling consider stochastic multi armed bandit problem prior distribution reward distributions interested studying prior free prior dependent regret bounds much spirit usual distribution free distribution dependent bounds non bayesian stochastic bandit first show thompson sampling attains optimal prior free bound sense prior distribution bayesian regret bounded sqrt result unimprovable sense exists prior distribution algorithm bayesian regret bounded frac sqrt also study case priors setting bubeck 2013 optimal mean known well lower bound smallest gap show case regret thompson sampling fact uniformly bounded time thus showing thompson sampling greatly take advantage nice properties priors
row column marginals reveal dataset numerous datasets ranging group memberships within social networks purchase histories commerce sites represented binary matrices data often either proprietary sensitive aggregated data notably row column marginals often viewed much less sensitive furnished analysis investigate data exploited make inferences underlying matrix instead assuming generative model view input marginals constraints dataspace possible realizations compute probability density function particular entries interest cells simultaneously without generating realizations rather via implicitly sampling datasets satisfy input marginals end result efficient algorithm running time equal time required standard sampling techniques generate single dataset dataspace experimental evaluation demonstrates efficiency efficacy framework multiple settings
solving inverse problem markov chain partial observations markov chain convenient tool represent dynamics complex systems traffic social systems probabilistic transition takes place internal states markov chain characterized initial state probabilities state transition probability matrix traditional setting major goal figure properties markov chain probabilities known paper tackles inverse version problem find probabilities partial observations limited number states observations include frequency visiting state rate reaching state another practical examples task include traffic monitoring systems cities need infer traffic volume every single link road network limited number observation points formulate task regularized optimization problem probability functions efficiently solved using notion natural gradient using synthetic real world data sets including city traffic monitoring data demonstrate effectiveness method
pac bayes empirical bernstein inequality present pac bayes empirical bernstein inequality inequality based combination pac bayesian bounding technique empirical bernstein bound allows take advantage small empirical variance especially useful regression show empirical variance significantly smaller empirical loss pac bayes empirical bernstein inequality significantly tighter pac bayes inequality seeger 2002 otherwise comparable pac bayes empirical bernstein inequality interesting example application pac bayesian bounding technique self bounding functions provide empirical comparison pac bayes empirical bernstein inequality pac bayes inequality synthetic example several uci datasets
approximate inference continuous determinantal processes determinantal point processes dpps random point processes well suited modeling repulsion machine learning focus dpp based models diverse subset selection discrete finite base set discrete setting admits efficient algorithm sampling based eigendecomposition defining kernel matrix recently growing interest using dpps defined continuous spaces discrete dpp sampler extends formally continuous case computationally steps required cannot directly extended except restricted cases paper present efficient approximate dpp sampling schemes based nystrom random fourier feature approximations apply wide range kernel functions demonstrate utility continuous dpps repulsive mixture modeling applications synthesizing human poses spanning activity spaces
inverse density inverse problem fredholm equation approach address problem estimating ratio frac density function another density generally arbitrary function knowing approximating ratio needed various problems inference integration particular needs average function respect probability distribution given sample another often referred importance sampling statistical inference also closely related problem covariate shift transfer learning well various mcmc methods approach based reformulating problem estimating ratio inverse problem terms integral operator corresponding kernel thus reducing integral equation known fredholm problem first kind formulation combined techniques regularization kernel methods leads principled kernel based framework constructing algorithms analyzing theoretically resulting family algorithms fire fredholm inverse regularized estimator flexible simple easy implement provide detailed theoretical analysis including concentration bounds convergence rates gaussian kernel densities defined smooth dimensional sub manifolds euclidean space model selection unsupervised semi supervised inference generally difficult problem interestingly turns density ratio estimation setting samples distributions available simple completely unsupervised methods choosing parameters call model selection mechanism cross density cross validation finally show encouraging experimental results including applications classification within covariate shift framework
robust data driven dynamic programming stochastic optimal control distribution exogenous noise typically unknown must inferred limited data dynamic programming based solution schemes applied conditional expectations recursions estimated via kernel regression however historical sample paths enter solution procedure directly determine evaluation points cost functions resulting data driven scheme asymptotically consistent admits efficient computational solution combined parametric value function approximations training data sparse however estimated cost functions display high variability optimistic bias corresponding control policies perform poorly sample tests mitigate small sample effects propose robust data driven scheme replaces expectations recursions worst case expectations set distributions close best estimate show arising min max problems recursions reduce tractable conic programs also demonstrate robust algorithm dominates state art benchmark algorithms sample tests across several application domains
visual concept learning combining machine vision bayesian generalization concept hierarchies learning visual concept small number positive examples significant challenge machine learning algorithms current methods typically fail find appropriate level generalization concept hierarchy given set visual examples recent work cognitive science bayesian models generalization addresses challenge prior results assumed objects perfectly recognized present algorithm learning visual concepts directly images using probabilistic predictions generated visual classifiers input bayesian generalization model existing challenge data tests paradigm collect make available new large scale dataset visual concept learning using imagenet hierarchy source possible concepts human annotators provide ground truth labels whether new image instance concept using paradigm similar used experiments studying word learning children compare performance system several baseline algorithms show significant advantage results combining visual classifiers ability identify appropriate level abstraction using bayesian generalization
learning gaussian graphical models observed latent fvss gaussian graphical models ggms gauss markov random fields widely used many applications trade modeling capacity efficiency learning inference important research problem paper study family ggms small feedback vertex sets fvss fvs set nodes whose removal breaks cycles exact inference computing marginal distributions partition function complexity using message passing algorithms size fvs total number nodes propose efficient structure learning algorithms cases nodes observed useful modeling social flight networks fvs nodes often correspond small number high degree nodes hubs rest networks modeled tree regardless maximum degree without knowing full graph structure exactly compute maximum likelihood estimate log fvs known polynomial time fvs unknown bounded size fvs nodes latent variables structure learning equivalent decomposing inverse covariance matrix exactly approximately sum tree structured matrix low rank matrix incorporating efficient inference learning steps obtain learning algorithm using alternating low rank correction complexity log per iteration also perform experiments using synthetic data well real data flight delays demonstrate modeling capacity fvss various sizes show empirically family ggms size log strikes good balance modeling capacity efficiency
lexical hierarchical topic regression inspired level theory unifies agenda setting ideological framing propose supervised hierarchical latent dirichlet allocation shlda jointly captures documents multi level topic structure polar response variables model extends nested chinese restaurant process discover tree structured topic hierarchy uses per topic hierarchical per word lexical regression parameters model response variables experiments political domain sentiment analysis tasks show shlda improves predictive accuracy adding new dimension insight topics discussion framed
poisson graphical models undirected graphical models gaussian graphical models ising multinomial categorical graphical models widely used variety applications modeling distributions large number variables standard instances however ill suited modeling count data increasingly ubiquitous big data settings genomic sequencing data user ratings data spatial incidence data climate studies site visits existing classes poisson graphical models arise joint distributions correspond poisson distributed node conditional distributions major drawback model negative conditional dependencies reasons normalizability given infinite domain paper objective modify poisson graphical model distribution capture rich dependence structure count valued variables begin discussing strategies truncating poisson distribution show leads valid joint distribution even model however limitations types variables dependencies modeled address propose novel variants poisson distribution corresponding joint graphical model distributions models provide class poisson graphical models capture positive negative conditional dependencies count valued variables learn graph structure model via penalized neighborhood selection demonstrate performance methods learning simulated networks well network microrna sequencing data
optimization learning games predictable sequences provide several applications optimistic mirror descent online learning algorithm based idea predictable sequences first recover mirror prox algorithm prove extension holder smooth functions apply results saddle point type problems second prove version optimistic mirror descent close relation exponential weights algorithm used strongly uncoupled players finite sum matrix game converge minimax equilibrium rate log addresses question daskalakis 2011 consider partial information version problem apply results approximate convex programming show simple algorithm approximate max flow problem
deep neural networks object detection deep neural networks dnns recently shown outstanding performance task whole image classification paper step address problem object detection classifying also precisely localizing objects various classes using dnns present simple yet powerful formulation object detection regression object masks define multi scale inference procedure able produce high resolution object detection low cost network applications approach achieves state art performance pascal 2007 voc
action eye beholder eye gaze driven model spatio temporal action localization propose new weakly supervised structured learning approach recognition spatio temporal localization actions video part proposed approach develop generalization max path search algorithm allows efficiently search structured space multiple spatio temporal paths also allowing incorporate context information model instead using spatial annotations form bounding boxes guide latent model training utilize human gaze data form weak supervisory signal achieved incorporating gaze along classification structured loss within latent svm learning framework experiments challenging benchmark dataset ucf sports show model accurate terms classification achieves state art results localization addition show model produce top saliency maps conditioned classification label localized latent paths
big quic sparse inverse covariance estimation million variables regularized gaussian maximum likelihood estimator mle shown strong statistical guarantees recovering sparse inverse covariance matrix even high dimensional settings however requires solving difficult non smooth log determinant program number parameters scaling quadratically number gaussian variables state art methods thus scale problems 000 variables paper develop algorithm bigquic solve million dimensional regularized gaussian mle problems would thus 1000 billion parameters using single machine bounded memory order carefully exploit underlying structure problem innovations include novel block coordinate descent method blocks chosen via clustering scheme minimize repeated computations allowing inexact computation specific components spite modifications able theoretically analyze procedure show bigquic achieve super linear even quadratic convergence rates
probabilistic movement primitives movement primitives well established approach representing modular usable robot movement generators many state art robot learning successes based mps due compact representation inherently continuous high dimensional robot movements major goal robot learning combine multiple mps building blocks modular control architecture solve complex tasks effect representation allow blending motions adapting altered task variables activating multiple mps parallel present probabilistic formulation concept maintains distribution trajectories probabilistic approach allows derivation new operations essential implementing aforementioned properties framework order use trajectory distribution robot movement control analytically derive stochastic feedback controller reproduces given trajectory distribution evaluate compare approach existing methods several simulated well real robot scenarios
estimation bias multi armed bandit algorithms search advertising search advertising search engine needs select profitable advertisements display formulated instance online learning partial feedback also known stochastic multi armed bandit mab problem paper show naive application mab algorithms search advertising advertisement selection produce sample selection bias harms search engine decreasing expected revenue estimation largest mean elm bias harms advertisers increasing game theoretic player regret propose simple bias correction methods benefits search engine advertisers
bellman error based feature generation using random projections sparse spaces paper addresses problem automatic generation features value function approximation reinforcement learning bellman error basis functions bebfs shown improve error policy evaluation function approximation convergence rate similar value iteration propose simple fast robust algorithm based random projections generates bebfs sparse feature spaces provide finite sample analysis proposed method prove projections logarithmic dimension original space guarantee contraction error empirical results demonstrate strength method domains choosing good state representation challenging
learning local statistics optical flow motivated recent progress natural image statistics use newly available datasets ground truth optical flow learn local statistics optical flow rigorously compare learned model prior models assumed computer vision optical flow algorithms find gaussian mixture model components provides significantly better model local flow statistics compared commonly used models investigate source gmms success show related explicit representation flow boundaries also learn model jointly models local intensity pattern local optical flow accordance assumptions often made computer vision model learns flow boundaries likely intensity boundaries however evaluated large dataset dependency weak benefit conditioning flow estimation local intensity pattern marginal
mapping paradigm ontologies brain imaging neuroscience links brain activation maps behavior cognition via correlational studies due nature individual experiments based eliciting neural response small number stimuli link incomplete unidirectional causal point view come conclusions function implied activation brain regions necessary combine wide exploration various brain functions inversion statistical inference introduce methodology accumulating knowledge towards bidirectional link observed brain activity corresponding function rely large corpus imaging studies predictive engine technically challenges find commonality studies without denaturing richness corpus key elements contribute labeling tasks performed cognitive ontology modeling long tail rare paradigms corpus knowledge approach first demonstration predicting cognitive content completely new brain images end propose method predicts experimental paradigms across different studies
factorized asymptotic bayesian inference latent feature models paper extends factorized asymptotic bayesian fab inference latent feature models lfms fab inference applicable models including lfms without specific condition hesqsian matrix complete log likelihood required derive factorized information criterion fic asymptotic analysis hessian matrix lfms shows fic lfms form mixture models fab lfms several desirable properties automatic hidden states selection parameter identifiability empirically perform better state art indian buffet processes terms model selection prediction computational efficiency
variational policy search via trajectory optimization order learn effective control policies dynamical systems policy search methods must able discover successful executions desired task random exploration work well simple domains complex high dimensional tasks present serious challenge particularly combined high dimensional policies make parameter space exploration infeasible present method uses trajectory optimization powerful exploration strategy guides policy search variational decomposition maximum likelihood policy objective allows use standard trajectory optimization algorithms differential dynamic programming interleaved standard supervised learning policy demonstrate resulting algorithm outperform prior methods challenging locomotion tasks
forgetful bayes myopic planning human learning decision making bandit setting humans achieve long term goals uncertain environment via repeated trials noisy observations important problem cognitive science investigate behavior context multi armed bandit task compare human behavior variety models vary representational computational complexity result shows subjects choices trial trial basis best captured forgetful bayesian iterative learning model combination partially myopic decision policy known knowledge gradient model accounts subjects trial trial choice better number previously proposed models including optimal bayesian learning risk minimization epsilon greedy win stay lose shift added benefit closest performance optimal bayesian model heuristic models computational complexity significantly less complex optimal model results constitute advancement theoretical understanding humans negotiate tension exploration exploitation noisy imperfectly known environment
pareto regret frontier performance guarantees online learning algorithms typically take form regret bounds express cumulative loss overhead compared best expert hindsight small common case large structured expert sets typically wish keep regret especially small compared simple experts cost modest additional overhead compared complex others study regret trade offs achieved analyse regret individual expert multi objective criterion simple fundamental case absolute loss characterise achievable pareto optimal trade offs corresponding optimal strategies sample size exactly finite horizon asymptotically
novel step method cross language representation learning cross language text classi cation important learning task natural language processing critical challenge cross language learning lies words different languages disjoint feature spaces paper propose step representation learning method bridge feature spaces different languages exploiting set parallel bilingual documents speci cally rst formulate matrix completion problem produce complete parallel document term matrix documents languages induce cross lingual document representation applying latent semantic indexing obtained matrix use projected gradient descent algorithm solve formulated matrix completion problem convergence guarantees proposed approach evaluated conducting set experiments cross language sentiment classi cation tasks amazon product reviews experimental results demonstrate proposed learning approach outperforms number comparison cross language representation learning methods especially number parallel bilingual documents small
mid level visual element discovery discriminative mode seeking recent work mid level visual representations aims capture information level complexity higher typical visual words lower full blown semantic objects several approaches proposed discover mid level visual elements representative frequently occurring within visual dataset visually discriminative however current approaches rather hoc difficult analyze evaluate work pose visual element discovery discriminative mode seeking drawing connections well known well studied mean shift algorithm given weakly labeled image collection method discovers visually coherent patch clusters maximally discriminative respect labels advantage formulation requires single pass data also propose purity coverage plot principled way experimentally analyzing evaluating different visual discovery approaches compare method prior work paris street view dataset also evaluate method task scene classification demonstrating state art performance mit scene dataset
online learning costly features labels paper introduces online probing problem round learner able purchase values subset feature values learner uses information come prediction given round option paying seeing loss evaluated either way learner pays imperfections predictions whatever chooses observe including cost observing loss function given round cost observed features consider variations problem depending whether learner observe label free provide algorithms upper lower bounds regret variants show positive cost observing label significantly increases regret problem
near optimal anomaly detection graphs using lovasz extended scan statistic detection anomalous activity graphs statistical problem arises many applications network surveillance disease outbreak detection activity monitoring social networks beyond wide applicability graph structured anomaly detection serves case study difficulty balancing computational complexity statistical power work develop first principles generalized likelihood ratio test determining well connected region activation vertices graph gaussian noise test computationally infeasible provide relaxation called lov asz extended scan statistic less uses submodularity approximate intractable generalized likelihood ratio demonstrate connection less maximum posteriori inference markov random fields provides poly time algorithm less using electrical network theory able control type error less prove conditions less risk consistent finally consider specific graph models torus nearest neighbor graphs epsilon random graphs show graphs results provide near optimal performance matching results known lower bounds
convergence monte carlo tree search simultaneous move games paper study monte carlo tree search mcts sum extensive form games perfect information simultaneous moves present general template mcts algorithms games instantiated various selection methods formally prove selection method epsilon hannan consistent matrix game satisfies additional requirements exploration mcts algorithm eventually converges approximate nash equilibrium extensive form game empirically evaluate claim using regret matching exp3 selection methods randomly generated worst case games confirm formal result show additional mcts variants also converge approximate evaluated games
sparse additive text models low rank background sparse additive model text modeling involves sum exp computing consuming costs large scales moreover assumption equal background across classes topics strong paper extends propose sparse additive model low rank background sam lrb simple yet efficient estimation particularly employing double majorization bound approximate log likelihood quadratic lower bound sum exp terms absent constraints low rank sparsity simply embodied nuclear norm ell_1 norm regularizers interestingly find optimization task manner transformed form robust pca consequently parameters supervised sam lrb efficiently learned using existing algorithm robust pca based accelerated proximal gradient besides supervised case extend sam lrb also favor unsupervised multifaceted scenarios experiments real world data demonstrate effectiveness efficiency sam lrb showing state art performances
algorithms sparse multi factor nmf nonnegative matrix factorization nmf popular data analysis method objective decompose matrix nonnegative components product nonnegative matrices work describe new simple efficient algorithm multi factor nonnegative matrix factorization problem mfnmf generalizes original nmf problem factors furthermore extend mfnmf algorithm incorporate regularizer based dirichlet distribution normalized columns encourage sparsity obtained factors sparse nmf algorithm affords closed form intuitive interpretation efficient comparison previous works use fix point iterations demonstrate effectiveness efficiency algorithms synthetic real data sets
generalizing analytic shrinkage arbitrary covariance structures analytic shrinkage statistical technique offers fast alternative cross validation regularization covariance matrices appealing consistency properties show proof consistency implies bounds growth rates eigenvalues dispersion often violated data prove consistency assumptions restrict covariance structure therefore better match real world data addition propose extension analytic shrinkage orthogonal complement shrinkage adapts covariance structure finally demonstrate superior performance novel approach data domains finance spoken letter optical character recognition neuroscience
reward mapping transfer long lived agents consider transfer knowledge previous tasks current task long lived bounded agents must solve sequence mdps finite lifetime novel aspect transfer approach reuse reward functions seem counterintuitive build insight recent work optimal rewards problem guiding agent behavior reward functions task specifying reward function help overcome computational bounds agent specifically use good guidance reward functions learned previous tasks sequence incrementally train reward mapping function maps task specifying reward functions good initial guidance reward functions subsequent tasks demonstrate approach substantially improve agent performance relative approaches including approach transfers policies
modeling clutter perception using parametric proto object partitioning visual clutter perception image crowded disordered affects aspects lives ranging object detection aesthetics yet relatively little effort made model important ubiquitous percept approach models clutter number proto objects segmented image proto objects defined groupings superpixels similar intensity color gradient orientation features introduce novel parametric method merging superpixels modeling mixture weibull distributions similarity distance statistics taking normalized number proto objects following partitioning estimate clutter perception validated model using new text image dataset realistic scenes rank ordered human raters clutter showed method predicted clutter extremely well spearman rho also outperformed existing clutter perception models even behavioral object segmentation ground truth conclude number proto objects image affects clutter perception number objects features
speeding permutation testing neuroimaging multiple hypothesis testing significant problem nearly neuroimaging studies order correct phenomena require reliable estimate family wise error rate fwer well known bonferroni correction method simple implement quite conservative substantially power study ignores dependencies test statistics permutation testing hand exact non parametric method estimating fwer given threshold acceptably low thresholds computational burden prohibitive paper observe permutation testing fact amounts populating columns large matrix analyzing spectrum matrix certain conditions see low rank plus low variance residual decomposition makes suitable highly sub sampled order matrix completion methods thus propose novel permutation testing methodology offers large speedup without sacrificing fidelity estimated fwer valuations different neuroimaging datasets show computational speedup factor roughly achieved recovering fwer distribution high accuracy show estimated threshold also recovered faithfully stable
learning invariant representations applications face verification approach computer object recognition modeling brain ventral stream involves unsupervised learning representations invariant common transformations however applications ideas usually limited affine transformations translation scaling since easiest solve via convolution accord recent theory transformation invariance propose model capturing common convolutional networks special cases also used arbitrary identity preserving transformations model wiring learned videos transforming objects grouping images sets depicted object series successively complex empirical tests study invariance discriminability properties model respect different transformations first empirically confirm theoretical predictions case affine transformations next apply model non affine transformations expected performs well face verification tasks requiring invariance relatively smooth transformations rotation depth changes illumination direction surprisingly also tolerate clutter transformations map image face background image face different background motivated empirical findings tested model face verification benchmark tasks computer vision literature labeled faces wild pubfig new dataset gathered achieving strong performance highly unconstrained cases well
binary bushy bayesian hierarchical clustering beta coalescent discovering hierarchical regularities data key problem interacting large datasets modeling cognition encoding knowledge previous bayesian solution kingman coalescent provides convenient probabilistic model data represented binary tree unfortunately inappropriate data better described bushier trees generalize existing belief propagation framework kingman coalescent beta coalescent models wider range tree structures complex combinatorial search possible structures develop new sampling schemes using sequential monte carlo dirichlet process mixture models render inference efficient tractable present results synthetic real data show beta coalescent outperforms kingman coalescent real datasets qualitatively better capturing data bushy hierarchies
heterogeneous neighborhood based multi task local learning algorithms existing multi task local learning methods defined homogeneous neighborhood consists data points task paper different existing methods propose local learning methods multi task classification regression problems based heterogeneous neighborhood defined data points tasks specifically extend nearest neighbor classifier formulating decision function data point weighted voting among neighbors tasks weights task specific defining regularizer enforce task specific weight matrix approach symmetric regularized objective function proposed efficient coordinate descent method developed solve regression problems extend kernel regression multi task setting similar way classification case experiments toy data real world datasets demonstrate effectiveness proposed methods
action still image dataset inverse optimal control learn task specific visual scanpaths human eye movements provide rich source information human visual processing complex interplay task visual stimulus believed determine human eye movements yet fully understood precluded development reliable dynamic eye movement prediction systems work makes contributions towards addressing problem first complement largest challenging static computer vision datasets voc 2012 actions human eye movement annotations collected task constraints action context recognition dataset unique among eyetracking datasets still images terms large scale million fixations 9157 images task control action single image emphasis second introduce models automatically discover areas interest aoi introduce novel dynamic consistency metrics based method automatically determine number spatial support aois addition locations based encodings show unconstrained read world stimuli task instructions significant influence visual behavior finally leverage large scale dataset conjunction powerful machine learning techniques computer vision features introduce novel dynamic eye movement prediction methods learn task sensitive reward functions eye movement data efficiently integrate rewards plan future saccades based inverse optimal control show propose methodology achieves state art scanpath modeling results
beyond pairwise provably fast algorithms approximate way similarity search beyond notion pairwise similarity look search problems way similarity functions paper focus problems related emph way jaccard similarity mathcal 3way frac s_1 cap s_2 cap s_3 s_1 cup s_2 cup s_3 s_1 s_2 s_3 mathcal mathcal size collection sets binary vectors show approximate mathcal 3way similarity search problems admit fast algorithms provable guarantees analogous pairwise case analysis speedup guarantees naturally extend way resemblance process extend traditional framework emph locality sensitive hashing lsh handle higher order similarities could independent theoretical interest applicability mathcal 3way search shown google sets application addition demonstrate advantage mathcal 3way resemblance pairwise case improving retrieval quality
geometric optimisation positive definite matrices elliptically contoured distributions hermitian positive definite matrices hpd recur throughout statistics machine learning paper develop emph geometric optimisation globally optimising certain nonconvex loss functions arising modelling data via elliptically contoured distributions ecds exploit remarkable structure convex cone positive definite matrices allows uncover hidden geodesic convexity objective functions nonconvex ordinary euclidean sense going even beyond manifold convexity show metric properties hpd matrices exploited globally optimise several ecd log likelihoods even geodesic convex present key results help recognise geometric structure well obtain efficient fixed point algorithms optimise corresponding objective functions knowledge general results geometric optimisation hpd matrices known far experiments reveal benefits approach avoids eigenvalue computations makes competitive
policy shaping integrating human feedback reinforcement learning long term goal interactive reinforcement learning incorporate non expert human feedback solve complex tasks state art methods approached problem mapping human information reward value signals indicate preferences iterating compute necessary control policy paper argue alternate effective characterization human feedback policy shaping introduce advise bayesian approach attempts maximize information gained human feedback utilizing direct labels policy compare advise state art approaches highlight scenarios outperforms importantly robust infrequent inconsistent human feedback
context sensitive active sensing humans humans animals readily utilize active sensing use self motion focus sensory cognitive resources behaviorally relevant stimuli events environment understanding computational basis natural active sensing important advancing brain sciences developing powerful artificial systems recently goal directed context sensitive bayesian control strategy active sensing termed dac context dependent active controller proposed ahmad 2013 contrast previously proposed algorithms human active vision tend optimize abstract statistical objectives therefore cannot adapt changing behavioral context task goals dac directly minimizes behavioral costs thus automatically adapts different task conditions however dac limited model human active sensing given computational representational requirements especially complex real world situations propose myopic approximation dac also takes behavioral costs account achieves significant reduction complexity looking step ahead also present data human active visual search experiment compare performance various models human behavior find dac myopic variant achieve better fit human data infomax butko movellan 2010 maximizes expected cumulative future information gain summary work provides novel experimental results differentiate theoretical models human active sensing well novel active sensing algorithm retains context sensitivity optimal controller achieving significant computational savings
invariant occlusive components image patches probabilistic generative approach study optimal image encoding based generative approach non linear feature combinations explicit position encoding far approaches unsupervised learning learning visual features sparse coding ica account translations representing features different positions earlier models used separate encoding features positions facilitate invariant data encoding recognition probabilistic generative models explicit position encoding far assumed linear superposition components encode image patches first time apply model non linear feature superposition explicit position encoding avoiding linear superpositions studied model represents closer match component occlusions ubiquitous natural images order account occlusions non linear model encodes patches qualitatively different linear models using component representations separated mask feature parameters first investigated encodings learned model using artificial data mutually occluding components find model extracts components correctly identify occlusive components hidden variables model natural image patches model learns component masks features typical image components using reverse correlation estimate receptive fields associated model hidden units find many gabor like globular receptive fields well fields sensitive complex structures results show probabilistic models capture occlusions invariances trained efficiently image patches resulting encoding represents alternative model neural encoding images primary visual cortex
parallel sampling mixture models using sub cluster splits present novel mcmc sampler dirichlet process mixture models used conjugate non conjugate prior distributions proposed sampler massively parallelized achieve significant computational gains non ergodic restricted gibbs iteration mixed split merge proposals produce valid sampler regular cluster augmented sub clusters construct likely split moves unlike many previous parallel samplers proposed sampler accurately enforces correct stationary distribution markov chain without need approximate models empirical results illustrate new sampler exhibits better convergence properties current methods
learning adaptive value information structured prediction discriminative methods learning structured models enabled wide spread use rich feature representations however computational cost feature extraction prohibitive large scale time sensitive applications often dominating cost inference models significant efforts devoted sparsity based model selection decrease cost feature selection methods control computation statically miss opportunity fine tune feature extraction input run time address key challenge learning control fine grained feature extraction adaptively exploiting non homogeneity data propose architecture uses rich feedback loop extraction prediction run time control policy learned using efficient value function approximation adaptively determines value information features level individual variables input demonstrate significant speedups state art methods challenging datasets articulated pose estimation video achieve accurate state art model simultaneously times faster using small fraction possible features similar results ocr task
high dimensional gaussian process bandits many applications machine learning require optimizing unknown functions defined high dimensional space noisy samples expensive obtain address notoriously hard challenge assumptions function varies along low dimensional subspace smooth low norm reproducible kernel hilbert space particular present algorithm leverages recent low rank matrix recovery techniques learn underlying subspace unknown function applies gaussian process upper confidence sampling optimization function carefully calibrate exploration exploitation tradeoff allocating sampling budget subspace estimation function optimization obtain first subexponential cumulative regret bounds convergence rates bayesian optimization high dimensions noisy observations numerical results demonstrate effectiveness approach difficult scenarios
robust multimodal graph matching sparse coding meets graph matching graph matching challenging problem important applications wide range fields image video analysis biological biomedical problems propose robust graph matching algorithm inspired sparsity related techniques cast problem resembling group collaborative sparsity formulations non smooth convex optimization problem efficiently solved using augmented lagrangian techniques method deal weighted unweighted graphs well multimodal data different graphs represent different types data proposed approach also naturally integrated collaborative graph inference techniques solving general network inference problems observed variables possibly coming different modalities correspondence algorithm tested compared state art graph matching techniques synthetic real graphs also present results multimodal graphs applications collaborative inference brain connectivity alignment free functional magnetic resonance imaging fmri data
learning word embeddings efficiently noise contrastive estimation continuous valued word embeddings learned neural language models recently shown capture semantic syntactic information words well setting performance records several word similarity tasks best results obtained learning high dimensional embeddings large quantities data makes scalability training method critical factor propose simple scalable new approach learning word embeddings based training log bilinear models noise contrastive estimation approach simpler faster produces better results current state art method mikolov 2013a achieve results comparable best ones reported obtained cluster using times less data order magnitude less computing time also investigate several model types find embeddings learned simpler models perform least well learned complex ones
sparse inverse covariance estimation calibration propose semiparametric procedure estimating high dimensional sparse inverse covariance matrix method named alice applicable elliptical family computationally develop efficient dual inexact iterative projection d_2 algorithm based alternating direction method multipliers admm theoretically prove alice estimator achieves parametric rate convergence parameter estimation model selection moreover alice calibrates regularizations estimating column inverse covariance matrix asymptotically tuning free also achieves improved finite sample performance present numerical simulations support theory real data example illustrate effectiveness proposed estimator
third order edge statistics contour continuation curvature cortical connections association field models used explain human contour grouping performance explain mean frequency long range horizontal connections across cortical columns however association fields essentially depend pairwise statistics edges natural scenes develop spectral test sufficiency pairwise statistics show significant higher order structure analysis using probabilistic spectral embedding reveals curvature dependent components association field reveals challenge biological learning algorithms
conditional random fields via univariate exponential families conditional random fields model distribution multivariate response conditioned set covariates using undirected graphs widely used variety multivariate prediction applications popular instances class models categorical discrete crfs ising crfs conditional gaussian based crfs however best suited varied types response variables many applications including count valued responses thus introduce novel subclass crfs derived imposing node wise conditional distributions response variables conditioned rest responses covariates arising univariate exponential families allows derive novel multivariate crfs given univariate exponential distribution including poisson negative binomial exponential distributions also particular addresses common crf problem specifying feature functions determining interactions response variables covariates develop class tractable penalized estimators learn crf distributions data well unified sparsistency analysis general class crfs showing exact structure recovery achieved high probability
efficient reinforcement learning via posterior sampling provably efficient learning algorithms introduce optimism poorly understood states actions encourage exploration study alternative approach efficient exploration posterior sampling reinforcement learning psrl algorithm proceeds repeated episodes known duration start episode psrl updates prior distribution markov decision processes takes sample posterior psrl follows policy optimal sample episode algorithm conceptually simple computationally efficient allows agent encode prior knowledge natural way establish tilde tau sqrt bound expected regret time tau episode length cardinalities state action spaces bound first algorithm based optimism close state art reinforcement learning algorithm show simulation psrl significantly outperforms existing algorithms similar regret bounds
non linear domain adaptation boosting common assumption machine vision training test samples drawn distribution however many problems assumption grossly violated bio medical applications different acquisitions generate drastic variations appearance data due changing experimental conditions problem accentuated data annotation time consuming limiting amount data labeled new acquisitions training paper present multi task learning algorithm domain adaptation based boosting unlike previous approaches learn task specific decision boundaries method learns single decision boundary shared feature space common tasks use boosting trick learn non linear mapping observations task need specific priori knowledge global analytical form yields parameter free domain adaptation approach successfully leverages learning new tasks labeled data scarce evaluate approach challenging bio medical datasets achieve significant improvement state art
parametric task learning introduce novel formulation multi task learning mtl called parametric task learning ptl systematically handle infinitely many tasks parameterized continuous parameter key finding certain class ptl problems path optimal task wise solutions represented piecewise linear functions continuous task parameter based fact employ parametric programming technique obtain common shared representation across continuously parameterized tasks efficiently show ptl formulation useful various scenarios learning non stationarity cost sensitive learning quantile regression demonstrate usefulness proposed method experimentally scenarios
lasso learning sparse bayesian network structure continuous variables address problem learning sparse bayesian network structure continuous variables high dimensional space constraint estimated bayesian network structure must directed acyclic graph dag makes problem challenging huge search space network structures previous methods based stage approach prunes search space first stage searches network structure satisfies dag constraint second stage although approach effective low dimensional setting difficult ensure correct network structure pruned first stage high dimensional setting paper propose single stage method called lasso recovers optimal sparse bayesian network structure solving single optimization problem search algorithm uses lasso scoring system approach substantially improves computational efficiency well known exact methods based dynamic programming also present heuristic scheme improves efficiency lasso without significantly compromising quality solutions demonstrate benchmark bayesian networks real data
global map optimality shrinking combinatorial search area convex relaxation consider energy minimization undirected graphical models also known map inference problem markov random fields although combinatorial methods return provably optimal integral solution problem made big progress past decade still typically unable cope large scale datasets hand large scale datasets typically defined sparse graphs convex relaxation methods linear programming relaxations often provide good approximations integral solutions propose novel method combining combinatorial convex programming techniques obtain global solution initial combinatorial problem based information obtained solution convex relaxation method confines application combinatorial solver small fraction initial graphical model allows optimally solve big problems demonstrate power approach computer vision energy minimization benchmark
higher order priors joint intrinsic image objects attributes estimation many methods proposed recover intrinsic scene properties shape reflectance illumination single image however models applied laboratory datasets work explore synergy effects intrinsic scene properties recovered image objects attributes present scene cast problem joint energy minimization framework thus model able encode strong correlations intrinsic properties reflectance shape illumination objects table monitor materials wooden plastic given scene tested approach nyu pascal datasets observe qualitative quantitative improvements overall accuracy
robust spatial filtering beta divergence efficiency brain computer interfaces bci largely depends upon reliable extraction informative features high dimensional eeg signal crucial step protocol computation spatial filters common spatial patterns csp algorithm computes filters maximize difference band power conditions thus tailored extract relevant information motor imagery experiments however csp highly sensitive artifacts eeg data outliers alter estimate drastically decrease classification performance inspired concepts field information geometry propose novel approach robustifying csp precisely formulate csp divergence maximization problem utilize property particular type divergence namely beta divergence robustifying estimation spatial filters presence artifacts data demonstrate usefulness method toy data eeg recordings subjects
deep fisher networks large scale image classification massively parallel computations become broadly available modern gpus deep architectures trained large datasets risen popularity discriminatively trained convolutional neural networks particular recently shown yield state art performance challenging image classification benchmarks imagenet however elements architectures similar standard hand crafted representations used computer vision paper explore extent analogy proposing version state art fisher vector image encoding stacked multiple layers architecture significantly improves standard fisher vectors obtains competitive results deep convolutional networks significantly smaller computational cost hybrid architecture allows measure performance improvement brought deeper image classification pipeline staying realms conventional sift features encodings
sinkhorn distances lightspeed computation optimal transport optimal transportation distances fundamental family parameterized distances histograms probability simplex despite appealing theoretical properties excellent performance intuitive formulation computation involves resolution linear program whose cost prohibitive whenever histograms dimension exceeds hundreds propose work new family optimal transportation distances look transportation problems maximum entropy perspective smooth classical optimal transportation problem entropic regularization term show resulting optimum also distance computed sinkhorn matrix scaling algorithm speed several orders magnitude faster transportation solvers also report improved performance mnist benchmark problem competing distances
fast template evaluation vector quantization applying linear templates integral part many object detection systems accounts significant portion computation time describe method achieves substantial end end speedup best current methods without loss accuracy method combination approximating scores vector quantizing feature windows number speedup techniques including cascade procedure allows speed accuracy traded ways choosing number vector quantization levels choosing rescore windows method directly plugged recognition system relies linear templates demonstrate method speed original exemplar svm detector order magnitude deformable part models orders magnitude loss accuracy
tracking time varying graphical structure structure learning algorithms graphical models focused almost exclusively stable environments underlying generative process change assume generating model globally stationary real world environments however changes often occur without warning signal real world data often come generating models locally stationary paper present losst novel heuristic structure learning algorithm tracks changes graphical model structure parameters dynamic real time manner show simulation algorithm performs comparably batch mode learning generating graphical structure globally stationary significantly better locally stationary
optimal integration visual speed across different spatiotemporal frequency channels human visual system compute speed coherent motion stimulus contains motion energy different spatiotemporal frequency bands propose perceived speed result optimal integration speed information independent spatiotemporal frequency tuned channels formalize hypothesis bayesian observer model treats channel activity independent cues optimally combined prior expectation slow speeds test model behavioral data 2afc speed discrimination task measured subjects perceived speed drifting sinusoidal gratings different contrasts spatial frequencies various combinations single gratings find perceived speed combined stimuli independent relative phase underlying grating components perceptual biases discrimination thresholds always smaller combined stimuli supporting cue combination hypothesis proposed bayesian model fits data well accounting perceptual biases thresholds simple combined stimuli fits improved assume channel responses subject divisive normalization line physiological evidence results provide important step toward complete model visual motion perception predict perceived speeds stimuli arbitrary spatial structure
learning trajectory preferences manipulators via iterative improvement consider problem learning good trajectories manipulation tasks challenging criterion defining good trajectory varies users tasks environments paper propose active online learning framework teaching robots preferences users object manipulation tasks key novelty approach lies type feedback expected user human user need demonstrate optimal trajectories training data merely needs iteratively provide trajectories slightly improve trajectory currently proposed system argue active preference feedback easily elicited user demonstrations optimal trajectories often challenging non intuitive provide high degrees freedom manipulators nevertheless theoretical regret bounds algorithm match asymptotic rates optimal trajectory algorithms also formulate score function capture contextual information demonstrate generalizability algorithm variety household tasks preferences influenced object manipulated also surrounding environment
reinforcement learning robust markov decision processes important challenge markov decision processes ensure robustness respect unexpected adversarial system behavior taking advantage well behaving parts system consider problem setting unknown parts state space arbitrary transitions parts purely stochastic devise algorithm adaptive potentially adversarial behavior show achieves similar regret bounds purely stochastic case
adaptive step size policy gradient methods last decade policy gradient methods significantly grown popularity reinforcement learning field particular largely employed motor control robotic applications thanks ability cope continuous state action domains partial observable problems policy gradient researches mainly focused identification effective gradient directions proposal efficient estimation algorithms nonetheless performance policy gradient methods determined gradient direction since convergence properties strongly influenced choice step size small values imply slow convergence rate large values lead oscillations even divergence policy parameters step size value usually chosen hand tuning still little attention paid automatic selection paper propose determine learning rate maximizing lower bound expected performance gain focusing gaussian policies derive lower bound second order polynomial step size show simplified version lower bound maximized gradient estimated trajectory samples properties proposed approach empirically evaluated linear quadratic regulator problem
estimating unseen improved estimators entropy properties recently valiant valiant showed class distributional properties includes practically relevant properties entropy number distinct elements distance metrics pairs distributions estimated given sublinear sized sample specifically given sample consisting independent draws distribution distinct elements properties estimated accurately using sample size log propose novel modification approach show theoretically estimator optimal constant factors worst case instances practice performs exceptionally well variety estimation tasks variety natural distributions wide range parameters perhaps unsurprisingly key step approach first use sample characterize unseen portion distribution goes beyond tools good turing frequency estimation scheme estimates total probability mass unobserved portion distribution seek estimate shape unobserved portion distribution approach robust general theoretically principled expect fruitfully used component within larger machine learning data analysis systems
online learning switching costs adaptive adversaries study power different types adaptive nonoblivious adversaries setting prediction expert advice full information bandit feedback measure player performance using new notion regret also known policy regret better captures adversary adaptiveness player behavior setting losses allowed drift characterize nearly complete manner power adaptive adversaries bounded memories switching costs particular show switching costs attainable rate bandit feedback interestingly rate significantly worse sqrt rate attainable switching costs full information case via novel reduction experts bandits also show bounded memory adversary force regret even full information case proving switching costs easier control bounded memory adversaries lower bounds rely new stochastic adversary strategy generates loss processes strong dependencies
despot online pomdp planning regularization pomdps provide principled framework planning uncertainty computationally intractable due curse dimensionality curse history paper presents online lookahead search algorithm alleviates difficulties limiting search set sampled scenarios execution policies sampled scenarios summarized using determinized sparse partially observable tree despot sparsely sampled belief tree algorithm named regularized despot despot searches despot policy optimally balances size policy accuracy value estimate obtained sampling give output sensitive performance bound policies derived despot show despot works well small optimal policy exists also give anytime approximation despot experiments show strong results compared fastest online pomdp algorithms
decision jungles compact rich models classification randomized decision trees forests rich history machine learning seen considerable success application perhaps particularly computer vision however face fundamental limitation given enough data number nodes decision trees grow exponentially depth certain applications example mobile embedded processors memory limited resource exponential growth trees limits depth thus potential accuracy paper proposes decision jungles revisiting idea ensembles rooted decision directed acyclic graphs dags shows compact powerful discriminative models classification unlike conventional decision trees allow path every node dag decision jungle allows multiple paths root leaf present compare new node merging algorithms jointly optimize features structure dags efficiently training node splitting node merging driven minimization exactly objective function weighted sum entropies leaves results varied datasets show compared decision forests several baselines decision jungles require dramatically less memory considerably improving generalization
minimax optimal algorithms unconstrained linear optimization design analyze minimax optimal algorithms online linear optimization games player choice unconstrained player strives minimize regret difference loss loss post hoc benchmark strategy standard benchmark loss best strategy chosen bounded comparator set whereas consider broad range benchmark functions consider problem sequential multi stage sum game give thorough analysis minimax behavior game providing characterizations value game well player adversary optimal strategy show objects computed efficiently certain circumstances selecting appropriate benchmark construct novel hedging strategy unconstrained betting game
approximate dynamic programming finally performs well game tetris tetris popular video game widely used benchmark various optimization techniques including approximate dynamic programming adp algorithms close look literature game shows adp algorithms almost entirely based approximating value function value function based performed poorly tetris methods search directly space policies learning policy parameters using optimization black box cross entropy method achieved best reported results makes conjecture tetris game good policies easier represent thus learn corresponding value functions order obtain good performance adp use adp algorithms search policy space instead traditional ones search value function space paper put conjecture test applying adp algorithm called classification based modified policy iteration cbmpi game tetris extensive experimental results show first time adp algorithm namely cbmpi obtains best results reported literature tetris small times large times boards although cbmpi results similar achieved method large board cbmpi uses considerably fewer almost samples call generative model game
learning deep compact image representation visual tracking paper study challenging problem tracking trajectory moving object video possibly complex background contrast existing trackers learn appearance tracked object online take different approach inspired recent advances deep learning architectures putting emphasis unsupervised feature learning problem specifically using auxiliary natural images train stacked denoising autoencoder offline learn generic image features robust variations followed knowledge transfer offline training online tracking process online tracking involves classification neural network constructed encoder part trained autoencoder feature extractor additional classification layer feature extractor classifier tuned adapt appearance changes moving object comparison state art trackers challenging benchmark video sequences shows deep learning tracker efficient well accurate
learning limited demonstrations propose approach learning demonstration lfd leverages expert data even expert examples inaccurate achieve integrating lfd approximate policy iteration algorithm key idea approach expert examples used generate linear constraints optimization similar fashion large margin classification prove upper bound true bellman error approximation computed algorithm iteration show empirically algorithm outperforms pure policy iteration well dagger state art lfd algorithm supervised learning variety scenarios including imperfect demonstrations available experiments include simulations well real robotic navigation task
actor critic algorithms risk sensitive mdps many sequential decision making problems want manage risk minimizing measure variability rewards addition maximizing standard criterion variance related risk measures among common risk sensitive criteria finance operations research however optimizing many criteria known hard problem paper consider discounted average reward markov decision processes formulation first define measure variability policy turn gives set risk sensitive criteria optimize criteria derive formula computing gradient devise actor critic algorithms estimating gradient updating policy parameters ascent direction establish convergence algorithms locally risk sensitive optimal policies finally demonstrate usefulness algorithms traffic signal control application
transfer learning transductive setting category models objects activities typically rely supervised learning requiring sufficiently large training sets transferring knowledge known categories novel classes labels however far less researched even though common scenario work extend transfer learning semi supervised learning exploit unlabeled instances novel categories labeled instances proposed approach propagated semantic transfer combines main ingredients first transfer information known novel categories incorporating external knowledge linguistic expert specified information mid level layer semantic attributes second exploit manifold structure novel classes specifically adapt graph based learning algorithm far used semi supervised learning shot shot learning third improve local neighborhood graph structures replacing raw feature based representation mid level object attribute based representation evaluate approach challenging datasets different applications namely animals attributes imagenet image classification mpii composites activity recognition approach consistently outperforms state art transfer semi supervised approaches datasets
multi agent control framework adaptation brain computer interfaces closed loop brain computer interface bci adaptive decoders used learn parameters suited decoding user neural response feedback user provides information permits neural tuning also adapt present approach model process adaptation encoding model neural signal decoding algorithm multi agent formulation linear quadratic gaussian lqg control problem simulation characterize decoding performance improves neural encoding adaptive decoder optimize qualitatively resembling experimentally demonstrated closed loop improvement propose novel modified decoder update rule aware fact encoder also changing show improve simulated adaptation dynamics modeling approach offers promise gaining insights adaptation well improving user learning bci control practical settings
scalable kernels graphs continuous attributes graphs continuous node attributes arise many applications state art graph kernels comparing continuous attributed graphs suffer high runtime complexity instance popular shortest path kernel scales mathcal number nodes paper present class path kernels computational complexity mathcal delta delta graph diameter number edges due sparsity small diameter real world graphs kernels scale comfortably large graphs experiments presented kernels outperform state art kernels terms speed accuracy classification benchmark datasets
point based value iteration optimal belief compression dec pomdps paper presents major results towards solving decentralized partially observable markov decision problems decpomdps culminating algorithm outperforms existing algorithms standard infinite horizon benchmark problems give integer program solves collaborative bayesian games cbgs program notable linear relaxation often integral show decpomdp bounded belief converted pomdp albeit actions exponential number beliefs actions correspond strategies cbg present method transform decpomdp decpomdp bounded beliefs number beliefs free parameter using optimal lossless belief compression show combination results opens door new classes decpomdp algorithms based previous pomdp algorithms choose algorithm point based valued iteration modify produce first tractable value iteration method decpomdps outperforms existing algorithms
devise deep visual semantic embedding model modern visual recognition systems often limited ability scale large numbers object categories limitation part due increasing difficulty acquiring sufficient training data form labeled images number object categories grows remedy leverage data sources text data train visual models constrain predictions paper present new deep visual semantic embedding model trained identify visual objects using labeled image data well semantic information gleaned unannotated text demonstrate model matches state art performance 1000 class imagenet object recognition challenge making semantically reasonable errors also show semantic information exploited make predictions tens thousands image labels observed training semantic knowledge improves shot predictions achieving hit rates across thousands novel labels never seen visual model
learning feature selection dependencies multi task learning probabilistic model based horseshoe prior proposed learning dependencies process identifying relevant features prediction exact inference intractable model however expectation propagation offers approximate alternative process estimating feature selection dependencies suffer fitting model proposed additional data multi task learning scenario considered induction model used setting modifications furthermore assumptions made less restrictive multi task methods different tasks must share feature selection dependencies different relevant features model coefficients experiments real synthetic data show model performs better multi task alternatives literature experiments also show model able induce suitable feature selection dependencies problems considered training data
extracting regions interest biological images convolutional sparse block coding biological tissue often composed cells similar morphologies replicated throughout large volumes many biological applications rely accurate identification cells locations image data develop generative model captures regularities present images composed repeating elements different types formally model described convolutional sparse block coding inference use variant convolutional matching pursuit adapted block based representations extend svd learning algorithm subspaces retaining several principal vectors svd decomposition instead good models little cross talk subspaces obtained learning blocks incrementally perform extensive experiments simulated images inference algorithm consistently recovers large proportion cells small number false positives fit convolutional model noisy gcamp6 photon images spiking neurons nissl stained slices cortical tissue show recovers cell body locations without supervision flexibility block based representation reflected variability recovered cell shapes
symbolic opportunistic policy iteration factored action mdps address scalability symbolic planning uncertainty factored states actions prior work focused almost exclusively factored states factored actions value iteration compared policy iteration rst contribution novel method symbolic policy backups via application constraints used yield new cient symbolic imple mentation modi mpi factored action spaces approach improves scalability cases naive handling policy constraints comes scalability issues leads second main contribution symbolic opportunistic policy iteration opi novel convergent gorithm lying mpi core idea symbolic procedure applies policy constraints reduce space time complexity update otherwise performs full bellman backups thus automatically adjusting backup per state also give memory bounded version algorithm allowing space time tradeoff empirical results show signi cantly improved scalability state art
analyzing harmonic structure graph based learning show either explicitly implicitly various well known graph based models exhibit common significant emph harmonic structure target function value vertex approximately weighted average values adjacent neighbors understanding structure analysis loss defined structure help reveal important properties target function graph paper show variation target function across cut upper lower bounded ratio harmonic loss cut cost use develop analytical tool analyze popular models graph based learning absorbing random walks partially absorbing random walks hitting times pseudo inverse graph laplacian eigenvectors laplacian matrices analysis well explains several open questions models reported literature furthermore provides theoretical justifications guidelines practical use simulations synthetic real datasets support analysis
model selection consistency penalized estimators geometric theory penalized estimators used diverse areas science engineering fit high dimensional models low dimensional structure often penalties emph geometrically decomposable expressed sum convex support functions generalize notion irrepresentable geometrically decomposable penalties develop general framework establishing consistency model selection consistency estimators penalties use framework derive results special cases interest bioinformatics statistical learning
optimistic policy iteration natural actor critic unifying view non optimality result approximate dynamic programming approaches reinforcement learning problem often categorized greedy value function methods value based policy gradient methods first main result show important subset latter methodology fact limiting special case general formulation former methodology optimistic policy iteration encompasses greedy value function methods also natural actor critic methods permits directly interpolate resulting continuum adjusts strength markov assumption policy improvement seen dual spirit continuum lambda style algorithms policy evaluation second main result show substantial subset soft greedy value function approaches potential avoid policy oscillation policy chattering subset never converge toward optimal policy except certain pathological case consequently context approximations majority greedy value function methods seem deemed suffer either risk oscillation chattering presence systematic sub optimality
understanding variable importances forests randomized trees despite growing interest practical use various scientific areas variable importances derived tree based ensemble methods well understood theoretical point view work characterize mean decrease impurity mdi variable importances measured ensemble totally randomized trees asymptotic sample ensemble size conditions derive level decomposition information jointly provided input variables output terms mdi importance input variable degree interaction given input variable input variables iii different interaction terms given degree show mdi importance variable equal variable irrelevant mdi importance relevant variable invariant respect removal addition irrelevant variables illustrate properties simple example discuss change case non totally randomized trees random forests extra trees
first order decomposition trees lifting attempts speedup probabilistic inference exploiting symmetries model exact lifted inference methods like propositional counterparts work recursively decomposing model problem propositional case exist formal structures decomposition trees dtrees represent decomposition allow determine complexity inference priori however currently equivalent structure analogous complexity results lifted inference paper introduce dtrees upgrade propositional dtrees first order level show trees characterize lifted inference solution probabilistic logical model terms sequence lifted operations make theoretical analysis complexity lifted inference terms novel notion lifted width tree
reservoir boosting online offline ensemble learning propose train ensemble help reservoir learning algorithm store limited number samples novel approach lies area offline online ensemble approaches seen either restriction former enhancement latter identify basic strategies used populate reservoir present main contribution dubbed greedy edge expectation maximization geem maintains reservoir content case boosting viewing samples projections weak classifier response space propose efficient algorithmic implementation makes tractable practice demonstrate efficiency experimentally several compute vision data sets outperforms online offline methods memory constrained setting
dimension free exponentiated gradient present new online learning algorithm extends exponentiated gradient infinite dimensional spaces analysis shows algorithm implicitly able estimate l_2 norm unknown competitor achieving regret bound order log sqrt instead standard sqrt achievable without knowing analysis introduce novel tools algorithms time varying regularizers use local smoothness lower bound also show algorithm optimal sqrt log term linear lipschitz losses
training analysing deep recurrent neural networks time series often temporal hierarchy information spread multiple time scales common recurrent neural networks however explicitly accommodate hierarchy research focusing training algorithms rather basic architecture per study effect hierarchy recurrent neural networks processing time series layer recurrent network receives hidden state previous layer input architecture allows perform erarchical processing difficult temporal tasks naturally capture structure time series show reach state art performance recurrent networks character level language modelling trained sim ple stochastic gradient descent also offer analysis different emergent time scales
documents multiple overlapping windows grids counts text analysis documents represented disorganized bags words models count features typically based mixing small number topics cite lda sam recently observed many text corpora documents evolve another smooth way features dropping new ones introduced counting grid cite cguai models spatial metaphor literally multidimensional grid word distributions learned way document distribution features modeled sum histograms found window grid major drawback method essentially mixture content much generated single contiguous area grid problematic especially lower dimensional grids paper overcome issue emph componential counting grid brings componential nature topic models basic counting grid also introduce generative kernel based document grid usage visualization strategy useful understanding large text corpora evaluate approach document classification multimodal retrieval obtaining state art results standard benchmarks
projected natural actor critic natural actor critics popular class policy search algorithms finding locally optimal policies markov decision processes paper address drawback natural actor critics limits real world applicability lack safety guarantees present principled algorithm performing natural gradient descent constrained domain context reinforcement learning allows natural actor critic algorithms guaranteed remain within known safe region policy space deriving class constrained natural actor critic algorithms call projected natural actor critics pnacs also elucidate relationship natural gradient descent mirror descent
direct loss minimization margin maximization boosting propose boosting method directboost greedy coordinate descent algorithm builds ensemble classifier weak classifiers directly minimizing empirical classification error labeled training examples training classification error reduced local coordinatewise minimum directboost runs greedy coordinate ascent algorithm continuously adds weak classifiers maximize targeted arbitrarily defined margins reaching local coordinatewise maximum margins certain sense experimental results collection machine learning benchmark datasets show directboost gives consistently better results adaboost logitboost lpboost column generation brownboost noise tolerant maximizes order bottom sample margin
distributed exploration multi armed bandits study exploration multi armed bandits mab setting players collaborate order identify epsilon optimal arm motivation comes recent employment mab algorithms computationally intensive large scale applications results demonstrate non trivial tradeoff number arm pulls required players amount communication particular main result shows allowing players communicate emph able learn sqrt times faster single player distributing learning players gives rise factor sqrt parallel speed complement result lower bound showing general best possible extreme present algorithm achieves ideal factor speed learning performance communication logarithmic epsilon
reshaping visual datasets domain adaptation visual recognition problems common data distribution mismatches training testing make domain adaptation essential however image data difficult manually divide discrete domains required adaptation algorithms standard practice equating datasets domains weak proxy real conditions alter statistics complex ways lighting pose background resolution etc propose approach automatically discover latent domains image video datasets formulation imposes key properties domains maximum distinctiveness maximum learnability maximum distinctiveness require underlying distributions identified domains different maximum learnability ensure strong discriminative model learned domain devise nonparametric representation efficient optimization procedure distinctiveness coupled learnability constraint successfully discover domains among training test data extensively evaluate approach object recognition human activity recognition tasks
chiru identifying groups strongly correlated variables smoothed ordered weighted norms failure lasso identify groups correlated predictors linear regression sparked significant research interest recently various norms proposed best described instances ordered weighted norms owl alternative regularization used lasso owl identify groups correlated variables forces model tobe constant within group artifactinduces unnecessary bias model estimation paper take submodular perspective show owl posed lov asz extension suitably defined submodular function submodular perspective explains groupwise constant behavior owl also suggests alternatives main contribution paper smoothed owl sowl new family norms identifies groups also allows model flexible inside group establish several algorithmic theoretical properties sowl including group identification model consistency also provide algorithmic toolsto compute sowl norm proximaloperator whose computational complexityo log significantly better thatof general purpose solvers experiments sowl compares favorably respect owl regimes interest
chiru clustering sum norms stochastic incremental algorithm convergence cluster recovery standard clustering methods means gaussian mixture models hierarchical clustering beset local minima sometimes drastically suboptimal moreover number clusters must known advance recently introduced sum norms son clusterpath convex relaxation means hierarchical clustering shrinks cluster centroids toward another ensure unique global minimizer give scalable stochastic incremental algorithm based proximal iterations solve son problem convergence guarantees also show algorithm recovers clusters quite general conditions similar form unifying proximity condition introduced approximation algorithms community covers paradigm cases gaussian mixtures planted partition models give experimental results confirm algorithm scales much better previous methods producing clusters comparable quality
chiru bayesian modeling temporal coherence videos entity discovery summarization video understood users terms entities present entity discovery task building appearance model entity person finding occurrences video represent video sequence tracklets spanning frames associated entity pose entity discovery tracklet clustering approach leveraging temporalcoherence property temporally neighboring tracklets likely associated entity major contributions first bayesian nonparametric models tracklet level extend chinese restaurant process crp temporally coherent chinese restaurant franchise crf jointly model entities temporal segments using mixture components sparse distributions discovering persons serial videos without meta data like scripts methods show considerable improvement state art approaches tracklet clustering terms clustering accuracy cluster purity entity coverage proposed methods perform online tracklet clustering streaming videos unlike existing approaches automatically reject false tracklets finally discuss entity driven video summarization temporal segments video selected based discovered entities create semantically meaningful summary index terms bayesian nonparametrics chinese restaurant process temporal coherence temporal segmentation tracklet clustering entity discovery entity driven video summarization
chiru non negative matrix factorization heavy noise noisy non negative matrix factorization given data matrix find non negative matrices noise matrix existing polynomial time algorithms proven error guarantees require column norm much smaller could restrictive important applications nmf topic modeling well theoretical noise models gaussian high almost every column violates condition introduce heavy noise model requires average noise large subsets columns small initiate study noisy nmf heavy noise model show noise model subsumes noise models theoretical practical interest gaussian noise maximum possible devise algorithm certain assumptions solves problem heavy noise error guarantees match previous algorithms running time substantially better previous best assumption weaker separability assumption made previous results provide empirical justification assumptions also provide first proof identifiability uniqueness noisy nmf based separability use hard check geometric conditions algorithm outperforms earlier polynomial time algorithms time error particularly presence high noise
chiru weighted theta functions embeddings applications max cut clustering summarization introduce unifying generalization lovasz theta function sociated geometric embedding graphs weights nodes edges show computed exactly semidefinite programming approximate using svm computations show theta function interpreted measure diversity graphs use idea graph embedding algorithms max cut correlation clustering document summarization well represented problems weighted graphs
chiru provable svd based algorithm learning topics dominant admixture corpus topic models latent dirichlet allocation lda posit documents drawn admixtures distributions words known topics inference problem recovering topics admixtures hard assuming separability strong assumption gave first provable algorithm inference lda model gave provable algorithm using tensor methods learn topic vectors bounded error natural measure probability vectors aim develop model makes intuitive empirically supported assumptions design algorithm natural simple components svd provably solves inference problem model bounded error topic lda models essentially characterized group occurring words motivated introduce topic specific catchwords group words occur strictly greater frequency topic topic individually required high frequency together rather individually major contribution paper show realistic assumption empirically verified real corpora singular value decomposition svd based algorithm crucial pre processing step thresholding provably recover topics collection documents drawn dominant admixtures dominant admixtures convex combination distributions distribution significantly higher contribution others apart simplicity algorithm sample complexity near optimal dependence lowest probability topic dominant better empirical evidence shows several real world corpora catchwords dominant admixture assumptions hold proposed algorithm substantially outperforms state art
chiru randomized algorithms large scale svms propose randomized algorithm training support vector machines svms large datasets using ideas random projections show combinatorial dimension svms logn high probability estimate combinatorial dimension used derive iterative algorithm called randsvm step calls existing solver train svms randomly chosen subset size logn algorithm probabilistic guarantees capable training svms kernels classification regression problems experiments done synthetic real life data sets demonstrate algorithm scales existing svm learners without loss accuracy
chiru randomized algorithm large scale support vector learning propose randomized algorithm large scale svm learning solves problem iterating random subsets data crucial algorithm scalability size subsets chosen context text classification show using ideas random projections sample size log used obtain solution close optimal high probability experiments done synthetic real life data sets demonstrate algorithm scales svm learners without loss accuracy
chiru minimax probability machine constructing classifier probability correct classification future data points maximized current paper desideratum translated direct way optimization problem solved using methods convex optimization also show exploit mercer kernels setting obtain nonlinear decision boundaries worst case bound probability misclassification future data obtained explicitly
chiru controlled sparsity kernel learning multiple kernel learning mkl support vector machines svms popular front research recent times due success application problems like object categorization success due fact mkl ability choose variety feature kernels identify optimal kernel combination initial formulation mkl able select best features misses many informative kernels presented overcome norm based formulation proposed kloft formulation capable choosing non sparse set kernels control parameter unfortunately parameter direct meaning number kernels selected observed stricter control number kernels selected gives edge techniques terms accuracy classification also helps fine tune algorithms time requirements hand work propose controlled sparsity kernel learning cskl formulation strictly control number kernels wish select cskl formulation introduces parameter directly corresponds number kernels selected important note search space finite fast compared also provided efficient reduced gradient descent based algorithm solve cskl formulation proven converge experiments caltech101 object categorization dataset also shown achieve better accuracies previous formulations right choice
chiru lov function svms finding dense subgraphs paper establish lov function graph restated kernel learning problem introduce notion svm graphs lov function approximated well support vector machine svm show erd nyi random graphs svm graphs log4 even embed large clique size graph resultant graph still remains svm graph immediately suggests svm based algorithm recovering large planted clique random graphs associated function notion orthogonal labellings introduce common orthogonal labellings extends idea orthogonal labellings multiple graphs allows propose multiple kernel learning mkl based solution capable identifying large common dense subgraph multiple graphs planted clique case common subgraph detection problem proposed solutions beat state art order magnitude
chiru mean field methods special class belief networks chief aim paper propose mean field approximations broad class belief networks sigmoid noisy networks seen special cases approximations based powerful mean field theory suggested plefka show saul jaakkola jordan approach first order approximation plefka approach via variational derivation application plefka theory belief networks computationally tractable tackle problem propose new approximations based taylor series small scale experiments show proposed schemes attractive
chiru efficient algorithms learning kernels multiple similarity matrices general convex loss functions paper consider problem learning kernel matrix similarity matrices general convex loss past research extensively studied case derived several algorithms require sophisticated techniques like accp socp etc existing algorithms apply uses arbitrary losses often handle case present several provably convergent iterative algorithms iteration requires either svm multiple kernel learning mkl solver case major contributions paper extend well known mirror descent framework handle cartesian product psd matrices novel extension leads algorithm called emkl solves problem log iterations iteration solves mkl involving kernels eigen decomposition matrices suitably defining restriction objective function faster version emkl proposed called rekl avoids eigen decomposition alternative emkl rekl also suggested requires svm solver experimental results real world protein data set involving several similarity matrices illustrate efficacy proposed algorithms
chiru conditional models non smooth ranking loss functions abstract
learning rank important area interface machine learning information retrieval web search central challenge optimizing various measures ranking loss objectives tend non convex discontinuous make functions amenable gradient based optimization procedures needs design clever bounds recent years boosting neural networks support vector machines many techniques applied however little work directly modeling conditional probability x_q permutation documents ranked x_q represents feature vectors respect query major reason space huge documents must ranked first propose intuitive appealing expected loss minimization objective give efficient shortcut evaluate despite huge space unfortunately optimization non convex propose convex approximation give new efficient monte carlo sampling method compute objective gradient approximation used quasi newton optimizer like lbfgs extensive experiments widely used letor dataset show large ranking accuracy improvements beyond recent competitive algorithms
chiru randomized algorithms large scale svms propose randomized algorithm training support vector machines svms large datasets using ideas random projections show combinatorial dimension svms logn high probability estimate combinatorial dimension used derive iterative algorithm called randsvm step calls existing solver train svms randomly chosen subset size logn algorithm probabilistic guarantees capable training svms kernels classification regression problems experiments done synthetic real life data sets demonstrate algorithm scales existing svm learners without loss accuracy
chiru structured learning non smooth ranking losses learning rank relevance judgment active research area itemwise score regression pairwise preference satisfaction listwise structured learning major techniques use listwise structured learning applied recently optimize important non decomposable ranking criteria like auc area roc curve map mean average precision propose new almost linear time algorithms optimize criteria widely used evaluate search systems mrr mean reciprocal rank ndcg normalized discounted cumulative gain max margin structured learning framework also demonstrate different ranking criteria need use different feature maps search applications optimized favor single criterion need cater variety queries mrr best navigational queries ndcg best informational queries key contribution paper fold multiple ranking loss functions multi criteria max margin optimization result single robust ranking model close best accuracy learners trained individual criteria fact experiments popular letor trec data sets show contrary conventional wisdom test criterion often best served training individual criterion
