<<<<<<< HEAD
learn activ learn data paper suggest novel data driven approach activ learn key idea train regressor predict expect error reduct candid sampl particular learn state formul queri select procedur regress problem restrict work exist heurist instead learn strategi base experi previous outcom show strategi learnt either simpl synthet dataset subset domain specif data method yield strategi work well real data wide rang domain
scalabl variat infer dynam system gradient match promis tool learn paramet state dynam ordinari differenti equat grid free infer approach fulli observ system time competit numer integr howev mani real world applic spars observ avail even unobserv variabl includ model descript case gradient match method difficult appli simpli provid satisfactori result despit high comput cost numer integr still gold standard mani applic use exist gradient match approach propos scalabl variat infer framework infer state paramet simultan offer comput speedup improv accuraci work well even model misspecif partial observ system
activ learn peer paper address challeng learn peer onlin multitask set instead alway request label human oracl propos method first determin learner task acquir label suffici confid peer either task similar weight sum singl similar task save oracl queri later use difficult case queri human oracl paper develop new algorithm exhibit behavior prove theoret mistak bound method compar best linear predictor hindsight experi multitask learn benchmark dataset show clear superior perform baselin assum task independ learn oracl learn peer task
gradient episod memori continuum learn major obstacl toward artifici intellig poor abil current model reus knowledg acquir past quick learn new task forget previous learn work formal emph continuum learn set train exampl emph iid generat continu stream task unknown relationship first propos new set metric continuum learn character learn system term averag accuraci also term abil transfer knowledg previous futur task second propos model continuum learn term gradient episod memori gem reduc forget allow potenti improv perform previous task experi variant mnist cifar100 dataset demonstr strong perform model compar varieti state art contend
consist multitask learn nonlinear output relat key multitask learn exploit relationship differ task improv predict perform relat linear regular approach use success howev practic assum task linear relat might restrict allow nonlinear structur challeng paper tackl issu cast problem within framework structur predict main contribut novel algorithm learn multipl task relat system nonlinear equat joint output need satisfi show algorithm consist effici implement experiment result show potenti propos method
joint distribut optim transport domain adapt paper deal unsupervis domain adapt problem want estim predict function
given target domain without label sampl exploit knowledg avail sourc domain label known work make follow assumpt exist non linear transform joint featur label space distribut domain propos solut problem optim transport allow recov estim target ptf optim simultan optim coupl show method correspond minim bound target error provid effici algorithm solut converg prove versatil approach term class hypothesi loss function demonstr real world classif regress problem reach surpass state art result
learn multipl task deep relationship network deep network train larg scale data learn transfer featur promot learn multipl task deep featur eventu transit general specif along deep network fundament problem exploit relationship across differ task improv featur transfer task specif layer paper propos deep relationship network drn discov task relationship base novel tensor normal prior paramet tensor multipl task specif layer deep convolut network joint learn transfer featur task relationship drn abl allevi dilemma negat transfer featur layer transfer classifi layer extens experi show drn yield state art result standard multi task learn benchmark
label effici learn transfer represent acrosss domain task propos framework learn represent transfer across differ domain task data effici manner approach battl domain shift domain adversari loss general embed novel task use metric learn base approach model simultan optim label sourc data unlabel spars label data target domain method show compel result novel class within new domain even label exampl per class avail outperform preval fine tune approach addit demonstr effect framework transfer learn task imag object recognit video action recognit
match neural path transfer recognit correspond search mani machin learn task requir find per part correspond object work focus low level correspond high ambigu match problem propos use hierarch semant represent object come convolut neural network solv ambigu train low level correspond predict direct might option domain ground truth correspond hard obtain show transfer recognit use avoid train idea mark part match featur close level convolut featur hierarchi neural path although overal number path exponenti number layer propos polynomi algorithm aggreg singl backward pass empir valid done task stereo correspond demonstr achiev competit result among method use label target domain data
deep neural network suffer crowd crowd visual effect suffer human object recogn isol longer recogn object call clutter place close work studi effect crowd artifici deep neural network dnns object recognit analyz deep convolut neural network dcnns well extens dcnns multi scale chang recept field size convolut filter posit imag call eccentr depend model latter network recent propos model feedforward path primat visual cortex result reveal incorpor clutter imag train set learn dnns lead robust clutter seen train also dnns train object isol find recognit accuraci dnns fall closer clutter target object clutter find visual similar target clutter also play role pool earli layer dnn lead crowd final show eccentr depend model train object isol recogn target object clutter object near center imag wherea dcnn cannot
svcca singular vector canon correl analysi deep understand improv continu empir success deep network becom increas import develop better method understand train model represent learn within paper propos singular valu canon correl analysi svcca tool quick compar represent way invari affin transform allow comparison differ layer network fast comput allow comparison calcul previous method deploy tool measur intrins dimension layer show case needless parameter probe learn dynam throughout train find network converg final represent bottom show class specif inform network form suggest new train regim simultan save comput overfit less
neural expect maxim mani real world task reason physic interact requir iden tific manipul conceptu entiti first step toward solv task autom discoveri symbol like represent distribut disentangl paper explicit formal problem infer spatial mixtur model compon parametr neural network base expect maxim framework deriv differenti cluster method simultan learn group repres individu entiti evalu method sequenti perceptu group task find accur abl recov constitu object demonstr learn represent use predict code
pointnet deep hierarch featur learn point set metric space prior work studi deep learn point set pointnet pioneer direct howev design pointnet captur local structur induc metric space point live limit abil recogn fine grain pattern generaliz complex scene work introduc hierarch neural network appli pointnet recurs nest partit input point set exploit metric space distanc network abl learn local featur increas contextu scale observ point set usual sampl vari densiti result great decreas perform network train uniform densiti propos novel set learn layer adapt combin featur multipl scale experi show network call pointnet abl learn deep point set featur effici robust particular result signific better state art obtain challeng benchmark point cloud
preserv proxim global rank node embed investig unsupervis generat approach network embed multi task siames neural network structur formul connect embed vector object preserv global node rank local proxim node provid deeper analysi connect propos proxim object link predict communiti detect network show model satisfi follow design properti scalabl asymmetri uniti simplic experi result verifi design properti also demonstr superior perform learn rank classif regress link predict task
unsupervis transform learn via convex relax goal extract meaning transform data thick line handwrit light portrait raw imag work propos unsupervis approach learn transform base reconstruct nearest neighbor use linear combin transform deriv new algorithm unsupervis linear transform learn handwritten digit celebr portrait dataset show even linear transform method extract meaning transform generat visual high qualiti transform output moreov method semiparametr model data distribut allow learn transform extrapol train data work new type imag
hunt uniqu stabl spars fast featur learn graph purpos learn graph hunt graph represent exhibit certain uniqu stabil sparsiti properti also amen fast comput lead graph represent base discoveri famili graph spectral distanc denot fgsd prove possess desir properti evalu qualiti graph featur produc fgsd demonstr util appli graph classif problem extens experi show simpl svm base classif algorithm driven power fgsd base graph featur signific outperform sophist state art algorithm unlabel node dataset term accuraci speed also yield competit result label dataset despit fact util node label inform
deep subspac cluster network present novel deep neural network architectur unsupervis subspac cluster architectur built upon deep auto encod non linear map input data latent space key idea introduc novel self express layer encod decod mimic self express properti proven effect tradit subspac cluster differenti new self express layer provid simpl effect way learn pairwis affin data point standard back propag procedur nonlinear neural network base method abl cluster data point complex often nonlinear structur propos pre train fine tune strategi let effect learn paramet subspac cluster network experi show propos method signific outperform state art unsupervis subspac cluster method
learn graph embed embed propag propos embed propag unsupervis learn framework graph structur data learn vector represent graph pass type messag neighbor node forward messag consist label represent represent word featur associ node backward messag consist gradient result aggreg represent appli reconstruct loss node represent final comput represent label signific fewer paramet hyperparamet instanc competit often outperform state art unsupervis learn method rang benchmark data set
unsupervis sequenc classif use sequenti output statist consid learn sequenc classifi without label data use sequenti output statist problem high valuabl sinc obtain label train data often cost sequenti output statist languag model could obtain independ input data thus low cost address problem propos unsupervis learn cost function studi properti show compar earlier work less inclin stuck trivial solut avoid need strong generat model although harder optim function form stochast primal dual gradient method develop effect solv problem experi result real world dataset demonstr new unsupervis learn method give drastic lower error baselin method specif reach test error twice obtain fulli supervis learn
context select embed model word embed effect tool analyz languag recent extend model type data beyond text item recommend system embed model consid probabl target observ word item condit element context word item paper show condit element context optim instead improv predict qualiti embed represent model probabl target condit subset element context develop model account use amort variat infer automat choos subset experi demonstr model outperform standard embed method dataset differ domain term held predict qualiti embed represent
probabilist rule realize select abstract realize bilater process key deriv intellig creativ mani domain process approach emph rule high level principl reveal invari within similar yet divers exampl probabilist set discret input space focus rule realize problem generat input sampl distribut follow given rule ambiti beyond mechan realize take whatev given instead ask proactiv select reason rule realiz goal demand practic sinc initi rule set alway consist thus intellig compromis need formul rule realize select strong connect compon within singl symmetr convex problem deriv effici algorithm work larg scale take music composit rule main exampl throughout paper demonstr model effici music realize composit also music interpret understand analysi
trim densiti ratio estim densiti ratio estim becom versatil tool machin learn communiti recent howev due unbound natur densiti ratio estim vulner corrupt data point often push estim ratio toward infin paper present robust estim automat identifi trim outlier propos estim convex formul global optimum obtain via subgradi descent analyz paramet estim error estim high dimension set experi conduct verifi effect estim
minimax optim algorithm crowdsourc consid problem accur estim reliabl worker base noisi label provid fundament question crowdsourc propos novel lower bound minimax estim error appli estim procedur propos triangular estim algorithm estim reliabl worker low complex implement stream set label provid worker real time reli iter procedur prove minimax optim match lower bound conclud assess perform state art algorithm synthet real world data
introspect classif convolut net propos introspect convolut network icn emphas import convolut neural network empow generat capabl employ reclassif synthesi algorithm perform train use formul stem bay theori icn tri iter synthes pseudo negat sampl enhanc improv classif singl cnn classifi learn time generat abl direct synthes new sampl within discrimin model conduct experi benchmark dataset includ mnist cifar svhn use state art cnn architectur observ improv classif result
adapt classif predict budget propos novel adapt approxim approach test time resourc constrain predict given input instanc test time gate function identifi predict model input among collect model object minim overal averag cost without sacrif accuraci learn gate predict model fulli label train data mean bottom strategi novel bottom method first train high accuraci complex model low complex gate predict model subsequ learnt adapt approxim high accuraci model region low cost model capabl make high accur predict pose empir loss minim problem cost constraint joint train gate predict model number benchmark dataset method outperform state art achiev higher accuraci cost
learn featur evolv stream learn stream data attract much attent past year though studi consid data stream fix featur real practic featur evolv exampl featur data gather limit lifespan sensor chang sensor substitut new one paper propos novel learn paradigm featur evolv stream learn old featur would vanish new featur occur rather reli current featur attempt recov vanish featur exploit improv perform specif learn model recov featur current featur respect benefit recov featur develop ensembl method first method combin predict model theoret show assist old featur perform new featur improv second approach dynam select best singl predict establish better perform guarante best model switch experi synthet real data valid effect propos
aggress sampl multi class binari reduct applic text classif address problem multi class classif case number class larg propos doubl sampl strategi top multi class binari reduct strategi transform origin multi class problem binari classif problem pair exampl aim sampl strategi overcom curs long tail class distribut exhibit major larg scale multi class classif problem reduc number pair exampl expand data show strategi alter consist empir risk minim principl defin doubl sampl reduct experi carri dmoz wikipedia collect 000 100 000 class show effici propos approach term train predict time memori consumpt predict perform respect state art approach
adversari surrog loss ordin regress ordin regress seek class label predict penalti incur mistak increas accord order label absolut error canon exampl mani exist method task reduc binari classif problem employ surrog loss hing loss instead deriv uniqu defin surrog ordin regress loss function seek predictor robust worst case approxim train data label subject match certain provid train data statist demonstr advantag approach surrog loss base hing loss approxim use uci ordin predict task
formal guarante robust classifi adversari manipul recent work shown state art classifi quit brittl sens small adversari chang origin high confid correct classifi input lead wrong classif high confid rais concern classifi vulner attack call question usag safeti critic system show paper first time formal guarante robust classifi give instanc specif emph lower bound norm input manipul requir chang classifi decis base analysi propos cross lipschitz regular function show use form regular kernel method resp neural network improv robust classifi without loss predict perform
cost effici gradient boost mani applic requir learn classifi regressor accur cheap evalu predict cost drastic reduc learn predictor construct major input use cheap featur fast evalu main challeng littl loss accuraci work propos budget awar strategi base deep boost regress tree contrast previous approach learn cost penalti method grow deep tree averag nonetheless cheap comput evalu method number dataset find outperform current state art larg margin algorithm easi implement learn time compar origin gradient boost sourc code made avail accept
high effici gradient boost decis tree gradient boost decis tree gbdt popular machin learn algorithm quit effect implement xgboost pgbrt although mani engin optim adopt implement effici scalabl still unsatisfactori featur dimens high data size larg major reason featur need scan data instanc estim inform gain possibl split point time consum tackl problem propos novel techniqu emph gradient base side sampl goss emph exclus featur bundl efb goss exclud signific proport data instanc small gradient use rest estim inform gain prove sinc data instanc larger gradient play import role comput inform gain goss obtain quit accur estim inform gain much smaller data size efb bundl mutual exclus featur rare take nonzero valu simultan reduc number featur prove find optim bundl exclus featur hard greedi algorithm achiev quit good approxim ratio thus effect reduc number featur without hurt accuraci split point determin much call new gbdt implement goss efb emph lightgbm experi multipl public dataset show lightgbm speed train process convent gbdt time achiev almost accuraci
estim accuraci unlabel data probabilist logic approach propos effici method estim accuraci classifi use unlabel data consid set multipl classif problem target class tie togeth logic constraint exampl set class mutual exclus mean data instanc belong propos method base intuit classifi agre like correct classifi make predict violat constraint least classifi must make error experi real world data set produc accuraci estim within percent true accuraci use sole unlabel data model also outperform exist state art solut estim accuraci combin multipl classifi output result emphas util logic constraint estim accuraci thus valid intuit
infer generat model structur static analysi obtain enough label train data complex discrimin model major bottleneck machin learn pipelin popular solut combin multipl sourc weak supervis use generat model structur model affect qualiti train label difficult learn without ground truth label instead reli weak supervis sourc structur virtu encod programmat present coral paradigm infer generat model structur static analyz code heurist thus reduc data requir learn structur signific prove coral sampl complex scale quasilinear number heurist number relat found improv standard sampl complex exponenti identifi degre relat experiment coral match outperform tradit structur learn approach point use coral model depend instead assum independ result perform better fulli supervis model accuraci point heurist use label radiolog data without ground truth label
scalabl model select belief network propos scalabl algorithm model select sigmoid belief network sbns base factor asymptot bayesian fab framework deriv correspond general factor inform criterion gfic sbn proven statist consist margin log likelihood captur depend within hidden variabl sbns recognit network employ model variat distribut result algorithm call fabia simultan execut model select infer maxim lower bound gfic synthet real data experi suggest fabia compar state art algorithm learn sbns
produc concis model thus enabl faster test improv predict perform iii acceler converg prevent overfit
time depend spatial vari graphic model applic brain fmri data analysi spatio tempor data often exhibit nonstationari chang spatial structur often mask strong tempor depend nonsepar work present addit model split data tempor correl signal spatial correl nois model spatial correl portion use time vari gaussian graphic model assumpt smooth chang graphic model structur deriv strong singl sampl converg result confirm abil estim track meaning graphic model evolv time appli methodolog discoveri time vari spatial structur human brain fmri signal
bayesian data augment approach learn deep model data augment essenti part train process appli deep learn model motiv robust train process deep learn model depend larg annot dataset expens acquir store process therefor reason altern abl automat generat new annot train sampl use process known data augment domin data augment approach field assum new train sampl obtain via random geometr appear transform appli annot train sampl strong assumpt unclear reliabl generat model produc new train sampl paper provid novel bayesian formul data augment allow introduc theoret sound algorithm base extens generat adversari network gan new annot train point treat miss variabl generat base distribut learn train set generalis mont carlo expect maximis process classif result mnist cifar cifar 100 show better perform propos method compar current domin data augment approach
union intersect uoi interpret data driven discoveri predict increas size complex scientif data could dramat enhanc discoveri predict basic scientif applic neurosci genet system biolog etc realiz potenti howev requir novel statist analysi method interpret predict introduc union intersect uoi method flexibl modular scalabl framework enhanc model select estim method perform model select model estim intersect union oper respect show uoi satisfi criteria low varianc near unbias estim small number interpret featur maintain high qualiti predict accuraci perform extens numer investig evalu uoi algorithm uoilasso synthet real data demonstr extract interpret function network human electrophysiolog record well accur predict ofphenotyp genotyp phenotyp data reduc featur also show uoil1logist uoicur variant basic framework improv predict parsimoni classif matrix factor sever benchmark biomed data set result suggest method base uoi framework could improv interpret predict data driven discoveri across scientif field
deep learn topolog signatur infer topolog geometr inform data offer altern perspect machin learn problem method topolog data analysi persist homolog enabl obtain inform typic form summari represent topolog featur howev topolog signatur often come unusu structur multiset interv high impract machin learn techniqu mani strategi propos map topolog signatur machin learn compat represent suffer agnost target learn task contrast propos techniqu enabl input topolog signatur deep neural network learn task optim represent train approach realiz novel input layer favor theoret properti classif experi object shape social network graph demonstr versatil approach case latter even outperform state art larg margin
practic hash function similar estim dimension reduct hash basic tool dimension reduct employ sever aspect machin learn howev perfom analysi often carri abstract assumpt truli random unit cost hash function use without concern concret hash function employ concret hash function work fine suffici random input question trust real world face structur input paper focus promin applic hash name similar estim permut hash oph scheme nip featur hash weinberg icml found numer applic approxim near neighbour search lsh classif svm consid recent mix tabul hash function dahlgaard foc prove theoret perform like truli random hash function mani applic includ oph first show improv concentr bound truli random hash argu mix tabul perform similar input vector dens main contribut howev experiment comparison differ hash scheme insid applic find mix tabul hash almost fast classic multipli mod prime scheme mod guarante work well suffici random data demonstr applic lead bias poor concentr real world synthet data also compar popular murmurhash3 proven guarante mix tabul murmurhash3 perform similar truli random hash experi howev mix tabul faster murmurhash3 proven guarante good perform possibl input make reliabl
max rank assumpt pac maximum maximum select max rank element via random pairwis comparison divers applic studi mani model assumpt simpl natur assumpt strong stochast transit show max perform linear mani comparison yet rank requir quadrat mani comparison assumpt show borda score metric maximum select perform linear mani comparison rank perform nlog comparison
kernel function base triplet comparison propos way defin kernel function data set avail inform data set consist similar triplet form object similar object object machin learn problem base restrict inform becom popular recent year previous approach construct low dimension euclidean embed data set reflect given similar triplet aim defin kernel function correspond high dimension embed kernel function subsequ use appli kernel method data set
learn structur optim bipartit graph cluster cluster method wide appli document cluster gene express analysi method make use dualiti featur sampl occur structur sampl featur cluster extract graph base cluster method bipartit graph construct depict relat featur sampl exist cluster method conduct cluster graph achiev origin data matrix explicit cluster structur thus requir post process step obtain cluster result paper propos novel cluster method learn bipartit graph exact connect compon number cluster new bipartit graph learn model approxim origin graph maintain explicit cluster structur immedi get cluster result without post process extens empir result present verifi effect robust model
multi way interact regress via factor machin propos bayesian regress method account multi way interact arbitrari order among predictor variabl model make use factor mechan repres regress coeffici interact among predictor interact select guid prior distribut random hypergraph construct general finit featur model present posterior infer algorithm base gibb sampl establish posterior consist regress model method evalu extens experi simul data demonstr abl identifi meaning interact sever applic genet retail demand forecast
maximum margin interv tree learn regress function use censor interv valu output data import problem field genom medicin goal learn real valu predict function train output label indic interv possibl valu wherea exist algorithm task linear model paper investig learn nonlinear tree model propos learn tree minim margin base discrimin object function provid dynam program algorithm comput optim solut log linear time show empir algorithm achiev state art speed predict accuraci benchmark sever data set
kernel featur select via condit covari minim propos framework featur select employ kernel base measur independ find subset covari maxim predict respons build past work kernel dimens reduct formul approach constrain optim problem involv trace condit covari oper
improv graph laplacian via geometr self consist address problem set kernel bandwidth epp use manifold learn algorithm construct graph laplacian exploit connect manifold geometri repres riemannian metric laplac beltrami oper set epp optim laplacian abil preserv geometri data experi show principl approach effect robust
mixtur rank matrix approxim collabor filter low rank matrix approxim lrma method achiev excel accuraci among today collabor filter method exist lrma method rank user item featur matric typic fix rank adopt describ user item howev studi show submatric differ rank could coexist user item rate matrix approxim fix rank cannot perfect describ intern structur rate matrix therefor lead inferior recommend accuraci paper mixtur rank matrix approxim mrma method propos user item rate character mixtur lrma model differ rank meanwhil learn algorithm capit iter condit mode propos tackl non convex optim problem pertain mrma experiment studi movielen netflix dataset demonstr mrma outperform state art lrma base method term recommend accuraci
predict state recurr neural network present new model call predict state recurr neural network psrnns filter predict dynam system psrnns draw insight recurr neural network rnns predict state represent psrs inherit advantag type model like mani success rnn architectur psrnns use potenti deepli compos bilinear transfer function combin inform multipl sourc sourc act gate anoth bilinear function aris natur connect state updat bay filter like psrs observ view gate belief state show psrnns learn effect combin backpropog time bptt initi base statist consist learn algorithm psrs call stage regress 2sr also show psrnns factor use tensor decomposit reduc model size suggest interest theoret connect exist multipl architectur lstms appli psrnns dataset show outperform sever popular altern approach model dynam system case
hierarch method moment spectral method moment provid power tool learn paramet latent variabl model despit theoret appeal applic method real data still limit due lack robust model misspecif paper present hierarch approach method moment circumv limit method base replac tensor decomposit step use previous algorithm approxim joint diagon experi topic model show method outperform previous tensor decomposit method term speed model qualiti
multitask spectral learn weight automata consid problem estim multipl relat function comput weight automata wfa first present natur notion related wfas consid extent sever wfas share common under represent introduc model vector valu wfa conveni help formal notion related final propos spectral learn algorithm vector valu wfas tackl multitask learn problem joint learn multipl task form vector valu wfa algorithm enforc discoveri represent space share task benefit propos multitask approach theoret motiv showcas experi synthet real world dataset
generat local metric learn kernel regress paper show metric learn use nadaraya watson kernel regress compar standard approach bandwidth select show metric learn signific reduc mean squar error mse kernel regress particular high dimension data propos method effici learn good metric function base upon analyz perform estim gaussian distribut data key featur approach estim learn metric use inform global local structur train data theoret empir result confirm learn metric consider reduc bias mse kernel regress
principl riemannian geometri neural network studi deal neural network sens differenti transform system differenti equat form part attempt construct formal general theori neural network branch riemannian geometri perspect follow theoret result develop proven feedforward network limit number network layer goe infin first shown residu neural network dynam system first order differenti equat oppos ordinari network static impli network learn system differenti equat organ data second shown limit metric tensor residu network converg smooth thus defin riemannian manifold third shown limit backpropag graph converg differenti tensor field result suggest analog einstein general relat particl trajectori geodes curv space time manifold neural network learn curv space layer manifold determin trajectori data move network
subset select sequenti data subset select task find small subset inform item larg ground set find numer applic differ area sequenti data includ time seri order data contain import structur relationship among item impos under dynam model data play vital role select repres howev near exist subset select techniqu ignor under dynam data treat item independ lead incompat set repres paper develop new framework sequenti subset select take advantag under dynam model data promot select set repres high qualiti divers also compat accord under dynam model equip item transit dynam model pose problem integ binari optim assign sequenti item repres lead high encod divers transit potenti propos formul non convex deriv max sum messag pass algorithm solv problem effici experi synthet real data includ instruct video summar motion captur segment show sequenti subset select framework achiev better encod divers state art also success incorpor dynam data lead compat repres
quadrat converg proxim newton algorithm nonconvex spars learn propos proxim newton algorithm solv nonconvex regular spars learn problem high dimens propos algorithm integr proxim newton algorithm multi stage convex relax base differ convex program enjoy strong comput statist guarante specif leverag sophist character spars model structur assumpt local restrict strong convex hessian smooth prove within stage convex relax propos algorithm achiev local quadrat converg eventu obtain spars approxim local optimum optim statist properti convex relax numer experi provid support theori
fast sampl effici algorithm structur phase retriev consid problem recov signal magnitud measur also known phase retriev problem fundament challeng nano bio astronom imag system astronom imag speech process problem ill pose therefor addit assumpt signal measur necessari paper first studi case under signal spars develop novel recoveri algorithm call compress phase retriev altern minim copram algorithm simpl obtain via natur combin classic altern minim approach phase retriev cosamp algorithm spars recoveri despit simplic prove algorithm achiev sampl complex log gaussian sampl match best known exist result also demonstr linear converg theori practic requir extra tune paramet signal sparsiti level consid case under signal aris structur sparsiti model specif examin case block spars signal uniform block size block sparsiti problem design recoveri algorithm call block copram reduc sampl complex log suffici larg block length theta bound equat log knowledg constitut first end end linear converg algorithm phase retriev gaussian sampl complex sub quadrat depend sparsiti level signal
support order weight sparsiti overlap group hard algorithm support owl norm general norm provid better predict accuraci better handl correl variabl studi norm obtain extend support norm owl norm set overlap group result norm general hard comput tractabl certain collect group demonstr fact develop dynam program problem project onto set vector support fix number group dynam program util tree decomposit complex scale treewidth program convert extend formul associ group structur model group support norm overlap group variant order weight norm numer result demonstr efficaci new penalti
parametr simplex method spars learn high dimension spars learn impos great comput challeng larg scale data analysi paper investiag broad class spars learn approach formul linear program parametr regular factor solv parametr simplex method psm psm offer signific advantag compet method psm natur obtain complet solut path valu regular paramet psm provid high precis dual certif stop criterion psm yield spars solut iter solut sparsiti signific reduc comput cost per iter particular demonstr superior psm various spars learn approach includ dantzig selector spars linear regress spars support vector machin spars linear classif spars differenti network estim provid suffici condit psm alway output spars solut comput perform signific boost thorough numer experi provid demonstr outstand perform psm method
learn amp principl neural network base compress imag recoveri compress imag recoveri challeng problem requir fast accur algorithm recent neural network appli problem promis result exploit massiv parallel gpu process architectur oodl train data abl run order magnitud faster exist techniqu unfortun method difficult train often time specif singl measur matrix larg unprincipl blackbox recent demonstr iter spars signal recoveri algorithm unrol form interpret deep neural network take inspir work develop novel neural network architectur mimic behavior denois base approxim messag pass amp algorithm call new network learn amp ldamp ldamp network easi train appli varieti differ measur matric come state evolut heurist accur predict perform import network outperform state art bm3d amp nlr algorithm term accuraci runtim high resolut use matric fast matrix multipli implement ldamp run faster bm3d amp hundr time faster nlr
falkon optim larg scale kernel method kernel method provid principl way perform non linear nonparametr learn reli solid function analyt foundat enjoy optim statist properti howev least basic form limit applic larg scale scenario stringent comput requir term time especi memori paper take substanti step scale kernel method propos falkon novel algorithm allow effici process million point falkon deriv combin sever algorithm principl name stochast subsampl iter solver precondit theoret analysi show optim statist accuraci achiev requir essenti
memori time extens experi show state art result avail larg scale dataset achiev even singl machin
recurs sampl nystrom method give first algorithm kernel nystrom approxim run linear time number train point provabl accur kernel matric without depend regular incoher condit algorithm project kernel onto set landmark point sampl ridg leverag score requir kernel evalu addit runtim leverag score sampl long known give strong theoret guarante nystrom approxim employ fast recurs sampl scheme algorithm first make approach scalabl empir show find accur kernel approxim less time popular techniqu classic nystrom approxim random fourier featur method
effici approxim algorithm string kernel base sequenc classif sequenc classif algorithm svm requir definit distanc similar measur sequenc common use notion similar number match mer length subsequ sequenc extend definit consid mer match distanc yield better classif perform howev make problem comput much complex known algorithm comput similar comput complex render applic small valu work develop novel techniqu effici accur estim pairwis similar score enabl use much larger valu get higher predict accuraci open broad avenu appli classif approach audio imag text sequenc algorithm achiev excel approxim perform theoret guarante process solv open combinatori problem pose major hindranc scalabl exist solut give analyt bound qualiti runtim algorithm report empir perform real world biolog music sequenc dataset
robust hypothesi test function effect gaussian process work construct hypothesi test detect whether data generat function realp real belong specif reproduc kernel hilbert space structur partial known util theori reproduc kernel reduc hypothesi simpl side score test scalar paramet develop test procedur robust mis specif kernel function also propos ensembl base estim null model guarante test perform small sampl demonstr util propos method appli test problem detect nonlinear interact group continu featur evalu finit sampl perform test differ data generat function estim strategi null model result reveal interest connect notion machin learn model underfit overfit statist infer type error power hypothesi test also highlight unexpect consequ common model estim strategi estim kernel hyperparamet use maximum likelihood estim model infer
invari stabil deep convolut represent paper studi deep signal represent near invari group transform stabl action diffeomorph without lose signal inform achiev general multilay kernel introduc context convolut kernel network studi geometri correspond reproduc kernel hilbert space show signal represent stabl model function space larg class convolut neural network enjoy stabil
test learn distribut symmetr nois invari kernel embed distribut maximum mean discrep mmd result distanc distribut use tool fulli nonparametr sampl test learn distribut howev rare possibl differ sampl interest discov differ due differ type measur nois data collect artefact irrelev sourc variabl propos distanc distribut encod invari addit symmetr nois aim test whether assum true under process differ moreov construct invari featur distribut lead learn algorithm robust impair input distribut symmetr addit nois
empir studi properti random base kernel method kernel machin neural network possess univers function approxim properti nevertheless practic way choos appropri function class differ thus limit usag emerg specif neural network learn represent adapt basi function data task kernel method typic use kernel adapt width rbf kernel chang anymor contribut work contrast neural network kernel method empir studi analysi reveal random adapt base affect qualiti learn furthermor present kernel basi adapt scheme make effici usag featur retain univers properti
max margin invari featur transform unlabel data studi represent invari common transform data import learn techniqu focus local approxim invari implement within expens optim framework lack explicit theoret guarante paper studi kernel invari unitari group theoret guarante address import practic issu unavail transform version label data problem call unlabel transform problem special form semi supervis learn shot learn present theoret motiv altern approach invari kernel svm base propos max margin invari featur mmif solv problem illustr design framework face recognit demonstr efficaci approach larg scale semi synthet dataset 153 000 imag new challeng protocol label face wild lfw perform strong baselin
safetynet verifi execut deep neural network untrust cloud infer use deep neural network often outsourc cloud sinc comput demand task howev rais fundament issu trust client sure cloud perform infer correct lazi cloud provid might use simpler less accur model reduc comput load wors malici modifi infer result sent client propos safetynet framework enabl untrust server cloud provid client short mathemat proof correct infer task perform behalf client specif safetynet develop implement special interact proof protocol verifi execut class deep neural network repres arithmet circuit empir result layer deep neural network demonstr run time cost safetynet client server low safetynet detect incorrect comput neural network untrust server high probabl achiev state art accuraci mnist digit recognit timit speech recognit task
multi output polynomi network factor machin factor machin polynomi network supervis polynomi model base effici low rank decomposit extend model multi output set learn vector valu function applic multi class multi task problem cast problem learn way tensor whose slice share common decomposit propos convex formul problem develop effici condit gradient algorithm prove global converg despit fact involv non convex hidden unit select step classif task show algorithm achiev excel accuraci much sparser model exist method recommend system task show combin algorithm reduct ordin regress multi output classif show result algorithm outperform exist baselin term rank accuraci
neural hawk process neural self modul multivari point process mani event occur world event type stochast excit inhibit sens probabl elev decreas pattern sequenc previous event discov pattern help predict type event happen next propos model stream discret event continu time construct neural self modul multivari point process intens multipl event type evolv accord novel continu time lstm generat model allow past event influenc futur complex realist way condit futur event intens hidden state recurr neural network consum stream past event model desir qualit properti achiev competit likelihood predict accuraci real synthet dataset includ miss data condit
maxim spread influenc train data consid canon problem influenc maxim social network sinc semin work kempt kleinberg tardo larg disjoint effort problem first studi problem associ learn generat model produc cascad second focus algorithm challeng identifi set influenc assum generat model known recent result learn optim impli general generat model known rather learn train data algorithm influenc maxim yield constant factor approxim guarante use polynomi mani sampl drawn distribut paper describ simpl algorithm maxim influenc train data main idea behind algorithm leverag strong communiti structur social network identifi set individu influenti whose communiti littl overlap although general approxim guarante algorithm unbound show algorithm perform well experiment analyz perform prove algorithm obtain constant factor approxim guarante graph generat stochast block model tradit use model network communiti structur
induct represent learn larg graph low dimension embed node larg graph prove extrem use varieti predict task content recommend identifi protein function howev exist approach requir node graph present train embed previous approach inher transduct natur general unseen node present graphsag general induct framework leverag node featur inform text attribut effici generat node embed instead train individu embed node learn function generat embed sampl aggreg featur node local neighborhood algorithm outperform strong baselin induct node classif benchmark classifi categori unseen node evolv inform graph base citat reddit post data show algorithm general complet unseen graph use multi graph dataset protein protein interact
meta learn perspect cold start recommend item matrix factor popular techniqu product recommend known suffer serious cold start problem item cold start problem particular acut set tweet recommend new item arriv continu paper present meta learn strategi address item cold start new item arriv continu propos deep neural network architectur implement meta learn strategi first architectur learn linear classifi whose weight determin item histori second architectur learn neural network whose bias instead adapt base item histori evalu techniqu real world problem tweet recommend product data twitter demonstr propos techniqu signific beat baselin lookup tabl base user embed also outperform state art product model tweet recommend
dropoutnet address cold start recommend system latent model becom default choic recommend system due perform scalabl howev research area primarili focus model user item interact latent model develop cold start deep learn recent achiev remark success show excel result divers input type inspir result propos neural network base latent model handl cold start recommend system unlik exist approach incorpor addit content base object term instead focus learn show neural network model explicit train handl cold start dropout model train top exist latent model effect provid cold start capabl full power deep architectur empir demonstr state art accuraci public avail benchmark
feder multi task learn feder learn pose new statist system challeng train machin learn model distribut network devic work show multi task learn natur suit handl statist challeng set propos novel system awar optim method mocha robust practic system issu method theori first time consid issu high communic cost straggler fault toler distribut multi task learn result method achiev signific speedup compar altern feder set demonstr extens simul real world feder dataset
flexpoint adapt numer format effici train deep neural network deep neural network common develop train bit float point format signific gain perform energi effici could realiz train infer numer format optim deep learn despit substanti advanc limit precis infer recent year train neural network low bit width remain challeng problem present flexpoint data format aim complet replac bit float point format train infer design support deep network topolog without modif flexpoint tensor share expon dynam adjust minim overflow maxim avail dynam rang valid flexpoint train alexnet deep residu network generat adversari network use simul implement emph neon deep learn framework demonstr bit flexpoint close match bit float point train model without need tune model hyper paramet result suggest flexpoint promis numer format futur hardwar train infer
bayesian infer individu treatment effect use multi task gaussian process predic increas abund electron health record investig problem infer individu treatment effect use observ data stem potenti outcom model propos novel multi task learn framework factual counterfactu outcom model output function vector valu reproduc kernel hilbert space vvrkhs develop nonparametr bayesian method learn treatment effect use multi task gaussian process linear coregion kernel prior vvrkhs bayesian approach allow comput individu measur confid estim via pointwis credibl interv crucial realiz full potenti precis medicin impact select bias allevi via risk base empir bay method adapt multi task prior joint minim empir error factual outcom uncertainti unobserv counterfactu outcom conduct experi observ dataset intervent social program appli prematur infant left ventricular assist devic appli cardiac patient wait list heart transplant experi show method signific outperform state art
tomographi london underground scalabl model origin destin data paper address classic network tomographi problem infer local traffic given origin destin observ focuss larg complex public transport system build scalabl model exploit input output inform estim unobserv link station load user path prefer base reconstruct user travel time distribut model flexibl enough captur possibl differ path choic strategi correl user travel similar path similar time correspond likelihood function intract medium larg scale network propos distinct strategi name exact maximum likelihood infer approxim tractabl model variat infer origin intract model applic approach consid emblemat case london underground network tap tap system track start exit time locat journey day set synthet simul real data provid transport london use valid test model predict observ unobserv quantiti
match balanc nonlinear represent treatment effect estim estim treatment effect observ data challeng problem due miss counterfactu match effect strategi tackl problem wide use match estim nearest neighbor match nnm pair treat unit similar control unit term covari estim treatment effect accord howev exist match estim poor perform distribut control treatment group unbalanc moreov theoret analysi suggest bias causal effect estim would increas dimens covari paper aim address problem learn low dimension balanc nonlinear represent bnr observ data particular convert counterfactu predict classif problem develop kernel learn model domain adapt constraint design novel match estim dimens covari signific reduc project data low dimension subspac experi sever synthet real world dataset demonstr effect approach
moleculenet continu filter convolut neural network model quantum interact deep learn potenti revolution quantum chemistri ideal suit learn represent structur data speed explor chemic space convolut neural network proven first choic imag audio video data atom molecul restrict grid instead precis locat contain essenti physic inform would get lost discret thus propos use textit continu filter convolut layer abl model local correl without requir data lie grid appli layer moleculenet novel deep learn architectur model quantum interact molecul obtain joint model total energi interatom forc follow fundament quantum chemic principl includ rotate invari energi predict smooth differenti potenti energi surfac architectur achiev state art perform benchmark equilibrium molecul molecular dynam trajectori final introduc challeng benchmark chemic structur variat suggest path work
hide imag plain sight deep steganographi steganographi practic conceal secret messag within anoth ordinari messag common steganographi use unobtrus hide small messag within noisi region larger imag studi attempt place full size color imag within anoth imag size deep neural network simultan train creat hide reveal process design specif work pair system train imag drawn random imagenet databas work well natur imag wide varieti sourc beyond demonstr success applic deep learn hide imag care examin result achiev explor extens unlik mani popular steganograph method encod secret messag within least signific bit carrier imag approach compress distribut secret imag represent across avail bit
univers style transfer via featur transform univers style transfer aim transfer arbitrari visual style content imag exist feed forward base method enjoy infer effici main limit inabl general unseen style compromis visual qualiti paper present simpl yet effect method tackl limit without train pre defin style key ingredi method pair featur transform whiten color embed imag reconstruct network whiten color transform reflect direct match featur covari content imag given style imag share similar spirit optim gram matrix base cost neural style transfer demonstr effect algorithm generat high qualiti styliz imag comparison number recent method also analyz method visual whiten featur synthes textur simpl featur color
attend predict understand gene regul select attent chromatin past decad seen revolut genom technolog enabl flood genom wide profil chromatin mark recent literatur tri understand gene regul predict gene express larg scale chromatin measur fundament challeng exist learn task genom wide chromatin signal spatial structur high dimension high modular core aim understand relev factor work togeth previous studi either fail model complex depend among input signal reli separ featur analysi explain decis paper present attent base deep learn approach call chromattent use unifi architectur model interpret depend among chromatin factor control gene regul chromattent use hierarchi multipl long short term memori lstm modul encod input signal model various chromatin mark cooper automat chromattent train level attent joint target predict enabl attend differenti relev mark locat import posit per mark evalu model across differ cell type task human propos architectur accur attent score also provid better interpret state art featur visual method salienc map
unbound cach model onlin languag model open vocabulari propos extens recurr network languag model adapt predict chang data distribut associ non parametr larg scale memori compon store hidden activ seen past approach seen unbound continu cach make use modern approxim search quantize algorithm store million represent search effici show approach help adapt pretrain neural network novel data distribut tackl call rare word problem
deconvolut paragraph represent learn learn latent represent long text sequenc import first step mani natur languag process applic recurr neural network rnns becom cornerston challeng task howev qualiti sentenc rnn base decod reconstruct decreas length text propos sequenc sequenc pure convolut deconvolut autoencod framework free issu also comput effici propos method simpl easi implement leverag build block mani applic show empir compar rnns framework better reconstruct correct long paragraph quantit evalu semi supervis text classif summar task demonstr potenti better util long unlabel text data
analyz hidden represent end end automat speech recognit system neural model becom ubiquit automat speech recognit system neural network typic use acoust model complex system recent studi explor end end speech recognit system base neural network train direct predict text input acoust featur although system conceptu eleg simpler tradit system less obvious interpret train model work analyz speech represent learn deep end end model base convolut recurr layer train connectionist tempor classif ctc loss use pre train model generat frame level featur given classifi train frame classif phone evalu represent differ layer deep model compar qualiti predict phone label experi shed light import aspect end end model layer depth model complex design choic
best world transfer knowledg discrimin learn generat visual dialog model present novel train framework neural sequenc model particular ground dialog generat standard train paradigm model maximum likelihood estim mle minim cross entropi human respons across varieti domain recur problem mle train generat neural dialog model tend produc safe generic respons like know tell contrast discrimin dialog model train rank list candid human respons outperform generat counterpart term automat metric divers inform respons howev use practic sinc deploy real convers user work aim achiev best world practic use strong perform via knowledg transfer primari contribut end end trainabl generat visual dialog model receiv gradient perceptu adversari loss sequenc sampl leverag recent propos gumbel softmax approxim discret distribut specif rnn augment sequenc sampler coupl straight gradient estim enabl end end differenti also introduc stronger encod visual dialog employ self attent mechan answer encod along metric learn loss aid better captur semant similar answer respons overal propos model outperform state art visdial dataset signific margin recal
teach machin describ imag natur languag feedback robot eventu part everi household thus critic enabl algorithm learn guid non expert user paper bring human loop enabl human teacher give feedback learn agent form natur languag descript sentenc provid stronger learn signal numer reward easili point mistak correct focus problem imag caption qualiti output easili judg non expert propos phrase base caption model train polici gradient design critic provid reward learner condit human provid feedback show exploit descript feedback model learn perform better given independ written human caption
high order attent model visual question answer quest algorithm enabl cognit abil import part machin learn common trait recent cognit like task take account differ data modal visual lingual paper propos novel general applic form attent mechan learn high order correl various data modal show high order correl effect direct appropri attent relev element differ data modal requir solv joint task demonstr effect high order attent mechan task visual question answer vqa achiev state art perform standard vqa dataset
visual refer resolut use attent memori visual dialog visual dialog task answer seri inter depend question given input imag often requir resolv visual refer among question problem differ visual question answer vqa reli spatial attent visual ground estim imag question pair propos novel attent mechan exploit visual attent past resolv current refer visual dialog scenario propos model equip associ attent memori store sequenc previous attent key pair memori model retriev previous attent take account recenc relev current question order resolv potenti ambigu refer model merg retriev attent tentat obtain final attent current question specif use dynam paramet predict combin attent condit question extens experi new synthet visual dialog dataset show model signific outperform state art point situat visual refer resolut play import role moreov propos model present superior perform point improv visual dialog dataset despit signific fewer paramet baselin
semi supervis learn optic flow generat adversari network convolut neural network cnns recent appli optic flow estim problem train cnns requir suffici larg ground truth train data exist approach resort synthet unrealist dataset hand unsupervis method capabl leverag real world video train ground truth flow field avail method howev reli fundament assumpt bright constanc spatial smooth prior hold near motion boundari paper propos exploit unlabel video semi supervis learn optic flow generat adversari network key insight adversari loss captur structur pattern flow warp error without make explicit assumpt extens experi benchmark dataset demonstr propos semi supervis algorithm perform favor pure supervis semi supervis learn scheme
associ embed end end learn joint detect group introduc associ embed novel method supervis convolut neural network task detect group number comput vision problem frame manner includ multi person pose estim instanc segment multi object track usual group detect achiev multi stage pipelin instead propos approach teach network simultan output detect group assign techniqu easili integr state art network architectur produc pixel wise predict show appli method multi person pose estim report state art perform multi person pose mpii dataset coco dataset
learn deep structur multi scale featur use attent gate crfs contour predict recent work shown exploit multi scale represent deepli learn via convolut neural network cnn tremend import accur contour detect paper present novel approach predict contour advanc state art fundament aspect multi scale featur generat fusion differ previous work direct consid multi scale featur map obtain inner layer primari cnn architectur introduc hierarch deep model produc rich complementari represent furthermor refin robust fuse represent learn differ scale novel attent gate condit random field crfs propos experi ran public avail dataset bsds500 nyudv2 demonstr effect latent crf model overal hierarch framework
incorpor side inform adapt convolut comput vision task often side inform avail help solv task exampl crowd count camera perspect camera angl height give clue appear scale peopl scene side inform shown use count system use tradit hand craft featur fulli util count system base deep learn order incorpor avail side inform propos adapt convolut neural network acnn convolut filter weight adapt current scene context via side inform particular model filter weight low dimension manifold within high dimension space filter weight filter weight generat use learn filter manifold sub network whose input side inform help side inform adapt weight acnn disentangl variat relat side inform extract discrimin featur relat current context camera perspect nois level blur kernel paramet demonstr effect acnn incorpor side inform task crowd count corrupt digit recognit imag deblur experi show acnn improv perform compar plain cnn similar number paramet sinc exist crowd count dataset contain ground truth side inform collect new dataset ground truth camera angl height side inform
learn multi view stereo machin show learn multi view stereopsi system contrast recent learn base method reconstruct leverag under geometri problem featur project unproject along view ray formul oper differenti manner abl learn system end end task metric reconstruct end end learn allow util prior object shape enabl reconstruct object much fewer imag even singl imag requir classic approach well complet unseen surfac thorough evalu approach shapenet dataset demonstr benefit classic approach recent learn base method
pose guid person imag generat paper propos novel pose guid person generat network allow synthes person imag arbitrari pose base imag person novel pose generat framework util pose inform explicit consist key stage coars structur generat detail appear refin first stage condit imag target pose fed net like network generat initi coars imag person target pose second stage refin initi blurri result base autoencod conjunct discrimin adversari way extens experiment result 12864 identif imag 256256 fashion photo show model generat high qualiti person imag convinc detail
work hard know neighbor margin local descriptor learn loss introduc novel loss learn local featur descriptor inspir sift match scheme show propos loss reli maxim distanc closest posit closest negat patch could replac complex regular method use local descriptor learn work well shallow deep convolut network architectur result descriptor compact dimension sift 128 show state art perform match patch verif retriev benchmark fast comput gpu
multimod imag imag translat enforc cycl consist mani imag imag translat problem ambigu singl input imag correspond multipl possibl output work aim model distribut possibl output condit generat model set ambigu map encod low dimension latent vector random sampl test time generat learn map input along latent code output explicit enforc cycl consist latent code output encourag invert help prevent mani map latent code output train also known problem mode collaps help produc divers result evalu relationship perceptu realism divers imag generat method test varieti domain
deep supervis discret hash rapid growth imag video data web hash extens studi imag video search recent year benefit recent advanc deep learn deep hash method achiev promis result imag retriev howev limit previous deep hash method semant inform fulli exploit paper develop deep supervis discret hash algorithm base assumpt learn binari code ideal classif pairwis label inform classif inform use learn hash code within stream framework constrain output last layer binari code direct rare investig deep hash algorithm discret natur hash code altern minim method use optim object function experiment result shown method outperform current state art method benchmark dataset
svd softmax fast softmax approxim larg vocabulari neural network propos fast approxim method softmax function larg vocabulari use singular valu decomposit svd svd softmax target fast accur probabl estim topmost probabl word infer recurr neural network languag model propos method transform weight matrix use calcul logit use svd approxim probabl word estim fraction svd transform matrix appli techniqu languag model neural machin translat present guidelin good approxim algorithm requir arithmet oper 800k vocabulari case show speedup gpu
hash embed effici word represent present hash embed effici method repres word continu vector form hash embed seen interpol standard word embed word embed creat use random hash function hash trick hash embed token repres dimension embed vector dimension weight vector final dimension represent token product rather fit embed vector token select hash trick share pool embed vector experi show hash embed easili deal huge vocabulari consist million token use hash embed need creat dictionari train perform kind vocabulari prune train show model train use hash embed exhibit least level perform model train use regular embed across wide rang task furthermor number paramet need embed fraction requir regular embed sinc standard embed embed construct use hash trick actual special case hash embed hash embed consid extens improv exist regular embed type
regular framework spars structur neural attent modern neural network often augment attent mechan tell network focus within input propos paper new framework spars structur attent build upon max oper regular strong convex function show oper differenti gradient defin map real valu probabl suitabl attent mechan framework includ softmax slight general recent propos sparsemax special case howev also show framework incorpor modern structur penalti result new attent mechan focus entir segment group input encourag parsimoni interpret deriv effici algorithm comput forward backward pass attent mechan enabl use neural network train backpropag showcas potenti drop replac exist attent mechan evalu larg scale task textual entail machin translat sentenc summar attent mechan improv interpret without sacrif perform notabl textual entail summar outperform exist attent mechan base softmax sparsemax
attent pool action recognit introduc simpl yet surpris power model incorpor attent action recognit human object interact task propos attent modul train without extra supervis give sizabl boost accuraci keep network size comput cost near lead signific improv state art base architectur standard action recognit benchmark across still imag video establish new state art mpii relat improv hmdb rgb dataset also perform extens analysi attent modul empir analyt term latter introduc novel deriv bottom top attent low rank approxim bilinear pool method typic use fine grain classif perspect attent formul suggest novel character action recognit fine grain recognit problem
plan attend generat plan sequenc sequenc model investig integr plan mechan encod decod architectur attent present model plan ahead comput align input output sequenc construct matrix propos futur align commit vector govern whether follow recomput plan mechan inspir strateg attent reader writer straw model propos model end end trainabl fulli differenti oper show outperform strong baselin charact level translat task wmt algorithm task find eulerian circuit graph among other analysi demonstr model comput qualit intuit align converg faster baselin achiev superior perform fewer paramet
dilat recurr neural network notori learn recurr neural network rnns long sequenc difficult task major challeng extract complex depend vanish explod gradient effici parallel paper introduc simpl yet effect rnn connect structur dilatedrnn simultan tackl challeng propos architectur character multi resolut dilat recurr skip connect combin flexibl differ rnn cell moreov dilatedrnn reduc number paramet enhanc train effici signific match state art perform even vanilla rnn cell task involv long term depend provid theori base quantif architectur advantag introduc memori capac measur mean recurr length suitabl rnns long skip connect exist measur rigor prove advantag dilatedrnn recurr neural architectur
thalamus gate recurr modul propos deep learn model inspir neurosci theori communic within neocortex model consist recurr modul send featur via rout center endow neural modul flexibl share featur multipl time step show model learn rout inform hierarch process input data chain modul observ common architectur feed forward neural network skip connect emerg special case architectur novel connect pattern learn text8 compress task model outperform multi layer recurr network sequenti task
wasserstein learn deep generat point process model point process becom popular model asynchron sequenti data due sound mathemat foundat strength model varieti real world phenomena current often character via intens function limit model express due unrealist assumpt parametr form use practic furthermor learn via maximum likelihood approach prone failur multi modal distribut sequenc paper propos intens free approach point process model transform nuisanc process target furthermor train model use likelihood free leverag wasserstein distanc point process experi various synthet real world data substanti superior propos point process model convent one
stabil train generat adversari network regular deep generat model base generat adversari network gan demonstr impress sampl qualiti order work requir care choic architectur paramet initi select hyper paramet fragil part due dimension mismatch model distribut true distribut caus densiti ratio associ diverg undefin overcom fundament limit propos new regular approach low comput cost yield stabl gan train procedur demonstr effect approach sever dataset includ common benchmark imag generat task approach turn gan model reliabl build block deep learn
neural variat infer learn undirect graphic model mani problem machin learn natur express languag undirect graphic model propos learn infer algorithm undirect model optim variat approxim log likelihood model central approach upper bound log partit function parametr function express flexibl neural network bound enabl accur track partit function learn speed sampl train broad class power hybrid direct undirect model via unifi variat infer framework empir demonstr effect method sever popular generat model dataset
adversari symmetr variat autoencod new form variat autoencod vae develop joint distribut data code consid symmetr form observ data fed encod yield code latent code drawn simpl prior propag decod manifest data lower bound learn margin log likelihood fit observ data latent code learn variat bound seek minim symmetr kullback leibler diverg joint densiti function simultan seek maxim margin log likelihood facilit learn new form adversari train develop extens set experi perform demonstr state art data reconstruct generat sever imag benchmark dataset
divers accur imag descript use variat auto encod addit gaussian encod space paper propos method generat imag descript use condit variat auto encod cvae data depend gaussian prior encod space standard cvae fix gaussian prior easili collaps generat descript littl variabl approach address problem linear combin multipl gaussian prior base semant content imag increas flexibl represent power generat model evalu addit gaussian cvae cvae approach mscoco dataset show produc caption divers accur strong lstm baselin cvae variant
forc train stochast recurr network mani effort devot incorpor stochast latent variabl sequenti neural model recurr neural network rnns rnns latent variabl success captur variabl observ natur structur data speech work propos novel recurr latent variabl model unifi success idea recent propos architectur model step sequenc associ latent variabl use condit recurr dynam futur step model train amortis variat infer infer network augment rnn run backward sequenc addit next step predict add auxiliari cost latent variabl forc reconstruct state backward recurr network provid latent variabl task independ object enhanc perform overal model although conceptu simpl model achiev state art result standard speech benchmark timit blizzard final appli model languag model imdb dataset auxiliari cost crucial learn interpret latent variabl set show regular evid lower bound signific underestim log likelihood model thus encourag futur work compar likelihood method use tighter bound
shot imit learn imit learn common appli solv differ task isol usual requir either care featur engin signific number sampl far desir ideal robot abl learn demonstr given task instant general new situat task without requir task specif engin paper propos meta learn framework achiev capabl call shot imit learn specif consid set larg mayb infinit set task task mani instanti exampl task could stack block tabl singl tower anoth task could place block tabl block tower etc case differ instanc task would consist differ set block differ initi state train time algorithm present pair demonstr subset task neural net train take input demonstr current state initi initi state demonstr pair output action goal result sequenc state action match close possibl second demonstr test time demonstr singl instanc new task present neural net expect perform well new instanc new task experi show use soft attent allow model general condit task unseen train data anticip train model much greater varieti task set obtain general system turn demonstr robust polici accomplish overwhelm varieti task
reconstruct crush network articl introduc energi base model adversari regard data minim energi given data distribut posit sampl maxim energi anoth given data distribut negat unlabel sampl model especi instanti autoencod energi repres reconstruct error provid general distanc measur unknown data result neural network thus learn reconstruct data first distribut crush data second distribut solut handl differ problem posit unlabel learn covari shift especi imbalanc data use autoencod allow handl larg varieti data imag text even dialogu experi show flexibl propos approach deal differ type data differ set imag cifar cifar 100 train set text amazon review learn dialogu facebook babi next respons classif dialogu complet
fader network generat imag variat slide attribut valu paper introduc new encod decod architectur train reconstruct imag disentangl salient inform imag valu attribut direct latent space result train model generat differ realist version input imag vari attribut valu use continu attribut valu choos much specif attribut perceiv generat imag properti could allow applic user modifi imag use slide knob like fader mix consol chang facial express portrait updat color object compar state art most reli train adversari network pixel space alter attribut valu train time approach result much simpler train scheme nice scale multipl attribut present evid model signific chang perceiv valu attribut preserv natur imag
predrnn recurr neural network video predict use spatiotempor lstms predict learn video sequenc aim generat futur imag learn histor frame spatial appear tempor variat crucial structur paper model structur present predict recurr neural network predrnn architectur enlighten idea video predict system memor spatial appear tempor variat unifi memori pool concret memori state longer constrain insid lstm unit instead allow zigzag direct across stack rnn layer vertic time step horizont core network new spatiotempor lstm lstm unit extract memor spatial tempor video represent simultan predrnn achiev state art predict perform standard video dataset believ general framework extend predict learn task beyond video predict
multi agent predict model attent commnet multi agent predict model essenti step understand physic social team play system recent interact network in propos task model multi agent physic system in scale number interact system typic quadrat higher order number agent paper introduc vain attent commnet multi agent predict model scale linear number agent show vain effect multi agent predict model represent learn transfer learn new data poor task method evalu task challeng multi agent predict domain chess soccer outperform compet multi agent approach
real time imag salienc black box classifi work develop fast salienc detect method appli differenti imag classifi train mask model manipul score classifi mask salient part input imag model generalis well unseen imag requir singl forward pass perform salienc detect therefor suitabl use real time system test approach cifar imagenet dataset show produc salienc map easili interpret sharp free artifact suggest new metric salienc test method imagenet object localis task achiev result outperform weak supervis method
prototyp network shot learn propos prototyp network problem shot classif classifi must general new class seen train set given small number exampl new class prototyp network learn metric space classif perform comput distanc prototyp represent class compar recent approach shot learn reflect simpler induct bias benefici limit data regim achiev excel result provid analysi show simpl design decis yield substanti improv recent approach involv complic architectur choic meta learn extend prototyp network shot learn achiev state art result bird dataset
shot learn inform retriev len shot learn refer understand new concept exampl propos inform retriev inspir approach problem motiv increas import maxim leverag avail inform low data regim defin train object aim extract much inform possibl train batch effect optim relat order batch point simultan particular view batch point queri rank remain one base predict relev defin model framework structur predict optim mean averag precis rank method produc state art result standard benchmark shot learn
revers residu network backpropag without store activ residu network resnet demonstr signific improv tradit convolut neural network cnns imag classif increas perform network grow deeper wider howev memori consumpt becom bottleneck need store intermedi activ calcul gradient use backpropag work present revers residu network revnet variant resnet layer activ reconstruct exact next layer therefor activ layer need store memori backprop demonstr effect revnet cifar imagenet establish near ident perform equal size resnet activ storag requir independ depth
gate recurr convolut neural network ocr optic charact recognit ocr aim recogn text natur imag wide research comput vision communiti paper present new architectur name gate recurr convolut layer grcl challeng grcl construct ad gate recurr convolut layer rcl find equip gate control context modul rcl balanc feed forward compon well recurr compon addit build bidirect long short term memori blstm sequenc model test sever variant blstm find suitabl architectur ocr final combin gate recurr convolut neural network grcnn blstm recogn text natur imag grcnn blstm train end end outperform benchmark dataset term state art result includ iiit street view text svt icdar
learn effici object detect model knowledg distil despit signific accuraci improv convolut neural network cnn base object detector often requir prohibit runtim process imag real time applic state art model often use deep network larg number float point oper effort model compress learn compact model fewer number paramet much reduc accuraci work propos new framework learn compact fast object detect network improv accuraci use knowledg distil hint learn although knowledg distil demonstr excel improv simpler classif setup complex detect pose new challeng form regress region propos less volumin label address sever innov weight cross entropi loss address class imbal teacher bound loss handl regress compon adapt layer better learn intermedi teacher distribut conduct comprehens empir evalu differ distil configur multipl dataset includ pascal kitti ilsvrc coco result show consist improv accuraci speed trade off modern multi class detect model
activ bias train accur neural network emphas high varianc sampl self pace learn hard exampl mine weight train instanc improv learn accuraci paper present improv altern base lightweight estim sampl uncertainti stochast gradient descent sgd varianc predict probabl correct class across iter mini batch sgd proxim correct class probabl decis threshold extens experiment result dataset show method reliabl improv accuraci various network architectur includ addit gain top popular train techniqu residu learn momentum adam batch normal dropout distil
decoupl updat updat deep learn requir data use approach obtain data creativ mine data various sourc creat differ purpos unfortun approach often lead noisi label paper propos meta algorithm tackl noisi label problem key idea decoupl updat fromhow updat demonstr effect algorithm mine data gender classif combin label face wild lfw face recognit dataset textual gender servic lead noisi dataset approach simpl implement lead state art result analyz converg properti propos algorithm
langevin dynam continu temper train deep neural network minim non convex high dimension object function challeng especi train modern deep neural network paper novel approach propos divid train process consecut phase obtain better general perform bayesian sampl stochast optim first phase explor energi landscap captur fat mode thesecondo etu theparamet domthefirstphas inthebayesian arn gphase weapplycont uoustemp gand imat othelan dynam createanefficientandeffectivesamp whichthetemperatureisadjustedau maticallyaord thedesig temperatur dynam strategi overcom challeng earli trap bad local minima achiev remark improv various type neural network shown theoret analysi empir experi
differenti learn logic rule knowledg base reason studi problem learn probabilist first order logic rule knowledg base reason learn problem difficult requir learn paramet continu space well structur discret space propos framework neural logic program combin paramet structur learn first order logic rule end end differenti model approach inspir recent develop differenti logic call tensorlog infer task compil sequenc differenti oper design neural control system learn compos oper empir method obtain state art result multipl knowledg base benchmark dataset includ freebas wikimovi
deliber network sequenc generat beyond pass decod encod decod framework achiev promis progress mani sequenc generat task includ machin translat text summar dialog system imag caption etc framework adopt pass forward process decod generat sequenc lack deliber process generat sequenc direct use final output without polish howev deliber common behavior human daili life like read news write paper articl book work introduc deliber process encod decod framework propos deliber network sequenc generat deliber network level decod first pass decod generat raw sequenc second pass decod polish refin raw sentenc deliber sinc second pass deliber decod overal pictur sequenc generat might potenti generat better sequenc look futur word raw sentenc experi neural machin translat text summar demonstr effect propos deliber network
neural program meta induct recent propos method neural program induct work assumpt larg set input output exampl learn given input output map paper aim address problem data comput effici program induct leverag inform relat task specif propos novel approach cross task knowledg transfer improv program induct limit data scenario first propos portfolio adapt set induct model pretrain set relat task best model adapt toward new task use transfer learn second approach meta program induct shot learn approach use make model general new task without addit train test efficaci method construct new benchmark program written karel program languag use extens experiment evalu karel benchmark demonstr propos dramat outperform baselin induct method use knowledg transfer also analyz relat perform approach studi condit perform best particular meta induct outperform exist approach extrem data sparsiti small number exampl avail fewer number avail exampl increas thousand portfolio adapt program induct becom best approach intermedi data size demonstr combin method adapt meta program induct strongest perform
salienc base sequenti imag attent multiset predict human process visual scene select sequenti use attent central model human visual attent salienc map propos hierarch visual architectur oper salienc map use novel attent mechan sequenti focus salient region take addit glimps within region architectur motiv human visual attent use multi label imag classif novel multiset task demonstr achiev high precis recal local object attent unlik convent multi label imag classif model model support multiset predict due reinforc learn base train process allow arbitrari label permut multipl instanc per label
protein interfac predict use graph convolut network present general framework graph convolut classif task label graph node edg featur perform convolut oper neighborhood node interest abl stack multipl layer convolut learn effect latent represent integr inform across input graph demonstr effect approach predict interfac protein challeng problem import applic drug discoveri design propos approach achiev accuraci better state art svm method task also outperform recent propos diffus convolut form graph convolut
dual agent gan photorealist ident preserv profil face synthesi synthes realist profil face promis effici train deep pose invari model larg scale unconstrain face recognit popul sampl extrem pose avoid tedious annot howev learn synthet face achiev desir perform due discrep distribut synthet real face imag narrow gap propos dual agent generat adversari network gan model improv realism face simul output use unlabel real face preserv ident inform realism refin dual agent specif design distinguish real fake ident simultan particular employ shelf face model simul generat profil face imag vari pose gan leverag fulli convolut network generat generat high resolut imag auto encod discrimin dual agent besid novel architectur make sever key modif standard gan preserv pose textur preserv ident stabil train process pose percept loss ident percept loss iii adversari loss boundari equilibrium regular term experiment result show gan present compel perceptu result also signific outperform state art larg scale challeng nist ijb unconstrain face recognit benchmark addit propos gan also promis new approach solv generic transfer learn problem effect
toward robust label nois train deep discrimin neural network collect larg train dataset annot high qualiti label cost process paper propos novel framework train deep convolut neural network noisi label dataset problem formul use undirect graphic model repres relationship noisi clean label train semi supervis set propos structur infer latent clean label tractabl regular train use auxiliari sourc inform propos model appli imag label problem shown effect label unseen imag well reduc label nois train cifar coco dataset
soft hard vector quantize end end learn compress represent present new approach learn compress represent deep architectur end end train strategi method base soft continu relax quantize entropi anneal discret counterpart throughout train showcas method challeng applic imag compress neural network compress task typic approach differ method soft hard quantize approach give result competit state art
select classif deep neural network select classif techniqu also known reject option yet consid context deep neural network dnns techniqu potenti signific improv dnns predict perform trade coverag paper propos method construct select classifi given train neural network method allow user set desir risk level test time classifi reject instanc need grant desir risk high probabl empir result cifar imagenet convinc demonstr viabil method open possibl oper dnns mission critic applic exampl use method unpreced error top imagenet classif guarante probabl almost test coverag
deep lattic network partial monoton function propos learn deep model monoton respect user specifi set input altern layer linear embed ensembl lattic calibr piecewis linear function appropri constraint monoton joint train result network implement layer project new comput graph node tensorflow use adam optim batch stochast gradient experi benchmark real world dataset show layer monoton deep lattic network achiev state art perform classif regress monoton guarante
learn prune deep neural network via layer wise optim brain surgeon develop slim accur deep neural network becom crucial real world applic especi employ embed system though previous work along research line shown promis result exist method either fail signific compress well train deep network requir heavi retrain process prune deep network boost predict perform paper propos new layer wise prune method deep neural network propos method paramet individu layer prune independ base second order deriv layer wise error function respect correspond paramet prove final predict perform drop prune bound linear combin reconstruct error caus layer therefor guarante need perform light retrain process prune network resum origin predict perform conduct extens experi benchmark dataset demonstr effect prune method compar sever state art baselin method
bayesian compress deep learn compress comput effici deep learn becom problem great signific work argu principl effect way attack problem take bayesian point view sparsiti induc prior prune larg part network introduc novelti paper use hierarch prior prune node instead individu weight use posterior uncertainti determin optim fix point precis encod weight factor signific contribut achiev state art term compress rate still stay competit method design optim speed energi effici
lower bound robust adversari perturb input output map learn state art neural network signific discontinu possibl caus neural network use imag recognit misclassifi input appli specif hard percept perturb input call adversari perturb mani hypothes propos explain exist peculiar sampl well sever method mitig proven explan remain elus howev work take step toward formal character adversari perturb deriv lower bound magnitud perturb necessari chang classif neural network bound experiment verifi mnist cifar data set
sobolev train neural network heart deep learn aim use neural network function approxim train produc output input emul ground truth function data creation process mani case access input output pair ground truth howev becom common access deriv target output respect input exampl ground truth function neural network network compress distil general target deriv comput ignor paper introduc sobolev train neural network method incorpor target deriv addit target valu train optimis neural network approxim function output also function deriv encod addit inform target function within paramet neural network therebi improv qualiti predictor well data effici general capabl learn function approxim provid theoret justif approach well exampl empir evid distinct domain regress classic optimis dataset distil polici agent play atari larg scale applic synthet gradient domain use sobolev train employ target deriv addit target valu result model higher accuraci stronger generalis
structur bayesian prune via log normal multipl nois dropout base regular method regard inject random nois pre defin magnitud differ part neural network train recent shown bayesian dropout procedur improv general also lead extrem spars neural architectur automat set individu nois magnitud per weight howev sparsiti hard use acceler sinc unstructur paper propos new bayesian model take account comput structur neural network provid structur sparsiti remov neuron convolut channel cnns inject nois neuron output keep weight unregular establish probabilist model proper truncat log uniform prior nois truncat log normal variat approxim ensur term evid lower bound comput close form model lead structur sparsiti remov element low snr comput graph provid signific acceler number deep neural architectur model easi implement correspond addit dropout like layer comput graph
popul match discrep applic deep learn differenti estim distanc distribut base sampl import mani deep learn task estim maximum mean discrep mmd howev mmd suffer sensit kernel bandwidth hyper paramet weak gradient larg mini batch size use train object paper propos popul match discrep pmd estim distribut distanc base sampl well algorithm learn paramet distribut use pmd object pmd defin minimum weight match sampl popul distribut prove pmd strong consist estim first wasserstein metric appli pmd deep learn task domain adapt generat model empir result demonstr pmd overcom aforement drawback mmd outperform mmd task term perform well converg speed
investig learn dynam deep neural network use random matrix theori evid well condit singular valu distribut input output jacobian lead substanti improv train perform deep neural network deep linear network conclus evid initi use orthogon random matric lead dramat improv train howev benefit initi strategi proven much less obvious realist nonlinear network use random matrix theori studi condit jacobian nonlinear neural network random initi show singular valu distribut jacobian sensit distribut weight also nonlinear surpris find benefit orthogon initi neglig rectifi linear network substanti tanh network provid rule thumb initi tanh network display dynam isometri full depth final perform experi mnist cifar10 use wide array optim show conclus singular valu distribut jacobian intim relat learn dynam final show spectral densiti jacobian evolv relat slowli train good initi affect learn dynam far initi set weight
robust imit divers behavior deep generat model recent shown great promis imit learn motor control given enough data even supervis approach shot imit learn howev vulner cascad failur agent trajectori diverg demonstr compar pure supervis method generat adversari imit learn gail learn robust control fewer demonstr inher mode seek difficult train paper show combin favour aspect approach base model new type variat autoencod demonstr trajectori learn semant polici embed show embed learn dof jaco robot arm reach task smooth interpol result smooth interpol reach behavior leverag polici represent develop new version gail much robust pure supervis control especi demonstr avoid mode collaps captur mani divers behavior gail demonstr approach learn divers gait demonstr bipe dof humanoid mujoco physic environ
question ask program generat hallmark human intellig abil ask rich creativ reveal question introduc cognit model capabl construct human like question approach treat question formal program execut state world output answer model specifi probabl distribut complex composit space program favor concis program help agent learn current context evalu approach model type open end question generat human attempt learn ambigu situat game find model predict question peopl ask general novel situat creativ way addit compar number model variant assess featur critic produc human like question
variat law visual attent dynam scene comput model visual attent crossroad disciplin like cognit scienc comput neurosci comput vision paper propos approach base principl foundat law drive emerg visual attent devis variat law eye movement reli general view least action principl physic potenti energi captur detail well peripher visual featur kinet energi correspond classic interpret analyt mechan addit lagrangian contain bright invari term character signific scanpath trajectori obtain differenti equat visual attent stationari point general action propos algorithm estim model paramet final report experiment result valid model task salienc detect
flexibl statist infer mechanist model neural dynam mechanist model singl neuron dynam extens studi comput neurosci howev identifi model quantit reproduc empir measur data challeng propos overcom limit use likelihood free infer approach also known approxim bayesian comput abc perform full bayesian infer singl neuron model approach build recent advanc abc learn neural network map featur observ data posterior distribut paramet learn bayesian mixtur densiti network approxim posterior multipl round adapt chosen simul furthermor propos effici approach handl miss featur paramet set simul fail preval issu model neural dynam well strategi automat learn relev featur use recurr neural network synthet data approach effici estim posterior distribut recov ground truth paramet vitro record membran voltag recov multivari posterior biophys paramet yield model predict voltag trace accur match empir data approach enabl neuroscientist perform bayesian infer complex neuron model without design model specif algorithm close gap mechanist statist approach singl neuron model
train recurr network generat hypothes brain solv hard navig problem self local navig noisi sensor ambigu world comput challeng yet anim human excel robot simultan locat map slam algorithm solv problem though joint sequenti probabilist infer coordin extern spatial landmark generat first neural solut slam problem train recurr lstm network perform set hard navig task requir general complet novel trajectori environ goal make sens divers phenomenolog brain spatial navig circuit relat function show hidden unit represent exhibit sever key properti hippocamp place cell includ stabl tune curv remap environ result also proof concept end end learn slam algorithm use recurr network demonstr approach advantag robot slam
yass yet anoth spike sorter spike sort critic first step extract neural signal larg scale electrophysiolog data manuscript describ automat effici reliabl pipelin spike sort dens multi electrod array mea neural signal appear across mani electrod spike sort current repres major comput bottleneck present sever new techniqu make dens mea spike sort robust scalabl pipelin base effici multi stage triag cluster pursuit approach initi extract clean high qualiti waveform electrophysiolog time seri temporarili discard noisi orcollid event repres neuron fire synchron accomplish develop neural net detect method follow effici outlier triag clean waveform use infer number neuron shape nonparametr bayesian cluster cluster approach adapt coreset approach data reduct use effici infer method dirichlet process mixtur model framework dramat improv scalabl reliabl entir pipelin thetriag waveform final recov match pursuit deconvolut techniqu propos method improv state art term accuraci stabil real biophys realist simul mea data furthermor propos pipelin effici learn templat cluster much faster real time 512 electrod dataset use primarili singl cpu core
neural system identif larg popul separ neuroscientist classifi neuron differ type perform similar comput differ locat visual field tradit neural system identif method capit separ learn deep convolut featur space share among mani neuron provid excit path forward architectur design need account data limit new experiment techniqu enabl record thousand neuron experiment time limit sampl small fraction neuron respons space show major bottleneck fit convolut neural network cnns neural data estim individu recept field locat problem scratch surfac thus far propos cnn architectur spars pool layer factor spatial featur dimens network scale well thousand neuron short record train end end explor architectur ground truth data explor challeng limit cnn base system identif moreov show network model outperform current state art system identif model mous primari visual cortex public avail dataset
simpl model recognit recal memori show sever strike differ memori perform recognit recal task explain ecolog bias endem classic memori experi experi univers involv stimuli retriev cue show sensibl think recal simpli retriev item probe cue typic item list better think recognit retriev cue probe item test theori manipul number item cue memori experi show crossov effect memori perform within subject recognit perform superior recal perform number item greater number cue recal perform better recognit convers hold build simpl comput model around theori use sampl approxim ideal bayesian observ encod retriev situat occurr frequenc stimuli retriev cue model robust reproduc number dissoci recognit recal previous use argu dual process account declar memori
gaussian process base nonlinear latent structur discoveri multivari spike train data larg bodi recent work focus method identifi low dimension latent structur multi neuron spike train data method employ either linear latent dynam linear log linear map latent space spike rate propos doubli nonlinear latent variabl model popul spike train identifi nonlinear low dimension structur under appar high dimension spike train data model poisson gaussian process latent variabl model gplvm defin low dimension latent variabl govern gaussian process nonlinear tune curv parametr exponenti sampl second gaussian process poisson observ nonlinear tune curv allow discoveri low dimension latent embed even spike rate span high dimension subspac hippocamp place cell code learn model introduc decoupl laplac approxim fast approxim infer method allow effici maxim margin likelihood latent path integr tune curv show method outperform previous approach maxim laplac approxim base margin likelihood converg speed valu final object appli model spike train record hippocamp place cell show outperform varieti previous method latent structur discoveri includ variat auto encod base method parametr nonlinear map latent space spike rate deep neural network
deep adversari neural decod present novel approach solv problem reconstruct perceiv stimuli brain respons combin probabilist infer deep learn approach first invert linear transform latent featur brain respons maximum posteriori estim invert nonlinear transform perceiv stimuli latent featur adversari train convolut neural network test approach function magnet reson imag experi show generat state art reconstruct perceiv face brain activ
cross spectral factor analysi neuropsychiatr disord schizophrenia depress often disrupt way differ region brain communic anoth order build greater understand neurolog basi disord introduc novel model multisit local field potenti lfps low frequenc voltag oscil measur electrod implant mani brain region simultan propos model call cross spectral factor analysi csfa break observ lfps electr function connectom electom defin differ spatiotempor properti electom defin uniqu frequenc power phase coher pattern mani brain region properti grant featur via gaussian process formul multipl kernel learn framework critic electom interpret use design follow causal studi furthermor use formul lfp signal map lower dimension space better tradit approach remark addit interpret propos approach achiev state art predict abil compar black box approach look behavior paradigm genotyp predict task mous model demonstr featur basi captur neural dynam relat outcom conclud discuss csfa analysi use conjunct experi design causal studi provid gold standard valid infer neural relationship
cognit impair predict alzheim diseas regular modal regress accur automat predict cognit assess via neuroimag marker critic earli detect alzheim diseas linear regress model success use associ studi neuroimag featur cognit perform alzheim diseas studi howev exist method built least squar mean squar error mse criterion sensit outlier perform degrad heavi tail nois complex brain disord data paper beyond criterion investig regular modal regress statist learn viewpoint new regular scheme base modal regress propos estim variabl select robust outlier heavi tail nois skew nois conduct theoret analysi establish approxim bound learn condit mode function sparsiti analysi variabl select robust character experiment evalu simul data adni cohort data provid support promis perform propos algorithm
stochast submodular maxim case coverag function continu optim techniqu sgd extens main workhors modern machin learn nevertheless varieti import machin learn problem requir solv discret optim problem submodular object goal paper unleash toolkit modern continu optim discret problem first introduc framework emph stochast submodular optim instead emph oracl access under object explicit consid statist comput aspect evalu object provid formal emph stochast submodular maxim class import discret optim problem show state art techniqu continu optim lift realm discret optim extens experiment evalu demonstr practic impact propos approach
gradient method submodular maxim paper studi problem maxim continu submodular function natur aris mani learn applic involv util function activ learn sens matrix approxim network infer despit appar lack convex function prove stochast project gradient method provid strong approxim guarante maxim continu submodular function convex constraint specif prove monoton continu submodular function fix point project gradient ascent provid factor approxim global maxima also studi stochast gradient mirror method show iter method reach solut achiev expectaion object valu exceed opt2 immedi implic result bridg discret continu submodular maxim final experi real data demonstr project gradient method consist achiev best util compar continu baselin remain competit term comput effort
non convex finit sum optim via scsg method develop class algorithm variant stochast control stochast gradient scsg method smooth nonconvex finit sum optim problem assum smooth compon complex scsg reach stationari point min 1n2 strict outperform stochast gradient descent moreov scsg never wors state art method base varianc reduct signific outperform target accuraci low similar acceler also achiev function satisfi polyak lojasiewicz condit empir experi demonstr scsg outperform stochast gradient method train multi layer neural network term train valid loss
influenc maxim almost submodular threshold function influenc maxim problem select node social network maxim influenc spread problem extens studi work focus submodular influenc diffus model paper motiv empir evid explor influenc maxim non submodular regim particular studi general threshold model fraction node non submodular threshold function threshold function close upper lower bound submodular function call almost submodular first show strong hard result approxim influenc maxim unless network almost submodular node paramet depend although threshold function close submodular influenc maxim still hard approxim provid approxim algorithm number almost submodular node final conduct experi number real world dataset result demonstr approxim algorithm outperform benchmark algorithm
subset select nois problem select best element subset univers involv mani applic previous studi assum nois free environ noisi monoton submodular object function paper consid realist general situat evalu subset noisi monoton function necessarili submodular multipl addit nois understand impact nois first show approxim ratio greedi algorithm poss power algorithm nois free subset select noisi environ propos incorpor nois awar strategi poss result new ponss algorithm better approxim ratio empir result influenc maxim spars regress problem show superior perform ponss
polynomi time algorithm dual volum sampl studi dual volum sampl method select column short wide matrix probabl select proport volum span row induc submatrix method propos avron boutsidi 2013 show promis method column subset select multipl applic howev wider adopt hamper lack polynomi time sampl algorithm remov hindranc develop exact random polynomi time sampl algorithm well derandom thereaft studi dual volum sampl via theori real stabl polynomi prove distribut satisfi strong rayleigh properti result remark consequ especi impli provabl fast mix markov chain sampler make dual volum sampl much attract practition sampler close relat classic algorithm popular experiment design method date lack theoret analysi known empir work well
lookahead bayesian optim inequ constraint consid task optim object function subject inequ constraint object constraint expens evalu bayesian optim popular way tackl optim problem expens object function evalu most appli unconstrain problem sever approach propos address expens constraint limit greedi strategi maxim immedi reward address limit propos lookahead approach select next evalu order maxim long term feasibl reduct object function present numer experi demonstr perform improv lookahead approach compar greedi algorithm constrain expect improv eic predict entropi search constraint pesc
non monoton continu submodular maxim structur algorithm submodular continu function import object wide real world applic span map infer determinant point process dpps mean field infer probabilist submodular model amongst other submodular captur subclass non convex function enabl exact minim approxim maxim polynomi time work studi problem maxim non monoton submodular continu function general close convex constraint start investig sever properti underli object use devis optim algorithm provabl guarante concret first devis phase algorithm approxim guarante algorithm allow use exist method ensur find approxim stationari point subroutin thus enabl util recent progress non convex optim present non monoton frank wolf variant approxim guarante sublinear converg rate final extend approach broader class general submodular continu function captur wider spectrum applic theoret find valid sever synthet real world problem instanc
solv almost system random quadrat equat paper deal find dimension solut bmx system quadrat equat bmai bmx general known hard put forth novel procedur start emph weight maxim correl initi obtain power iter follow success refin base emph iter reweight gradient type iter novel techniqu distinguish prior work inclus fresh weight regular certain random measur model propos procedur return true solut bmx high probabl time proport read data bmai provid number equat constant time number unknown name empir upshot contribut perfect signal recoveri high dimension regim given inform theoret limit number equat near optim statist accuraci presenc addit nois extens numer test use synthet data real imag corrobor improv signal recoveri perform comput effici relat state art approach
learn relus via gradient descent paper studi problem learn rectifi linear unit relus function form vctx max vctw vctx vctw denot weight vector studi problem high dimension regim number observ fewer dimens weight vector assum weight vector belong close set convex nonconvex captur known side inform structur focus realiz model input chosen gaussian distribut label generat accord plant weight vector show project gradient descent initi vct0 converg linear rate plant model number sampl optim numer constant result dynam converg shallow neural net provid insight toward understand dynam deeper architectur
stochast mirror descent non convex optim paper examin class non convex stochast program call emph variat coher proper includ quasi pseudo convex optim problem establish converg class problem studi well known smd method show algorithm last iter converg problem global optimum probabl result contribut landscap non convex optim clarifi convex quasi convex essenti global converg rather variat coher much weaker requir suffic local class account local variat coher problem show last iter stochast mirror descent converg local optima high probabl final consid last iter converg rate problem sharp minima deriv special case conclus probabl last iter stochast gradient descent reach exact global optimum finit number step result contrast exist work linear program exhibit asymptot converg rate
acceler first order method geodes convex optim riemannian manifold paper propos acceler first order method geodes convex optim general standard nesterov acceler method euclidean space nonlinear riemannian space first deriv equat approxim linear gradient like updat euclidean space geodes convex optim particular analyz global converg properti acceler method geodes strong convex problem show method improv converg rate sqrt moreov method also improv global converg rate geodes general convex problem final give specif iter scheme matrix karcher mean problem valid theoret result experi
fine grain complex empir risk minim kernel method neural network empir risk minim erm ubiquit machin learn under supervis learn method larg bodi work algorithm various erm problem exact comput complex erm still understood address issu multipl popular erm problem includ kernel svms kernel ridg regress train final layer neural network particular give condit hard result problem base complex theoret assumpt strong exponenti time hypothesi assumpt show algorithm solv aforement erm problem high accuraci sub quadrat time also give similar hard result comput gradient empir loss main comput burden mani non convex learn task
larg scale quadrat constrain quadrat program via low discrep sequenc consid problem solv larg scale quadrat constrain quadrat program problem occur natur mani scientif web applic although effici method tackl problem most scalabl paper develop method transform quadrat constraint linear form sampl set low discrep point transform problem solv appli state art larg scale solver show converg approxim solut true solut well finit sampl error bound experiment result also shown prove scalabl practic
new altern direct method linear program well known linear program constraint matrix altern direct method multipli converg global linear rate log howev rate relat problem dimens algorithm exhibit slow fluctuat tail converg practic paper propos new variabl split method prove method converg rate 2log proof base simultan estim distanc pair primal dual iter optim primal dual solut set certain residu practic result new first order solver exploit sparsiti specif structur matrix signific speedup import problem basi pursuit invers covari matrix estim svm nonneg matrix factor problem compar current fastest solver
dykstra algorithm admm coordin descent connect insight extens studi connect dykstra algorithm project onto intersect convex set augment lagrangian method multipli admm block coordin descent prove coordin descent regular regress problem separ penalti function seminorm exact equival dykstra algorithm appli dual problem admm dual problem also seen equival special case set linear subspac connect asid interest right suggest new way analyz extend coordin descent exampl exist converg theori dykstra algorithm polyhedra discern coordin descent lasso problem converg asymptot linear rate also develop parallel version coordin descent base dykstra admm connect
smooth primal dual coordin descent algorithm nonsmooth convex optim propos new random coordin descent method convex optim templat broad applic analysi reli novel combin idea appli primal dual gap function smooth acceler homotopi non uniform sampl result method featur first converg rate guarante best known varieti common structur assumpt templat provid numer evid support theoret result comparison state art algorithm
first order adapt sampl size method reduc complex empir risk minim paper studi empir risk minim erm problem larg scale dataset incorpor idea adapt sampl size method improv guarante converg bound first order stochast determinist method contrast tradit method attempt solv erm problem correspond full dataset direct adapt sampl size scheme start small number sampl solv correspond erm problem statist accuraci sampl size grown geometr scale factor use solut previous erm warm start new erm theoret analys show use adapt sampl size method reduc overal comput cost achiev statist accuraci whole dataset broad rang determinist stochast first order method gain specif choic method particular acceler gradient descent stochast varianc reduc gradient comput cost advantag logarithm number train sampl numer experi various dataset confirm theoret claim showcas gain use propos adapt sampl size scheme
acceler consensus via min sum split appli min sum messag pass protocol solv consensus problem distribut optim show ordinari min sum algorithm converg modifi version known split yield converg problem solut prove proper choic tune paramet allow min sum split yield subdiffus acceler converg rate match rate obtain shift regist method acceler scheme embodi min sum split consensus problem bear similar lift markov chain techniqu multi step first order method convex optim
integr method optim algorithm show acceler optim method seen particular instanc multi step integr scheme numer analysi appli gradient flow equat compar recent advanc vein differenti equat consid basic gradient flow deriv class multi step scheme includ acceler algorithm use classic condit numer analysi multi step scheme integr differenti equat use larger step size intuit explain acceler phenomenon
effici use limit memori resourc acceler linear learn work propos generic approach effici use comput acceler gpus fpgas train larg scale machin learn model train data exceed memori capac techniqu build upon primal dual coordin select use dualiti gap select criteria dynam decid part data made avail fast process provid strong theoret guarante motiv gap base select scheme provid effici practic implement thereof illustr power approach demonstr perform train general linear model larg scale dataset exceed memori size modern gpu show order magnitud speedup exist approach
screen rule regular ise model estim discov screen rule regular ise model estim simpl close form screen rule necessari suffici condit exact recov blockwis structur solut given regular paramet enough sparsiti screen rule combin exact inexact optim procedur deliv solut effici practic screen rule especi suitabl larg scale exploratori data analysi number variabl dataset thousand interest relationship among hand variabl within moder size cluster interpret experiment result various dataset demonstr effici insight gain introduct screen rule
uproot reroot higher order graphic model idea uproot reroot graphic model introduc specif binari pairwis model weller way transform model whole equival class relat model infer model yield infer result other help sinc infer relev bound much easier obtain accur model class introduc method extend approach model higher order potenti develop theoret insight exampl demonstr triplet consist polytop tri uniqu univers root demonstr empir reroot signific improv accuraci method infer higher order model neglig comput cost
concentr multilinear function ise model applic network data prove near tight concentr measur polynomi function ise model high temperatur improv radius concentr guarante known result polynomi factor dimens number node ise model show result optim logarithm factor dimens obtain result extend strengthen exchang pair approach use prove concentr measur set chatterje demonstr efficaci function statist test strength interact social network synthet real world data
infer graphic model via semidefinit program hierarchi maximum posteriori probabl map infer graphic model amount solv graph structur combinatori optim problem popular infer algorithm belief propag general belief propag gbp intim relat linear program relax within sherali adam hierarchi despit popular algorithm well understood sum squar sos hierarchi base semidefinit program sdp provid superior guarante unfortun sos relax graph vertic requir solv sdp variabl degre hierarchi practic approach scale beyond ten variabl paper propos sdp relax map infer use sos hierarchi innov focus comput effici first analog variant introduc decis variabl correspond contigu region graphic model second solv result sdp use non convex burer monteiro style method develop sequenti round procedur demonstr result algorithm solv problem ten thousand variabl within minut signific outperform gbp practic problem imag denois ise spin glass final specif graph type establish suffici condit tight propos partial sos relax
beyond normal learn spars probabilist graphic model non gaussian set present algorithm identifi spars depend structur continu non gaussian probabl distribut given correspond set data condit independ structur arbitrari distribut repres undirect graph markov random field algorithm learn structur restrict discret gaussian case new approach allow realist accur descript distribut question turn better estim spars markov structur sparsiti graph interest acceler infer improv sampl method reveal import depend variabl algorithm reli exploit connect sparsiti graph sparsiti transport map determinist coupl probabl measur anoth
dynam import sampl anytim bound partit function comput partit function key infer task mani graphic model paper propos dynam import sampl scheme provid anytim finit sampl bound partit function algorithm balanc advantag major infer strategi heurist search variat bound mont carlo method blend sampl search refin variat defin propos algorithm combin general recent work anytim search probabilist bound partit function use intellig chosen weight averag sampl construct unbias estim partit function strong finit sampl confid interv inherit rapid earli improv rate sampl long term benefit improv propos search give signific improv anytim behavior flexibl trade off memori time solut qualiti demonstr effect approach empir real world problem instanc taken recent uai competit
nonbacktrack bound influenc independ cascad model paper develop upper lower bound influenc measur network precis expect number node seed set influenc independ cascad model particular bound exploit nonbacktrack walk fortuin kasteleyn ginibr fkg type inequ comput messag pass implement nonbacktrack walk recent allow headway communiti detect paper show use also impact influenc comput provid knob control trade effici accuraci bound final tight bound illustr simul various network model
rigor dynam consist estim arbitrarili condit linear system problem estim random vector noisi linear measur unknown paramet distribut must also learn aris wide rang statist learn linear invers problem show comput simpl iter messag pass algorithm provabl obtain asymptot consist estim certain high dimension larg system limit lsl general parameter previous messag pass techniqu requir sub gaussian matric often fail matrix ill condit propos algorithm call adapt vector approxim messag pass adapt vamp auto tune appli right rotate random import class includ matric arbitrarili bad condit show paramet estim mean squar error mse iter converg determinist limit precis predict simpl set state evolut equat addit simpl testabl condit provid mse match bay optim valu predict replica method paper thus provid comput simpl method provabl guarante optim consist larg class linear invers problem
learn disentangl represent semi supervis deep generat model variat autoencod vae learn represent data joint train probabilist encod decod network typic model encod featur data singl variabl interest learn disentangl represent encod distinct aspect data separ variabl propos learn represent use model architectur general standard vae employ general graphic model structur encod decod allow train partial specifi model make relat strong assumpt subset interpret variabl reli flexibl neural network learn represent remain variabl defin general object semi supervis learn model class approxim use import sampl procedur appli general class model evalu framework abil learn disentangl represent qualit explor generat capac quantit evalu discrimin abil varieti model dataset
gaug variat infer comput partit function import statist infer task aris applic graphic model sinc comput intract approxim method use resolv issu practic mean field belief propag arguabl popular success approach variat type paper propos new variat scheme coin gaug gaug improv respect provid lower bound partit function util call gaug transform modifi factor keep partit function invari moreov prove exact gms singl loop special structur even though bare perform bad case extens experi complet gms relat small size larg 300 variabl confirm newli propos algorithm outperform general
variat infer via upper bound minim variat infer wide use effici altern mcmc posit famili approxim distribut find member closest true posterior close usual measur via diverg though success approach also problem notabl typic lead underestim posterior varianc paper propos chivi new black box variat infer algorithm minim diverg chivi minim upper bound model evid term cubo minim cubo lead better estim posterior use classic lower bound elbo provid sandwich estim margin likelihood studi chivi model probit regress gaussian process classif cox process model basketbal play compar classic chivi produc better error rate accur estim posterior varianc
collaps variat bay markov jump process markov jump process continu time stochast process wide use statist applic natur scienc recent machin learn infer model typic proceed via markov chain mont carlo suffer various comput challeng work propos novel collaps variat infer algorithm address issu work leverag idea discret time markov chain exploit connect idea call uniform algorithm proceed margin paramet markov jump process approxim distribut trajectori factor distribut segment piecewis constant function unlik mcmc scheme margin transit time piecewis constant process scheme optim discret time result signific comput save appli idea synthet data well dataset check record demonstr superior perform state art mcmc method
bayesian dyadic tree histogram regress mani machin learn tool regress base recurs partit covari space smaller region regress function estim local among regress tree ensembl demonstr impress empir perform work shed light machineri behind bayesian variant method particular studi bayesian regress histogram bayesian dyadic tree simpl regress case predictor focus reconstruct regress surfac piecewis constant number jump unknown show suitabl design prior posterior distribut concentr around true step regress function minimax rate log factor result requir knowledg true number step width true partit cell thus bayesian dyadic regress tree fulli adapt recov true piecewis regress function near well knew exact number locat jump result constitut first step toward understand bayesian tree ensembl work well practic asid discuss prior distribut balanc interv partit relat problem geometr probabl name quantifi probabl cover circumfer circl random arc whose endpoint confin grid new variant origin problem
differenti privat bayesian learn distribut data mani applic machin learn exampl health care would benefit method guarante privaci data subject differenti privaci becom establish standard protect learn result standard algorithm requir singl trust parti access entir data clear weak consid bayesian learn distribut set parti hold singl sampl sampl data propos learn strategi base secur multi parti sum function aggreg summari data holder gaussian mechan method build asymptot optim practic effici bayesian infer rapid diminish extra cost
model power condit independ test consid problem non parametr condit independ test test continu random variabl given sampl joint distribut continu random vector determin whether independenti approach convert condit independ test classif problem allow har power classifi like gradient boost tree deep neural network model handl complex probabl distribut allow perform signific better compar prior state art high dimension test main technic challeng classif problem need sampl condit product distribut fci joint distribut independenti given access sampl true joint distribut tackl problem propos novel nearest neighbor bootstrap procedur theoret show generat sampl inde close fci term total variat distanc develop theoret result regard general bound classif problem translat error bound test provid novel analysi rademach type classif bound presenc non textit near independ sampl empir valid perform algorithm simul real dataset show perform gain previous method
world collid integr differ counterfactu assumpt fair machin learn use make crucial decis peopl live near decis risk individu certain race gender sexual orient subpopul unfair discrimin recent method demonstr use techniqu counterfactu infer make predict fair across differ subpopul method requir provid causal model generat data hand genera valid causal model imposs use observ data alon without assumpt henc desir integr compet causal model provid counterfactu fair decis regardless world correct paper show possibl make predict approxim fair respect multipl possibl causal model thus bypass problem exact causal specif frame goal learn fair classifi optim problem fair constraint provid techniqu relax solv optim problem demonstr flexibl model real world fair classif problem show model seamless balanc fair multipl world predict accuraci
lda uncov latent pattern text base sequenti decis process sequenti decis make often import use end user understand under pattern caus lead correspond decis howev typic deep reinforc learn algorithm seldom provid inform due black box natur paper present probabilist model lda uncov latent pattern text base sequenti decis process model understood variant latent topic model tailor maxim total reward draw interest connect approxim maximum likelihood estim lda celebr learn algorithm demonstr text game domain propos method provid viabl mechan uncov latent pattern decis process also obtain state art reward game
probabilist model integr error assess function cardiac model paper studi numer comput integr repres estim predict output comput model respect distribut uncertain input model function cardiac model motiv work neither possess close form express evalu either requir 100 cpu hour preclud standard numer integr method propos treat integr estim problem joint model priori unknown function priori unknown distribut result posterior distribut integr explicit account dual sourc numer approxim error due sever limit comput budget construct appli account statist principl manner impact numer error present confound factor function cardiac model assess
expect propag exponenti famili use algebra exponenti famili distribut high use machin learn sinc calcul perform effici natur paramet exponenti famili recent extend emph exponenti famili contain student distribut famili member thus allow handl noisi data well howev sinc exponenti famili defin emph deform exponenti cannot deriv effici learn algorithm exponenti famili expect propag paper borrow mathemat tool algebra statist physic show pseudo addit distribut allow perform calcul exponenti famili distribut natur paramet develop expect propag algorithm exponenti famili provid determinist approxim posterior predict distribut simpl moment match final appli propos algorithm bay point machin student process classif demonstr perform numer
probabilist framework nonlinear stochast neural network present probabilist framework nonlinear base doubli truncat gaussian distribut set truncat point appropri abl generat various type nonlinear within unifi framework includ sigmoid tanh relu common use nonlinear neural network framework readili integr exist stochast neural network hidden unit character random variabl allow first time learn nonlinear alongsid model weight network extens experi demonstr perform improv brought propos framework integr restrict boltzmann machin rbm tempor rbm truncat gaussian graphic model tggm
clone mcmc parallel high dimension gaussian gibb sampl propos general gibb sampler algorithm obtain sampl approx imat distribut high dimension gaussian distribut similar hogwild method approach target origin gaussian distribut interest approxim contrari hogwild method singl paramet allow trade bias varianc show empir method flexibl perform well compar hogwild type algorithm
learn spatiotempor piecewis geodes trajectori longitudin manifold valu data introduc hierarch model allow estim group averag piecewis geodes trajectori riemannian space measur individu variabl model fall well defin mix effect model subject specif trajectori defin spatial tempor transform group averag piecewis geodes path compon compon thus appli model wide varieti situat due non linear model use stochast approxim expect maxim algorithm estim model paramet experi synthet data valid choic model appli metastat renal cancer chemotherapi monitor run estim recist score treat patient estim time escap treatment experi highlight role differ paramet respons treatment
scalabl levi process prior spectral kernel learn gaussian process rich distribut function generalis properti determin kernel function propos distribut kernel form model spectral densiti levi process result distribut support stationari covari includ popular rbf period matern kernel combin induct bias enabl automat data effici learn long rang extrapol state art predict perform posterior infer develop revers jump mcmc approach includ automat select model order exploit algebra structur propos process train predict show propos model empir recov flexibl ground truth covari demonstr extrapol sever benchmark
infer latent structur human decis make raw visual input goal imit learn match exampl expert behavior without access reinforc signal expert demonstr provid human howev often show signific variabl due latent factor explicit model introduc extens generat adversari imit learn method infer latent structur human decis make unsupervis way method imit complex behavior also learn interpret meaning represent demonstr approach applic high dimension environ includ raw visual input highway drive domain show model learn demonstr abl produc differ drive style accur anticip human action method surpass various baselin term perform function
hybrid reward architectur reinforc learn main challeng reinforc learn generalis typic deep method achiev approxim optim valu function low dimension represent use deep network approach work well mani domain domain optim valu function cannot easili reduc low dimension represent learn slow unstabl paper contribut toward tackl challeng domain propos new method call hybrid reward architectur hydra hydra take input decompos reward function learn separ valu function compon reward function compon typic depend subset featur overal valu function much smoother easier approxim low dimension represent enabl effect learn demonstr hydra toy problem atari game pac man hydra achiev human perform
shallow updat deep reinforc learn deep reinforc learn drl method deep network dqn achiev state art result varieti challeng high dimension domain success main attribut power deep neural network learn rich domain represent approxim valu function polici batch reinforc learn method linear represent hand stabl requir less hyper paramet tune yet substanti featur engin necessari achiev good result work propos hybrid approach least squar deep network dqn combin rich featur represent learn drl algorithm stabil linear least squar method period train last hidden layer drl network batch least squar updat key approach bayesian regular term least squar updat prevent fit recent data test dqn atari game demonstr signific improv vanilla dqn doubl dqn also investig reason superior perform method interest found perform improv attribut larg batch size use method optim last layer
toward general simplic continu control remark success deep learn speech recognit comput vision motiv effort adapt similar techniqu problem domain includ reinforc learn consequ method produc rich motor behavior simul robot task success larg attribut use multi layer neural network work among first care studi might respons recent advanc main result call emerg narrat question show much simpler architectur base linear rbf parameter achiev compar perform state art result studi differ polici represent regard perform measur hand also toward robust extern perturb find learn neural network polici standard train scenario robust linear rbf polici fact remark brittl final direct modifi train scenario order favor robust polici find compel case favor multi layer architectur overal studi suggest multi layer architectur default choic unless side side comparison simpler architectur show otherwis general hope result lead interest care studi architectur choic associ trade off train generaliz robust polici
interpol polici gradient merg polici polici gradient estim deep reinforc learn polici model free deep reinforc learn method use previous collect data improv sampl effici polici polici gradient techniqu hand polici algorithm often stabl easier use paper examin theoret empir approach merg polici updat deep reinforc learn theoret result show polici updat valu function estim interpol polici polici gradient updat whilst still satisfi perform bound analysi use control variat method produc famili polici gradient algorithm sever recent propos algorithm special case famili provid empir comparison techniqu remain algorithm detail fix show differ mix polici gradient estim polici sampl contribut improv empir perform final algorithm provid general unif exist deep polici gradient techniqu theoret guarante bias introduc polici updat improv state art model free deep method number openai gym continu control benchmark
scalabl plan tensorflow hybrid nonlinear domain given recent deep learn result demonstr abil effect optim high dimension non convex function gradient descent optim gpus ask paper whether symbol gradient optim tool tensorflow effect plan hybrid mix discret continu nonlinear domain high dimension state action space end demonstr hybrid plan tensorflow rmsprop gradient descent competit mix integ linear program milp base optim piecewis linear plan domain comput optim solut substanti outperform state art interior point method nonlinear plan domain furthermor remark tensorflow high scalabl converg strong polici larg scale concurr domain total 576 000 continu action horizon time step minut provid number insight clarifi strong perform includ observ despit long horizon rmsprop avoid vanish explod gradient problem togeth result suggest new frontier high scalabl plan nonlinear hybrid domain leverag gpus power recent advanc gradient descent high optmiz toolkit like tensorflow
task base end end model learn stochast optim machin learn techniqu becom widespread becom common see predict algorithm oper within larger process howev criteria train algorithm often differ ultim criteria evalu paper propos end end approach learn probabilist machin learn model within context stochast program manner direct captur ultim task base object use present experiment evalu propos approach classic inventori stock problem real world electr grid schedul task case show propos approach outperform tradit model pure black box polici optim approach
valu predict network paper propos novel deep reinforc learn approach call valu predict network vpn integr model free model base method singl neural network contrast typic model base method vpn learn dynam model whose abstract state train make option condit predict futur valu rather futur observ experiment result show vpn sever advantag model free model base baselin stochast environ care plan requir build accur observ predict model difficult furthermor vpn outperform deep network dqn sever atari game even short lookahead plan demonstr potenti new way learn good state represent
variabl import use decis tree decis tree random forest well establish model offer good predict perform also provid rich featur import inform practition often employ variabl import method reli impur base inform method remain poor character theoret perspect provid novel insight perform method deriv finit sampl perform guarante high dimension set various model assumpt demonstr effect impur base method via extens set simul
express power neural network view width express power neural network import understand deep learn exist work consid problem view depth network paper studi width affect express neural network classic result state emph depth bound depth network suitabl activ function univers approxim show univers approxim theorem emph width bound relu network width relu network input dimens univers approxim moreov except measur set function cannot approxim width relu network exhibit phase transit sever recent work demonstr benefit depth prove depth effici neural network class deep network cannot realiz shallow network whose size emph exponenti bound pose dual question width effici relu network wide network cannot realiz narrow network whose size substanti larger show exist class wide network cannot realiz narrow network whose depth emph polynomi bound hand demonstr extens experi narrow network whose depth exceed polynomi bound constant factor approxim wide shallow network high accuraci result provid comprehens evid depth effect width express relu network
sgd learn conjug kernel class network show standard stochast gradient decent sgd algorithm guarante learn polynomi time function competit best function conjug kernel space network defin dani frostig singer result hold log depth network rich famili architectur best knowledg first polynomi time guarante standard neural network learn algorithm network depth corollari follow neural network depth log sgd guarante learn polynomi time constant degre polynomi polynomi bound coeffici likewis follow sgd larg enough network learn continu function polynomi time complement classic express result
radon machin effect parallelis machin learn order simplifi adapt learn algorithm grow amount data well grow need accur confid predict critic applic paper propos novel provabl effect parallelis scheme contrast parallelis techniqu scheme appli broad class learn algorithm without mathemat deriv without write singl line addit code achiev treat learn algorithm black box appli parallel random data subset result hypothes assign leav aggreg tree bottom replac set hypothes correspond inner node tree radon point consid confid paramet epsilon delta input learn algorithm effici sampl complex polynomi epsilon delta time complex polynomi sampl complex parallelis scheme algorithm achiev guarante appli polynomi number core polylogarithm time result allow effect parallelis broad class learn algorithm intrins relat nick class decis problem well learnabl exact learn cost parallelis form slight larger sampl complex empir studi confirm potenti parallis scheme rang data set sever learn algorithm
nois toler interact learn use pairwis comparison studi problem interact learn binari classifi use noisi label pairwis comparison oracl comparison oracl answer given instanc like posit learn oracl multipl applic obtain direct label harder pairwis comparison easier algorithm leverag type oracl paper attempt character access easier comparison oracl help improv label total queri complex show comparison oracl reduc learn problem learn threshold function present algorithm interact queri label comparison oracl character queri complex tsybakov adversari nois condit comparison label oracl lower bound show label total queri complex almost optim
pac bayesian analysi random learn applic stochast gradient descent analyz general properti random learn algorithm focus stochast gradient descent sgd use novel combin pac bay algorithm stabil import risk bound hold posterior distribut algorithm hyperparamet includ distribut depend train data inspir adapt sampl algorithm sgd optim posterior runtim analyz algorithm context risk bound evalu empir benchmark dataset
revisit perceptron effici label optim learn halfspac long stand problem effici learn linear separ use label possibl presenc nois work propos effici perceptron base algorithm activ learn homogen linear separ uniform distribut bound nois label flip probabl eta algorithm achiev near optim tild frac 2eta frac epsilon label complex time tild frac epsilon 2eta adversari nois tild omega epsilon fraction label flip algorithm achiev near optim tild frac epsilon label complex time tild frac epsilon furthermor show activ learn algorithm convert effici passiv learn algorithm near optim sampl complex respect epsilon
sampl comput effici learn algorithm concav distribut provid new result nois toler sampl effici learn algorithm concav distribut new class concav distribut broad natur general log concav includ mani import addit distribut pareto distribut distribut class studi context effici sampl integr optim much remain unknown geometri class distribut applic context learn challeng unlik common use distribut learn uniform general log concav distribut broader class close margin oper mani distribut fat tail work introduc new convex geometri tool studi properti concav distribut use properti provid bound quantiti interest learn includ probabl disagr halfspac disagr outsid band disagr coeffici use result signific general prior result margin base activ learn disagr base activ learn passiv learn intersect halfspac analysi geometr properti concav distribut might independ interest optim broad
nearest neighbor sampl compress effici consist infinit dimens examin bay consist recent propos nearest neighbor base multiclass learn algorithm algorithm deriv sampl compress bound enjoy statist advantag tight fulli empir general bound well algorithm advantag runtim memori save prove algorithm strong bay consist metric space finit doubl dimens first consist result effici nearest neighbor sampl compress scheme rather surpris discov algorithm continu bay consist even certain infinit dimension set basic measur theoret condit classic consist proof hing violat surpris sinc known bay consist set pose sever challeng open problem futur research
learn identifi gaussian bayesian network polynomi time sampl complex learn direct acycl graph dag structur bayesian network observ data notori difficult problem mani non identifi hard result known paper propos provabl polynomi time algorithm learn spars gaussian bayesian network equal nois varianc class bayesian network dag structur uniqu identifi observ data high dimension set show k4log number sampl suffic method recov true dag structur high probabl number variabl maximum markov blanket size obtain theoret guarante condit call emph restrict strong adjac faith rsaf strict weaker strong faith condit method base condit independ test need success sampl complex method match inform theoret limit term depend valid theoret find synthet experi
world graph discov statist structur link fundament problem analysi social network choos misspecifi model equival incorrect infer algorithm result invalid analysi even fals uncov pattern fact artifact model work focus unifi wide use link format model stochast block model sbm small world latent space model swm integr techniqu kernel learn spectral graph theori nonlinear dimension reduct develop first statist sound polynomi time algorithm discov latent pattern spars graph model network come sbm algorithm output block structur swm algorithm output estim node latent posit
mean field residu network edg chao studi random initi residu network use mean field theori theori differ equat classic feedforward neural network tanh activ exhibit exponenti behavior averag propag input forward gradient backward exponenti forward dynam caus rapid collaps input space geometri exponenti backward dynam caus drastic vanish explod gradient show contrast convert residu connect activ tanh power relu unit network adopt subexponenti forward backward dynam mani case fact polynomi expon polynomi obtain analyt method prove verifi empir correct term edg chao hypothesi subexponenti polynomi law allow residu network tohov boundari stabil chao thus preserv geometri input space gradient inform flow also train grid tanh residu network mnist observ predict theori develop paper peak perform model determin product standard deviat weight squar root depth thus addit improv understand residu network theoret tool guid research toward better initi scheme
learn uncertain curv wasserstein metric gaussian process introduc novel framework statist analysi popul non degener gaussian process gps natur represent uncertain curv allow inher variat uncertainti function valu data proper incorpor popul analysi use wasserstein metric geometr space gps mean covari function compact index space prove exist uniqu barycent popul gps well converg metric barycent finit dimension counterpart justifi practic comput final demonstr framework experiment valid dataset repres brain connect climat chang sourc code releas upon public
cluster network valu data communiti detect focus cluster node detect communiti most singl network problem consider practic interest receiv great deal attent research communiti abl cluster within network import emerg need abl cluster multipl network larg motiv routin collect network data generat potenti differ popul network node correspond node correspond present cluster summar network graphon estim wherea node correspond present propos novel solut cluster network associ comput feasibl featur vector network base trace power adjac matrix illustr method simul real data set theoret justif given term consist
power truncat svd general high rank matrix estim problem show given estim mata close general high rank posit semi definit psd matrix mata spectral norm mata mata simpl truncat singular valu decomposit mata produc multipl approxim mata frobenius norm observ lead mani interest result general high rank matrix estim problem high rank matrix complet show possibl recov general high rank matrix mata relat error frobenius norm partial observ sampl complex independ spectral gap mata high rank matrix denois design algorithm recov matrix mata relat error frobenius norm nois perturb observ without assum mata exact low rank low dimension estim high dimension covari given sampl dimens mat0 mata show possibl estim covari matrix mata relat error frobenius norm improv classic covari estim result requir
adagan boost generat model generat adversari network gan effect method train generat model complex data natur imag howev notori hard train suffer problem miss mode model abl produc exampl certain region space propos iter procedur call adagan everi step add new compon mixtur model run gan algorithm weight sampl inspir boost algorithm mani potenti weak individu predictor greedili aggreg form strong composit predictor prove analyt increment procedur lead converg true distribut finit number step step optim converg exponenti rate otherwis also illustr experiment procedur address problem miss mode
adagan boost generat model generat adversari network gan effect method train generat model complex data natur imag howev notori hard train suffer problem miss mode model abl produc exampl certain region space propos iter procedur call adagan everi step add new compon mixtur model run gan algorithm weight sampl inspir boost algorithm mani potenti weak individu predictor greedili aggreg form strong composit predictor prove analyt increment procedur lead converg true distribut finit number step step optim converg exponenti rate otherwis also illustr experiment procedur address problem miss mode
discov potenti influenc via inform bottleneck discov potenti influenc variabl anoth variabl fundament scientif practic interest exist correl measur suitabl discov averag influenc fail discov potenti influenc bridg gap postul set natur axiom expect measur potenti influenc satisfi show rate inform bottleneck hypercontract coeffici satisfi propos axiom iii provid novel estim estim hypercontract coeffici sampl numer experi demonstr propos estim discov potenti influenc various indic dataset robust discov gene interact gene express time seri data statist power estim correl measur binari hypothesi test canon potenti influenc
phase transit pool data problem code distribut comput invers problem comput intens distribut parallel comput often bottleneck small set slow worker known straggler paper util emerg idea code comput design novel error correct code inspir techniqu solv linear invers problem specif iter method parallel implement affect straggler exampl applic includ invers problem person pagerank sampl graph provabl show code comput techniqu reduc mean squar error comput deadlin constraint fact ratio mean squar error replic base code techniqu diverg infin deadlin increas experi person pagerank perform real system real social network show ratio larg 104 unlik code comput techniqu propos thus far strategi combin output worker includ straggler produc accur estim comput deadlin also ensur accuraci degrad grace event number straggler larg paper studi pool data problem identifi label associ larg collect item base sequenc pool test reveal count label within pool noiseless set exact recoveri identifi exact asymptot threshold requir number test optim decod prove phase transit complet success complet failur addit present novel noisi variat problem provid inform theoret framework character requir number test general nois model result reveal nois make problem consider difficult strict increas scale law even low nois level
queri complex cluster side inform suppos given set element cluster unknown cluster oracl interact answer pair wise queri form element belong cluster goal recov optimum cluster ask minimum number queri paper initi rigor theoret studi basic problem queri complex interact cluster provid strong inform theoret lower bound well near match upper bound cluster problem come similar matrix use autom process cluster similar point togeth howev obtain ideal similar function extrem challeng due ambigu data represent poor data qualiti etc primari reason make cluster hard improv accuraci cluster fruit approach recent year ask domain expert crowd obtain label data interact mani heurist propos use similar function come queri strategi howev systemat theoret studi main contribut paper show dramat power side inform aka similar matrix reduc queri complex cluster natur model similar matrix similar valu drawn independ arbitrari probabl distribut under pair element belong cluster otherwis show given similar matrix queri complex reduc drastic similar matrix log denot squar helling diverg moreov also inform theoret optim within logn factor algorithm effici paramet free work without knowledg depend logarithm
revisit fuzzi neural network demystifi batch normal relu general ham network revisit fuzzi neural network cornerston notion textit general ham distanc provid novel theoret justifi approach rectifi understand tradit neural comput turn mani use neural network method batch normal rectifi linear unit could interpret new framework rectifi general ham network gnn propos accord ghn lend rigiour analysi within fuzzi logic theori also demonstr superior perform varieti learn task term fast learn speed well control behaviour simpl paramet set
posterior sampl reinforc learn worst case regret bound present algorithm base posterior sampl aka thompson sampl achiev near optim worst case regret bound under markov decis process mdp communic finit though unknown diamet main result high probabl regret upper bound dsat communic mdp state action diamet s5a regret compar total reward achiev algorithm total expect reward optim infinit horizon undiscount averag reward polici time horizon result improv best previous known upper bound dsat achiev algorithm set match depend establish lower bound dsat problem
framework multi rmed andit test onlin fdr control propos altern framework exist setup control fals alarm multipl test run time setup aris mani practic applic pharmaceut compani test new treatment option control pill differ diseas internet compani test default webpag versus various altern time framework propos replac sequenc test sequenc best arm mab instanc continu monitor data scientist interleav mab test onlin fals discoveri rate fdr algorithm obtain best world low sampl complex time onlin fdr control main contribut propos reason definit null hypothesi mab instanc demonstr deriv alway valid sequenti valu allow continu monitor mab test iii show use reject threshold onlin fdr algorithm confid level mab algorithm result sampl optim high power low fdr point time run extens simul verifi claim also report result real data collect new yorker cartoon caption contest
mont carlo tree search best arm identif recent advanc bandit tool techniqu sequenti learn steadili enabl new applic promis resolut rang challeng relat problem studi game tree search problem goal quick identifi optim move given game tree sequenti sampl stochast payoff develop new algorithm tree arbitrari depth oper summar deeper level tree confid interv depth appli best arm identif procedur root prove new sampl complex guarante refin depend problem instanc show experiment algorithm outperform exist elimin base algorithm match previous special purpos method depth tree
minim explor structur stochast bandit paper introduc address wide class stochast bandit problem function map arm correspond reward exhibit known structur properti exist structur linear lipschitz unimod combinatori duel cover framework deriv asymptot instanc specif regret lower bound problem develop ossb algorithm whose regret match fundament limit ossb base classic principl optim face uncertainti thompson sampl rather aim match minim explor rate sub optim arm character deriv regret lower bound illustr effici ossb use numer experi case linear bandit problem show ossb outperform exist algorithm includ thompson sampl
regret analysi continu duel bandit duel bandit learn framework feedback inform learn process restrict noisi comparison pair action paper address duel bandit problem base cost function continu space propos stochast mirror descent algorithm show algorithm achiev sqrtt logt regret bound strong convex smooth assumpt cost function clarifi equival regret minim duel bandit convex optim cost function moreov consid lower bound convex optim turn algorithm achiev optim converg rate convex optim optim regret duel bandit except logarithm factor
elementari symmetr polynomi optim experiment design revisit classic problem optim experiment design o new mathemat model ground geometr motiv specif introduc model base elementari symmetr polynomi polynomi captur partial volum offer grade interpol wide use optim optim design model obtain special case analyz properti model deriv greedi convex relax algorithm comput associ design analysi establish approxim guarante algorithm empir result substanti claim demonstr curious phenomenon concern greedi algorithm final byproduct obtain new result theori elementari symmetr polynomi independ interest
onlin learn linear dynam system present effici practic algorithm onlin predict discret time linear dynam system despit non convex optim problem use improp learn convex relax algorithm come provabl guarante near optim regret bound compar best lds hindsight overparameter small logarithm factor analysi bring togeth idea improp learn convex relax onlin regret minim spectral theori hankel matric
effici flexibl infer stochast system mani real world dynam system describ stochast differenti equat thus paramet infer challeng import problem mani disciplin provid grid free flexibl algorithm offer paramet state infer stochast system compar approch base variat approxim state art method show signific advantag runtim accuraci
group spars addit machin famili learn algorithm generat addit model attract much attent recent flexibl interpret high dimension data analysi among learn model group variabl shown competit perform predict variabl select howev previous work main focus least squar regress problem classif task thus desir design new addit classif model variabl select capabl mani real world applic focus high dimension data classif address challeng problem paper investig classif group spars addit model reproduc kernel hilbert space novel classif method call emph group spars addit machin groupsam propos explor util structur inform among input variabl general error bound deriv prove integr sampl error analysi empir cover number hypothesi error estim step stone techniqu new bound show groupsam achiev satisfactori learn rate polynomi decay experiment result synthet data benchmark dataset consist show effect new approach
bregman diverg stochast varianc reduct saddl point adversari predict adversari machin learner compet adversari gain much recent interest machin learn natur form saddl point optim often separ structur sometim also unmanag larg dimens work show adversari predict multivari loss solv much faster use first reduc problem size exponenti use appropri suffici statist adapt new stochast varianc reduc algorithm balamurugan bach 2016 allow bregman diverg prove linear rate converg retain show adversari predict use diverg achiev speedup exampl time compar euclidean altern verifi theoret find extens experi exampl applic adversari predict lpboost
onlin multiclass boost recent work extend theoret analysi boost algorithm multiclass problem onlin set howev multiclass extens batch set onlin extens consid binari classif fill gap literatur defin justifi weak learn condit onlin multiclass boost condit lead optim boost algorithm requir minim number weak learner achiev certain accuraci addit propos adapt algorithm near optim enjoy excel perform real data due adapt properti
univers consist minimax rate onlin mondrian forest establish consist algorithm mondrian forest cite lakshminarayanan2014mondrianforest lakshminarayanan2016mondrianuncertainti random classif algorithm implement onlin first amend origin mondrian forest algorithm propos cite lakshminarayanan2014mondrianforest consid emph fix lifetim paramet inde fact paramet fix actual hinder statist consist origin procedur modifi mondrian forest algorithm grow tree increas lifetim paramet use altern updat rule allow work also onlin fashion second provid theoret analysi establish simpl condit consist theoret analysi also exhibit surpris fact algorithm achiev minimax rate optim rate estim lipschitz regress function strong extens previous result cite arlot2014purf_bia emph arbitrari dimens
mean teacher better role model weight averag consist target improv semi supervis deep learn result recent propos tempor ensembl achiev state art result sever semi supervis learn benchmark maintain exponenti move averag label predict train exampl penal predict inconsist target howev target chang per epoch tempor ensembl becom unwieldi learn larg dataset overcom problem propos mean teacher method averag model weight instead label predict addit benefit mean teacher improv test accuraci enabl train fewer label tempor ensembl mean teacher achiev error rate svhn 250 label better tempor ensembl 1000 label
learn complementari label collect label data cost thus critic bottleneck real world classif task mitig problem consid complementari label specifi class pattern belong collect complementari label would less labori ordinari label sinc user care choos correct class mani candid class howev complementari label less inform ordinari label thus suitabl approach need better learn complementari label paper show unbias estim classif risk obtain complementari label loss function satisfi particular symmetr condit theoret prove estim error bound propos method experiment demonstr use propos algorithm
posit unlabel learn non negat risk estim emph posit emph unlabel data binari classifi train learn state art emph unbias learn howev model flexibl empir risk train data negat suffer serious overfit paper propos emph non negat risk estim learn minim robust overfit thus abl train flexibl model given limit data moreov analyz emph bias emph consist emph mean squar error reduct propos risk estim emph estim error correspond risk minim experi show propos risk estim success fix overfit problem unbias counterpart
semisupervis cluster queri local encod sourc code sourc code canon problem data compress inform theori local encod sourc code compress bit depend bit input paper show recent popular model semisupervis cluster equival local encod sourc code model task perform multiclass label unlabel element begin ask parallel set simpl queri oracl provid possibl erron binari answer queri queri cannot involv fix constant number element label element cluster must done base noisi queri answer goal recov correct label minim number queri equival local encod sourc code lead find lower bound number queri requir varieti scenario also abl show fundament limit pairwis cluster queri propos pairwis queri provabl perform better
learn error structur predict approxim infer work tri understand differ exact approxim infer algorithm structur predict compar estim approxim error underestim overestim model result show perspect learn error perform approxim infer could good exact infer error analys also suggest new margin exist learn algorithm empir evalu text classif sequenti label depend pars wit success approxim infer benefit propos margin
optim generaliz parametr learn consid parametr learn problem object learner determin parametr loss function employ empir risk minim possibl regular infer paramet vector bias toward train sampl bias measur cross valid procedur practic data set partit train set use train valid set use train left measur sampl perform classic cross valid strategi leav cross valid loocv sampl left valid train done rest sampl present learner process repeat sampl loocv rare use practic due high comput complex paper first develop comput effici approxim loocv aloocv provid theoret guarante perform use aloocv provid optim algorithm find optim regular empir risk minim framework numer experi illustr accuraci effici aloocv well propos framework optim regular
multi object non parametr sequenti predict onlin learn research main focus minim object function mani real world applic howev sever object function consid simultan recent algorithm deal sever object function case present paper extend multi object framework case stationari ergod process thus allow depend among observ first identifi asymptomat lower bound predict strategi present algorithm whose predict achiev optim solut fulfil continu convex constrain criterion
fix rank approxim posit semidefinit matrix stream data sever import applic stream pca semidefinit program involv larg scale posit semidefinit psd matrix present sequenc linear updat storag limit possibl retain sketch psd matrix paper develop new algorithm fix rank psd approxim sketch approach combin nystr approxim novel mechan rank truncat theoret analysi establish propos method achiev prescrib relat error schatten norm exploit spectral decay input matrix comput experi show propos method domin altern techniqu fix rank psd matrix approxim across wide rang exampl
communic effici stochast gradient descent applic neural network parallel implement stochast gradient descent sgd receiv signific research attent thank excel scalabl properti fundament barrier parallel sgd high bandwidth cost communic gradient updat node consequ sever lossi compres heurist propos node communic quantiz gradient although effect practic heurist alway guarante converg clear whether improv paper propos quantiz sgd qsgd famili compress scheme gradient updat provid converg guarante qsgd allow user smooth trade emph communic bandwidth emph converg time node adjust number bit sent per iter cost possibl higher varianc show trade inher sens improv past threshold would violat inform theoret lower bound qsgd guarante converg convex non convex object asynchroni extend stochast varianc reduc techniqu appli train deep neural network imag classif autom speech recognit qsgd lead signific reduct end end train time exampl 16gpus train resnet152 network full accuraci imagenet faster full precis variant
machin learn adversari byzantin toler gradient descent studi resili byzantin failur distribut implement stochast gradient descent sgd far distribut machin learn framework larg ignor possibl failur especi arbitrari byzantin one caus failur includ softwar bug network asynchroni bias local dataset well attack tri compromis entir system assum set worker byzantin ask resili sgd without limit dimens size paramet space first show gradient aggreg rule base linear combin vector propos worker current approach toler singl byzantin failur formul resili properti aggreg rule captur basic requir guarante converg despit byzantin worker propos emph krum aggreg rule satisfi resili properti argu first provabl byzantin resili algorithm distribut sgd also report experiment evalu krum
rank data continu label orient recurs partit formul supervis learn problem refer continu rank continu real valu label assign observ take valu featur space goal order possibl observ mean score function tend increas decreas togeth highest probabl problem general multi partit rank certain extent task find optim score function natur cast optim dedic function cri terion call iroc curv maxim kendal relat pair theoret side describ optim element problem provid statist guarante empir kendal maximiza tion appropri condit class score function candid also propos recurs statist learn algorithm tailor empir iroc curv optim produc piecewis constant score function fulli describ orient binari tree preliminari numer experi highlight differ natur regress continu rank provid strong empir evid perform empir optim criteria propos
practic data depend metric compress provabl guarante introduc new distanc preserv compact represent multi dimension point set given point dimension space coordin repres use bit bit per point produc represent size log epsilon log bit per point approxim distanc factor epsilon algorithm almost match recent bound indyk 2017 much simpler compar algorithm product quantize jegou 2011 state art heurist metric compress method evalu algorithm sever data set sift mnist new york citi taxi time seri synthet dimension data set embed high dimension space algorithm produc represent compar better produc provabl guarante perform
simpl strategi recov inner product coars quantiz random project random project increas adopt divers set task machin learn involv dimension reduct specif line research topic investig use quantize subsequ project aim addit data compress motiv applic nearest neighbor search linear learn revisit problem recov inner product respect cosin similar set show even coars scalar quantize bit per project loss accuraci tend rang neglig tomoder implic scenario practic interest need sophist recoveri approach like maximum likelihood estim consid previous work subject propos herein also yield consider improv term accuraci ham distanc base approach icml 2014 compar term simplic
cluster stabl instanc euclidean mean euclidean mean problem arguabl wide studi cluster problem machin learn mean object hard worst case practition enjoy remark success appli heurist like lloyd algorithm problem address disconnect studi follow question properti real world instanc enabl design effici algorithm prove guarante find optim cluster consid natur notion call addit perturb stabil believ captur mani practic instanc euclidean mean cluster stabl instanc uniqu optim mean solut chang even point perturb littl euclidean distanc captur properti mean optim solut toler measur error uncertainti point design effici algorithm provabl recov optim cluster instanc addit perturb stabl instanc addit separ design simpl effici algorithm provabl guarante also robust outlier also complement result studi amount stabil real dataset demonstr algorithm perform well benchmark dataset
distribut hierarch cluster graph cluster fundament task mani data mine machin learn pipelin particular identifi good hierarch structur time fundament challeng problem sever applic amount data analyz increas astonish rate day henc need new solut effici comput effect hierarch cluster huge data main focus paper minimum span tree mst base cluster particular propos affin novel hierarch cluster base boruvka mst algorithm prove certain theoret guarante affin well classic algorithm show practic superior sever state art cluster algorithm furthermor present mapreduc algorithm affin first work case input graph dens take constant round base mst algorithm dens graph improv upon prior work karloff second algorithm assumpt densiti input graph find affin cluster log round use distribut hash tabl dhts show experiment algorithm scalabl huge data set
spars mean embed mean cluster algorithm ubiquit tool data mine machin learn show promis perform howev high comput cost hinder applic broad domain research success address obstacl dimension reduct method recent cite dblp journal tit boutsidiszmd15 develop state art random project method faster mean cluster method deliv mani improv dimension reduct method exampl compar advanc singular valu decomposit base featur extract approach cite dblp journal tit boutsidiszmd15 reduc run time factor min 2log data matrix data point featur lose factor approxim accuraci unfortun still requir ndk 2log matrix multipl cost prohibit larg valu break bottleneck care build spars embed mean cluster algorithm requir nnz nnz denot number non zero fast matrix multipl moreov propos algorithm improv cite dblp journal tit boutsidiszmd15 result approxim accuraci factor empir studi corrobor theoret find demonstr approach abl signific acceler mean cluster achiev satisfactori cluster perform
medoid mean seed show experiment algorithm claran han 1994 find better medoid solut voronoi iter algorithm hasti 2001 find along similar voronoi iter algorithm lloyd mean algorithm motiv use claran mean initi show claran outperform algorithm dataset mean decreas mean initi mean squar error mse final mse introduc algorithm improv claran improv complex runtim make extrem viabl initi scheme larg dataset
appli algorithm foundat hierarch cluster hierarch cluster data analysi method use decad despit widespread use lack analyt foundat method foundat would support method current use guid futur improv paper give appli algorithm foundat hierarch cluster goal paper give analyt framework support observ seen practic paper consid dual problem framework hierarch cluster introduc dasgupta main result popular algorithm use practic averag linkag agglom cluster small constant approxim ratio paper establish use recurs mean divis cluster poor lower bound approxim ratio perhap explain popular practic motiv poor perform mean seek find divis algorithm perform well theoret paper give constant approxim algorithm paper repres first work give foundat hierarch cluster algorithm use practic
inhomogoen hypergraph cluster applic hypergraph partit import problem machin learn comput vision network analyt wide use method hypergraph partit reli minim normal sum cost partit hyperedg across cluster algorithm solut base approach assum differ partit hyperedg incur cost howev assumpt fail leverag fact differ subset vertic within hyperedg differ structur import henc propos new hypergraph cluster techniqu term inhomogen hypergraph partit assign differ cost differ hyperedg cut prove inhomogen partit produc quadrat approxim optim solut inhomogen cost satisfi submodular constraint moreov demonstr inhomogen partit offer signific perform improv applic structur learn rank subspac segment motif cluster
subspac cluster via tangent cone given sampl lie near number fix subspac subspac cluster task group sampl base correspond subspac mani subspac cluster method oper assign affin pair point feed affin common cluster algorithm paper propos new paradigm subspac cluster comput affin base under conic geometri union subspac propos conic subspac cluster csc approach consid convex hull collect normal data point tangent cone sampl union subspac under data impos strong associ tangent cone point origin subspac contain addit describ novel geometr perspect paper provid practic algorithm subspac cluster leverag perspect tangent cone membership test estim affin algorithm accompani determinist stochast guarante properti learn affin matrix direct translat overal cluster accuraci
tensor biclust consid dataset data collect multipl featur multipl individu multipl time type data repres dimension individu featur time tensor becom increas promin various area scienc tensor biclust problem comput subset individu subset featur whose signal trajectori time lie low dimension subspac model similar among signal trajectori allow differ scale across differ individu differ featur studi inform theoret limit problem generat model moreov propos effici spectral algorithm solv tensor biclust problem analyz achiev bound asymptot regim final show effici propos method sever synthet real dataset
unifi approach interpret model predict understand model made certain predict crucial mani applic howev larg modern dataset best accuraci often achiev complex model even expert struggl interpret ensembl deep learn model creat tension accuraci interpret respons varieti method recent propos help user interpret predict complex model present unifi framework interpret predict name shap shapley addit explan assign featur import particular predict key compon shap framework identif class addit featur import measur theoret result uniqu solut class set desir properti class unifi exist method sever recent method class desir properti mean framework inform develop new method explain predict model demonstr sever new method present paper base shap framework show better comput perform better consist human intuit exist method
effici sublinear regret algorithm onlin spars linear regress onlin spars linear regress task appli linear regress analysi exampl arriv sequenti subject resourc constraint limit number featur exampl observ despit import mani practic applic recent shown polynomi time sublinear regret algorithm unless bpp exponenti time sublinear regret algorithm known paper introduc mild assumpt solv problem assumpt present polynomi time sublinear regret algorithm onlin spars linear regress addit thorough experi public avail data demonstr algorithm outperform known algorithm
unbias estim linear regress via volum sampl given full rank matrix column row consid task estim pseudo invers base pseudo invers sampl subset column size least number row show possibl subset column chosen proport squar volum span row chosen submatrix volum sampl result estim unbias surpris covari estim also close form equal specif factor time pseudo invers play import part solv linear least squar problem tri predict label column assum label expens given label small subset column sampl use method show weight vector solut sub problem unbias estim optim solut whole problem base column label believ new formula establish fundament connect linear least squar volum sampl use method obtain algorithm volum sampl faster state art obtain bound total loss estim least squar solut label column
separ loss function revisit discrimin generat model revisit classic analysi generat discrimin model general exponenti famili high dimension set toward develop novel technic machineri includ notion separ general loss function allow provid general framework obtain converg rate general estim use machineri analyz converg rate generat discrimin model provid insight nuanc behavior high dimens result also applic differenti paramet estim quantiti interest differ generat model paramet
general linear model regress distanc set penalti estim general linear model glm complic presenc constraint handl constraint maxim penal log likelihood penalti lasso effect high dimens often lead sever shrinkag paper explor instead penal squar distanc constraint set distanc penalti flexibl algebra regular penalti avoid drawback shrinkag optim distanc penal object make use major minim principl result algorithm construct within framework amen acceler come global converg guarante applic shape constraint spars regress rank restrict matrix regress synthet real data showcas strong empir perform distanc penal even non convex constraint
group addit structur identif kernel nonparametr regress addit model popular use model high dimension nonparametr regress analysi howev main drawback neglect possibl interact predictor variabl paper reexamin group addit model propos literatur rigor defin intrins group addit structur relationship respons variabl
predictor vector vectx develop effect structur penal kernel method simultan identif intrins group addit structur nonparametr function estim method util novel complex measur deriv group addit structur show propos method consist identifi intrins group addit structur simul studi real data applic demonstr effect propos method general tool high dimension nonparametr regress
learn overcomplet hmms studi basic problem learn overcomplet hmms mani hidden state small output alphabet despit signific practic import hmms poor understood known posit negat result effici learn paper present sever new result posit negat help defin boundari tractabl learn set intract set show posit result larg subclass hmms whose transit matric spars well condit small probabl mass short cycl also show learn imposs given polynomi number sampl hmms small output alphabet whose transit matric random regular graph larg degre
matrix norm estim entri singular valu data matrix form provid insight structur data effect dimension choic hyper paramet higher level data analysi tool howev mani practic applic collabor filter network analysi get partial observ scenario consid fundament problem recov various spectral properti under matrix sampl entri propos framework first estim schatten norm matrix sever valu use surrog estim spectral properti interest spectrum rank paper focus technic challeng accur estim schatten norm sampl matrix introduc novel unbias estim base count small structur graph provid guarante match empir perform theoret analysi show schatten norm recov accur strict smaller number sampl compar need recov under low rank matrix numer experi suggest signific improv upon compet approach use matrix complet method
optim shrinkag singular valu random data contamin low rank matrix contamin uniform distribut nois miss valu outlier corrupt entri reconstruct singular valu singular vector contamin matrix key problem machin learn comput vision data scienc paper show common contamin model includ arbitrari combin uniform nois miss valu outlier corrupt entri describ effici use singl framework develop asymptot optim algorithm estim manipul singular valu appli contamin model consid final find explicit signal nois cutoff estim singular valu decomposit must fail well defin sens
new theori nonconvex matrix complet preval matrix complet theori repli assumpt locat miss data distribut uniform random uniform sampl nevertheless reason observ miss often depend unseen observ thus miss data practic usual occur nonuniform fashion rather random break limit random assumpt paper introduc new hypothesi call isomer condit provabl weaker random assumpt arguabl hold even miss data place irregular equip new tool prove seri theorem miss data recoveri matrix complet particular prove exact solut identifi target matrix includ critic point common use nonconvex program unlik exist nonconvex theori use condit convex program theori show nonconvex program work much weaker condit compar exist theori nonuniform sampl theori flexibl power
learn low dimension metric paper investig theoret foundat metric learn focus key question fulli address prior work consid learn general low dimension low rank metric well spars metric develop upper lower minimax bound general error quantifi sampl complex metric learn term dimens featur space dimens rank under metric also bound accuraci learn metric relat under true generat metric result involv novel mathemat approach metric learn problem also shed new light special case ordin embed aka non metric multidimension scale
fast altern minim algorithm dictionari learn present theoret guarante altern minim algorithm dictionari learn spars code problem dictionari learn problem factor sampl appropri basi dictionari time spars vector algorithm simpl altern minim procedur switch gradient descent minim everi step dictionari learn specif altern minim algorithm dictionari learn well studi theoret empir howev contrast previous theoret analysi problem replac condit oper norm true under dictionari condit matrix infin norm allow get converg rate term error estim dictionari infin norm also allow initi random converg global optimum guarante reason generat model allow dictionari grow oper norm handl arbitrari level overcomplet sparsiti inform theoret optim incoher dictionari also present statist guarante present sampl complex guarante algorithm
consist robust regress present first effici provabl consist estim robust regress problem area robust learn optim generat signific amount interest learn statist communiti recent year owe applic scenario corrupt data well handl model mis specif particular special interest devot fundament problem robust linear regress estim toler corrupt constant fraction respons variabl wide studi surpris howev date awar polynomi time estim offer consist estim presenc dens unbound corrupt work present estim call crr solv open problem put forward work bhatia 2015 consist analysi requir novel stage proof techniqu involv care analysi stabil order list independ interest show crr offer consist estim empir far superior sever recent propos algorithm robust regress problem includ extend lasso torrent algorithm comparison crr offer compar better model recoveri runtim faster order magnitud
partial hard threshold toward unifi analysi support recoveri machin learn compress sens central import understand tractabl algorithm recov support spars signal compress measur paper present toward principl analysi support recoveri perform famili hard threshold algorithm end appeal partial hard threshold pht oper propos recent jain ieee tran inform theori 2017 show proper condit pht recov arbitrari spars signal within log iter condit number special pht oper obtain best known result hard threshold pursuit orthogon match pursuit replac experi simul data complement theoret find also illustr interest phase transit iter number cannot signific reduc
minimax estim bandabl precis matric invers covari matrix provid consider insight understand statist model multivari set particular distribut variabl assum multivari normal sparsiti pattern invers covari matrix common refer precis matrix correspond adjac matrix represent gauss markov graph encod condit independ statement variabl minimax result spectral norm previous establish covari matric spars band spars precis matric establish minimax estim bound estim band precis matric spectral norm result great improv upon exist bound particular find minimax rate estim band precis matric match estim band covari matric key insight analysi abl obtain bare noisi estim time subblock precis matrix invert slight wider block empir covari matrix along diagon theoret result complement experi demonstr sharp bound
diffus approxim onlin princip compon estim global converg paper propos adopt diffus approxim tool studi dynam oja iter onlin stochast gradient method princip compon analysi oja iter maintain run estim true princip compon stream data enjoy less tempor spatial complex show oja iter top eigenvector generat continu state discret time markov chain unit sphere character oja iter phase use diffus approxim weak converg tool phase analysi provid finit sampl error bound run estim match minimax inform lower bound pca bound nois
estim covari structur heavi tail distribut propos analyz new estim covari matrix admit strong theoret guarante weak assumpt under distribut exist moment low order estim covari matric correspond sub gaussian distribut well understood much less known case heavi tail data balasubramanian yuan write data real world experi oftentim tend corrupt outlier exhibit heavi tail case clear covari matrix estim remain optim possibl strategi deal heavi tail distribut warrant studi make step toward answer question prove tight deviat inequ propos estim depend paramet control intrins dimens associ covari matrix oppos dimens ambient space particular result applic case high dimension observ
learn koopman invari subspac dynam mode decomposit spectral decomposit koopman oper attract attent tool analysi nonlinear dynam system dynam mode decomposit popular numer algorithm koopman spectral analysi howev often need prepar nonlinear observ manual accord under dynam alway possibl sinc priori knowledg paper propos fulli data driven method koopman spectral analysi base principl learn koopman invari subspac observ data end propos minim residu sum squar linear least squar regress estim set function transform data form linear regress fit well introduc implement neural network evalu perform empir use nonlinear dynam system applic
tochast approxim canon correl analysi propos novel first order stochast approxim algorithm canon correl analysi cca algorithm present instanc noisi matrix stochast gradient msg noisi matrix exponenti gradient meg achiev suboptim popul object time poli probabl least input dimension also consid practic variant propos algorithm compar method cca theoret empir
dive shallow comput perspect larg scale shallow learn remark recent success deep neural network easi analyz theoret particular hard disentangl relat signific architectur optim achiev accur classif larg dataset flip side shallow method kernel method encount obstacl scale larg data practic method variant gradient descent use success deep learn seem perform par appli kernel method difficulti sometim attribut limit shallow architectur paper first identifi basic limit gradient descent base optim method use conjunct smooth kernel analysi demonstr vanish small fraction function space reachabl polynomi number gradient descent iter drastic limit approxim power gradient descent fix comput budget lead serious regular issu pure algorithm persist even limit infinit data address shortcom practic introduc eigenpro iter base simpl direct precondit scheme use small number approxim eigenvector also view learn new kernel optim gradient descent turn inject small amount approxim second order inform lead major improv converg larg data translat signific perform boost state art kernel method particular abl match improv result recent report literatur small fraction comput budget final feel result show need broader comput perspect modern larg scale learn complement tradit statist converg analys
unreason effect structur random orthogon embed examin class embed base structur random matric orthogon row appli mani machin learn applic includ dimension reduct kernel approxim johnson lindenstrauss transform angular kernel show select matric yield guarante improv perform accuraci speed compar earlier method introduc matric complex entri give signific accuraci improv provid geometr markov chain base perspect help understand benefit empir result suggest approach help wider rang applic
general properti learn random featur studi general properti ridg regress random featur statist learn framework show first time learn bound achiev
nlog random featur rather suggest previous result prove faster learn rate show might requir random featur unless sampl accord possibl problem depend distribut result shed light statist comput trade off larg scale kernel learn show potenti effect random featur reduc comput complex keep optim general properti
gaussian quadratur kernel featur kernel method recent attract resurg interest match perform deep neural network task speech recognit random fourier featur map techniqu common use scale kernel machin employ random featur map mean sampl requir achiev approxim error paper investig altern scheme construct featur map determinist rather random approxim kernel frequenc domain use gaussian quadratur show determinist featur map construct achiev error sampl goe valid method dataset differ domain mnist timit show determinist featur faster generat achiev compar accuraci state art kernel method base random fourier featur
linear time kernel good fit test propos novel adapt test good fit comput cost linear number sampl learn test featur best indic differ observ sampl refer model minim fals negat rate featur construct via stein method mean necessari comput normalis constant model analys asymptot bahadur effici new test prove mean shift altern test alway greater relat effici previous linear time kernel test regardless choic paramet test experi perform method exceed earlier linear time test match exceed power quadrat time kernel test high dimens model structur exploit good fit test perform far better quadrat time sampl test base maximum mean discrep sampl drawn model
converg rate partit base bayesian multivari densiti estim method studi class non parametr densiti estim bayesian set estim obtain adapt partit sampl space suitabl prior analyz concentr rate posterior distribut demonstr rate direct depend dimens problem sever special case anoth advantag class bayesian densiti estim adapt unknown smooth true densiti function thus achiev optim conv
power absolut discount dimension distribut estim categor model natur fit mani problem learn distribut categori sampl high dimension dilut data minimax optim pessimist remedi issu serendipit discov estim absolut discount correct empir frequenc subtract constant observ categori redistribut among unobserv outperform classic estim empir use extens natur languag model paper rigor explain prowess estim use less pessimist notion show absolut discount recov classic minimax risk rate emph adapt effect dimens rather true dimens strong relat good ture estim inherit emph competit properti use power law distribut corner stone result valid theori via synthet data applic global terror databas
optim learn popul paramet consid follow fundament estim problem entiti unknown paramet observ independ random variabl binomi accur recov histogram cumul densiti function empir estim would recov histogram earth mover distanc equival distanc cdfs show provid suffici larg achiev error inform theoret optim also extend result multi dimension paramet case captur set member popul multipl associ paramet beyond theoret result demonstr recoveri algorithm perform well practic varieti dataset provid illumin insight sever domain includ polit sport analyt
communic effici distribut learn discret distribut initi systemat studi distribut learn densiti estim distribut model problem data drawn unknown distribut partit across multipl machin machin must succinct communic refere end refere estim under distribut data problem motiv press need build communic effici protocol various distribut system power consumpt limit bandwidth impos stringent communic constraint give first upper lower bound communic complex nonparametr densiti estim discret probabl distribut distanc specif result includ follow case unknown distribut arbitrari machin sampl show interact protocol learn distribut must essenti communic entir sampl case structur distribut
histogram monoton design distribut protocol achiev better communic guarante trivial one show tight bound regim
improv dynam regret non degeneraci function recent grow research interest analysi dynam regret measur perform onlin learner sequenc local minim exploit strong convex previous studi shown dynam regret upper bound path length compar sequenc paper illustr dynam regret improv allow learner queri gradient function multipl time meanwhil strong convex weaken non degeneraci condit specif introduc squar path length could much smaller path length new regular compar sequenc multipl gradient access learner first demonstr dynam regret strong convex function upper bound minimum path length squar path length extend theoret guarante function semi strong convex self concord best knowledg first time semi strong convex self concord util tighten dynam regret
paramet free onlin learn via model select introduc new framework deriv effici algorithm obtain model select oracl inequ adversari onlin learn set also sometim describ paramet free onlin learn work area focus specif high structur function class nest ball hilbert space eschew approach propos generic meta algorithm framework achiev oracl inequ minim structur assumpt allow deriv new comput effici algorithm oracl bound wide rang set result previous unavail give first comput effici algorithm work arbitrari banach space mild smooth assumpt previous result appli hilbert case deriv new oracl inequ various matrix class non nest convex set generic regular final general provid oracl inequ arbitrari non linear class contextu learn model particular give new algorithm learn multipl kernel result deriv unifi meta algorithm scheme base novel multi scale algorithm predict expert advic base random playout independ interest
fast rate bandit optim upper confid frank wolf consid problem bandit optim inspir stochast optim onlin learn problem bandit feedback problem object minim global loss function action necessarili cumul loss framework allow studi general class problem applic statist machin learn field solv problem analyz upper confid frank wolf algorithm inspir techniqu bandit convex optim give theoret guarante perform algorithm various class function discuss optim result
onlin learn transduct regret studi onlin learn general notion transduct regret regret modif rule appli expert sequenc oppos singl expert represent weight finit state transduc show transduct regret general exist notion regret includ extern regret intern regret swap regret condit swap regret present general onlin learn algorithm minim transduct regret extend work design effici algorithm time select sleep expert set product studi algorithm swap regret mild assumpt effici exist method
multi arm bandit metric movement cost consid non stochast multi arm bandit problem set fix known metric action space determin cost switch pair action loss onlin learner compon first usual loss select action second addit loss due switch action main contribut give tight character expect minimax regret set term complex measur under metric depend cover number finit metric space max setc1 3t2 show best possibl regret bound general previous known regret bound special case unit switch cost regret max setk1 3t2 interv metric regret max sett2 infinit metric space lipschitz loss function deriv tight regret bound minkowski dimens space known tight even switch cost
differenti privat empir risk minim revisit faster general paper studi differenti privat empir risk minim erm differ set smooth strong convex loss function without non smooth regular give algorithm achiev either optim near optim util bound less gradient complex compar previous work erm smooth convex loss function high dimens set give algorithm achiev upper bound less gradient complex previous one last general expect excess empir risk convex polyak lojasiewicz condit give tighter upper bound util compar result cite dblp journal corr zhangzmw17
certifi defens data poison attack machin learn system train user provid data suscept data poison attack wherebi malici user inject data aim corrupt learn model recent work propos number attack defens littl understood worst case perform defens face determin attack remedi construct upper bound loss across broad famili attack defend oper via outlier remov follow empir risk minim bound come pair candid attack near realiz upper bound give power tool quick assess defens given dataset empir find even simpl defens mnist dogfish dataset certifi resili attack contrast imdb sentiment dataset driven test error ad poison data
spars approxim conic hull consid problem comput restrict nonneg matrix factor nmf matrix specif seek factor column subset equival given matrix consid problem find small subset column conic hull ep approxim conic hull column distanc everi column conic hull column ep fraction angular diamet size smallest ep approxim produc eps2 size eps1 approxim yield first provabl polynomi time ep approxim class nmf problem also desir approxim independ furthermor prove approxim conic caratheodori theorem general sparsiti result show column ep approxim eps2 spars combin result facilit reduct problem approxim convex hull prove convex conic hull variant sum hard resolv open problem final provid experiment result convex conic algorithm varieti featur select task
estim high dimension non gaussian multipl index model via stein lemma consid estim parametr compon semi parametr multipl index model high dimension non gaussian set estim leverag score function base second order stein lemma requir gaussian ellipt symmetri assumpt made literatur show estim achiev near optim statist rate converg even score function respons variabl heavi tail util data driven truncat argument base requir concentr result establish supplement theoret result via simul experi confirm theori
solid harmon wavelet scatter predict quantum molecular energi invari descriptor electron densiti introduc solid harmon wavelet scatter represent invari rigid movement stabl deform regress classif imag solid harmon wavelet comput multipli solid harmon function gaussian window dilat differ scale invari scatter coeffici obtain cascad wavelet transform complex modulus nonlinear studi applic solid harmon scatter invari estim quantum molecular energi also invari rigid movement stabl respect deform introduc neural network multipl non linear regress scatter invari provid close state art result databas organ molecul
cluster billion read dna data storag store data synthet dna offer possibl improv inform densiti durabl sever order magnitud compar current storag technolog howev dna data storag requir comput intens process retriev data particular crucial step data retriev pipelin involv cluster billion string respect edit distanc observ dataset domain mani notabl properti contain larg number small cluster well separ edit distanc metric space regim exist algorithm unsuit either long run time low accuraci address issu present novel distribut algorithm approxim comput under cluster algorithm converg effici dataset satisfi certain separ properti come dna storag system also prove assumpt algorithm robust outlier high level nois provid empir justif accuraci scalabl converg algorithm real synthet data compar state art algorithm cluster dna sequenc algorithm simultan achiev higher accuraci 1000x speedup real dataset
deep recurr neural network base identif precursor microrna microrna mirna small non code ribonucl acid rnas play key role post transcript gene regul direct identif matur mirna infeas due short length research instead aim identifi precursor mirna pre mirna mani known pre mirna distinct stem loop secondari structur structur base filter usual first step predict possibl given sequenc pre mirna identifi new pre mirna often non canon structur howev need consid addit featur structur obtain addit characterist exist comput method reli manual featur extract inevit limit effici robust general comput identif address limit exist approach propos pre mirna identif method incorpor deep recurr neural network rnn autom featur learn classif multimod architectur seamless integr prior knowledg secondari structur attent mechan improv long term depend model rnn base class activ map highlight learn represent contrast pre mirna non pre mirna experi recent benchmark propos approach outperform compar state art altern term various perform metric
decod valu network neural machin translat neural machin translat nmt becom popular technolog recent year beam search facto decod method due shrunk search space reduc comput complex issu beam search sinc search local optima time step step forward look usual cannot output best target sentenc inspir success methodolog alphago paper propos use predict network improv beam search take sourc sentenc current avail decod output candid word step input predict long term valu bleu score partial target sentenc complet nmt model follow practic reinforc learn call predict network emph valu network specif propos recurr structur valu network train paramet bilingu data test time choos word decod consid condit probabl given nmt model long term valu predict valu network experi show approach signific improv translat accuraci translat task english french translat chines english translat
toward imagenet cnn nlp pretrain sentenc encod machin translat comput vision benefit initi multipl deep layer weight pre train larg supervis train set like imagenet contrast deep model languag task current benefit transfer unsupervis word vector random initi higher layer paper use encod attent sequenc sequenc model train machin translat initi model differ languag task show transfer improv perform use word vector wide varieti common nlp task sentiment analysi sst imdb question classif entail snli question answer squad
deep voic multi speaker neural text speech introduc techniqu augment neural text speech tts low dimension trainabl speaker embed generat differ voic singl model start point show improv state art approach singl speaker neural tts deep voic tacotron introduc deep voic base similar pipelin deep voic construct higher perform build block demonstr signific audio qualiti improv deep voic improv tacotron introduc post process neural vocod demonstr signific audio qualiti improv demonstr techniqu multi speaker speech synthesi deep voic tacotron multi speaker tts dataset show singl neural tts system learn hundr uniqu voic less half hour data per speaker achiev high audio qualiti synthesi preserv speaker ident almost perfect
modul earli visual process languag common assum languag refer high level visual concept leav low level visual process unaffect view domin current literatur comput model languag vision task visual linguist input most process independ fuse singl represent paper deviat classic pipelin propos modul emph entir visual process linguist input specif condit batch normal paramet pretrain residu network languag embed approach call modul residu network mrn signific improv strong baselin visual question answer task ablat studi show modul earli stage visual process benefici
multimod learn reason visual question answer reason entiti relationship multimod data key goal artifici general intellig visual question answer vqa problem excel way test reason capabl model multimod represent learn howev current vqa model oversimplifi deep neural network compris long short term memori lstm unit question comprehens convolut neural network cnn learn singl imag represent argu singl visual represent contain limit general inform imag content thus limit model reason capabl work introduc modular neural network model learn multimod multifacet represent imag question propos model learn use multimod represent reason imag entiti achiev new state art perform vqa benchmark dataset vqa wide margin
learn model tail describ approach learn long tail imbalanc dataset preval real world set challeng learn accur shot model class tail littl data avail cast problem transfer learn knowledg data rich class head transfer data poor class tail key insight follow first propos transfer meta knowledg learn learn head knowledg encod meta network oper space model paramet train predict mani shot model paramet shot model paramet second transfer meta knowledg progress manner class head thebodi bodi tail transfer knowledg gradual fashion regular meta network shot regress train train data allow final network captur dynam transfer meta knowledg data rich data poor regim demonstr result imag classif dataset sun place imagenet tune long tail set signific outperform widespread heurist data resampl reweight
interpret global optim predict textual ground use imag concept textual ground import challeng task human comput interact robot knowledg mine exist algorithm general formul task select solut set bound box propos obtain deep net base system work demonstr cast problem textual ground unifi framework permit effici search possibl bound box henc abl consid signific propos due unifi formul approach reli success first stage beyond demonstr train paramet model use word embed captur spatial imag relationship provid interpret last approach outperform current state art method flickr 30k entiti referitgam dataset respect
multiscal quantize fast similar search propos multiscal quantize approach fast similar search larg high dimension dataset key insight approach quantize method particular product quantize perform poor varianc norm data point common scenario real world dataset especi product quantize residu obtain coars vector quantize address issu propos multiscal formul learn separ scalar quantize residu norm paramet learn joint stochast gradient descent framework minim overal quantize error provid theoret motiv propos techniqu conduct comprehens experi larg scale public dataset demonstr substanti improv recal exist state art method
maskrnn instanc level video object segment instanc level video object segment import techniqu video edit compress captur tempor coher paper develop maskrnn recurr neural net approach fuse frame output deep net object instanc binari segment net provid mask local net provid bound box due recurr compon local compon method abl take advantag long term tempor structur video data well reject outlier valid propos algorithm challeng benchmark dataset davi 2016 dataset davi 2017 dataset segtrack dataset achiev state art perform
flat2spher learn spheric convolut fast featur 360 imageri 360 camera offer tremend new possibl vision graphic augment realiti spheric imag produc make core featur extract non trivial convolut neural network cnns train imag perspect camera yield flat filter yet 360 imag cannot project singl plane without signific distort naiv solut repeat project view sphere tangent plane accur much comput intens real problem propos learn spheric convolut network translat planar cnn process 360 imageri direct equirectangular project approach learn reproduc flat filter output 360 data sensit vari distort effect across view sphere key benefit effici featur extract 360 imag video abil leverag power pre train network research care hone togeth massiv label imag train set perspect imag valid approach compar sever altern method term raw cnn output accuraci well appli state art flat object detector 360 data method yield accur result save order magnitud comput versus exist exact reproject solut
deep mean shift prior imag restor paper introduc natur imag prior direct repres gaussian smooth version natur imag distribut includ prior formul imag restor bay estim also allow solv nois blind imag restor problem gradient bound estim involv gradient logarithm prior gradient correspond mean shift vector natur imag distribut learn mean shift vector field use denois autoencod demonstr competit result nois blind deblur super resolut demosa
pixel graph associ embed graph use abstract imag content graph repres detail individu object scene captur interact pair object present method train convolut neural network take input imag produc full graph definit done end end singl stage use associ embed network learn simultan identifi element make graph piec togeth benchmark visual genom dataset demonstr state art perform challeng task scene graph generat
shape reconstruct model sketch object reconstruct singl imag high determin problem requir strong prior knowledg plausibl shape introduc challeng learn base approach object annot real imag scarc previous work chose train synthet data ground truth inform suffer domain adapt issu test real data work propos end end trainabl framework sequenti estim sketch object shape disentangl step formul advantag first compar full shape sketch much easier recov imag transfer synthet real imag second reconstruct sketch easili transfer learn model synthet data real imag render sketch invari object appear variat real imag includ light textur etc reliev domain adapt problem third deriv differenti project function shape sketch make framework end end trainabl real imag requir real imag annot framework achiev state art perform shape reconstruct
tempor coher base criteria predict video frame use deep multi stage generat adversari network predict futur sequenc video frame recent sought yet challeng task field comput vision machin learn although effort track use motion trajectori flow featur complex problem generat unseen frame studi extens paper deal problem use convolut model within multi stage generat adversari network gan framework propos method use stage gan generat crisp clear set futur frame although gan use past predict futur none work consid relat subsequ frame tempor dimens main contribut lie formul object function base normal cross correl ncc pairwis contrast diverg pcd solv problem method coupl tradit loss experi real world video dataset viz sport ucf 101 kitti perform analysi reveal superior result recent state art method
learn general intrins imag structur disentangl autoencod intrins decomposit singl imag high challeng task due inher ambigu scarciti train data contrast tradit fulli supervis learn approach paper propos learn intrins imag decomposit explain input imag model render intrins network rin join togeth imag decomposit pipelin predict reflect shape light condit given singl imag recombin function learn shade model use recompos origin input base intrins imag predict network use unsupervis reconstruct error addit signal improv intermedi represent allow larg scale unlabel data use train also enabl transfer learn knowledg imag unseen object categori light condit shape extens experi demonstr method perform well intrins imag decomposit knowledg transfer
unsupervis object learn dens equivari imag label key challeng visual percept extract abstract model object object categori visual measur affect complex nuisanc factor viewpoint occlus motion deform start recent idea viewpoint factor propos new approach given larg number imag object supervis extract dens object centric coordin frame coordin frame invari deform imag come dens equivari label neural network map imag pixel correspond object coordin demonstr applic method simpl articul object deform object human face learn embed random synthet transform optic flow correspond without manual supervis
side unsupervis domain map unsupervis domain map learner given unmatch dataset goal learn map gab translat sampl analog sampl recent approach shown learn simultan gab invers map gba convinc map obtain work present method learn gab without learn gba done learn map maintain distanc pair sampl moreov good map obtain even maintain distanc differ part sampl map present experiment result new method allow side map learn also lead prefer numer result exist circular base constraint entir code made public avail
contrast learn imag caption imag caption popular topic comput vision achiev substanti progress recent year howev distinct natur descript often overlook previous work close relat qualiti caption distinct caption like describ imag uniqu aspect work propos new learn method contrast learn imag caption specif via constraint formul top refer model propos method encourag distinct maintain overal qualiti generat caption test method challeng dataset improv baselin model signific margin also show studi propos method generic use model various structur
dynam rout capsul capsul group neuron whose activ vector repres instanti paramet specif type entiti object object part use length activ vector repres probabl entiti exist orient repres instanti paramt activ capsul level make predict via transform matric instanti paramet higher level capsul multipl predict agre higher level capsul becom activ show discriminin train multi layer capsul system achiev state art perform mnist consider better convolut net recogn high overlap digit achiev result use iter rout agreement mechan lower level capsul prefer send output higher level capsul whose activ vector big scalar product predict come lower level capsul
uncertainti need bayesian deep learn comput vision major type uncertainti model aleator uncertainti captur nois inher observ hand epistem uncertainti account uncertainti model uncertainti explain away given enough data tradit difficult model epistem uncertainti comput vision new bayesian deep learn tool possibl studi benefit model epistem aleator uncertainti bayesian deep learn model vision task present bayesian deep learn framework combin input depend aleator uncertainti togeth epistem uncertainti studi model framework per pixel semant segment depth regress task explicit uncertainti formul lead new loss function task interpret learn attenu make loss robust noisi data also give new state art result segment depth regress benchmark
effici optim linear dynam system applic cluster spars code linear dynam system ldss fundament tool model spatio tempor data various disciplin though rich model analyz ldss free difficulti main ldss compli euclidean geometri henc convent learn techniqu appli direct paper propos effici project gradient descent method minim general form loss function demonstr cluster spars code ldss solv propos method effici end first deriv novel canon form repres paramet lds show gradient descent updat project space ldss achiev dexter contrast previous studi solut avoid approxim lds model optim process extens experi reveal superior perform propos method term converg classif accuraci state art techniqu
label distribut learn forest label distribut learn ldl general learn framework assign instanc distribut set label rather singl label multipl label current ldl method either restrict assumpt express form label distribut limit represent learn learn deep featur end end manner paper present label distribut learn forest ldlfs novel label distribut learn algorithm base differenti decis tree sever advantag decis tree potenti model general form label distribut mixtur leaf node predict learn differenti decis tree combin represent learn defin distribut base loss function forest enabl tree learn joint show updat function leaf node predict guarante strict decreas loss function deriv variat bound effect propos ldlfs verifi sever ldl task comput vision applic show signific improv state art ldl method
graph match via multipl updat algorithm graph match fundament problem comput vision machin learn area problem usual formul quadrat program problem doubli stochast discret integ constraint sinc hard approxim algorithm requir paper present new algorithm call multipl updat graph match mpgm develop multipl updat techniqu solv match problem mpgm main benefit theoret mpgm solv general problem doubli stochast constraint natur direct whose converg kkt optim guarante empir mpgm general return spars solut thus also incorpor discret constraint approxim optim effici simpl implement experi synthet real world match task show benefit mpgm algorithm
train quantiz net deeper understand current deep neural network deploy low power embed devic first train full precis model use power comput hardwar deriv correspond low precis model effici infer system howev train model direct coars quantiz weight key step toward learn embed platform limit comput resourc memori capac power consumpt numer recent public studi method train quantiz network weight studi most empir work investig train method quantiz neural network theoret viewpoint first explor accuraci guarante train method convex assumpt look behavior algorithm non convex problem show train algorithm exploit high precis represent import anneal properti pure quantiz train method lack explain mani observ empir differ type algorithm
inner loop free admm use auxiliari deep neural network propos new method use appli deep learn techniqu acceler popular altern direct method multipli admm solut invers problem admm updat consist proxim oper least squar regress includ big matrix invers explicit solut updat dual variabl typic inner loop requir solv first sub minim problem due intract prior matrix invers avoid drawback limit propos textit inner loop free updat rule pre train deep convolut architectur specif learn condit denois auto encod impos implicit data depend prior regular ground truth first sub minim problem design follow empir bayesian strategi lead call amort infer matrix invers second sub problem learn convolut neural network approxim matrix invers invers map learn feed input learn forward network note train neural network requir ground truth measur data independ extens experi synthet data real dataset demonstr effici accuraci propos method compar convent admm solut use inner loop solv invers problem
toward accur binari convolut neural network introduc novel scheme train binari convolut neural network cnns cnns weight activ constrain run time known use binari weight activ drastic reduc memori size access replac arithmet oper effici bitwis oper lead much faster test time infer lower power consumpt howev previous work binar cnns usual result sever predict accuraci degrad paper address issu major innov approxim full precis weight linear combin multipl binari weight base employ multipl binari activ allevi inform loss implement result binari cnn denot abc net shown achiev much closer perform full precis counterpart even reach compar predict accuraci imagenet forest trail dataset given adequ binari weight base activ
runtim neural prune paper propos runtim neural prune rnp framework prune deep neural network dynam runtim unlik exist neural prune method produc fix prune model deploy method preserv full abil origin network conduct prune accord input imag current featur map adapt prune perform bottom layer layer manner model markov decis process use reinforc learn train agent judg import convolut kernel conduct channel wise prune condit differ sampl network prune imag easier task sinc abil network fulli preserv balanc point easili adjust accord avail resourc method appli shelf network structur reach better tradeoff speed accuraci especi larg prune rate
structur embed model group data word embed power approach analyz languag exponenti famili embed efe extend type data develop structur exponenti famili embed efe method discov embed vari across relat group data studi word usag congression speech vari across state parti affili word use differ across section arxiv purchas pattern groceri vari across season key success method group share statist inform develop share strategi hierarch model amort demonstr benefit approach empir studi speech abstract shop basket show sefe enabl group specif interpret word usag outperform efe predict held data
poincar embed learn hierarch represent represent learn becom invalu approach learn symbol data text graph howev complex symbol dataset often exhibit latent hierarch structur state art method typic learn embed euclidean vector space account properti purpos introduc new approach learn hierarch represent symbol data embed hyperbol space precis dimension poincar ball due under hyperbol geometri allow learn parsimoni represent symbol data simultan captur hierarchi similar introduc effici algorithm learn embed base riemannian optim show experiment poincar embed outperform euclidean embed signific data latent hierarchi term represent capac term general abil
languag model recurr highway hypernetwork provid extens experiment theoret support efficaci recurr highway network rhns recurr hypernetwork complimentari origin work demonstr experiment rhns benefit far better gradient flow lstms coupl great improv task accuraci rais provid solut sever theoret issu hypernetwork believ yield gain futur along dramat reduc comput cost combin rhns hypernetwork make signific improv current state art languag model perform penn treebank reli much simpler regular final argu rhns drop replac lstms analog lstms vanilla rnns hypernetwork facto augment analog attent recurr architectur
prevent gradient explos gate recurr unit gate recurr unit gru success recurr neural network architectur time seri data gru typic train use gradient base method subject explod gradient problem gradient increas signific problem caus abrupt chang dynam gru due small variat paramet paper find condit dynam gru chang drastic propos learn method address explod gradient problem method constrain dynam gru drastic chang evalu method experi languag model polyphon music model experi show method prevent explod gradient problem improv model accuraci
wider deeper cheaper faster tensor lstms sequenc learn long short term memori lstm popular approach boost abil recurr neural network store longer term tempor inform capac lstm network increas widen ad layer howev former introduc addit paramet latter increas runtim altern propos tensor lstm hidden state repres tensor updat via cross layer convolut increas tensor size network widen effici without addit paramet sinc paramet share across differ locat tensor delay output network deepen implicit littl addit runtim sinc deep comput timestep merg tempor comput sequenc experi conduct challeng sequenc learn task show potenti propos model
fast slow recurr neural network process sequenti data variabl length major challeng wide rang applic speech recognit languag model generat imag model machin translat address challeng propos novel recurr neural network rnn architectur fast slow rnn rnn rnn incorpor strength multiscal rnns deep transit rnns process sequenti data differ timescal learn complex transit function time step next evalu rnn charact base languag model data set penn treebank hutter prize wikipedia improv state art result bit per charact bpc respect addit ensembl rnns achiev bpc hutter prize wikipedia outperform best known compress algorithm respect bpc measur also present empir investig learn network dynam rnn explain improv perform compar rnn architectur approach general kind rnn cell possibl build block rnn architectur thus flexibl appli differ task
cold start reinforc learn softmax polici gradient present learn algorithm target effici solv fundament problem structur output predict exposur bias problem model expos train data distribut fail expos predict wrong object problem train model conveni object function give suboptim perform method base polici gradient approach reinforc learn succeed avoid common overhead procedur associ approach name warm start train varianc reduct polici updat propos cold start reinforc learn method base new softmax polici gradient softmax polici combin effici simplic maximum likelihood approach effect reward base signal empir evid valid method structur output predict automat summar imag caption task
deep learn precipit nowcast benchmark new model goal make high resolut forecast region rainfal precipit nowcast becom import fundament technolog under various public servic rang rainfal alert flight safeti recent convolut lstm convlstm model shown outperform tradit optic flow base method precipit nowcast suggest deep learn model huge potenti solv problem howev convolut recurr structur convlstm base model locat invari natur motion transform rotat locat variant general furthermor sinc deep learn base precipit nowcast newli emerg area clear evalu protocol yet establish address problem propos new model benchmark precipit nowcast specif beyond convlstm propos trajectori gru trajgru model activ learn locat variant structur recurr connect besid provid benchmark includ real world larg scale dataset hong kong observatori new train loss comprehens evalu protocol facilit futur research gaug state art
recurr ladder network propos recurr extens ladder network cite ladder motiv infer requir hierarch latent variabl model demonstr recurr ladder abl handl wide varieti complex learn task need iter infer tempor model architectur show close optim result tempor model video data competit result music model improv perceptu group base higher order abstract stochast textur motion cue present result fulli supervis semi supervis unsupervis task result suggest propos architectur principl power tool learn hierarchi abstract handl tempor inform model relat interact object
predict state decod encod futur recurr network recurr neural network rnns vital model techniqu reli intern state learn indirect optim supervis unsupervis reinforc train loss rnns use model dynam process character under latent state whose form often unknown preclud analyt represent insid rnn predict state represent psr literatur latent state process model intern state represent direct model distribut futur observ recent work area reli explicit repres target suffici statist probabl distribut seek combin advantag rnns psrs augment exist state art recurr neural network predict state decod psds add supervis network intern state represent target predict futur observ psds simpl implement easili incorpor exist train pipelin via addit loss regular demonstr effect psds experiment result differ domain probabilist filter imit learn reinforc learn method improv statist perform state art recurr baselin fewer iter less data
qmdp net deep learn plan partial observ paper introduc qmdp net neural network architectur plan partial observ qmdp net combin strength model free learn model base plan recurr polici network repres polici connect model plan algorithm solv model thus embed solut structur plan network learn architectur qmdp net fulli differenti allow end end train train qmdp net set differ environ general new one transfer larger environ well preliminari experi qmdp net show strong perform sever robot task simul interest qmdp net encod qmdp algorithm sometim outperform qmdp algorithm experi qmdp net increas robust end end learn
filter variat object evid lower bound elbo appear mani algorithm maximum likelihood estim mle latent variabl sharp lower bound margin log likelihood neural latent variabl model optim elbo joint variat posterior model paramet produc state art result inspir success elbo surrog mle object consid extens elbo famili lower bound defin mont carlo estim margin likelihood show tight bound asymptot relat varianc under estim introduc special case filter variat object take argument elbo pass particl filter form tighter bound filter variat object optim tractabl stochast gradient particular suit mle sequenti latent variabl model standard sequenti generat model task present uniform improv comput budget model train elbo iwa object includ whole nat per timestep improv
unsupervis learn disentangl latent represent sequenti data present factor hierarch variat autoencod learn disentangl represent sequenti data without supervis specif exploit multi scale natur inform sequenti data formul explicit within factor hierarch graphic model impos sequenc specif prior global prior differ set latent variabl model evalu speech corpora demonstr qualit abil transform speaker linguist content manipul differ set latent variabl quantit abil outperform vector baselin speaker verif reduc word error rate much mismatch train test scenario automat speech recognit task
neural discret represent learn learn use represent without supervis remain key challeng machin learn paper propos simpl yet power generat model learn discret represent model vector quantis variat autoencod vae differ vae key way encod network output discret rather continu code prior learnt rather static order learn discret latent represent incorpor idea vector quantis use method allow model circumv issu posterior collaps latent ignor pair power autoregress decod typic observ vae framework pair represent autoregress prior model generat high qualiti imag video speech well high qualiti speaker inpaint provid evid util learnt represent
variat memori address generat model aim augment generat model extern memori interpret output memori modul stochast address condit mixtur distribut read oper correspond sampl discret memori address retriev correspond content memori perspect allow appli variat infer memori address enabl effect train memori modul use target inform guid memori lookup stochast address particular well suit generat model natur encourag multimod promin aspect high dimension dataset treat chosen address latent variabl also allow quantifi amount inform gain memori lookup measur contribut memori modul generat process illustr advantag approach incorpor variat autoencod appli result model task generat shot learn intuit behind architectur memori modul pick relev templat memori continu part model concentr model remain variat demonstr empir model abl identifi access relev memori content even hundr unseen omniglot charact memori
cortic microcircuit gate recurr neural network cortic circuit exhibit intric recurr architectur remark similar across differ brain area stereotyp structur suggest exist common comput principl remain larg elus inspir gate memori network name long short term memori lstm net introduc recurr neural network rnn inform gate inhibitori unit subtract balanc subrnn propos subrnn natur map onto known canon excitatori inhibitori cortic microcircuit show network subtract gate easier optimis standard multipl gate moreov subrnn yield near exact solut standard long term depend task tempor addit task empir result across sever long term depend task generalis tempor addit multipl tempor mnist word level languag model show subrnn outperform achiev similar perform lstm network test work suggest novel view cortex solv complex contextu problem provid first step toward unifi machin learn recurr network biolog counterpart
continu learn deep generat replay attempt train comprehens artifici intellig capabl solv multipl task imped chronic problem call catastroph forget although simpli replay previous data allevi problem requir larg memori even wors often infeas real world applic access past data limit inspir generat natur hippocampus short term memori system primat brain propos deep generat replay novel framework cooper dual model architectur consist deep generat model generat task solv model solver model train data previous task easili sampl interleav new task test method sever sequenti learn set involv imag classif task
hierarch attent recurr track class agnost object track particular difficult clutter environ target specif discrimin model cannot learn priori inspir human visual cortex employ spatial attent separ andwhat process pathway activ suppress irrelev visual featur work develop hierarch attent recurr model singl object track video first layer attent discard major background select region contain object interest subsequ layer tune visual featur particular track object framework fulli differenti train pure data driven fashion gradient method improv train converg augment loss function term number auxiliari task relev track evalu propos model perform dataset increas difficulti pedestrian track kth activ recognit dataset kitti object track dataset
vae learn via stein variat gradient descent new method learn variat autoencod vae develop base stein variat gradient descent key advantag approach need make parametr assumpt form encod distribut perform enhanc integr propos encod import sampl excel perform demonstr across multipl unsupervis semi supervis problem includ semi supervis analysi imagenet data demonstr scalabl model larg dataset
learn inpaint imag compress studi design deep architectur lossi imag compress present architectur recip context multi stage progress encod empir demonstr import compress perform specif show predict origin imag data residu multi stage progress architectur facilit learn lead improv perform approxim origin content learn inpaint neighbor imag pixel perform compress reduc amount inform must store achiev high qualiti approxim incorpor design choic baselin progress encod yield averag reduct file size similar qualiti compar origin residu encod
visual interact network glanc human make rich predict futur state wide rang physic system modern approach engin robot graphic often restrict narrow domain requir direct measur under state introduc visual interact network general purpos model learn dynam physic system raw visual observ predict futur state model consist perceptu front end base convolut neural network dynam predictor base interact network joint train perceptu front end learn pars dynam visual scene set factor latent object represent dynam predictor learn roll state forward time comput interact dynam produc predict physic trajectori arbitrari length found input video frame visual interact network generat accur futur trajectori hundr time step wide rang physic system model also appli scene invis object infer futur state effect visibl object implicit infer unknown mass object result demonstr perceptu modul object base dynam predictor modul induc factor latent represent support accur dynam predict work open new opportun model base decis make plan raw sensori observ complex physic environ
neuralfdr learn discoveri threshold hypothesi featur dataset grow richer import challeng leverag full featur data maxim number use discoveri control fals posit address problem context multipl hypothes test hypothesi observ valu along set featur specif hypothesi exampl genet associ studi hypothesi test correl variant trait rich set featur variant locat conserv epigenet etc could inform like variant true associ howev popular test approach benjamini hochberg procedur independ hypothesi weight ihw either ignor featur assum featur categor propos new algorithm neuralfdr automat learn discoveri threshold function hypothesi featur parametr discoveri threshold neural network enabl flexibl handl multi dimension discret continu featur well effici end end optim prove neuralfdr strong fals discoveri rate fdr guarante show make substanti discoveri synthet real dataset moreov demonstr learn discoveri threshold direct interpret
eigen distort hierarch represent develop method compar hierarch imag represent term abil explain perceptu sensit human specif util fisher inform establish model deriv predict local sensit perturb around given natur imag given imag comput eigenvector fisher inform matrix largest smallest eigenvalu correspond model predict least notic imag distort respect human subject measur amount distort reliabl detect ad imag compar threshold predict correspond model use method test abil varieti represent mimic human perceptu sensit find earli layer vgg16 deep neural network optim object recognit provid better match human percept later layer better match stage convolut neural network cnn train databas human rate distort imag qualiti hand find simpl model earli visual process incorpor stage local gain control train databas distort rate predict human sensit signific better cnn layer vgg16
fli oper batch dynam comput graph dynam neural network toolkit pytorch dynet chainer offer flexibl implement model cope data vari dimens structur relat toolkit oper static declar comput tensorflow cntk theano howev exist toolkit static dynam requir develop organ comput batch necessari exploit high perform data parallel algorithm hardwar batch task general difficult becom major hurdl architectur becom complex paper present algorithm implement dynet toolkit automat batch oper develop simpli write minibatch comput aggreg singl instanc comput batch algorithm seamless execut fli comput effici batch varieti task obtain throughput similar manual batch well compar speedup singl instanc learn architectur impract batch manual
learn affin via spatial propag network paper propos spatial propag network learn affin matrix show construct row column linear propag model spatial variant transform matrix constitut affin matrix model dens global pairwis similar imag specif develop way connect linear propag model formul spars transform matrix element output deep cnn result dens affin matrix effect model task specif pairwis similar instead design similar kernel accord imag featur point direct output similar pure data driven manner spatial propag network generic framework appli numer task tradit benefit design affin imag mat color guid filter name furthermor model also learn semant awar affin high level vision task due learn capabl deep model valid propos framework refin object segment experi helen face pars pascal voc 2012 semant segment task show spatial propag network provid general effect effici solut generat high qualiti segment result
supervis adversari domain adapt work provid framework address problem supervis domain adapt deep model main idea exploit adversari learn learn embed subspac simultan maxim confus domain semant align embed version supervis set becom attract especi target data sampl need label scenario align separ semant probabl distribut difficult lack data found care design train scheme wherebi typic binari adversari discrimin augment distinguish differ class possibl effect address supervis adapt problem addit approach high speed adapt requir extrem low number label target train sampl even per categori effect extens compar approach state art domain adapt experi use dataset handwritten digit recognit use dataset visual object recognit
deep hyperspher learn convolut inner product found basi convolut neural network cnns key end end visual represent learn benefit deeper architectur recent cnns demonstr increas strong represent abil despit improv increas depth larger paramet space also led challeng proper train network light challeng propos hyperspher convolut sphereconv novel learn framework give angular represent hyperspher introduc spherenet deep hyperspher convolut network distinct convent inner product base convolut network particular spherenet adopt sphereconv basic convolut oper supervis general angular softmax loss natur loss formul sphereconv show spherenet effect encod discrimin represent allevi train difficulti lead easier optim faster converg better classif perform convolut counterpart also provid theoret justif advantag hyperspher optim experi ablat studi verifi conclus
riemannian approach batch normal batch normal proven effect algorithm deep neural network train normal input neuron reduc intern covari shift space weight vector layer natur interpret riemannian manifold invari linear scale weight follow intrins geometri manifold provid new learn rule effici easier analyz also propos intuit effect gradient clip regular method propos algorithm util geometri manifold result algorithm consist outperform origin various type network architectur dataset
backprop without learn rate coin bet deep learn method achiev state art perform mani applic scenario yet method requir signific amount hyperparamet tune order achiev best result particular tune learn rate stochast optim process still main bottleneck paper propos new stochast gradient descent procedur deep network requir learn rate set contrari previous method adapt learn rate make use assum curvatur object function instead reduc optim process game bet coin propos learn rate free optim algorithm scenario theoret converg proven convex quasi convex function empir evid show advantag algorithm popular stochast gradient algorithm
converg block coordin descent train dnns tikhonov regular lift relu function higher dimension space develop smooth multi convex formul train feed forward deep neural network dnns allow develop block coordin descent bcd train algorithm consist sequenc numer well behav convex optim use idea proxim point method convex analysi prove bcd algorithm converg global stationari point linear converg rate order experi mnist databas dnns train bcd algorithm consist yield better test set error rate ident dnn architectur tarin via stochast gradient descent sgd variant caff toolbox
collabor deep learn fix topolog network signific recent interest parallel deep learn algorithm order handl enorm growth data model size advanc focus model parallel engag multipl comput agent via use central paramet server aspect data parallel along decentr comput explor suffici context paper present new consensus base distribut sgd cdsgd momentum variant cdmsgd algorithm collabor deep learn fix topolog network enabl data parallel well decentr comput framework extrem use learn agent access local privat data communic constrain environ analyz converg properti propos algorithm strong convex nonconvex object function fix diminish step size use concept lyapunov function construct demonstr efficaci algorithm comparison baselin central sgd recent propos feder averag algorithm also enabl data parallel base benchmark dataset mnist cifar cifar 100
regular affect critic point linear network paper concern problem repres learn linear transform use linear neural network recent year grow interest studi network part due success deep learn main question bodi research also paper pertain exist optim properti critic point mean squar loss function primari concern robust critic point regular loss function optim control model introduc purpos learn algorithm regular form backprop deriv use hamilton formul optim control formul use provid complet character critic point term solut nonlinear matrix valu equat refer characterist equat analyt numer tool bifurc theori use comput critic point via solut characterist equat main conclus critic point diagram fundament differ even arbitrari small amount regular
predict organ reaction outcom weisfeil lehman network predict organ reaction outcom fundament problem comput chemistri sinc reaction involv hundr atom fulli explor space possibl transform intract current solut util reaction templat limit space suffer coverag effici issu paper propos templat free approach effici explor space product molecul first pinpoint reaction center set node edg graph edit occur sinc small number atom contribut reaction center direct enumer candid product generat candid score weisfeil lehman differ network model high order interact chang occur node across molecul framework outperform top perform templat base approach margin run order magnitud faster final demonstr model accuraci rival perform domain expert
predict scene pars motion dynam futur import intellig system textit autonom vehicl robot anticip futur order plan earli make decis accord predict futur scene pars motion dynam help agent understand visual environ better former provid dens semant segment textit object present later provid dens motion inform textit object move futur paper propos novel model predict futur scene pars motion dynam unobserv video frame simultan use histori inform preced frame correspond scene pars result input model abl predict scene pars motion arbitrari time step ahead import model superior compar method predict pars motion individu solv predict task joint fulli exploit complementari relationship best knowledg paper first aim learn predict futur scene pars motion dynam simultan larg scale cityscap dataset demonstr model produc signific better pars motion predict compar well establish baselin addit also present predict steer angl vehicl use model good result verifi capabl model learn under latent paramet
houdini democrat adversari exampl generat adversari exampl critic step evalu improv robust learn machin far exist method work classif design alter true perform measur problem hand introduc novel flexibl approach name houdini generat adversari specif tailor final perform measur task consid success appli houdini rang applic speech recognit pose estim
geometr matrix complet recurr multi graph neural network matrix complet model among common formul recommend system recent work show boost perform techniqu introduc pairwis relationship user item form graph impos smooth prior graph howev techniqu fulli exploit local stationari structur user item graph number paramet learn linear number user item propos novel approach overcom limit use geometr deep learn graph matrix complet architectur combin novel multi graph convolut neural network learn meaning statist graph structur pattern user item recurr neural network appli learnabl diffus score matrix neural network system comput attract requir constant number paramet independ matrix size appli method sever standard dataset show outperform state art matrix complet techniqu
compress awar train deep neural network recent year great progress made varieti applic domain thank develop increas deeper neural network unfortun huge number unit network make expens comput memori wise overcom exploit fact deep network parametr sever compress strategi propos method howev typic start network train standard manner without consid futur compress paper propos explicit account compress train process end introduc regular encourag paramet matrix layer low rank train show allow learn much compact yet least effect model state art compress techniqu
non parametr neural network deep neural network dnns probabilist graphic model pgms main tool statist model dnns provid abil model rich complex relationship input independ output variabl pgms provid abil encod depend among output variabl end end train model structur graphic depend top independ neural predict recent emerg principl way combin paradigm type model proven power discrimin set discret output extens structur continu space well perform effici infer space lack propos non parametr neural network n3s modular approach clean separ non parametr structur posterior represent discrimin infer scheme allow end end train compon experi evalu abil n3s captur structur posterior densiti model comput complex statist densiti infer compar model number baselin includ popular variat sampl base infer scheme term accuraci speed
gibbsnet iter adversari infer deep graphic model direct latent variabl model formul joint distribut advantag sampl fast exact yet weak need specifi often simpl fix prior limit express model undirect latent variabl model discard requir specifi prior yet sampl general requir iter procedur block gibb sampl requir mani step achiev sampl joint distribut propos novel approach learn joint distribut data latent code use adversari learn iter procedur gradual refin joint distribut better match data distribut step gibbsnet best world theori practic achiev speed simplic direct latent variabl model guarante assum adversari game reach virtual train criteria global minimum produc sampl sampl iter achiev express flexibl undirect latent variabl model gibbsnet away need explicit abil classif class condit generat joint imag attribut model singl model train specif task show empir gibbsnet abl learn complex show lead improv inpaint iter refin dozen step stabl generat without collaps thousand step despit train step
explor general deep learn goal understand drive general deep network consid sever recent suggest explan includ norm base control sharp robust studi measur ensur general highlight import scale normal make connect sharp pac bay theori investig well measur explain differ observ phenomena
regular deep neural network nois interpret optim overfit critic challeng deep neural network various type regular method improv general perform inject nois hidden unit train dropout known success regular still clear enough train techniqu work well practic maxim benefit presenc conflict object optim true data distribut prevent overfit regular paper address issu interpret convent train method regular nois inject optim lower bound true object propos techniqu achiev tighter lower bound use multipl nois sampl per mini batch demonstr effect idea sever comput vision applic
extract low dimension dynam multipl larg scale neural popul record learn predict correl power approach understand neural popul dynam extract low dimension trajectori popul record use dimension reduct method current approach dimension reduct neural data limit singl popul record identifi dynam embed across multipl measur propos approach extract low dimension dynam multipl sequenti record algorithm scale data compris million observ dimens make possibl access dynam distribut across larg popul multipl brain area build subspac identif approach dynam system perform paramet estim minim moment match object use scalabl stochast gradient descent algorithm model optim predict tempor covari across neuron across time show approach natur handl miss data multipl partial record identifi dynam predict correl even presenc sever subsampl small overlap record demonstr effect approach simul data whole brain larval zebrafish imag dataset
adapt sampl popul neuron adapt sampl method neurosci primarili focus maxim fire rate singl record neuron record neuron usual possibl find singl stimulus maxim fire rate neuron motiv object function take account record popul neuron togeth propos adept adapt sampl method optim popul object function simul experi first confirm popul object function elicit vari stimulus respons singl neuron object function test adept close loop electrophysiolog experi popul activ record macaqu cortic area known mid level visual process adept use output deep convolut neural network model featur embed predict neural respons adept elicit mean stimulus respons larger random chosen natur imag well larger scatter stimulus respons adapt sampl method enabl new scientif discoveri record popul neuron heterogen respons properti
onacid onlin analysi calcium imag data real time optic imag method use calcium indic critic monitor activ larg neuron popul vivo imag experi typic generat larg amount data need process extract activ imag neuron sourc deriv process algorithm activ area research exist method requir process larg amount data time render vulner volum record data prevent real time experiment interrog introduc onacid onlin framework analysi stream calcium imag data includ motion artifact correct neuron sourc extract iii activ denois deconvolut approach combin extend previous work onlin dictionari learn calcium imag data analysi deliv autom pipelin discov track activ hundr cell real time therebi enabl new type close loop experi appli algorithm larg scale experiment dataset benchmark perform manual annot data show outperform popular offlin approach
detrend partial cross correl brain connect analysi brain connect analysi critic compon ongo human connectom project deciph healthi diseas brain recent work highlight power law multi time scale properti brain signal howev remain lack method specif quantifi short long rang brain connect paper use detrend partial cross correl analysi dpcca propos novel function connect measur delin brain interact multipl time scale control covari use rich simul fmri data valid propos method appli real fmri data cocain depend predict task show compar extant method dpcca base approach distinguish short long rang function connect also improv featur extract subsequ increas classif accuraci togeth paper contribut broad new comput methodolog understand neural inform process
practic bayesian optim model fit bayesian adapt direct search comput model field comput neurosci often evalu via stochast simul numer approxim fit model impli difficult optim problem complex possibl noisi paramet landscap bayesian optim success appli solv expens black box problem engin machin learn explor whether appli general tool model fit first present novel algorithm bayesian adapt direct search bad achiev competit perform afford comput overhead run time typic model perform extens benchmark bad mani common state art nonconvex deriv free optim set model fit problem real data model studi behavior cognit comput neurosci default set bad consist find compar better solut method show great promis bad particular general model fit tool
error detect correct framework connectom signific advanc made recent year problem neural circuit reconstruct electron microscop imageri improv imag acquisit imag align boundari detect great reduc achiev error rate order make progress argu autom error detect essenti focuss effort attent human machin paper report use autom error detect attent signal flood fill error correct modul demonstr signific improv upon state art segment perform
cake effect brain connect causal kernel fundament goal network neurosci understand activ region drive activ elsewher process refer effect connect propos model causal interact use integro differenti equat causal kernel allow rich analysi effect connect approach combin tractabl flexibl autoregress model biophys interpret dynam causal model causal kernel learn nonparametr use gaussian process regress yield effici framework causal infer construct novel class causal covari function enforc desir properti causal kernel approach call cake construct model hyperparamet biophys mean therefor easili interpret demonstr efficaci cake number simul give exampl realist applic magnetoencephalographi meg data
learn neural represent human cognit across mani fmri studi cognit neurosci enjoy rapid increas extens public brain imag dataset open door design deploy larg scale statist model target unifi perspect avail data impli find scalabl autom solut old challeng aggreg heterogen inform brain function univers cognit system relat psycholog behavior brain network cast challeng machin learn approach predict condit statist brain map across differ studi leverag multi task learn multi scale dimens reduct learn low dimension represent brain imag carri robust cognit inform robust associ psycholog stimuli multi dataset classif model achiev best predict perform sever larg refer dataset compar model forgo learn cognit awar low dimens represent bring substanti perform boost analysi small dataset introspect identifi univers templat cognit concept
map distinct timescal function interact among brain network brain process occur various timescal rang millisecond neuron minut hour behavior character function coupl among brain region divers timescal key understand brain produc behavior appli instantan lag base measur condit linear depend base granger gewek causal infer network connect distinct timescal function magnet reson imag fmri data due slow sampl rate fmri wide held produc spurious unreli estim function connect appli fmri data challeng claim combin simul novel machin learn approach first show simul fmri data instantan lag base identifi distinct timescal complementari pattern function connect next analyz fmri record 500 human subject show linear classifi train either instantan lag base connect reliabl distinguish task versus rest brain state cross valid accuraci import instantan lag base exploit mark differ spatial tempor pattern connect achiev robust classif approach provid novel framework uncov valid function connect network oper distinct timescal brain
robust estim neural signal calcium imag calcium imag promin technolog neurosci research allow simultan record larg number neuron awak anim autom extract neuron tempor activ imag dataset import step path produc neurosci result howev near imag dataset typic contain gross contamin sourc could contribut technolog use under biolog tissu although attempt made better extract neural signal limit gross contamin scenario effort address contamin full general statist estim work proceed new direct propos extract cell activ use robust estim deriv optim robust loss base simpl abstract calcium imag data also find simpl practic optim routin loss provabl fast converg use propos robust loss matrix factor framework extract neuron tempor activ calcium imag dataset demonstr superior robust estim approach exist method simul real dataset
learn morpholog brain signal use alpha stabl convolut spars code neural time seri data contain wide varieti prototyp signal waveform atom signific import clinic cognit research goal analyz data henc extract shift invari atom even though success report exist algorithm limit applic due heurist natur moreov often vulner artifact impuls nois typic present raw neural record studi address issu propos novel probabilist convolut spars code csc model learn shift invari atom raw neural signal contain potenti sever artifact core model call csc lie famili heavi tail distribut call stabl distribut develop novel comput effici mont carlo expect maxim algorithm infer maxim step boil weight csc problem develop comput effici optim algorithm result show propos algorithm achiev state art converg speed besid csc signific robust artifact compar compet algorithm extract spike burst oscil even reveal subtl phenomena cross frequenc coupl appli noisi neural time seri
stream weak submodular interpret neural network fli mani machin learn applic import explain predict black box classifi exampl deep neural network assign imag particular class cast interpret black box classifi combinatori maxim problem propos effici stream algorithm solv subject cardin constraint extend idea badanidiyuru 2014 provid constant factor approxim guarante algorithm case random stream order weak submodular object function first theoret guarante general class function also show algorithm exist worst case stream order algorithm obtain similar explan incept predict time faster state art lime framework ribeiro 2016
decompos submodular function minim discret continu paper investig connect discret continu approach decompos submodular function minim provid improv run time estim state art continu algorithm problem use combinatori argument also provid systemat experiment comparison type method base clear distinct level level algorithm
differenti learn submodular function incorpor discret optim algorithm within modern machin learn model exampl possibl use deep architectur layer whose output minim cut parametr graph given model train end end leverag gradient inform introduct layer seem challeng due non continu output paper focus problem submodular minim show layer inde possibl key idea continu relax output without sacrif guarante provid easili comput approxim jacobian complement complet theoret analysi final contribut let experiment learn probabilist log supermodular model via level variat infer formul
robust optim non convex object consid robust optim problem goal optim worst case class object function develop reduct robust improp optim bayesian optim given oracl return approxim solut distribut object comput distribut solut approxim worst case show derandom solut hard general done broad class statist learn task appli result robust neural network train submodular optim evalu approach experiment charact classif task subject adversari distort robust influenc maxim larg network
optim landscap tensor decomposit non convex optim local search heurist wide use machin learn achiev mani state art result becom increas import understand work hard problem typic data landscap mani object function learn conjectur geometr properti local optima approxim global optima thus solv effici local search algorithm howev establish properti difficult paper analyz optim landscap random complet tensor decomposit problem mani applic unsupervis lean especi learn latent variabl model practic effici solv gradient ascent non convex object show small constant among set point function valu factor larger expect function local maxima approxim global maxima previous best known result character geometri small neighborhood around true compon result impli even initi bare better random guess gradient ascent algorithm guarante solv problem main techniqu use kac rice formula random matrix theori best knowledg first time kac rice formula success appli count number local minima high structur random polynomi depend coeffici
gradient descent take exponenti time escap saddl point although gradient descent almost alway escap saddl point asymptot lee 2016 paper show even fair natur random initi scheme non patholog function signific slow saddl point take exponenti time escap hand gradient descent perturb 2015 jin 2017 slow saddl point find approxim local minim polynomi time result conclud gradient descent inher slower justifi import ad perturb effici non convex optim experi also provid demonstr theoret find
convolut phase retriev studi convolut phase retriev problem ask recov unknown signal length measur consist magnitud cyclic convolut known kernel length model motiv applic channel estim optic underwat acoust communic signal interest act given channel filter phase inform difficult imposs acquir show random mathbf effici recov global phase use combin spectral initi general gradient descent main challeng cope depend measur oper overcom challeng use idea decoupl theori suprema chao process restrict isometri properti random circul matric recent analysi altern minim method
implicit regular matrix factor studi implicit regular optim underdetermin quadrat object matrix gradient descent factor conjectur provid empir theoret evid small enough step size initi close enough origin gradient descent full dimension factor converg minimum nuclear norm solut
near linear time approxim algorithm optim transport via sinkhorn iter comput optim transport distanc earth mover distanc fundament problem machin learn statist comput vision despit recent introduct sever algorithm good empir perform unknown whether general optim transport distanc approxim near linear time paper demonstr ambiti goal fact achiev cuturi sinkhorn distanc provid guidanc toward paramet tune algorithm result reli new analysi sinkhorn iter also direct suggest new algorithm greenkhorn theoret guarante numer simul clear illustr greenkhorn signific outperform classic sinkhorn algorithm practic
frank wolf equilibrium comput consid frank wolf method constrain convex optim first order project free procedur show algorithm recast differ light emerg special case particular meta algorithm comput equilibria saddl point convex concav sum game equilibrium comput trick reli exist regret onlin learn generat sequenc iter also provid proof converg vanish regret show state equival sever nice properti particular exhibit modular give rise various old new algorithm explor result method provid experiment result demonstr correct effici
greedi algorithm cone constrain optim converg guarante greedi optim method match pursuit frank wolf algorithm regain popular recent year due simplic effect theoret guarante address optim textit linear span textit convex hull set atom respect paper consid intermedi case optim textit convex cone parametr conic hull generic atom set lead first principl definit non negat algorithm give explicit converg rate demonstr excel empir perform novel algorithm analysi tailor particular function atom set particular deriv sublinear converg general smooth convex object linear converg strong convex object case general set atom furthermor establish clear correspond algorithm known algorithm literatur novel algorithm analys target general atom set general object function henc direct applic larg varieti learn set
cyclic coordin descent beat random coordin descent coordin descent method seen resurg recent interest applic machin learn well larg scale data analysi superior empir perform method variant cyclic coordin descent ccd random coordin descent rcd determinist random version method light recent result literatur common percept rcd alway domin ccd term perform paper question percept provid exampl general problem class ccd determinist order faster rcd term asymptot worst case converg furthermor provid lower upper bound amount improv rate determinist relat rcd amount improv depend determinist order use also provid character best determinist order lead maximum improv converg rate term combinatori properti hessian matrix object function
linear converg frank wolf type algorithm trace norm ball propos rank variant classic frank wolf algorithm solv convex minim trace norm ball algorithm replac top singular vector comput svd frank wolf top singular vector comput svd done repeat appli svd time algorithm linear converg rate object function smooth strong convex optim solut rank improv converg rate total complex frank wolf method variant
adapt acceler gradient converg method lderian error bound condit recent studi shown proxim gradient method acceler gradient method apg restart enjoy linear converg weaker condit strong convex name quadrat growth condit qgc howev faster converg restart apg method reli potenti unknown constant qgc appropri restart apg restrict applic address issu develop novel adapt gradient converg method leverag magnitud proxim gradient criterion restart termin analysi extend much general condit beyond qgc name lderian error bound heb condit key techniqu develop novel synthesi adapt regular condit restart scheme extend previous work focus strong convex problem much broader famili problem furthermor demonstr result import implic applic machin learn object function coerciv semi algebra converg speed essenti total number iter object function consist huber norm regular convex smooth piecewis quadrat loss squar loss squar hing loss huber loss propos algorithm paramet free enjoy faster linear converg without assumpt restrict eigen valu condit notabl linear converg result aforement problem global instead local best knowledg improv result first shown work
search dark practic svrg method error bound condit guarante paper develop practic stochast varianc reduc gradient svrg method error bound condit theoret guarante error bound condit inher properti optim problem recent reviv optim develop fast algorithm improv global converg without strong convex particular condit interest quadrat error bound aka second order growth condit weaker strong convex leverag develop linear converg mani gradient proxim gradient method sever recent studi also deriv linear converg quadrat error bound condit stochast varianc reduc gradient method import mileston stochast optim solv machin learn problem howev studi overlook critic issu algorithm depend unknown paramet analog strong convex modulus error bound condit usual difficult estim therefor make algorithm practic solv mani interest machin learn problem address issu propos novel techniqu automat search unknown paramet fli optim maintain almost converg rate oracl set assum involv paramet given
geometr descent method convex composit minim paper extend geometr descent method recent propos bubeck lee singh tackl nonsmooth strong convex composit problem prove propos algorithm dub geometr proxim gradient method geopg converg linear rate
condit number problem numer result linear regress logist regress elast net regular show geopg compar favor nesterov acceler proxim gradient method especi problem ill condit
faster non ergod stochast altern direct method multipli studi stochast convex optim subject linear equal constraint tradit stochast altern direct method multipli nesterov acceler scheme achiev ergod sqrt converg rate number iter introduc varianc reduct techniqu converg rate improv ergod paper propos new stochast admm elabor integr nesterov extrapol techniqu nesterov extrapol algorithm achiev non ergod converg rate optim separ linear constrain non smooth convex problem converg rate base admm method actual tight sqrt non ergod sens best knowledg first work achiev truli acceler stochast converg rate constrain convex problem experiment result demonstr algorithm signific faster exist state art stochast admm method
doubli acceler stochast varianc reduc dual averag method regular empir risk minim develop new acceler stochast gradient method effici solv convex regular empir risk minim problem mini batch set use mini batch becom golden standard machin learn communiti mini batch set stabil gradient estim easili make good use parallel comput core propos method incorpor new doubl acceler techniqu varianc reduct techniqu theoret analyz propos method show method much improv mini batch effici previous acceler stochast method essenti need mini batch achiev optim iter complex non strong strong convex object train set size show even non mini batch set method surpass best known converg rate non strong convex object achiev strong convex object
limit varianc reduct acceler scheme finit sum optim studi condit abl effici appli varianc reduct acceler scheme finit sum problem first show perhap surpris finit sum structur suffici obtain complex bound
smooth strong convex finit sum must also know exact individu function refer oracl iter next show broad class first order coordin descent finit sum algorithm includ sdca svrg sag possibl get acceler complex bound unless strong convex paramet given explicit last show class algorithm use minim
smooth non strong convex finit sum optim complex bound
nonlinear acceler stochast algorithm extrapol method use last iter optim algorithm produc better estim optimum shown achiev optim converg rate determinist set use simpl gradient iter studi extrapol method stochast set iter produc either simpl acceler stochast gradient algorithm first deriv converg bound arbitrari potenti bias perturb produc asymptot bound use ratio varianc nois accuraci current point final appli acceler techniqu stochast algorithm sgd saga svrg katyusha differ set show signific perform gain
acceler averag stochast descent dynam formul studi general famili continu time stochast dynam acceler first order minim smooth convex function build averag formul acceler mirror descent propos stochast variant gradient contamin nois studi result stochast differenti equat prove bound rate chang energi function associ problem use deriv estim converg rate function valu expect persist asymptot vanish nois discuss interact paramet dynam learn rate averag weight variat nois process show particular asymptot rate variat affect choic paramet ultim converg rate
multiscal semi markov dynam intracort brain comput interfac intracort brain comput interfac allow peopl tetraplegia control comput cursor imagin motion paralyz limb standard decod deriv kalman filter assum markov dynam angl intend movement unimod likelihood channel neural activ due error made decod noisi neural data user attempt move cursor goal angl cursor goal posit chang rapid propos dynam bayesian network includ screen goal posit part latent state thus allow motion cue aggreg much longer histori neural activ multiscal model explicit captur relationship instantan angl motion long term goal incorpor semi markov dynam motion trajectori also propos flexibl likelihood model record neural popul offlin experi record neural data demonstr signific improv predict motion direct compar kalman filter baselin deriv effici onlin infer algorithm enabl clinic trial particip tetraplegia control comput cursor neural activ real time
eeg graph factor graph base model captur spatial tempor observ relationship electroencephalogram paper report factor graph base model brain activ joint describ instantan observ base tempor spatial depend factor function repres depend defin manual base domain knowledg model valid use clinic collect intracrani electroencephalogram eeg data epilepsi patient applic seizur onset local result indic model outperform convent approach devis use observ depend alon better auc furthermor also show manual definit factor function allow solv graph infer exact use graph cut algorithm experi show propos infer techniqu provid gain auc compar sampl base altern
asynchron parallel coordin minim map infer find maximum posteriori map assign central task graphic model sinc modern applic give rise larg problem instanc increas need effici solver work propos improv effici coordin minim base dual decomposit solver run updat asynchron parallel case messag pass infer perform multipl process unit simultan without coordin read write share memori analyz converg properti result algorithm identifi set speedup gain expect numer evalu show approach inde achiev signific speedup common comput vision task
speed latent variabl gaussian graphic model estim via nonconvex optim studi estim latent variabl gaussian graphic model lvggm precis matrix superposit spars matrix low rank matrix order speed estim spars plus low rank compon propos sparsiti constrain maximum likelihood estim base matrix factor effici altern gradient descent algorithm hard threshold solv algorithm order magnitud faster convex relax base method lvggm addit prove algorithm guarante linear converg unknown spars low rank compon optim statist precis experi synthet genom data demonstr superior algorithm state art algorithm corrobor theori
expxorcist nonparametr graphic model via condit exponenti densiti non parametr multivari densiti estim face strong statist comput bottleneck practic approach impos near parametr assumpt form densiti function paper leverag recent develop propos class non parametr model attract comput statist properti approach reli simpl function space assumpt condit distribut variabl condit variabl non parametr exponenti famili form
reduc reparameter gradient varianc optim noisi gradient becom ubiquit statist machin learn reparameter gradient gradient estim comput via reparameter trick repres class noisi gradient often use mont carlo variat infer mcvi howev gradient estim noisi optim procedur slow fail converg way reduc nois use sampl gradient estim comput expens instead view noisi gradient random variabl form inexpens approxim generat procedur gradient sampl approxim high correl noisi gradient construct make use control variat varianc reduct demonstr approach non conjug multi level hierarch model bayesian neural net observ gradient varianc reduct multipl order magnitud 000
robust condit probabl condit probabl core concept machin learn exampl optim predict label given input correspond maxim condit probabl common approach infer task learn model condit probabl howev model often base strong assumpt log linear model henc estim condit probabl robust high depend valid assumpt propos framework reason condit probabl without assum anyth under distribut except knowledg second order margin estim data show set lead guarante bound condit probabl calcul effici varieti set includ structur predict final appli semi supervis deep learn obtain result competit variat autoencod
stein variat gradient descent gradient flow stein variat gradient descent svgd determinist sampl algorithm iter transport set particl approxim given distribut base effici gradient base updat guarante optim decreas diverg within function space paper develop first theoret analysi svgd establish empir measur svgd sampl weak converg target distribut show asymptot behavior svgd character nonlinear fokker planck equat known vlasov equat physic develop geometr perspect view svgd gradient flow diverg function new metric structur space distribut induc stein oper
parallel stream wasserstein barycent effici aggreg data differ sourc challeng problem particular sampl sourc distribut differ differ inher infer task present reason sensor sensor network place far apart affect individu measur convers comput advantag split bayesian infer task across subset data data need ident distribut across subset principl way fuse probabl distribut via len optim transport wasserstein barycent singl distribut summar collect input measur respect geometri howev comput barycent scale poor requir discret input distribut barycent improv situat present scalabl communic effici parallel algorithm comput wasserstein barycent arbitrari distribut algorithm oper direct continu input distribut optim stream data method even robust nonstationari input distribut produc barycent estim track input measur time algorithm semi discret need discret barycent estim best knowledg also provid first bound qualiti approxim barycent discret becom finer final demonstr practic effect method track move distribut sphere well larg scale bayesian infer task
aid algorithm measur accuraci probabilist infer algorithm approxim probabilist infer algorithm central mani field exampl includ sequenti mont carlo infer robot variat infer machin learn markov chain mont carlo infer statist key problem face practition measur accuraci approxim infer algorithm specif dataset exist techniqu measur infer accuraci often brittl special type infer algorithm paper introduc auxiliari infer diverg estim aid algorithm measur accuraci approxim infer algorithm aid base observ infer algorithm treat probabilist model random variabl use within infer algorithm view auxiliari variabl view lead new estim symmetr diverg output distribut infer algorithm paper illustr applic aid algorithm infer regress hidden markov dirichlet process mixtur model experi show aid captur qualit behavior broad class infer algorithm detect failur mode infer algorithm miss standard heurist
deep dynam poisson factor model new model name deep dynam poisson factor model analyz sequenti count vector propos paper model base poisson factor analysi method captur depend among time step neural network repres implicit distribut local complic relationship obtain local implicit distribut deep latent structur exploit get long time depend variat infer latent variabl gradient descent base loss function deriv variat distribut perform infer synthet dataset real world dataset appli propos model result show good predict fit perform interpret latent structur
model shrinkag effect gamma process edg partit model edg partit model epm fundament bayesian nonparametr model extract overlap structur binari matrix epm adopt gamma proce prior automat shrink number activ atom howev empir found model shrinkag epm typic work appropri lead overfit solut analysi expect epm intens function suggest gamma prior epm hyperparamet disturb model shrinkag effect intern order ensur model shrinkag effect epm work appropri manner propos novel generat construct epm cepm incorpor constrain gamma prior depm incorpor dirichlet prior instead gamma prior furthermor depm model paramet includ infinit atom prior could margin thus possibl deriv truli infinit depm idepm effici infer use collaps gibb sampler experiment confirm model shrinkag propos model work well idepm indic state art perform general abil link predict accuraci mix effici converg speed
model evid nonequilibrium simul margin likelihood model evid key quantiti bayesian paramet estim model comparison mani probabilist model comput margin likelihood challeng involv sum integr enorm paramet space markov chain mont carlo mcmc power approach comput margin likelihood various mcmc algorithm evid estim propos literatur discuss use nonequilibrium techniqu estim margin likelihood nonequilibrium estim build recent develop statist physic known anneal import sampl ai revers ai probabilist machin learn introduc new estim model evid combin forward backward simul show various challeng model new evid estim outperform forward revers ai
nice adversari train mcmc exist markov chain mont carlo mcmc method either base general purpos domain agnost scheme lead slow converg requir hand craft problem specif propos expert propos nice novel method train flexibl parametr markov chain kernel produc sampl desir properti first propos effici likelihood free adversari train method train markov chain mimic given data distribut leverag flexibl volum preserv flow obtain parametr kernel mcmc use bootstrap approach show train effici markov chain sampl prescrib posterior distribut iter improv qualiti model sampl nice provid first framework automat design effici domain specif mcmc propos empir result demonstr nice combin strong guarante mcmc express deep neural network abl signific outperform compet method hamiltonian mont carlo
identif gaussian process state space model gaussian process state space model gpssm non linear dynam system unknown transit measur map describ gps research gpssms focuss state estim problem howev key challeng gpssms satisfactorili address yet system identif address challeng impos structur gaussian variat posterior distribut latent state parameteris recognit model form direct recurr neural network infer structur allow recov posterior smooth entir sequenc data provid practic algorithm effici comput lower bound margin likelihood use reparameteris trick addit allow arbitrari kernel use within gpssm demonstr effici generat plausibl futur trajectori system seek model gpssm requir small number interact true system
stream spars gaussian process approxim spars approxim gaussian process model provid suit method enabl model deploy larg data regim enabl analyt intract sidestep howev field lack principl method handl stream data posterior distribut function valu hyperparamet updat onlin fashion small number exist approach either use suboptim hand craft heurist hyperparamet learn suffer catastroph forget slow updat new data arriv paper develop new principl framework deploy gaussian process probabilist model stream set provid principl method learn hyperparamet optimis pseudo input locat propos framework experiment valid use synthet real world dataset
bayesian optim gradient bayesian optim shown success global optim expens evalu multimod object function howev unlik optim method bayesian optim typic use deriv inform paper show bayesian optim exploit deriv inform find good solut fewer object function evalu particular develop novel bayesian optim algorithm deriv enabl knowledg gradient dkg step bay optim asymptot consist provid greater step valu inform deriv free set dkg accommod noisi incomplet deriv inform come sequenti batch form option reduc comput cost infer automat select retent singl direct deriv also comput dkg acquisit function gradient use novel fast discret free techniqu show dkg provid state art perform compar wide rang optim procedur without gradient benchmark includ logist regress deep learn kernel learn nearest neighbor
variat infer gaussian process model linear complex larg scale gaussian process infer long face practic challeng due time space complex superlinear dataset size spars variat gaussian process model capabl learn larg scale data standard strategi sparsifi model prevent approxim complex function work propos novel variat gaussian process model decoupl represent mean covari function reproduc kernel hilbert space show new parametr general previous model yield variat infer problem solv stochast gradient ascent time space complex linear number mean function paramet strategi make adopt larg scale express gaussian process model possibl run sever experi regress task show decoupl approach great outperform previous spars variat gaussian process infer procedur
effici model latent inform supervis learn use gaussian process often machin learn data collect combin multipl condit voic record multipl person label could build model captur latent inform relat condit general new data present new model call latent variabl multipl output gaussian process lvmogp allow joint model multipl condit regress general new condit data point test time lvmogp infer posterior gaussian process togeth latent space repres inform differ condit deriv effici variat infer method lvmogp comput complex low spars gaussian process show lvmogp signific outperform relat gaussian process method various task synthet real data
non stationari spectral kernel propos non stationari spectral kernel gaussian process regress propos model spectral densiti non stationari kernel function mixtur input depend gaussian process frequenc densiti surfac solv generalis fourier transform model present famili non stationari non monoton kernel learn input depend potenti long rang non monoton covari input deriv effici infer use model whiten margin posterior show case studi kernel necessari model even rather simpl time seri imag geospati data non stationari characterist
scalabl log determin gaussian process kernel learn applic vari bayesian neural network determinant point process ellipt graphic model kernel learn gaussian process gps must comput log determin posit definit matrix deriv lead prohibit comput propos novel approach estim quantiti fast matrix vector multipl mvms stochast approxim base chebyshev lanczo surrog model converg quick even kernel matric challeng spectra leverag approxim develop scalabl gaussian process approach kernel learn find lanczo general superior chebyshev kernel learn surrog approach high effici accur popular kernel
spectral mixtur kernel multi output gaussian process initi multipl output gaussian process mogp model reli linear transform independ latent singl output gaussian process gps result cross covari function limit parametr interpret thus conflict singl output gps intuit understand lengthscal frequenc magnitud name contrari current approach mogp abl better interpret relationship differ channel direct model cross covari spectral mixtur kernel phase shift propos parametr famili complex valu cross spectral densiti build cramer theorem multivari version bochner theorem provid principl approach design multivari covari function construct kernel abl model delay among channel addit phase differ thus express previous method also provid full parametr interpret relationship across channel propos method valid synthet data compar exist mogp method real world exampl
linear constrain gaussian process consid modif covari function gaussian process correct account known linear constraint model target function transform under function constraint explicit incorpor model guarante fulfil sampl drawn predict made also propos construct procedur design transform oper illustr result simul real data exampl
hindsight experi replay pieter abbeel wojciech zaremba
deal spars reward biggest challeng reinforc learn present novel techniqu call hindsight experi replay allow sampl effici learn reward spars binari therefor avoid need complic reward engin combin arbitrari polici algorithm seen form implicit curriculum demonstr approach task manipul object robot arm particular run experi differ task push slide pick place case use binari reward indic whether task complet ablat studi show hindsight experi replay crucial ingredi make train possibl challeng environ show polici train physic simul deploy physic robot success complet task video present experi avail https goo smrqni
log normal skew estim state action valu reinforc learn overestim state action valu harm reinforc learn agent paper show state action valu estim use bellman equat decompos weight sum path wise valu follow log normal distribut sinc log normal distribut skew distribut estim valu also skew lead imbalanc likelihood overestim degre imbal vari great among action polici within problem instanc make agent prone select action polici inferior expect return higher likelihood overestim present comprehens analysi skew examin factor impact theoret empir result discuss possibl way reduc undesir effect skew
finit sampl analysi gtd polici evalu algorithm markov set reinforc learn key compon polici evalu aim estim valu function expect long term accumul reward start state given polici good polici evalu method algorithm estim valu function given polici accur find better polici state space larg continu emph gradient base tempor differ gtd algorithm linear function approxim valu function wide use consid collect evalu data like time reward consum get clear understand finit sampl perform gtd algorithm import effici polici evalu entir algorithm previous work convert gtd algorithm convex concav saddl point problem provid finit sampl analysi gtd algorithm constant step size assumpt data generat howev know problem data generat markov process rather step size set differ way paper realist markov set deriv finit sampl bound expect high probabl general convex concav saddl point problem henc gtd algorithm bound show markov set variant step size gtd algorithm converg converg rate determin step size relat mix time markov process explain experi repli trick effect sinc improv mix properti markov process best knowledg analysi first provid finit sampl bound gtd algorithm markov set
invers filter hidden markov model paper consid relat invers filter problem hidden markov model hmms given sequenc state posterior system dynam estim correspond sequenc observ estim observ likelihood iii joint estim observ likelihood observ sequenc problem motiv challeng revers engin sensor includ calibr diagnost show avoid comput expens mix integ linear program milp exploit structur hmm filter provid condit quantiti uniqu recov final also consid case posterior corrupt nois shown problem natur pose cluster problem propos algorithm evalu real world polysomnograph data use automat sleep stage
safe model base reinforc learn stabil guarante reinforc learn power paradigm learn optim polici experiment data howev find optim polici reinforc learn algorithm explor possibl action harm real world system consequ learn algorithm rare appli safeti critic system real world paper present learn algorithm explicit consid safeti term stabil guarante specif extend control theoret result lyapunov stabil verif show use statist model dynam obtain high perform control polici provabl stabil certif moreov addit regular assumpt term gaussian process prior prove effect safe collect data order learn dynam thus improv control perform expand safe region state space experi show result algorithm safe optim neural network polici simul invert pendulum without pendulum ever fall
data effici reinforc learn continu state action gaussian pomdp present data effici reinforc learn method continu state action system signific observ nois data effici solut small nois exist pilco learn cartpol swing task 30s pilco evalu polici plan state trajectori use dynam model howev pilco appli polici observ state therefor plan observ space extend pilco filter instead plan belief space consist partial observ markov decis process pomdp plan enabl data effici learn signific observ nois outperform naiv method post hoc applic filter polici optimis origin unfilt pilco algorithm test method cartpol swing task involv nonlinear dynam requir nonlinear control
linear regress without correspond articl consid algorithm statist aspect linear regress correspond covari respons unknown first fulli polynomi time approxim scheme given natur least squar optim problem constant dimens next averag case nois free set respons exact correspond linear function draw standard multivari normal distribut effici algorithm base lattic basi reduct shown exact recov unknown linear function arbitrari dimens final lower bound signal nois ratio establish approxim recoveri unknown linear function
complex learn neural network stun empir success neural network current lack rigor theoret eplan form would explan take face exist complex theoret lower bound first step might show data generat neural network singl hidden layer smooth activ function benign input distribut learn effici demonstr comprehens lower bound rule possibl wide class activ function includ current use input drawn logconcav distribut famili hidden layer function whose output sum gate hard learn precis sens statist queri algorithm includ known variant stochast gradient descent loss function need exponenti number queri even use toler invers proport input dimension moreov hard famili function realiz small sublinear dimens number activ unit singl hidden layer lower bound also robust small perturb true weight systemat experi illustr phase transit train error predict analysi
near optim sketch low rank tensor regress studi least squar regress problem low rank tensor defin vector rpd small compar denot outer product vector linear function problem motiv fact number paramet 1dpd signific smaller 1dpd number paramet ordinari least squar regress consid decomposit model tensor well tucker decomposit model show appli data dimension reduct techniqu base spars random project reduc problem much smaller problem min hold simultan obtain signific smaller dimens sparsiti random linear map possibl ordinari least squar regress final give number numer simul support theori
input sparsiti time possibl kernel low rank approxim low rank approxim common tool use acceler kernel method kernel matrix approxim via rank matrix store much less space process quick work studi limit comput effici low rank kernel approxim show broad class kernel includ popular gaussian polynomi kernel comput relat error rank approxim least difficult multipli input data matrix arbitrari matrix bar breakthrough fast matrix multipl larg requir nnz time nnz number non zero lower bound match mani paramet regim recent work subquadrat time algorithm low rank approxim general kernel mm16 mm17 demonstr algorithm unlik signific improv particular nnz input sparsiti runtim time hope show first time nnz time approxim possibl general radial basi function kernel gaussian kernel close relat problem low rank approxim kernel dataset
higher order total variat class grid minimax theori trend filter method consid problem estim valu function node dimension grid graph equal side length smash noisi observ function assum smooth allow exhibit differ amount smooth differ region grid heterogen elud classic measur smooth nonparametr statist holder smooth meanwhil total variat smooth class allow heterogen restrict anoth sens constant function count perfect smooth achiev move past consid higher order class base way compil discret deriv paramet across node relat class holder class deriv minimax error rate higher order class analyz natur associ trend filter method seen optim appropri class
adapt cluster semidefinit program analyz cluster problem flexibl probabilist model aim identifi optim partit sampl perform exact cluster high probabl use convex semidefinit estim interpret correct relax version mean estim analyz non asymptot framework show optim near optim recov partit furthermor perform shown adapt problem effect dimens well unknown number group partit illustr method perform comparison classic cluster algorithm numer experi simul data
compress gram matrix learn neural network polynomi time consid problem learn function class comput neural network various activ relu sigmoid task believ intract worst case major open problem understand minim assumpt class admit effici algorithm work show natur distribut assumpt eigenvalu decay gram matrix yield polynomi time algorithm non realiz set express class network feed forward network relus make assumpt network architectur label given suffici strong polynomi eigenvalu decay obtain fulli polynomi time algorithm paramet respect squar loss milder decay also lead improv algorithm awar prior work assumpt margin distribut alon lead polynomi time algorithm network relus even hidden layer unlik prior assumpt margin distribut gaussian eigenvalu decay observ practic common data set algorithm appli function class embed suitabl rkhs main technic contribut new approach prove general bound kernel regress use compress scheme oppos rademach bound general known sampl complex bound kernel method must depend norm correspond rkhs quick becom larg depend kernel function employ sidestep worst case bound sparsifi gram matrix use recent work recurs nystrom sampl due musco musco prove approxim spars hypothesi admit compress scheme whose true error depend rate eigenvalu decay
learn averag top loss work introduc averag top atk loss new ensembl loss supervis learn atk loss provid natur general wide use ensembl loss name averag loss maximum loss furthermor atk loss combin advantag allevi correspond drawback better adapt differ data distribut show atk loss afford intuit interpret reduc penalti continu convex individu loss correct classifi data atk loss lead convex optim problem solv effect convent sub gradient base method studi statist learn theori matk establish classif calibr statist consist matk provid use insight practic choic paramet demonstr applic matk learn combin differ individu loss function binari multi class classif regress use synthet real dataset
hierarch cluster beyond worst case hiererach cluster comput recurs partit dataset obtain cluster increas finer granular fundament problem data analysi although hierarch cluster most studi procedur linkag algorithm top heurist rather optim problem recent dasgupta propos object function hierarch cluster initi line work develop algorithm explicit optim object see also paper consid fair general random graph model hierarch cluster call hierarch stochast blockmodel hsbm show certain regim svd approach mcsherri combin specif linkag method result cluster give approxim dasgupta cost function also show approach base sdp relax balanc cut base work makarychev combin recurs sparsest cut algorithm dasgupta yield approxim slight larger regim also semi random set adversari remov edg random graph generat accord hsbm final report empir evalu synthet real world data show propos svd base method inde achiev better cost wide use heurstic also result better classif accuraci under problem multi class classif
net trim convex prune deep neural network perform guarante introduc analyz new techniqu model reduct deep neural network larg network theoret capabl learn arbitrarili complex model overfit model redund negat affect predict accuraci model varianc net trim algorithm prune sparsifi train network layer wise remov connect layer solv convex optim program program seek spars set weight layer keep layer input output consist origin train model algorithm associ analysi applic neural network oper rectifi linear unit relu nonlinear activ present parallel cascad version algorithm latter achiev slight simpler model general perform former comput distribut manner case net trim signific reduc number connect network also provid enough regular slight reduc general error also provid mathemat analysi consist initi network retrain model analyz model sampl complex deriv general suffici condit recoveri spars transform matrix singl layer take independ gaussian random vector input show network respons describ use maximum number non weight per node weight learn slog sampl
graph theoret approach multitask key featur neural network architectur abil support simultan interact among larg number unit learn process represent howev rich interact trade abil network simultan carri multipl independ process salient limit mani domain human cognit remain larg unexplor paper use graph theoret analysi network architectur address question task repres edg bipartit graph defin new measur multitask capac network base assumpt task emph need multitask reli independ resourc form match task emph perform without interfer form induc match main result inher tradeoff multitask capac averag degre network hold emph regardless network architectur result also extend network depth greater
posit side demonstr network random like local spars desir multitask properti result shed light parallel process limit neural system provid insight use analysi design parallel architectur
inform theoret analysi general capabl learn algorithm deriv upper bound general error learn algorithm term mutual inform input output upper bound provid theoret guidelin strike right balanc data fit general control input output mutual inform learn algorithm result also use analyz general capabl learn algorithm adapt composit bias accuraci tradeoff adapt data analyt work extend lead nontrivi improv recent result russo zou
independ cluster without matrix independ cluster problem consid follow formul given set random variabl requir find finest partit cluster cluster mutual independ sinc mutual independ target pairwis similar measur use thus tradit cluster algorithm inapplic distribut random variabl general unknown sampl avail thus problem cast term time seri form sampl consid stationari time seri main emphasi latter general case consist comput tractabl algorithm set propos number fascin open direct research outlin
polynomi code optim design high dimension code matrix multipl consid larg scale matrix multipl problem comput carri use distribut system master node multipl worker node worker store part input matric propos comput strategi leverag idea code theori design intermedi comput worker node order effici deal straggl worker propos strategi name emph polynomi code achiev optimum recoveri threshold defin minimum number worker master need wait order comput output furthermor leverag algebra structur polynomi code map reconstruct problem final output polynomi interpol problem solv effici polynomi code provid order wise improv state art term recoveri threshold also optim term sever metric furthermor extend code distribut convolut show order wise optim
estim mutual inform discret continu mixtur estim mutual inform observ sampl basic primit machin learn use sever learn task includ correl mine inform bottleneck chow liu tree condit independ test causal graphic model mutual inform quantiti well defin general probabl space estim develop special case discret continu pair random variabl estim oper use principl calcul differenti entropi pair howev general mixtur space individu entropi well defin even though mutual inform paper develop novel estim estim mutual inform discret continu mixtur prove consist estim theoret well demonstr excel empir perform problem relev wide array applic variabl discret continu other mixtur continu discret compon
best respons regress regress task predictor given set instanc along real valu point subsequ identifi valu new instanc accur possibl work initi studi strateg predict machin learn consid regress task tackl player payoff player proport point predict accur player first revis probabl approxim correct learn framework deal case duel predictor devis algorithm find linear regress predictor best respons necessarili linear regress algorithm show linearithm sampl complex polynomi time complex dimens instanc domain fix also test approach high dimension set show signific defeat classic regress algorithm predict duel togeth work introduc novel machin learn task lend well current competit onlin set provid theoret foundat illustr applic
statist cost share studi cost share problem cooper game situat cost function avail via oracl queri must instead learn sampl drawn distribut repres tupl differ subset player formal approach call statist cost share consid comput core shapley valu expand work balcan 2015 give precis sampl complex bound comput cost share satisfi core properti high probabl function non empti core shapley valu never studi set show submodular cost function curvatur bound curvatur approxim sampl uniform distribut factor bound tight defin statist analogu shapley axiom deriv notion statist shapley valu approxim arbitrarili well sampl distribut function
sampl complex measur applic learn optim auction introduc new sampl complex measur refer split sampl growth rate hypothesi sampl size split sampl growth rate count mani differ hypothes empir risk minim output sub sampl size show expect general error upper bound log result enabl strengthen rademach complex analysi expect general error show sampl complex measur great simplifi analysi sampl complex optim auction design mani auction class studi literatur sampl complex deriv sole notic auction class erm sampl sub sampl pick paramet equal point sampl
multipl weight updat constant step size congest game converg limit cycl chao multipl weight updat mwu method ubiquit meta algorithm work follow distribut maintain certain set step probabl assign action multipli cost action rescal ensur new valu form distribut analyz mwu congest game agent use textit arbitrari admiss constant learn rate prove converg textit exact nash equilibria interest converg result carri near homolog mwu variant step probabl assign action multipli even innocu case agent strategi load balanc game dynam provabl lead limit cycl even chaotic behavior
effici guarante data analysi effici outcom game theoret set main item studi intersect econom comput scienc notion price anarchi take worst case stanc effici analysi consid instanc independ guarante effici propos data depend analog price anarchi refin worst case assum access sampl strateg behavior focus auction set latter non trivial due privat inform held particip approach bound effici data robust statist error mis specif unlik tradit econometr seek learn privat inform player observ behavior analyz properti outcom direct quantifi ineffici without go privat inform appli approach dataset sponsor search auction system find empir result signific improv bound worst case analysi
safe nest subgam solv imperfect inform game unlik perfect inform game imperfect inform game cannot solv decompos game subgam solv independ thus comput intens equilibrium find techniqu use decis must consid strategi game whole possibl solv imperfect inform game exact decomposit possibl approxim solut improv exist solut solv disjoint subgam process refer subgam solv introduc subgam solv techniqu outperform prior method theori practic also show adapt past subgam solv techniqu respond oppon action outsid origin action abstract signific outperform prior state art approach action translat final show subgam solv repeat game progress tree lead signific lower exploit appli techniqu develop first defeat top human head limit texa hold poker
primer optim transport optim transport provid power flexibl way compar probabl measur discret continu includ therefor point cloud histogram dataset parametr generat model origin propos eighteenth centuri theori later led nobel prize koopman kantorovich well villani field medal 2010 recent reach machin learn communiti tackl challeng learn scenario includ dimension reduct structur predict problem involv histogram output estim generat model gan high degener high dimension problem despit recent success bring theori practic remain challeng machin learn communiti mathemat formal tutori introduc approach way crucial theoret comput algorithm practic aspect need machin learn applic
fast black box variat infer stochast trust region optim introduc trustvi fast second order algorithm black box variat infer base trust region optim reparameter trick iter trustvi propos assess step base minibatch draw variat distribut algorithm provabl converg stationari point implement trustvi stan framework compar advi trustvi typic converg ten iter solut least good advi reach thousand iter trustvi iter comput expens total comput typic order magnitud less experi
optim transport machin learn optim transport gradual establish power essenti tool compar probabl measur machin learn take form point cloud histogram bag featur general dataset compar probabl densiti generat model trace back earli work mong later kantorovich dantzig birth linear program mathemat theori produc sever import develop sinc crown dric villani field medal 2010 transit appli sphere includ recent applic machin learn tackl challeng learn scenario includ dimension reduct structur predict problem involv histogram estim generat model high degener high dimension problem workshop follow organ year ago nip 2014 seek amplifi trend provid audienc updat recent success brought forward effici solver innov applic long list invit talk add contribut present oral need poster final panel invit speaker take question audienc formul nuanc opinion nascent field
bayesian optim scienc engin ruben martinez cantin jose miguel hern ndez lobato javier gonzalez
bayesian optim recent subfield machin learn compris collect methodolog effici optim expens black box function techniqu work fit model black box function data use model predict decid collect data next optim problem solv use small number function evalu result method character high sampl effici compar altern black box optim algorithm enabl solut new challeng problem exampl recent year becom popular tool machin learn communiti excel perform attain problem hyperparamet tune import result academia industri success made crucial player current trend automat machin learn new method develop area applic continu expand problem hyperparamet tune permeat disciplin field move toward specif problem scienc engin requir new advanc methodolog today bayesian optim promis approach acceler autom scienc engin therefor chosen year theme workshop bayesian optim scienc engin
opt 2017 optim machin learn year mark major mileston histori opt 10th anniversari edit long run nip workshop
previous opt workshop enjoy pack overpack attend huge interest surpris optim 2nd largest topic nip inde foundat wider communiti
look back past decad strong trend appar intersect opt grown monoton point sever cut edg advanc optim aris communiti distinct featur optim within departur textbook approach particular differ set goal driven big data model practic implement crucial
revenu optim approxim bid predict context advertis auction find good reserv price notori challeng learn problem due heterogen opportun type non convex object function work show reduc reserv price optim standard set predict squar loss well understood problem learn communiti bound gap expect bid revenu term averag loss predictor first result formal relat revenu gain qualiti standard machin learn model
multi inform sourc optim consid bayesian method multi inform sourc optim miso seek optim expens evalu black box object function also access cheaper bias noisi approxim inform sourc present novel algorithm outperform state art problem use joint statist model inform sourc better suit miso use previous approach novel acquisit function base step optim analysi support effici parallel provid guarante asymptot qualiti solut provid algorithm experiment evalu demonstr algorithm consist find design higher valu less cost previous approach
straggler mitig distribut optim data encod slow run straggler task signific reduc comput speed distribut comput recent code theori inspir approach appli mitig effect straggl embed redund certain linear comput step optim algorithm thus complet comput without wait straggler paper propos altern approach emb redund data instead comput allow node oper complet oblivi encod propos sever encod scheme demonstr popular batch algorithm gradient descent bfgs appli code oblivi manner determinist achiev sampl path linear converg approxim solut origin problem use arbitrarili vari subset node iter moreov approxim control choic encod matrix number node use iter provid experiment result demonstr advantag approach uncod replic strategi
beyond worst case probabilist analysi affin polici dynam optim affin polici control wide use solut approach dynam optim comput optim adjust solut usual intract worst case perform affin polici signific bad empir perform observ near optim larg class problem instanc instanc stage dynam robust optim problem linear cover constraint uncertain right hand side worst case approxim bound affin polici also tight see bertsima goyal 2012 wherea observ empir perform near optim paper optim aim address stark contrast worst case empir perform affin polici particular show affin polici give good approxim stage adjust robust optim problem high probabl random instanc constraint coeffici generat larg class distribut therebi provid theoret justif observ empir perform hand also present distribut perform bound affin polici instanc generat accord distribut high probabl howev constraint coeffici demonstr empir perform affin polici depend generat model instanc
break nonsmooth barrier scalabl parallel method composit optim due simplic excel perform parallel asynchron variant stochast gradient descent becom popular method solv wide rang larg scale optim problem multi core architectur yet despit practic success support nonsmooth object still lack make unsuit mani problem interest machin learn lasso group lasso empir risk minim box constraint key technic issu explain pauciti design algorithm asynchron analysi work propos analyz proxasaga fulli asynchron spars method inspir saga varianc reduc increment gradient algorithm propos method easi implement signific outperform state art sever nonsmooth larg scale problem prove method achiev theoret linear speedup respect sequenti version assumpt sparsiti gradient block separ proxim term empir benchmark multi core architectur illustr practic speedup 13x core machin
structur predict theori calibr convex surrog loss provid novel theoret insight structur predict context effici convex surrog loss minim consist guarante task loss construct convex surrog optim via stochast gradient descent prove tight bound call calibr function relat excess surrog risk actual risk contrast prior relat work care monitor effect exponenti number class learn guarante well optim complex interest consequ formal intuit task loss make learn harder other classic loss ill suit structur predict
terngrad ternari gradient reduc communic distribut deep learn high network communic cost synchron gradient paramet well known bottleneck distribut train work propos terngrad use ternari gradient acceler distribut deep learn data parallel approach requir numer level aggress reduc communic time mathemat prove converg terngrad assumpt bound gradient guid bound propos layer wise ternar gradient clip improv converg experi show appli terngrad alexnet incur accuraci loss even improv accuraci accuraci loss googlenet induc terngrad less averag final perform model propos studi scalabl terngrad experi show signific speed gain various deep neural network
rebar low varianc unbias gradient estim discret latent variabl model learn model discret latent variabl challeng due high varianc gradient estim general approach reli control variat reduc varianc reinforc estim recent work citep jang2016categor maddison2016concret taken differ approach introduc continu relax discret variabl produc low varianc bias gradient estim work combin approach novel control variat produc low varianc emph unbias gradient estim introduc novel continu relax show tight relax adapt onlin remov hyperparamet show state art varianc reduct sever benchmark generat model task general lead faster converg better final log likelihood
train longer general better close general gap larg batch train neural network background deep learn model typic train use stochast gradient descent variant method updat weight use gradient estim small fraction train data observ use larg batch size persist degrad general perform known general gap phenomena identifi origin gap close remain open problem contribut examin initi high learn rate train phase find weight distanc initi grow logarithmicali number weight updat therefor propos random walk random landscap statist model known exhibit similar ultra slow diffus behavior follow hypothesi conduct experi show empir general gap stem relat small number updat rather batch size complet elimin adapt train regim use investig differ techniqu train model larg batch regim present novel algorithm name ghost batch normal enabl signific decreas general gap without increas number updat valid find conduct sever addit experi mnist cifar cifar 100 imagenet final reassess common practic belief concern train deep model suggest optim achiev good general
unsupervis imag imag translat network exist imag imag translat framework map imag domain correspond imag anoth base supervis learn pair correspond imag domain requir learn translat function larg limit applic captur correspond imag differ domain often difficult task address issu propos unsupervis imag imag translat unit framework propos framework base variat autoencod generat adversari network learn translat function without correspond imag show learn capabl enabl combin weight share constraint adversari object verifi effect propos framework extens experi result
bayesian gan generat adversari network gan implicit learn rich distribut imag audio data hard model explicit likelihood present practic bayesian formul unsupervis semi supervis learn gan use stochast gradient hamiltonian mont carlo margin weight generat discrimin network result approach straightforward obtain good perform without standard intervent featur match mini batch discrimin explor express posterior paramet generat bayesian gan avoid mode collaps produc interpret candid sampl notabl variabl particular provid state art quantit result semi supervis learn benchmark includ svhn celeba cifar outperform dcgan wasserstein gan dcgan ensembl
imagin augment agent deep reinforc learn
reinforc learn deep learn introduc imagin augment agent i2a novel architectur deep reinforc learn combin model free model base aspect contrast exist model base reinforc learn plan method prescrib model use arriv polici i2a learn interpret predict train environ model construct implicit plan arbitrari way use predict addit context deep polici network i2a show improv data effici perform robust model misspecif compar sever strong baselin
multi inform sourc optim consid bayesian method multi inform sourc optim miso seek optim expens evalu black box object function also access cheaper bias noisi approxim inform sourc present novel algorithm outperform state art problem use joint statist model inform sourc better suit miso use previous approach novel acquisit function base step optim analysi support effici parallel provid guarante asymptot qualiti solut provid algorithm experiment evalu demonstr algorithm consist find design higher valu less cost previous approach
chiru identifi translat compar corpora well known problem sever applic exist method reli linguist tool high qualiti corpora absenc resourc especi indian languag make problem hard exampl state art techniqu achiev mean reciproc rank english italian mere 187 telugu kannada work address problem compar corpora base translat correspond induct resourc avail small noisi compar corpora extract wikipedia observ translat sourc target languag mani topic relat word common auxiliari languag model defin notion translingu theme set topic relat word auxiliari languag corpora present probabilist framework extens experi compar corpora show dramat improv perform extend idea propos method measur cross lingual semant related word stimul research area make public avail new high qualiti human annot dataset clsr experi clsr dataset show improv correl clsr task appli method real world problem cross lingual wikipedia titl suggest build wikitsu system user studi wikitsu show improv qualiti titl suggest ccs concept comput methodolog natur languag process addit key word phrase compar corpora translat correspond induct bilingu lexicon cross lingual semant related auxiliari languag wikipedia titl suggest
chiru identifi group strong correl variabl smooth order weight norm failur lasso identifi group correl predictor linear regress spark signific research interest recent various norm propos best describ instanc order weight norm owl altern regular use lasso owl identifi group correl variabl forc model tobe constant within group artifactinduc unnecessari bias model estim paper take submodular perspect show owl pose lov asz extens suitabl defin submodular function submodular perspect explain groupwis constant behavior owl also suggest altern main contribut paper smooth owl sowl new famili norm identifi group also allow model flexibl insid group establish sever algorithm theoret properti sowl includ group identif model consist also provid algorithm toolsto comput sowl norm proximaloper whose comput complexityo log signific better thatof general purpos solver experi sowl compar favor respect owl regim interest
chiru vine copula mix data multi view clusteringfor mix data beyond meta gaussian depend copula enabl exibl parameter multivari distribut termsof constitu margin depend famili vine copula hierarch collect ofbivari copula model wide varieti depend multivari data includingasymmetr tail depend wide use gaussian copula use inmeta gaussian distribut cannot howev current infer algorithm vine cannot data mix combin continu binari ordin featur arecommon mani domain design new infer algorithm vine mix datatherebi extend use severalappl illustr algorithm develop adepend seek multi view cluster model base dirichlet process mixtur vinesthat general previous model arbitrari depend well mix margin empir result synthet real dataset demonstr perform clusteringsingl view multi view data asymmetr tail depend mixedmargin vine copula mix data multi view depend seek cluster
chiru cluster sum norm stochast increment algorithm converg cluster recoveri standard cluster method mean gaussian mixtur model hierarch cluster beset local minima sometim drastic suboptim moreov number cluster must known advanc recent introduc sum norm son clusterpath convex relax mean hierarch cluster shrink cluster centroid toward anoth ensur uniqu global minim give scalabl stochast increment algorithm base proxim iter solv son problem converg guarante also show algorithm recov cluster quit general condit similar form unifi proxim condit introduc approxim algorithm communiti cover paradigm case gaussian mixtur plant partit model give experiment result confirm algorithm scale much better previous method produc cluster compar qualiti
chiru bayesian model tempor coher video entiti discoveri summar video understood user term entiti present entiti discoveri task build appear model entiti person find occurr video repres video sequenc tracklet span frame associ entiti pose entiti discoveri tracklet cluster approach leverag temporalcoher properti tempor neighbor tracklet like associ entiti major contribut first bayesian nonparametr model tracklet level extend chines restaur process crp tempor coher chines restaur franchis crf joint model entiti tempor segment use mixtur compon spars distribut discov person serial video without meta data like script method show consider improv state art approach tracklet cluster term cluster accuraci cluster puriti entiti coverag propos method perform onlin tracklet cluster stream video unlik exist approach automat reject fals tracklet final discuss entiti driven video summar tempor segment video select base discov entiti creat semant meaning summari index term bayesian nonparametr chines restaur process tempor coher tempor segment tracklet cluster entiti discoveri entiti driven video summar
chiru non negat matrix factor heavi nois noisi non negat matrix factor given data matrix find non negat matric nois matrix exist polynomi time algorithm proven error guarante requir column norm much smaller could restrict import applic nmf topic model well theoret nois model gaussian high almost everi column violat condit introduc heavi nois model requir averag nois larg subset column small initi studi noisi nmf heavi nois model show nois model subsum nois model theoret practic interest gaussian nois maximum possibl devis algorithm certain assumpt solv problem heavi nois error guarante match previous algorithm run time substanti better previous best assumpt weaker separ assumpt made previous result provid empir justif assumpt also provid first proof identifi uniqu noisi nmf base separ use hard check geometr condit algorithm outperform earlier polynomi time algorithm time error particular presenc high nois
chiru copula hdp hmm non parametr model tempor multivari data effici bulk cach preload cach import determin storag system perform bulk cach preload process preload larg batch relev data cach minut hour advanc actual request applic address bulk preload analyz high level spatio tempor motif raw noisi trace aggreg trace tempor sequenc correl count vector tempor multivari data trace aggreg aris divers
set workload lead divers data distribut complex spatio tempor depend motiv propos copula hdphmm new bayesian non parametr model techniqu base gaussian copula suitabl tempor multivari data arbitrari margin avoid limit assumpt margin distribut awar prior work copula base extens
bayesian non parametr model algorithm discret data infer copula hard data continu propos semi parametr infer techniqu base extend rank likelihood circumv specifi margin make infer suitabl count data even data combin discret continu margin enabl use bayesian non parametr model sever data type without assumpt margin final propos effici bulk cach preload use copula model leverag high level spatio tempor motif block trace experi benchmark trace show near perfect hitrat use hulk tremend improv baselin use multivari poisson fourth overhead
chiru relat roman comment news articl infer multi glyphic topic correspond comment popular facil provid news site analyz user generat content recent attract research interest howev multilingu societi india analyz user generat content hard due sever reason offici languag linguist resourc avail main hindi observ peopl frequent use roman text easi quick use english keyboard result multiglyph comment text languag differ script roman text almost unexplor machin learn far mani case comment made specif part articl rather topic entir articl shelf method correspond lda insuffici model relationship articl comment paper extend notion correspond model multi lingual multi script inter lingual topic unifi probabilist model call multi glyphic correspond topic model mctm use sever metric verifi approach show improv state art
chiru weight theta function embed applic max cut cluster summar introduc unifi general lovasz theta function sociat geometr embed graph weight node edg show comput exact semidefinit program approxim use svm comput show theta function interpret measur divers graph use idea graph embed algorithm max cut correl cluster document summar well repres problem weight graph
=======
learning active learning data paper suggest novel data driven approach active learning key idea train regressor predicts expected error reduction candidate sample particular learning state formulating query selection procedure regression problem restricted working existing heuristics instead learn strategies based experience previous outcomes show strategy learnt either simple synthetic datasets subset domain specific data method yields strategies work well real data wide range domains
scalable variational inference dynamical systems gradient matching promising tool learning parameters state dynamics ordinary differential equations grid free inference approach fully observable systems times competitive numerical integration however many real world applications sparse observations available even unobserved variables included model description cases gradient matching methods difficult apply simply provide satisfactory results despite high computational cost numerical integration still gold standard many applications using existing gradient matching approach propose scalable variational inference framework infer states parameters simultaneously offers computational speedups improved accuracy works well even model misspecifications partially observable system
active learning peers paper addresses challenge learning peers online multitask setting instead always requesting label human oracle proposed method first determines learner task acquire label sufficient confidence peers either task similarity weighted sum single similar task saves oracle query later use difficult cases queries human oracle paper develops new algorithm exhibit behavior proves theoretical mistake bound method compared best linear predictor hindsight experiments multitask learning benchmark datasets show clearly superior performance baselines assuming task independence learning oracle learning peer tasks
gradient episodic memory continuum learning major obstacle towards artificial intelligence poor ability current models reuse knowledge acquired past quickly learn new tasks forgetting previously learned work formalize emph continuum learning setting training examples emph iid generated continuous stream tasks unknown relationship first propose new set metrics continuum learning characterize learning systems terms average accuracy also terms ability transfer knowledge previous future tasks second propose model continuum learning termed gradient episodic memory gem reduces forgetting allows potential improvements performance previous tasks experiments variants mnist cifar100 datasets demonstrate strong performance model compared variety state art contenders
consistent multitask learning nonlinear output relations key multitask learning exploiting relationships different tasks improve prediction performance relations linear regularization approaches used successfully however practice assuming tasks linearly related might restrictive allowing nonlinear structures challenge paper tackle issue casting problem within framework structured prediction main contribution novel algorithm learning multiple tasks related system nonlinear equations joint outputs need satisfy show algorithm consistent efficiently implemented experimental results show potential proposed method
joint distribution optimal transportation domain adaptation paper deals unsupervised domain adaptation problem wants estimate prediction function
given target domain without labeled sample exploiting knowledge available source domain labels known work makes following assumption exists non linear transformation joint feature label space distributions domain propose solution problem optimal transport allows recover estimated target ptf optimizing simultaneously optimal coupling show method corresponds minimization bound target error provide efficient algorithmic solution convergence proved versatility approach terms class hypothesis loss functions demonstrated real world classification regression problems reach surpass state art results
learning multiple tasks deep relationship networks deep networks trained large scale data learn transferable features promote learning multiple tasks deep features eventually transition general specific along deep networks fundamental problem exploit relationship across different tasks improve feature transferability task specific layers paper propose deep relationship networks drn discover task relationship based novel tensor normal priors parameter tensors multiple task specific layers deep convolutional networks jointly learning transferable features task relationships drn able alleviate dilemma negative transfer feature layers transfer classifier layer extensive experiments show drn yields state art results standard multi task learning benchmarks
label efficient learning transferable representations acrosss domains tasks propose framework learns representation transferable across different domains tasks data efficient manner approach battles domain shift domain adversarial loss generalizes embedding novel task using metric learning based approach model simultaneously optimized labeled source data unlabeled sparsely labeled data target domain method shows compelling results novel classes within new domain even labeled examples per class available outperforming prevalent fine tuning approach addition demonstrate effectiveness framework transfer learning task image object recognition video action recognition
matching neural paths transfer recognition correspondence search many machine learning tasks require finding per part correspondences objects work focus low level correspondences highly ambiguous matching problem propose use hierarchical semantic representation objects coming convolutional neural network solve ambiguity training low level correspondence prediction directly might option domains ground truth correspondences hard obtain show transfer recognition used avoid training idea mark parts matching features close levels convolutional feature hierarchy neural paths although overall number paths exponential number layers propose polynomial algorithm aggregating single backward pass empirical validation done task stereo correspondence demonstrates achieve competitive results among methods use labeled target domain data
deep neural networks suffer crowding crowding visual effect suffered humans object recognized isolation longer recognized objects called clutter placed close work study effect crowding artificial deep neural networks dnns object recognition analyze deep convolutional neural networks dcnns well extension dcnns multi scale change receptive field size convolution filters position image called eccentricity dependent models latter networks recently proposed modeling feedforward path primate visual cortex results reveal incorporating clutter images training set learning dnns lead robustness clutter seen training also dnns trained objects isolation find recognition accuracy dnns falls closer clutter target object clutter find visual similarity target clutter also plays role pooling early layers dnn leads crowding finally show eccentricity dependent model trained objects isolation recognize target objects clutter objects near center image whereas dcnn cannot
svcca singular vector canonical correlation analysis deep understanding improvement continuing empirical successes deep networks becomes increasingly important develop better methods understanding training models representations learned within paper propose singular value canonical correlation analysis svcca tool quickly comparing representations way invariant affine transform allowing comparison different layers networks fast compute allowing comparisons calculated previous methods deploy tool measure intrinsic dimensionality layers showing cases needless parameterization probe learning dynamics throughout training finding networks converge final representations bottom show class specific information networks formed suggest new training regimes simultaneously save computation overfit less
neural expectation maximization many real world tasks reasoning physical interaction require iden tification manipulation conceptual entities first step towards solving tasks automated discovery symbol like representations distributed disentangled paper explicitly formalize problem inference spatial mixture model component parametrized neural network based expectation maximization framework derive differentiable clustering method simultaneously learns group represent individual entities evaluate method sequential perceptual grouping task find accurately able recover constituent objects demonstrate learned representations useful predictive coding
pointnet deep hierarchical feature learning point sets metric space prior works study deep learning point sets pointnet pioneer direction however design pointnet capture local structures induced metric space points live limiting ability recognize fine grained patterns generalizability complex scenes work introduce hierarchical neural network applies pointnet recursively nested partitioning input point set exploiting metric space distances network able learn local features increasing contextual scales observation point sets usually sampled varying densities results greatly decreased performance networks trained uniform densities propose novel set learning layers adaptively combine features multiple scales experiments show network called pointnet able learn deep point set features efficiently robustly particular results significantly better state art obtained challenging benchmarks point clouds
preserving proximity global ranking node embedding investigate unsupervised generative approach network embedding multi task siamese neural network structure formulated connect embedding vectors objective preserve global node ranking local proximity nodes provide deeper analysis connect proposed proximity objective link prediction community detection network show model satisfy following design properties scalability asymmetry unity simplicity experiment results verify design properties also demonstrate superior performance learning rank classification regression link prediction tasks
unsupervised transformation learning via convex relaxations goal extract meaningful transformations data thickness lines handwriting lighting portrait raw images work propose unsupervised approach learn transformations based reconstructing nearest neighbors using linear combination transformations derive new algorithm unsupervised linear transformation learning handwritten digits celebrity portrait datasets show even linear transformations method extracts meaningful transformations generates visually high quality transformed outputs moreover method semiparametric model data distribution allowing learned transformations extrapolate training data work new types images
hunt unique stable sparse fast feature learning graphs purpose learning graphs hunt graph representation exhibit certain uniqueness stability sparsity properties also amenable fast computation leads graph representation based discovery family graph spectral distances denoted fgsd prove possess desired properties evaluate quality graph features produced fgsd demonstrate utility apply graph classification problem extensive experiments show simple svm based classification algorithm driven powerful fgsd based graph features significantly outperforms sophisticated state art algorithms unlabeled node datasets terms accuracy speed also yields competitive results labeled datasets despite fact utilize node label information
deep subspace clustering network present novel deep neural network architecture unsupervised subspace clustering architecture built upon deep auto encoders non linearly map input data latent space key idea introduce novel self expressive layer encoder decoder mimic self expressiveness property proven effective traditional subspace clustering differentiable new self expressive layer provides simple effective way learn pairwise affinities data points standard back propagation procedure nonlinear neural network based method able cluster data points complex often nonlinear structures propose pre training fine tuning strategies let effectively learn parameters subspace clustering networks experiments show proposed method significantly outperforms state art unsupervised subspace clustering methods
learning graph embeddings embedding propagation propose embedding propagation unsupervised learning framework graph structured data learns vector representations graphs passing types messages neighboring nodes forward messages consist label representations representations words features associated nodes backward messages consist gradients results aggregating representations applying reconstruction loss node representations finally computed representation labels significantly fewer parameters hyperparameters instance competitive often outperforms state art unsupervised learning methods range benchmark data sets
unsupervised sequence classification using sequential output statistics consider learning sequence classifier without labeled data using sequential output statistics problem highly valuable since obtaining labels training data often costly sequential output statistics language models could obtained independently input data thus low cost address problem propose unsupervised learning cost function study properties show compared earlier works less inclined stuck trivial solutions avoids need strong generative model although harder optimize functional form stochastic primal dual gradient method developed effectively solve problem experiment results real world datasets demonstrate new unsupervised learning method gives drastically lower errors baseline methods specifically reaches test errors twice obtained fully supervised learning
context selection embedding models word embeddings effective tool analyze language recently extended model types data beyond text items recommendation systems embedding models consider probability target observation word item conditioned elements context words items paper show conditioning elements context optimal instead improve predictions quality embedding representations modeling probability target conditioned subset elements context develop model account use amortized variational inference automatically choose subset experiments demonstrate model outperforms standard embedding methods datasets different domains terms held predictions quality embedding representations
probabilistic rule realization selection abstraction realization bilateral processes key deriving intelligence creativity many domains processes approached emph rules high level principles reveal invariances within similar yet diverse examples probabilistic setting discrete input spaces focus rule realization problem generates input sample distributions follow given rules ambitiously beyond mechanical realization takes whatever given instead ask proactively selecting reasonable rules realize goal demanding practice since initial rule set always consistent thus intelligent compromises needed formulate rule realization selection strongly connected components within single symmetric convex problem derive efficient algorithm works large scale taking music compositional rules main example throughout paper demonstrate model efficiency music realization composition also music interpretation understanding analysis
trimmed density ratio estimation density ratio estimation become versatile tool machine learning community recently however due unbounded nature density ratio estimation vulnerable corrupted data points often pushes estimated ratio toward infinity paper present robust estimator automatically identifies trims outliers proposed estimator convex formulation global optimum obtained via subgradient descent analyze parameter estimation error estimator high dimensional settings experiments conducted verify effectiveness estimator
minimax optimal algorithm crowdsourcing consider problem accurately estimating reliability workers based noisy labels provide fundamental question crowdsourcing propose novel lower bound minimax estimation error applies estimation procedure propose triangular estimation algorithm estimating reliability workers low complexity implemented streaming setting labels provided workers real time rely iterative procedure prove minimax optimal matches lower bound conclude assessing performance state art algorithms synthetic real world data
introspective classification convolutional nets propose introspective convolutional networks icn emphasize importance convolutional neural networks empowered generative capabilities employ reclassification synthesis algorithm perform training using formulation stemmed bayes theory icn tries iteratively synthesize pseudo negative samples enhance improving classification single cnn classifier learned time generative able directly synthesize new samples within discriminative model conduct experiments benchmark datasets including mnist cifar svhn using state art cnn architectures observe improved classification results
adaptive classification prediction budget propose novel adaptive approximation approach test time resource constrained prediction given input instance test time gating function identifies prediction model input among collection models objective minimize overall average cost without sacrificing accuracy learn gating prediction models fully labeled training data means bottom strategy novel bottom method first trains high accuracy complex model low complexity gating prediction model subsequently learnt adaptively approximate high accuracy model regions low cost models capable making highly accurate predictions pose empirical loss minimization problem cost constraints jointly train gating prediction models number benchmark datasets method outperforms state art achieving higher accuracy cost
learning feature evolvable streams learning streaming data attracted much attention past years though studies consider data stream fixed features real practice features evolvable example features data gathered limited lifespan sensors change sensors substituted new ones paper propose novel learning paradigm feature evolvable streaming learning old features would vanish new features occur rather relying current features attempt recover vanished features exploit improve performance specifically learn models recovered features current features respectively benefit recovered features develop ensemble methods first method combine predictions models theoretically show assistance old features performance new features improved second approach dynamically select best single prediction establish better performance guarantee best model switches experiments synthetic real data validate effectiveness proposal
aggressive sampling multi class binary reduction applications text classification address problem multi class classification case number classes large propose double sampling strategy top multi class binary reduction strategy transforms original multi class problem binary classification problem pairs examples aim sampling strategy overcome curse long tailed class distributions exhibited majority large scale multi class classification problems reduce number pairs examples expanded data show strategy alter consistency empirical risk minimization principle defined double sample reduction experiments carried dmoz wikipedia collections 000 100 000 classes show efficiency proposed approach terms training prediction time memory consumption predictive performance respect state art approaches
adversarial surrogate losses ordinal regression ordinal regression seeks class label predictions penalty incurred mistakes increases according ordering labels absolute error canonical example many existing methods task reduce binary classification problems employ surrogate losses hinge loss instead derive uniquely defined surrogate ordinal regression loss functions seeking predictor robust worst case approximations training data labels subject matching certain provided training data statistics demonstrate advantages approach surrogate losses based hinge loss approximations using uci ordinal prediction tasks
formal guarantees robustness classifier adversarial manipulation recent work shown state art classifiers quite brittle sense small adversarial change originally high confidence correctly classified input leads wrong classification high confidence raises concerns classifiers vulnerable attacks calls question usage safety critical systems show paper first time formal guarantees robustness classifier giving instance specific emph lower bounds norm input manipulation required change classifier decision based analysis propose cross lipschitz regularization functional show using form regularization kernel methods resp neural networks improves robustness classifier without loss prediction performance
cost efficient gradient boosting many applications require learning classifiers regressors accurate cheap evaluate prediction cost drastically reduced learned predictor constructed majority inputs uses cheap features fast evaluations main challenge little loss accuracy work propose budget aware strategy based deep boosted regression trees contrast previous approaches learning cost penalties method grow deep trees average nonetheless cheap compute evaluate method number datasets find outperforms current state art large margin algorithm easy implement learning time comparable original gradient boosting source code made available acceptance
highly efficient gradient boosting decision tree gradient boosting decision tree gbdt popular machine learning algorithm quite effective implementations xgboost pgbrt although many engineering optimizations adopted implementations efficiency scalability still unsatisfactory feature dimension high data size large major reason feature need scan data instances estimate information gain possible split points time consuming tackle problem propose novel techniques emph gradient based side sampling goss emph exclusive feature bundling efb goss exclude significant proportion data instances small gradients use rest estimate information gain prove since data instances larger gradients play important role computation information gain goss obtain quite accurate estimation information gain much smaller data size efb bundle mutually exclusive features rarely take nonzero values simultaneously reduce number features prove finding optimal bundling exclusive features hard greedy algorithm achieve quite good approximation ratio thus effectively reduce number features without hurting accuracy split point determination much call new gbdt implementation goss efb emph lightgbm experiments multiple public datasets show lightgbm speeds training process conventional gbdt times achieving almost accuracy
estimating accuracy unlabeled data probabilistic logic approach propose efficient method estimate accuracy classifiers using unlabeled data consider setting multiple classification problems target classes tied together logical constraints example set classes mutually exclusive meaning data instance belong proposed method based intuition classifiers agree likely correct classifiers make prediction violates constraints least classifier must making error experiments real world data sets produce accuracy estimates within percent true accuracy using solely unlabeled data models also outperform existing state art solutions estimating accuracies combining multiple classifier outputs results emphasize utility logical constraints estimating accuracy thus validating intuition
inferring generative model structure static analysis obtaining enough labeled training data complex discriminative models major bottleneck machine learning pipeline popular solution combining multiple sources weak supervision using generative models structure models affects quality training labels difficult learn without ground truth labels instead rely weak supervision sources structure virtue encoded programmatically present coral paradigm infers generative model structure statically analyzing code heuristics thus reducing data required learn structure significantly prove coral sample complexity scales quasilinearly number heuristics number relations found improving standard sample complexity exponential identifying degree relations experimentally coral matches outperforms traditional structure learning approaches points using coral model dependencies instead assuming independence results performing better fully supervised model accuracy points heuristics used label radiology data without ground truth labels
scalable model selection belief networks propose scalable algorithm model selection sigmoid belief networks sbns based factorized asymptotic bayesian fab framework derive corresponding generalized factorized information criterion gfic sbn proven statistically consistent marginal log likelihood capture dependencies within hidden variables sbns recognition network employed model variational distribution resulting algorithm call fabia simultaneously execute model selection inference maximizing lower bound gfic synthetic real data experiments suggest fabia compared state art algorithms learning sbns
produces concise model thus enabling faster testing improves predictive performance iii accelerates convergence prevents overfitting
time dependent spatially varying graphical models application brain fmri data analysis spatio temporal data often exhibits nonstationary changes spatial structure often masked strong temporal dependencies nonseparability work present additive model splits data temporally correlated signal spatially correlated noise model spatially correlated portion using time varying gaussian graphical model assumptions smoothness changes graphical model structure derive strong single sample convergence results confirming ability estimate track meaningful graphical models evolve time apply methodology discovery time varying spatial structure human brain fmri signals
bayesian data augmentation approach learning deep models data augmentation essential part training process applied deep learning models motivation robust training process deep learning models depends large annotated datasets expensive acquired stored processed therefore reasonable alternative able automatically generate new annotated training samples using process known data augmentation dominant data augmentation approach field assumes new training samples obtained via random geometric appearance transformations applied annotated training samples strong assumption unclear reliable generative model producing new training samples paper provide novel bayesian formulation data augmentation allowing introduce theoretically sound algorithm based extension generative adversarial network gan new annotated training points treated missing variables generated based distribution learned training set generalised monte carlo expectation maximisation process classification results mnist cifar cifar 100 show better performance proposed method compared current dominant data augmentation approach
union intersections uoi interpretable data driven discovery prediction increasing size complexity scientific data could dramatically enhance discovery prediction basic scientific applications neuroscience genetics systems biology etc realizing potential however requires novel statistical analysis methods interpretable predictive introduce union intersections uoi method flexible modular scalable framework enhanced model selection estimation method performs model selection model estimation intersection union operations respectively show uoi satisfy criteria low variance nearly unbiased estimation small number interpretable features maintaining high quality prediction accuracy perform extensive numerical investigation evaluate uoi algorithm uoilasso synthetic real data demonstrate extraction interpretable functional networks human electrophysiology recordings well accurate prediction ofphenotypes genotype phenotype data reduced features also show uoil1logistic uoicur variants basic framework improved prediction parsimony classification matrix factorization several benchmark biomedical data sets results suggest methods based uoi framework could improve interpretation prediction data driven discovery across scientific fields
deep learning topological signatures inferring topological geometrical information data offer alternative perspective machine learning problems methods topological data analysis persistent homology enable obtain information typically form summary representations topological features however topological signatures often come unusual structure multisets intervals highly impractical machine learning techniques many strategies proposed map topological signatures machine learning compatible representations suffer agnostic target learning task contrast propose technique enables input topological signatures deep neural networks learn task optimal representation training approach realized novel input layer favorable theoretical properties classification experiments object shapes social network graphs demonstrate versatility approach case latter even outperform state art large margin
practical hash functions similarity estimation dimensionality reduction hashing basic tool dimensionality reduction employed several aspects machine learning however perfomance analysis often carried abstract assumption truly random unit cost hash functions used without concern concrete hash function employed concrete hash functions work fine sufficiently random input question trusted real world faced structured input paper focus prominent applications hashing namely similarity estimation permutation hashing oph scheme nips feature hashing weinberger icml found numerous applications approximate near neighbour search lsh classification svm consider recent mixed tabulation hash function dahlgaard focs proved theoretically perform like truly random hash function many applications including oph first show improved concentration bounds truly random hashing argue mixed tabulation performs similar input vectors dense main contribution however experimental comparison different hashing schemes inside applications find mixed tabulation hashing almost fast classic multiply mod prime scheme mod guaranteed work well sufficiently random data demonstrate applications lead bias poor concentration real world synthetic data also compare popular murmurhash3 proven guarantees mixed tabulation murmurhash3 perform similar truly random hashing experiments however mixed tabulation faster murmurhash3 proven guarantee good performance possible input making reliable
maxing ranking assumptions pac maximum maximum selection maxing ranking elements via random pairwise comparisons diverse applications studied many models assumptions simple natural assumption strong stochastic transitivity show maxing performed linearly many comparisons yet ranking requires quadratically many comparisons assumptions show borda score metric maximum selection performed linearly many comparisons ranking performed nlog comparisons
kernel functions based triplet comparisons propose ways defining kernel function data set available information data set consists similarity triplets form object similar object object machine learning problems based restricted information become popular recent years previous approaches construct low dimensional euclidean embedding data set reflects given similarity triplets aim defining kernel functions correspond high dimensional embeddings kernel functions subsequently used apply kernel method data set
learning structured optimal bipartite graph clustering clustering methods widely applied document clustering gene expression analysis methods make use duality features samples occurring structure sample feature clusters extracted graph based clustering methods bipartite graph constructed depict relation features samples existing clustering methods conduct clustering graph achieved original data matrix explicit cluster structure thus require post processing step obtain clustering results paper propose novel clustering method learn bipartite graph exactly connected components number clusters new bipartite graph learned model approximates original graph maintains explicit cluster structure immediately get clustering results without post processing extensive empirical results presented verify effectiveness robustness model
multi way interacting regression via factorization machines propose bayesian regression method accounts multi way interactions arbitrary orders among predictor variables model makes use factorization mechanism representing regression coefficients interactions among predictors interaction selection guided prior distribution random hypergraphs construction generalizes finite feature model present posterior inference algorithm based gibbs sampling establish posterior consistency regression model method evaluated extensive experiments simulated data demonstrated able identify meaningful interactions several applications genetics retail demand forecasting
maximum margin interval trees learning regression function using censored interval valued output data important problem fields genomics medicine goal learn real valued prediction function training output labels indicate interval possible values whereas existing algorithms task linear models paper investigate learning nonlinear tree models propose learn tree minimizing margin based discriminative objective function provide dynamic programming algorithm computing optimal solution log linear time show empirically algorithm achieves state art speed prediction accuracy benchmark several data sets
kernel feature selection via conditional covariance minimization propose framework feature selection employs kernel based measures independence find subset covariates maximally predictive response building past work kernel dimension reduction formulate approach constrained optimization problem involving trace conditional covariance operator
improved graph laplacian via geometric self consistency address problem setting kernel bandwidth epps used manifold learning algorithms construct graph laplacian exploiting connection manifold geometry represented riemannian metric laplace beltrami operator set epps optimizing laplacian ability preserve geometry data experiments show principled approach effective robust
mixture rank matrix approximation collaborative filtering low rank matrix approximation lrma methods achieved excellent accuracy among today collaborative filtering methods existing lrma methods rank user item feature matrices typically fixed rank adopted describe users items however studies show submatrices different ranks could coexist user item rating matrix approximations fixed ranks cannot perfectly describe internal structures rating matrix therefore leading inferior recommendation accuracy paper mixture rank matrix approximation mrma method proposed user item ratings characterized mixture lrma models different ranks meanwhile learning algorithm capitalizing iterated condition modes proposed tackle non convex optimization problem pertaining mrma experimental studies movielens netflix datasets demonstrate mrma outperform state art lrma based methods terms recommendation accuracy
predictive state recurrent neural networks present new model called predictive state recurrent neural networks psrnns filtering prediction dynamical systems psrnns draw insights recurrent neural networks rnns predictive state representations psrs inherit advantages types models like many successful rnn architectures psrnns use potentially deeply composed bilinear transfer functions combine information multiple sources source act gate another bilinear functions arise naturally connection state updates bayes filters like psrs observations viewed gating belief states show psrnns learned effectively combining backpropogation time bptt initialization based statistically consistent learning algorithm psrs called stage regression 2sr also show psrnns factorized using tensor decomposition reducing model size suggesting interesting theoretical connections existing multiplicative architectures lstms applied psrnns datasets showed outperform several popular alternative approaches modeling dynamical systems cases
hierarchical methods moments spectral methods moments provide powerful tool learning parameters latent variable models despite theoretical appeal applicability methods real data still limited due lack robustness model misspecification paper present hierarchical approach methods moments circumvent limitations method based replacing tensor decomposition step used previous algorithms approximate joint diagonalization experiments topic modeling show method outperforms previous tensor decomposition methods terms speed model quality
multitask spectral learning weighted automata consider problem estimating multiple related functions computed weighted automata wfa first present natural notion relatedness wfas considering extent several wfas share common underlying representation introduce model vector valued wfa conveniently helps formalize notion relatedness finally propose spectral learning algorithm vector valued wfas tackle multitask learning problem jointly learning multiple tasks form vector valued wfa algorithm enforces discovery representation space shared tasks benefits proposed multitask approach theoretically motivated showcased experiments synthetic real world datasets
generative local metric learning kernel regression paper shows metric learning used nadaraya watson kernel regression compared standard approaches bandwidth selection show metric learning significantly reduce mean square error mse kernel regression particularly high dimensional data propose method efficiently learning good metric function based upon analyzing performance estimator gaussian distributed data key feature approach estimator learned metric uses information global local structure training data theoretical empirical results confirm learned metric considerably reduce bias mse kernel regression
principles riemannian geometry neural networks study deals neural networks sense differential transformations systems differential equations forms part attempt construct formalized general theory neural networks branch riemannian geometry perspective following theoretical results developed proven feedforward networks limit number network layers goes infinity first shown residual neural networks dynamical systems first order differential equations opposed ordinary networks static implying network learning systems differential equations organize data second shown limit metric tensor residual networks converges smooth thus defines riemannian manifold third shown limit backpropagation graphs converges differentiable tensor fields results suggest analogy einstein general relativity particle trajectories geodesics curved space time manifolds neural networks learning curved space layer manifolds determine trajectory data moves network
subset selection sequential data subset selection task finding small subset informative items large ground set finds numerous applications different areas sequential data including time series ordered data contain important structural relationships among items imposed underlying dynamic models data play vital role selection representatives however nearly existing subset selection techniques ignore underlying dynamics data treat items independently leading incompatible set representatives paper develop new framework sequential subset selection takes advantage underlying dynamic models data promoting select set representatives high quality diversity also compatible according underlying dynamic models equip items transition dynamic models pose problem integer binary optimization assignments sequential items representatives leads high encoding diversity transition potentials proposed formulation non convex derive max sum message passing algorithm solve problem efficiently experiments synthetic real data including instructional video summarization motion capture segmentation show sequential subset selection framework achieves better encoding diversity state art also successfully incorporates dynamic data leading compatible representatives
quadratic convergence proximal newton algorithm nonconvex sparse learning propose proximal newton algorithm solving nonconvex regularized sparse learning problems high dimensions proposed algorithm integrates proximal newton algorithm multi stage convex relaxation based difference convex programming enjoys strong computational statistical guarantees specifically leveraging sophisticated characterization sparse modeling structures assumptions local restricted strong convexity hessian smoothness prove within stage convex relaxation proposed algorithm achieves local quadratic convergence eventually obtains sparse approximate local optimum optimal statistical properties convex relaxations numerical experiments provided support theory
fast sample efficient algorithms structured phase retrieval consider problem recovering signal magnitude measurements also known phase retrieval problem fundamental challenge nano bio astronomical imaging systems astronomical imaging speech processing problem ill posed therefore additional assumptions signal measurements necessary paper first study case underlying signal sparse develop novel recovery algorithm call compressive phase retrieval alternating minimization copram algorithm simple obtained via natural combination classical alternating minimization approach phase retrieval cosamp algorithm sparse recovery despite simplicity prove algorithm achieves sample complexity log gaussian samples matches best known existing results also demonstrates linear convergence theory practice requires extra tuning parameters signal sparsity level consider case underlying signal arises structured sparsity models specifically examine case block sparse signals uniform block size block sparsity problem design recovery algorithm call block copram reduces sample complexity log sufficiently large block lengths theta bound equates log knowledge constitutes first end end linearly convergent algorithm phase retrieval gaussian sample complexity sub quadratic dependence sparsity level signal
support ordered weighted sparsity overlapping groups hardness algorithms support owl norms generalize norm providing better prediction accuracy better handling correlated variables study norms obtained extending support norm owl norms setting overlapping groups resulting norms general hard compute tractable certain collections groups demonstrate fact develop dynamic program problem projecting onto set vectors supported fixed number groups dynamic program utilizes tree decompositions complexity scales treewidth program converted extended formulation associated group structure models group support norms overlapping group variant ordered weighted norm numerical results demonstrate efficacy new penalties
parametric simplex method sparse learning high dimensional sparse learning imposed great computational challenge large scale data analysis paper investiage broad class sparse learning approaches formulated linear programs parametrized regularization factor solve parametric simplex method psm psm offers significant advantages competing methods psm naturally obtains complete solution path values regularization parameter psm provides high precision dual certificate stopping criterion psm yields sparse solutions iterations solution sparsity significantly reduces computational cost per iteration particularly demonstrate superiority psm various sparse learning approaches including dantzig selector sparse linear regression sparse support vector machine sparse linear classification sparse differential network estimation provide sufficient conditions psm always outputs sparse solutions computational performance significantly boosted thorough numerical experiments provided demonstrate outstanding performance psm method
learned amp principled neural network based compressive image recovery compressive image recovery challenging problem requires fast accurate algorithms recently neural networks applied problem promising results exploiting massively parallel gpu processing architectures oodles training data able run orders magnitude faster existing techniques unfortunately methods difficult train often times specific single measurement matrix largely unprincipled blackboxes recently demonstrated iterative sparse signal recovery algorithms unrolled form interpretable deep neural networks taking inspiration work develop novel neural network architecture mimics behavior denoising based approximate message passing amp algorithm call new network learned amp ldamp ldamp network easy train applied variety different measurement matrices comes state evolution heuristic accurately predicts performance importantly network outperforms state art bm3d amp nlr algorithms terms accuracy runtime high resolutions used matrices fast matrix multiply implementations ldamp runs faster bm3d amp hundreds times faster nlr
falkon optimal large scale kernel method kernel methods provide principled way perform non linear nonparametric learning rely solid functional analytic foundations enjoy optimal statistical properties however least basic form limited applicability large scale scenarios stringent computational requirements terms time especially memory paper take substantial step scaling kernel methods proposing falkon novel algorithm allows efficiently process millions points falkon derived combining several algorithmic principles namely stochastic subsampling iterative solvers preconditioning theoretical analysis shows optimal statistical accuracy achieved requiring essentially
memory time extensive experiments show state art results available large scale datasets achieved even single machine
recursive sampling nystrom method give first algorithm kernel nystrom approximation runs linear time number training points provably accurate kernel matrices without dependence regularity incoherence conditions algorithm projects kernel onto set landmark points sampled ridge leverage scores requiring kernel evaluations additional runtime leverage score sampling long known give strong theoretical guarantees nystrom approximation employing fast recursive sampling scheme algorithm first make approach scalable empirically show finds accurate kernel approximations less time popular techniques classic nystrom approximation random fourier features method
efficient approximation algorithms strings kernel based sequence classification sequence classification algorithms svm require definition distance similarity measure sequences commonly used notion similarity number matches mers length subsequences sequences extending definition considering mers match distance yields better classification performance however makes problem computationally much complex known algorithms compute similarity computational complexity render applicable small values work develop novel techniques efficiently accurately estimate pairwise similarity score enables use much larger values get higher predictive accuracy opens broad avenue applying classification approach audio images text sequences algorithm achieves excellent approximation performance theoretical guarantees process solve open combinatorial problem posed major hindrance scalability existing solutions give analytical bounds quality runtime algorithm report empirical performance real world biological music sequences datasets
robust hypothesis test functional effect gaussian processes work constructs hypothesis test detecting whether data generating function realp real belongs specific reproducing kernel hilbert space structure partially known utilizing theory reproducing kernels reduce hypothesis simple sided score test scalar parameter develop testing procedure robust mis specification kernel functions also propose ensemble based estimator null model guarantee test performance small samples demonstrate utility proposed method apply test problem detecting nonlinear interaction groups continuous features evaluate finite sample performance test different data generating functions estimation strategies null model results revealed interesting connection notions machine learning model underfit overfit statistical inference type error power hypothesis test also highlighted unexpected consequences common model estimating strategies estimating kernel hyperparameters using maximum likelihood estimation model inference
invariance stability deep convolutional representations paper study deep signal representations near invariant groups transformations stable action diffeomorphisms without losing signal information achieved generalizing multilayer kernel introduced context convolutional kernel networks studying geometry corresponding reproducing kernel hilbert space show signal representation stable models functional space large class convolutional neural networks enjoy stability
testing learning distributions symmetric noise invariance kernel embeddings distributions maximum mean discrepancy mmd resulting distance distributions useful tools fully nonparametric sample testing learning distributions however rarely possible differences samples interest discovered differences due different types measurement noise data collection artefacts irrelevant sources variability propose distances distributions encode invariance additive symmetric noise aimed testing whether assumed true underlying processes differ moreover construct invariant features distributions leading learning algorithms robust impairment input distributions symmetric additive noise
empirical study properties random bases kernel methods kernel machines neural networks possess universal function approximation properties nevertheless practice way choosing appropriate function class differ thus limits usage emerge specifically neural networks learn representation adapting basis functions data task kernel methods typically use kernels adapted width rbf kernel change anymore contribute work contrasting neural network kernel methods empirical study analysis reveals random adaptive bases affect quality learning furthermore present kernel basis adaptation schemes make efficient usage features retaining universality properties
max margin invariant features transformed unlabelled data study representations invariant common transformations data important learning techniques focused local approximate invariance implemented within expensive optimization frameworks lacking explicit theoretical guarantees paper study kernels invariant unitary group theoretical guarantees addressing important practical issue unavailability transformed versions labelled data problem call unlabeled transformation problem special form semi supervised learning shot learning present theoretically motivated alternate approach invariant kernel svm based propose max margin invariant features mmif solve problem illustration design framework face recognition demonstrate efficacy approach large scale semi synthetic dataset 153 000 images new challenging protocol labelled faces wild lfw performing strong baselines
safetynets verifiable execution deep neural networks untrusted cloud inference using deep neural networks often outsourced cloud since computationally demanding task however raises fundamental issue trust client sure cloud performed inference correctly lazy cloud provider might use simpler less accurate model reduce computational load worse maliciously modify inference results sent client propose safetynets framework enables untrusted server cloud provide client short mathematical proof correctness inference tasks perform behalf client specifically safetynets develops implements specialized interactive proof protocol verifiable execution class deep neural networks represented arithmetic circuits empirical results layer deep neural networks demonstrate run time costs safetynets client server low safetynets detects incorrect computations neural network untrusted server high probability achieving state art accuracy mnist digit recognition timit speech recognition tasks
multi output polynomial networks factorization machines factorization machines polynomial networks supervised polynomial models based efficient low rank decomposition extend models multi output setting learning vector valued functions application multi class multi task problems cast problem learning way tensor whose slices share common decomposition propose convex formulation problem develop efficient conditional gradient algorithm prove global convergence despite fact involves non convex hidden unit selection step classification tasks show algorithm achieves excellent accuracy much sparser models existing methods recommendation system tasks show combine algorithm reduction ordinal regression multi output classification show resulting algorithm outperforms existing baselines terms ranking accuracy
neural hawkes process neurally self modulating multivariate point process many events occur world event types stochastically excited inhibited sense probabilities elevated decreased patterns sequence previous events discovering patterns help predict type event happen next propose model streams discrete events continuous time constructing neurally self modulating multivariate point process intensities multiple event types evolve according novel continuous time lstm generative model allows past events influence future complex realistic ways conditioning future event intensities hidden state recurrent neural network consumed stream past events model desirable qualitative properties achieves competitive likelihood predictive accuracy real synthetic datasets including missing data conditions
maximizing spread influence training data consider canonical problem influence maximization social networks since seminal work kempte kleinberg tardos largely disjoint efforts problem first studies problem associated learning generative model produces cascades second focuses algorithmic challenge identifying set influencers assuming generative model known recent results learning optimization imply general generative model known rather learned training data algorithm influence maximization yield constant factor approximation guarantee using polynomially many samples drawn distribution paper describe simple algorithm maximizing influence training data main idea behind algorithm leverage strong community structure social networks identify set individuals influentials whose communities little overlap although general approximation guarantee algorithm unbounded show algorithm performs well experimentally analyze performance prove algorithm obtains constant factor approximation guarantee graphs generated stochastic block model traditionally used model networks community structure
inductive representation learning large graphs low dimensional embeddings nodes large graphs proved extremely useful variety prediction tasks content recommendation identifying protein functions however existing approaches require nodes graph present training embeddings previous approaches inherently transductive naturally generalize unseen nodes present graphsage general inductive framework leverages node feature information text attributes efficiently generate node embeddings instead training individual embeddings node learn function generates embeddings sampling aggregating features node local neighborhood algorithm outperforms strong baselines inductive node classification benchmarks classify category unseen nodes evolving information graphs based citation reddit post data show algorithm generalizes completely unseen graphs using multi graph dataset protein protein interactions
meta learning perspective cold start recommendations items matrix factorization popular techniques product recommendation known suffer serious cold start problems item cold start problems particularly acute settings tweet recommendation new items arrive continuously paper present meta learning strategy address item cold start new items arrive continuously propose deep neural network architectures implement meta learning strategy first architecture learns linear classifier whose weights determined item history second architecture learns neural network whose biases instead adapted based item history evaluate techniques real world problem tweet recommendation production data twitter demonstrate proposed techniques significantly beat baseline lookup table based user embeddings also outperform state art production model tweet recommendation
dropoutnet addressing cold start recommender systems latent models become default choice recommender systems due performance scalability however research area primarily focused modeling user item interactions latent models developed cold start deep learning recently achieved remarkable success showing excellent results diverse input types inspired results propose neural network based latent model handle cold start recommender systems unlike existing approaches incorporate additional content based objective terms instead focus learning show neural network models explicitly trained handle cold start dropout model trained top existing latent model effectively providing cold start capabilities full power deep architectures empirically demonstrate state art accuracy publicly available benchmarks
federated multi task learning federated learning poses new statistical systems challenges training machine learning models distributed networks devices work show multi task learning naturally suited handle statistical challenges setting propose novel systems aware optimization method mocha robust practical systems issues method theory first time consider issues high communication cost stragglers fault tolerance distributed multi task learning resulting method achieves significant speedups compared alternatives federated setting demonstrate extensive simulations real world federated datasets
flexpoint adaptive numerical format efficient training deep neural networks deep neural networks commonly developed trained bit floating point format significant gains performance energy efficiency could realized training inference numerical formats optimized deep learning despite substantial advances limited precision inference recent years training neural networks low bit width remains challenging problem present flexpoint data format aiming complete replacement bit floating point format training inference designed support deep network topologies without modifications flexpoint tensors shared exponent dynamically adjusted minimize overflows maximizing available dynamic range validate flexpoint training alexnet deep residual network generative adversarial network using simulator implemented emph neon deep learning framework demonstrate bit flexpoint closely matches bit floating point training models without need tuning model hyper parameters results suggest flexpoint promising numerical format future hardware training inference
bayesian inference individualized treatment effects using multi task gaussian processes predicated increasing abundance electronic health records investigate problem inferring individualized treatment effects using observational data stemming potential outcomes model propose novel multi task learning framework factual counterfactual outcomes modeled outputs function vector valued reproducing kernel hilbert space vvrkhs develop nonparametric bayesian method learning treatment effects using multi task gaussian process linear coregionalization kernel prior vvrkhs bayesian approach allows compute individualized measures confidence estimates via pointwise credible intervals crucial realizing full potential precision medicine impact selection bias alleviated via risk based empirical bayes method adapting multi task prior jointly minimizes empirical error factual outcomes uncertainty unobserved counterfactual outcomes conduct experiments observational datasets interventional social program applied premature infants left ventricular assist device applied cardiac patients wait listed heart transplant experiments show method significantly outperforms state art
tomography london underground scalable model origin destination data paper addresses classical network tomography problem inferring local traffic given origin destination observations focussing large complex public transportation systems build scalable model exploits input output information estimate unobserved link station loads users path preferences based reconstruction users travel time distribution model flexible enough capture possible different path choice strategies correlations users travelling similar paths similar times corresponding likelihood function intractable medium large scale networks propose distinct strategies namely exact maximum likelihood inference approximate tractable model variational inference original intractable model application approach consider emblematic case london underground network tap tap system tracks start exit time location journeys day set synthetic simulations real data provided transport london used validate test model predictions observable unobservable quantities
matching balanced nonlinear representations treatment effects estimation estimating treatment effects observational data challenging problem due missing counterfactuals matching effective strategy tackle problem widely used matching estimators nearest neighbor matching nnm pair treated units similar control units terms covariates estimate treatment effects accordingly however existing matching estimators poor performance distributions control treatment groups unbalanced moreover theoretical analysis suggests bias causal effect estimation would increase dimension covariates paper aim address problems learning low dimensional balanced nonlinear representations bnr observational data particular convert counterfactual prediction classification problem develop kernel learning model domain adaptation constraint design novel matching estimator dimension covariates significantly reduced projecting data low dimensional subspace experiments several synthetic real world datasets demonstrate effectiveness approach
moleculenet continuous filter convolutional neural network modeling quantum interactions deep learning potential revolutionize quantum chemistry ideally suited learn representations structured data speed exploration chemical space convolutional neural networks proven first choice images audio video data atoms molecules restricted grid instead precise locations contain essential physical information would get lost discretized thus propose use textit continuous filter convolutional layers able model local correlations without requiring data lie grid apply layers moleculenet novel deep learning architecture modeling quantum interactions molecules obtain joint model total energy interatomic forces follows fundamental quantum chemical principles includes rotationally invariant energy predictions smooth differentiable potential energy surface architecture achieves state art performance benchmarks equilibrium molecules molecular dynamics trajectories finally introduce challenging benchmark chemical structural variations suggests path work
hiding images plain sight deep steganography steganography practice concealing secret message within another ordinary message commonly steganography used unobtrusively hide small message within noisy regions larger image study attempt place full size color image within another image size deep neural networks simultaneously trained create hiding revealing processes designed specifically work pair system trained images drawn randomly imagenet database works well natural images wide variety sources beyond demonstrating successful application deep learning hiding images carefully examine result achieved explore extensions unlike many popular steganographic methods encode secret message within least significant bits carrier image approach compresses distributes secret image representation across available bits
universal style transfer via feature transforms universal style transfer aims transfer arbitrary visual styles content images existing feed forward based methods enjoying inference efficiency mainly limited inability generalizing unseen styles compromised visual quality paper present simple yet effective method tackles limitations without training pre defined styles key ingredient method pair feature transforms whitening coloring embedded image reconstruction network whitening coloring transforms reflect direct matching feature covariance content image given style image shares similar spirits optimization gram matrix based cost neural style transfer demonstrate effectiveness algorithm generating high quality stylized images comparisons number recent methods also analyze method visualizing whitened features synthesizing textures simple feature coloring
attend predict understanding gene regulation selective attention chromatin past decade seen revolution genomic technologies enable flood genome wide profiling chromatin marks recent literature tried understand gene regulation predicting gene expression large scale chromatin measurements fundamental challenges exist learning tasks genome wide chromatin signals spatially structured high dimensional highly modular core aim understand relevant factors work together previous studies either failed model complex dependencies among input signals relied separate feature analysis explain decisions paper presents attention based deep learning approach call chromattention uses unified architecture model interpret dependencies among chromatin factors controlling gene regulation chromattention uses hierarchy multiple long short term memory lstm modules encode input signals model various chromatin marks cooperate automatically chromattention trains levels attention jointly target prediction enabling attend differentially relevant marks locate important positions per mark evaluate model across different cell types tasks human proposed architecture accurate attention scores also provide better interpretation state art feature visualization methods saliency map
unbounded cache model online language modeling open vocabulary propose extension recurrent networks language modeling adapt prediction changes data distribution associate non parametric large scale memory component stores hidden activations seen past approach seen unbounded continuous cache make use modern approximate search quantization algorithms stores millions representations searching efficiently show approach helps adapting pretrained neural networks novel data distribution tackle called rare word problem
deconvolutional paragraph representation learning learning latent representations long text sequences important first step many natural language processing applications recurrent neural networks rnns become cornerstone challenging task however quality sentences rnn based decoding reconstruction decreases length text propose sequence sequence purely convolutional deconvolutional autoencoding framework free issue also computationally efficient proposed method simple easy implement leveraged building block many applications show empirically compared rnns framework better reconstructing correcting long paragraphs quantitative evaluation semi supervised text classification summarization tasks demonstrate potential better utilization long unlabeled text data
analyzing hidden representations end end automatic speech recognition systems neural models become ubiquitous automatic speech recognition systems neural networks typically used acoustic models complex systems recent studies explored end end speech recognition systems based neural networks trained directly predict text input acoustic features although systems conceptually elegant simpler traditional systems less obvious interpret trained models work analyze speech representations learned deep end end model based convolutional recurrent layers trained connectionist temporal classification ctc loss use pre trained model generate frame level features given classifier trained frame classification phones evaluate representations different layers deep model compare quality predicting phone labels experiments shed light important aspects end end model layer depth model complexity design choices
best worlds transferring knowledge discriminative learning generative visual dialog model present novel training framework neural sequence models particularly grounded dialog generation standard training paradigm models maximum likelihood estimation mle minimizing cross entropy human responses across variety domains recurring problem mle trained generative neural dialog models tend produce safe generic responses like know tell contrast discriminative dialog models trained rank list candidate human responses outperform generative counterparts terms automatic metrics diversity informativeness responses however useful practice since deployed real conversations users work aims achieve best worlds practical usefulness strong performance via knowledge transfer primary contribution end end trainable generative visual dialog model receives gradients perceptual adversarial loss sequence sampled leverage recently proposed gumbel softmax approximation discrete distribution specifically rnn augmented sequence samplers coupled straight gradient estimator enables end end differentiability also introduce stronger encoder visual dialog employ self attention mechanism answer encoding along metric learning loss aid better capturing semantic similarities answer responses overall proposed model outperforms state art visdial dataset significant margin recall
teaching machines describe images natural language feedback robots eventually part every household thus critical enable algorithms learn guided non expert users paper bring human loop enable human teacher give feedback learning agent form natural language descriptive sentence provide stronger learning signal numeric reward easily point mistakes correct focus problem image captioning quality output easily judged non experts propose phrase based captioning model trained policy gradients design critic provides reward learner conditioning human provided feedback show exploiting descriptive feedback model learns perform better given independently written human captions
high order attention models visual question answering quest algorithms enable cognitive abilities important part machine learning common trait recent cognitive like tasks take account different data modalities visual lingual paper propose novel generally applicable form attention mechanism learns high order correlations various data modalities show high order correlations effectively direct appropriate attention relevant elements different data modalities required solve joint task demonstrate effectiveness high order attention mechanism task visual question answering vqa achieve state art performance standard vqa dataset
visual reference resolution using attention memory visual dialog visual dialog task answering series inter dependent questions given input image often requires resolve visual references among questions problem different visual question answering vqa relies spatial attention visual grounding estimated image question pair propose novel attention mechanism exploits visual attentions past resolve current reference visual dialog scenario proposed model equipped associative attention memory storing sequence previous attention key pairs memory model retrieves previous attention taking account recency relevant current question order resolve potentially ambiguous reference model merges retrieved attention tentative obtain final attention current question specifically use dynamic parameter prediction combine attentions conditioned question extensive experiments new synthetic visual dialog dataset show model significantly outperforms state art points situation visual reference resolution plays important role moreover proposed model presents superior performance points improvement visual dialog dataset despite significantly fewer parameters baselines
semi supervised learning optical flow generative adversarial networks convolutional neural networks cnns recently applied optical flow estimation problem training cnns requires sufficiently large ground truth training data existing approaches resort synthetic unrealistic datasets hand unsupervised methods capable leveraging real world videos training ground truth flow fields available methods however rely fundamental assumptions brightness constancy spatial smoothness priors hold near motion boundaries paper propose exploit unlabeled videos semi supervised learning optical flow generative adversarial network key insight adversarial loss capture structural patterns flow warp errors without making explicit assumptions extensive experiments benchmark datasets demonstrate proposed semi supervised algorithm performs favorably purely supervised semi supervised learning schemes
associative embedding end end learning joint detection grouping introduce associative embedding novel method supervising convolutional neural networks task detection grouping number computer vision problems framed manner including multi person pose estimation instance segmentation multi object tracking usually grouping detections achieved multi stage pipelines instead propose approach teaches network simultaneously output detections group assignments technique easily integrated state art network architecture produces pixel wise predictions show apply method multi person pose estimation report state art performance multi person pose mpii dataset coco dataset
learning deep structured multi scale features using attention gated crfs contour prediction recent works shown exploiting multi scale representations deeply learned via convolutional neural networks cnn tremendous importance accurate contour detection paper presents novel approach predicting contours advances state art fundamental aspects multi scale feature generation fusion different previous works directly considering multi scale feature maps obtained inner layers primary cnn architecture introduce hierarchical deep model produces rich complementary representations furthermore refine robustly fuse representations learned different scales novel attention gated conditional random fields crfs proposed experiments ran publicly available datasets bsds500 nyudv2 demonstrate effectiveness latent crf model overall hierarchical framework
incorporating side information adaptive convolution computer vision tasks often side information available helpful solve task example crowd counting camera perspective camera angle height gives clue appearance scale people scene side information shown useful counting systems using traditional hand crafted features fully utilized counting systems based deep learning order incorporate available side information propose adaptive convolutional neural network acnn convolution filter weights adapt current scene context via side information particular model filter weights low dimensional manifold within high dimensional space filter weights filter weights generated using learned filter manifold sub network whose input side information help side information adaptive weights acnn disentangle variations related side information extract discriminative features related current context camera perspective noise level blur kernel parameters demonstrate effectiveness acnn incorporating side information tasks crowd counting corrupted digit recognition image deblurring experiments show acnn improves performance compared plain cnn similar number parameters since existing crowd counting datasets contain ground truth side information collect new dataset ground truth camera angle height side information
learning multi view stereo machine show learn multi view stereopsis system contrast recent learning based methods reconstruction leverage underlying geometry problem feature projection unprojection along viewing rays formulating operations differentiable manner able learn system end end task metric reconstructions end end learning allows utilize priors object shapes enabling reconstruction objects much fewer images even single image required classical approaches well completion unseen surfaces thoroughly evaluate approach shapenet dataset demonstrate benefits classical approaches recent learning based methods
pose guided person image generation paper proposes novel pose guided person generation network allows synthesize person images arbitrary poses based image person novel pose generation framework utilizes pose information explicitly consists key stages coarse structure generation detailed appearance refinement first stage condition image target pose fed net like network generate initial coarse image person target pose second stage refines initial blurry result based autoencoder conjunction discriminator adversarial way extensive experimental results 12864 identification images 256256 fashion photos show model generates high quality person images convincing details
working hard know neighbor margins local descriptor learning loss introduce novel loss learning local feature descriptors inspired sift matching scheme show proposed loss relies maximization distance closest positive closest negative patches could replace complex regularization methods used local descriptor learning works well shallow deep convolution network architectures resulting descriptor compact dimensionality sift 128 shows state art performance matching patch verification retrieval benchmarks fast compute gpu
multimodal image image translation enforcing cycle consistency many image image translation problems ambiguous single input image corresponding multiple possible outputs work aim model distribution possible outputs conditional generative modeling setting ambiguity mapping encoded low dimensional latent vector randomly sampled test time generator learns map input along latent code output explicitly enforce cycle consistency latent code output encouraging invertibility helps prevent many mapping latent code output training also known problem mode collapse helps produce diverse results evaluate relationship perceptual realism diversity images generated method test variety domains
deep supervised discrete hashing rapid growth image video data web hashing extensively studied image video search recent years benefit recent advances deep learning deep hashing methods achieved promising results image retrieval however limitations previous deep hashing methods semantic information fully exploited paper develop deep supervised discrete hashing algorithm based assumption learned binary codes ideal classification pairwise label information classification information used learn hash codes within stream framework constrain outputs last layer binary codes directly rarely investigated deep hashing algorithm discrete nature hash codes alternating minimization method used optimize objective function experimental results shown method outperforms current state art methods benchmark datasets
svd softmax fast softmax approximation large vocabulary neural networks propose fast approximation method softmax function large vocabulary using singular value decomposition svd svd softmax targets fast accurate probability estimation topmost probable words inference recurrent neural network language models proposed method transforms weight matrix used calculation logits using svd approximate probability word estimated fraction svd transformed matrix apply technique language modeling neural machine translation present guideline good approximation algorithm requires arithmetic operations 800k vocabulary case shows speedup gpu
hash embeddings efficient word representations present hash embeddings efficient method representing words continuous vector form hash embedding seen interpolation standard word embedding word embedding created using random hash function hashing trick hash embeddings token represented dimensional embeddings vectors dimensional weight vector final dimensional representation token product rather fitting embedding vectors token selected hashing trick shared pool embedding vectors experiments show hash embeddings easily deal huge vocabularies consisting millions tokens using hash embedding need create dictionary training perform kind vocabulary pruning training show models trained using hash embeddings exhibit least level performance models trained using regular embeddings across wide range tasks furthermore number parameters needed embedding fraction required regular embedding since standard embeddings embeddings constructed using hashing trick actually special cases hash embedding hash embeddings considered extension improvement existing regular embedding types
regularized framework sparse structured neural attention modern neural networks often augmented attention mechanism tells network focus within input propose paper new framework sparse structured attention building upon max operator regularized strongly convex function show operator differentiable gradient defines mapping real values probabilities suitable attention mechanism framework includes softmax slight generalization recently proposed sparsemax special cases however also show framework incorporate modern structured penalties resulting new attention mechanisms focus entire segments groups input encouraging parsimony interpretability derive efficient algorithms compute forward backward passes attention mechanisms enabling use neural network trained backpropagation showcase potential drop replacement existing attention mechanisms evaluate large scale tasks textual entailment machine translation sentence summarization attention mechanisms improve interpretability without sacrificing performance notably textual entailment summarization outperform existing attention mechanisms based softmax sparsemax
attentional pooling action recognition introduce simple yet surprisingly powerful model incorporate attention action recognition human object interaction tasks proposed attention module trained without extra supervision gives sizable boost accuracy keeping network size computational cost nearly leads significant improvements state art base architecture standard action recognition benchmarks across still images videos establishes new state art mpii relative improvement hmdb rgb datasets also perform extensive analysis attention module empirically analytically terms latter introduce novel derivation bottom top attention low rank approximations bilinear pooling methods typically used fine grained classification perspective attention formulation suggests novel characterization action recognition fine grained recognition problem
plan attend generate planning sequence sequence models investigate integration planning mechanism encoder decoder architectures attention present model plans ahead computes alignments input output sequences constructing matrix proposed future alignments commitment vector governs whether follow recompute plan mechanism inspired strategic attentive reader writer straw model proposed model end end trainable fully differentiable operations show outperforms strong baseline character level translation tasks wmt algorithmic task finding eulerian circuits graphs among others analysis demonstrates model computes qualitatively intuitive alignments converges faster baselines achieves superior performance fewer parameters
dilated recurrent neural networks notoriously learning recurrent neural networks rnns long sequences difficult task major challenges extracting complex dependencies vanishing exploding gradients efficient parallelization paper introduce simple yet effective rnn connection structure dilatedrnn simultaneously tackles challenges proposed architecture characterized multi resolution dilated recurrent skip connections combined flexibly different rnn cells moreover dilatedrnn reduces number parameters enhances training efficiency significantly matching state art performance even vanilla rnn cells tasks involving long term dependencies provide theory based quantification architecture advantages introduce memory capacity measure mean recurrent length suitable rnns long skip connections existing measures rigorously prove advantages dilatedrnn recurrent neural architectures
thalamus gated recurrent modules propose deep learning model inspired neuroscience theories communication within neocortex model consists recurrent modules send features via routing center endowing neural modules flexibility share features multiple time steps show model learns route information hierarchically processing input data chain modules observe common architectures feed forward neural networks skip connections emerging special cases architecture novel connectivity patterns learned text8 compression task model outperforms multi layer recurrent networks sequential tasks
wasserstein learning deep generative point process models point processes becoming popular modeling asynchronous sequential data due sound mathematical foundation strength modeling variety real world phenomena currently often characterized via intensity function limits model expressiveness due unrealistic assumptions parametric form used practice furthermore learned via maximum likelihood approach prone failure multi modal distributions sequences paper propose intensity free approach point processes modeling transforms nuisance processes target furthermore train model using likelihood free leveraging wasserstein distance point processes experiments various synthetic real world data substantiate superiority proposed point process model conventional ones
stabilizing training generative adversarial networks regularization deep generative models based generative adversarial networks gans demonstrated impressive sample quality order work require careful choice architecture parameter initialization selection hyper parameters fragility part due dimensional mismatch model distribution true distribution causing density ratio associated divergence undefined overcome fundamental limitation propose new regularization approach low computational cost yields stable gan training procedure demonstrate effectiveness approach several datasets including common benchmark image generation tasks approach turns gan models reliable building blocks deep learning
neural variational inference learning undirected graphical models many problems machine learning naturally expressed language undirected graphical models propose learning inference algorithms undirected models optimize variational approximation log likelihood model central approach upper bound log partition function parametrized function express flexible neural network bound enables accurately track partition function learning speed sampling train broad class powerful hybrid directed undirected models via unified variational inference framework empirically demonstrate effectiveness method several popular generative modeling datasets
adversarial symmetric variational autoencoder new form variational autoencoder vae developed joint distribution data codes considered symmetric forms observed data fed encoder yield codes latent codes drawn simple prior propagated decoder manifest data lower bounds learned marginal log likelihood fits observed data latent codes learning variational bound seeks minimize symmetric kullback leibler divergence joint density functions simultaneously seeking maximize marginal log likelihoods facilitate learning new form adversarial training developed extensive set experiments performed demonstrate state art data reconstruction generation several image benchmarks datasets
diverse accurate image description using variational auto encoder additive gaussian encoding space paper proposes method generate image descriptions using conditional variational auto encoder cvae data dependent gaussian prior encoding space standard cvaes fixed gaussian prior easily collapse generate descriptions little variability approach addresses problem linearly combining multiple gaussian priors based semantic content image increasing flexibility representational power generative model evaluate additive gaussian cvae cvae approach mscoco dataset show produces captions diverse accurate strong lstm baseline cvae variants
forcing training stochastic recurrent networks many efforts devoted incorporate stochastic latent variables sequential neural models recurrent neural networks rnns rnns latent variables successful capturing variability observed natural structured data speech work propose novel recurrent latent variable model unifies successful ideas recently proposed architectures model step sequence associated latent variable used condition recurrent dynamics future steps model trained amortised variational inference inference network augmented rnn runs backward sequence addition next step prediction add auxiliary cost latent variables forces reconstruct state backward recurrent network provides latent variables task independent objective enhances performance overall model although conceptually simple model achieves state art results standard speech benchmarks timit blizzard finally apply model language modeling imdb dataset auxiliary cost crucial learning interpretable latent variables setting show regular evidence lower bound significantly underestimates log likelihood model thus encouraging future works compare likelihoods methods using tighter bounds
shot imitation learning imitation learning commonly applied solve different tasks isolation usually requires either careful feature engineering significant number samples far desire ideally robots able learn demonstrations given task instantly generalize new situations task without requiring task specific engineering paper propose meta learning framework achieving capability call shot imitation learning specifically consider setting large maybe infinite set tasks task many instantiations example task could stack blocks table single tower another task could place blocks table block towers etc case different instances task would consist different sets blocks different initial states training time algorithm presented pairs demonstrations subset tasks neural net trained takes input demonstration current state initially initial state demonstration pair outputs action goal resulting sequence states actions matches closely possible second demonstration test time demonstration single instance new task presented neural net expected perform well new instances new task experiments show use soft attention allows model generalize conditions tasks unseen training data anticipate training model much greater variety tasks settings obtain general system turn demonstrations robust policies accomplish overwhelming variety tasks
reconstruct crush network article introduces energy based model adversarial regarding data minimizes energy given data distribution positive samples maximizing energy another given data distribution negative unlabeled samples model especially instantiated autoencoders energy represented reconstruction error provides general distance measure unknown data resulting neural network thus learns reconstruct data first distribution crushing data second distribution solution handle different problems positive unlabeled learning covariate shift especially imbalanced data using autoencoders allows handling large variety data images text even dialogues experiments show flexibility proposed approach dealing different types data different settings images cifar cifar 100 training setting text amazon reviews learning dialogues facebook babi next response classification dialogue completion
fader networks generating image variations sliding attribute values paper introduces new encoder decoder architecture trained reconstruct images disentangling salient information image values attributes directly latent space result training model generate different realistic versions input image varying attribute values using continuous attribute values choose much specific attribute perceivable generated image property could allow applications users modify image using sliding knobs like faders mixing console change facial expression portrait update color objects compared state art mostly relies training adversarial networks pixel space altering attribute values train time approach results much simpler training schemes nicely scales multiple attributes present evidence model significantly change perceived value attributes preserving naturalness images
predrnn recurrent neural networks video prediction using spatiotemporal lstms predictive learning video sequences aims generate future images learning historical frames spatial appearance temporal variations crucial structures paper models structures presenting predictive recurrent neural network predrnn architecture enlightened idea video prediction system memorize spatial appearance temporal variations unified memory pool concretely memory states longer constrained inside lstm unit instead allowed zigzag directions across stacked rnn layers vertically time steps horizontally core network new spatiotemporal lstm lstm unit extracts memorizes spatial temporal video representations simultaneously predrnn achieves state art prediction performance standard video datasets believed general framework extended predictive learning tasks beyond video prediction
multi agent predictive modeling attentional commnets multi agent predictive modeling essential step understanding physical social team play systems recently interaction networks ins proposed task modeling multi agent physical systems ins scale number interactions system typically quadratic higher order number agents paper introduce vain attentional commnet multi agent predictive modeling scales linearly number agents show vain effective multi agent predictive modeling representation learned transferable learning new data poor tasks method evaluated tasks challenging multi agent prediction domains chess soccer outperforms competing multi agent approaches
real time image saliency black box classifiers work develop fast saliency detection method applied differentiable image classifier train masking model manipulate scores classifier masking salient parts input image model generalises well unseen images requires single forward pass perform saliency detection therefore suitable use real time systems test approach cifar imagenet datasets show produced saliency maps easily interpretable sharp free artifacts suggest new metric saliency test method imagenet object localisation task achieve results outperforming weakly supervised methods
prototypical networks shot learning propose prototypical networks problem shot classification classifier must generalize new classes seen training set given small number examples new class prototypical networks learn metric space classification performed computing distances prototype representations class compared recent approaches shot learning reflect simpler inductive bias beneficial limited data regime achieve excellent results provide analysis showing simple design decisions yield substantial improvements recent approaches involving complicated architectural choices meta learning extend prototypical networks shot learning achieve state art results birds dataset
shot learning information retrieval lens shot learning refers understanding new concepts examples propose information retrieval inspired approach problem motivated increased importance maximally leveraging available information low data regime define training objective aims extract much information possible training batch effectively optimizing relative orderings batch points simultaneously particular view batch point query ranks remaining ones based predicted relevance define model framework structured prediction optimize mean average precision rankings method produces state art results standard benchmarks shot learning
reversible residual network backpropagation without storing activations residual networks resnets demonstrated significant improvement traditional convolutional neural networks cnns image classification increasing performance networks grow deeper wider however memory consumption becomes bottleneck needs store intermediate activations calculating gradients using backpropagation work present reversible residual network revnet variant resnets layer activations reconstructed exactly next layer therefore activations layers need stored memory backprop demonstrate effectiveness revnets cifar imagenet establishing nearly identical performance equally sized resnets activation storage requirements independent depth
gated recurrent convolution neural network ocr optical character recognition ocr aims recognize text natural images widely researched computer vision community paper present new architecture named gated recurrent convolution layer grcl challenge grcl constructed adding gate recurrent convolution layer rcl find equipped gate control context modulation rcl balancing feed forward component well recurrent component addition build bidirectional long short term memory blstm sequence modelling test several variants blstm find suitable architecture ocr finally combine gated recurrent convolution neural network grcnn blstm recognize text natural image grcnn blstm trained end end outperforms benchmark datasets terms state art results including iiit street view text svt icdar
learning efficient object detection models knowledge distillation despite significant accuracy improvement convolutional neural networks cnn based object detectors often require prohibitive runtimes process image real time applications state art models often use deep networks large number floating point operations efforts model compression learn compact models fewer number parameters much reduced accuracy work propose new framework learn compact fast object detection networks improved accuracy using knowledge distillation hint learning although knowledge distillation demonstrated excellent improvements simpler classification setups complexity detection poses new challenges form regression region proposals less voluminous labels address several innovations weighted cross entropy loss address class imbalance teacher bounded loss handle regression component adaptation layers better learn intermediate teacher distributions conduct comprehensive empirical evaluation different distillation configurations multiple datasets including pascal kitti ilsvrc coco results show consistent improvement accuracy speed trade offs modern multi class detection models
active bias training accurate neural network emphasizing high variance samples self paced learning hard example mining weight training instances improve learning accuracy paper presents improved alternatives based lightweight estimates sample uncertainty stochastic gradient descent sgd variance predicted probability correct class across iterations mini batch sgd proximity correct class probability decision threshold extensive experimental results datasets show methods reliably improve accuracy various network architectures including additional gains top popular training techniques residual learning momentum adam batch normalization dropout distillation
decoupling update update deep learning requires data useful approach obtain data creative mine data various sources created different purposes unfortunately approach often leads noisy labels paper propose meta algorithm tackling noisy labels problem key idea decouple update fromhow update demonstrate effectiveness algorithm mining data gender classification combining labeled faces wild lfw face recognition dataset textual genderizing service leads noisy dataset approach simple implement leads state art results analyze convergence properties proposed algorithm
langevin dynamics continuous tempering training deep neural networks minimizing non convex high dimensional objective functions challenging especially training modern deep neural networks paper novel approach proposed divides training process consecutive phases obtain better generalization performance bayesian sampling stochastic optimization first phase explore energy landscape capture fat modes thesecondo etu theparameter domthefirstphase inthebayesian arn gphase weapplycont uoustemper gands imation othelan dynamics createanefficientandeffectivesamp whichthetemperatureisadjustedau maticallyaord thedesig temperature dynamics strategies overcome challenge early trapping bad local minima achieved remarkable improvements various types neural networks shown theoretical analysis empirical experiments
differentiable learning logical rules knowledge base reasoning study problem learning probabilistic first order logical rules knowledge base reasoning learning problem difficult requires learning parameters continuous space well structure discrete space propose framework neural logic programming combines parameter structure learning first order logical rules end end differentiable model approach inspired recently developed differentiable logic called tensorlog inference tasks compiled sequences differentiable operations design neural controller system learns compose operations empirically method obtains state art results multiple knowledge base benchmark datasets including freebase wikimovies
deliberation networks sequence generation beyond pass decoding encoder decoder framework achieved promising progress many sequence generation tasks including machine translation text summarization dialog system image captioning etc framework adopts pass forward process decoding generating sequence lacks deliberation process generated sequence directly used final output without polishing however deliberation common behavior human daily life like reading news writing papers articles books work introduce deliberation process encoder decoder framework propose deliberation networks sequence generation deliberation network levels decoders first pass decoder generates raw sequence second pass decoder polishes refines raw sentence deliberation since second pass deliberation decoder overall picture sequence generated might potential generate better sequence looking future words raw sentence experiments neural machine translation text summarization demonstrate effectiveness proposed deliberation networks
neural program meta induction recently proposed methods neural program induction work assumption large set input output examples learning given input output mapping paper aims address problem data computation efficiency program induction leveraging information related tasks specifically propose novel approaches cross task knowledge transfer improve program induction limited data scenarios first proposal portfolio adaptation set induction models pretrained set related tasks best model adapted towards new task using transfer learning second approach meta program induction shot learning approach used make model generalize new tasks without additional training test efficacy methods constructed new benchmark programs written karel programming language using extensive experimental evaluation karel benchmark demonstrate proposals dramatically outperform baseline induction method use knowledge transfer also analyze relative performance approaches study conditions perform best particular meta induction outperforms existing approaches extreme data sparsity small number examples available fewer number available examples increase thousand portfolio adapted program induction becomes best approach intermediate data sizes demonstrate combined method adapted meta program induction strongest performance
saliency based sequential image attention multiset prediction humans process visual scenes selectively sequentially using attention central models human visual attention saliency map propose hierarchical visual architecture operates saliency map uses novel attention mechanism sequentially focus salient regions take additional glimpses within regions architecture motivated human visual attention used multi label image classification novel multiset task demonstrating achieves high precision recall localizing objects attention unlike conventional multi label image classification models model supports multiset prediction due reinforcement learning based training process allows arbitrary label permutation multiple instances per label
protein interface prediction using graph convolutional networks present general framework graph convolution classification tasks labeled graphs node edge features performing convolution operation neighborhood node interest able stack multiple layers convolution learn effective latent representations integrate information across input graph demonstrate effectiveness approach prediction interfaces proteins challenging problem important applications drug discovery design proposed approach achieves accuracy better state art svm method task also outperforms recently proposed diffusion convolution form graph convolution
dual agent gans photorealistic identity preserving profile face synthesis synthesizing realistic profile faces promising efficiently training deep pose invariant models large scale unconstrained face recognition populating samples extreme poses avoiding tedious annotations however learning synthetic faces achieve desired performance due discrepancy distributions synthetic real face images narrow gap propose dual agent generative adversarial network gan model improve realism face simulator output using unlabeled real faces preserving identity information realism refinement dual agents specifically designed distinguishing real fake identities simultaneously particular employ shelf face model simulator generate profile face images varying poses gan leverages fully convolutional network generator generate high resolution images auto encoder discriminator dual agents besides novel architecture make several key modifications standard gan preserve pose texture preserve identity stabilize training process pose perception loss identity perception loss iii adversarial loss boundary equilibrium regularization term experimental results show gan presents compelling perceptual results also significantly outperforms state arts large scale challenging nist ijb unconstrained face recognition benchmark addition proposed gan also promising new approach solving generic transfer learning problems effectively
toward robustness label noise training deep discriminative neural networks collecting large training datasets annotated high quality labels costly process paper proposes novel framework training deep convolutional neural networks noisy labeled datasets problem formulated using undirected graphical model represents relationship noisy clean labels trained semi supervised setting proposed structure inference latent clean labels tractable regularized training using auxiliary sources information proposed model applied image labeling problem shown effective labeling unseen images well reducing label noise training cifar coco datasets
soft hard vector quantization end end learning compressible representations present new approach learn compressible representations deep architectures end end training strategy method based soft continuous relaxation quantization entropy anneal discrete counterparts throughout training showcase method challenging applications image compression neural network compression tasks typically approached different methods soft hard quantization approach gives results competitive state art
selective classification deep neural networks selective classification techniques also known reject option yet considered context deep neural networks dnns techniques potentially significantly improve dnns prediction performance trading coverage paper propose method construct selective classifier given trained neural network method allows user set desired risk level test time classifier rejects instances needed grant desired risk high probability empirical results cifar imagenet convincingly demonstrate viability method opens possibilities operate dnns mission critical applications example using method unprecedented error top imagenet classification guaranteed probability almost test coverage
deep lattice networks partial monotonic functions propose learning deep models monotonic respect user specified set inputs alternating layers linear embeddings ensembles lattices calibrators piecewise linear functions appropriate constraints monotonicity jointly training resulting network implement layers projections new computational graph nodes tensorflow use adam optimizer batched stochastic gradients experiments benchmark real world datasets show layer monotonic deep lattice networks achieve state art performance classification regression monotonicity guarantees
learning prune deep neural networks via layer wise optimal brain surgeon develop slim accurate deep neural networks become crucial real world applications especially employed embedded systems though previous work along research line shown promising results existing methods either fail significantly compress well trained deep network require heavy retraining process pruned deep network boost prediction performance paper propose new layer wise pruning method deep neural networks proposed method parameters individual layer pruned independently based second order derivatives layer wise error function respect corresponding parameters prove final prediction performance drop pruning bounded linear combination reconstructed errors caused layer therefore guarantee needs perform light retraining process pruned network resume original prediction performance conduct extensive experiments benchmark datasets demonstrate effectiveness pruning method compared several state art baseline methods
bayesian compression deep learning compression computational efficiency deep learning become problem great significance work argue principled effective way attack problem taking bayesian point view sparsity inducing priors prune large parts network introduce novelties paper use hierarchical priors prune nodes instead individual weights use posterior uncertainties determine optimal fixed point precision encode weights factors significantly contribute achieving state art terms compression rates still staying competitive methods designed optimize speed energy efficiency
lower bounds robustness adversarial perturbations input output mappings learned state art neural networks significantly discontinuous possible cause neural network used image recognition misclassify input applying specific hardly perceptible perturbations input called adversarial perturbations many hypotheses proposed explain existence peculiar samples well several methods mitigate proven explanation remains elusive however work take steps towards formal characterization adversarial perturbations deriving lower bounds magnitudes perturbations necessary change classification neural networks bounds experimentally verified mnist cifar data sets
sobolev training neural networks heart deep learning aim use neural networks function approximators training produce outputs inputs emulation ground truth function data creation process many cases access input output pairs ground truth however becoming common access derivatives target output respect input example ground truth function neural network network compression distillation generally target derivatives computed ignored paper introduces sobolev training neural networks method incorporating target derivatives addition target values training optimising neural networks approximate function outputs also function derivatives encode additional information target function within parameters neural network thereby improve quality predictors well data efficiency generalization capabilities learned function approximation provide theoretical justifications approach well examples empirical evidence distinct domains regression classical optimisation datasets distilling policies agent playing atari large scale applications synthetic gradients domains use sobolev training employing target derivatives addition target values results models higher accuracy stronger generalisation
structured bayesian pruning via log normal multiplicative noise dropout based regularization methods regarded injecting random noise pre defined magnitude different parts neural network training recently shown bayesian dropout procedure improves generalization also leads extremely sparse neural architectures automatically setting individual noise magnitude per weight however sparsity hardly used acceleration since unstructured paper propose new bayesian model takes account computational structure neural networks provides structured sparsity removes neurons convolutional channels cnns inject noise neurons outputs keeping weights unregularized establish probabilistic model proper truncated log uniform prior noise truncated log normal variational approximation ensures term evidence lower bound computed closed form model leads structured sparsity removing elements low snr computation graph provides significant acceleration number deep neural architectures model easy implement corresponds addition dropout like layer computation graph
population matching discrepancy applications deep learning differentiable estimation distance distributions based samples important many deep learning tasks estimation maximum mean discrepancy mmd however mmd suffers sensitive kernel bandwidth hyper parameter weak gradients large mini batch size used training objective paper propose population matching discrepancy pmd estimating distribution distance based samples well algorithm learn parameters distributions using pmd objective pmd defined minimum weight matching sample populations distribution prove pmd strongly consistent estimator first wasserstein metric apply pmd deep learning tasks domain adaptation generative modeling empirical results demonstrate pmd overcomes aforementioned drawbacks mmd outperforms mmd tasks terms performance well convergence speed
investigating learning dynamics deep neural networks using random matrix theory evidence well conditioned singular value distribution input output jacobian lead substantial improvements training performance deep neural networks deep linear networks conclusive evidence initializing using orthogonal random matrices lead dramatic improvements training however benefit initialization strategies proven much less obvious realistic nonlinear networks use random matrix theory study conditioning jacobian nonlinear neural networks random initialization show singular value distribution jacobian sensitive distribution weights also nonlinearity surprisingly find benefit orthogonal initialization negligible rectified linear networks substantial tanh networks provide rule thumb initializing tanh networks display dynamical isometry full depth finally perform experiments mnist cifar10 using wide array optimizers show conclusively singular value distribution jacobian intimately related learning dynamics finally show spectral density jacobian evolves relatively slowly training good initialization affects learning dynamics far initial setting weights
robust imitation diverse behaviors deep generative models recently shown great promise imitation learning motor control given enough data even supervised approaches shot imitation learning however vulnerable cascading failures agent trajectory diverges demonstrations compared purely supervised methods generative adversarial imitation learning gail learn robust controllers fewer demonstrations inherently mode seeking difficult train paper show combine favourable aspects approaches base model new type variational autoencoder demonstration trajectories learns semantic policy embeddings show embeddings learned dof jaco robot arm reaching tasks smoothly interpolated resulting smooth interpolation reaching behavior leveraging policy representations develop new version gail much robust purely supervised controller especially demonstrations avoids mode collapse capturing many diverse behaviors gail demonstrate approach learning diverse gaits demonstration biped dof humanoid mujoco physics environment
question asking program generation hallmark human intelligence ability ask rich creative revealing questions introduce cognitive model capable constructing human like questions approach treats questions formal programs executed state world output answer model specifies probability distribution complex compositional space programs favoring concise programs help agent learn current context evaluate approach modeling types open ended questions generated humans attempting learn ambiguous situation game find model predicts questions people ask generalize novel situations creative ways addition compare number model variants assess features critical producing human like questions
variational laws visual attention dynamic scenes computational models visual attention crossroad disciplines like cognitive science computational neuroscience computer vision paper proposes approach based principle foundational laws drive emergence visual attention devise variational laws eye movement rely generalized view least action principle physics potential energy captures details well peripheral visual features kinetic energy corresponds classic interpretation analytic mechanics addition lagrangian contains brightness invariance term characterizes significantly scanpath trajectories obtain differential equations visual attention stationary point generalized action propose algorithm estimate model parameters finally report experimental results validate model tasks saliency detection
flexible statistical inference mechanistic models neural dynamics mechanistic models single neuron dynamics extensively studied computational neuroscience however identifying models quantitatively reproduce empirically measured data challenging propose overcome limitation using likelihood free inference approaches also known approximate bayesian computation abc perform full bayesian inference single neuron models approach builds recent advances abc learning neural network maps features observed data posterior distribution parameters learn bayesian mixture density network approximating posterior multiple rounds adaptively chosen simulations furthermore propose efficient approach handling missing features parameter settings simulator fails prevalent issues models neural dynamics well strategy automatically learning relevant features using recurrent neural networks synthetic data approach efficiently estimates posterior distributions recovers ground truth parameters vitro recordings membrane voltages recover multivariate posteriors biophysical parameters yield model predicted voltage traces accurately match empirical data approach enable neuroscientists perform bayesian inference complex neuron models without design model specific algorithms closing gap mechanistic statistical approaches single neuron modelling
training recurrent networks generate hypotheses brain solves hard navigation problems self localization navigation noisy sensors ambiguous world computationally challenging yet animals humans excel robotics simultaneous location mapping slam algorithms solve problem though joint sequential probabilistic inference coordinates external spatial landmarks generate first neural solution slam problem training recurrent lstm networks perform set hard navigation tasks require generalization completely novel trajectories environments goal make sense diverse phenomenology brain spatial navigation circuits related function show hidden unit representations exhibit several key properties hippocampal place cells including stable tuning curves remap environments result also proof concept end end learning slam algorithm using recurrent networks demonstration approach advantages robotic slam
yass yet another spike sorter spike sorting critical first step extracting neural signals large scale electrophysiological data manuscript describes automatic efficient reliable pipeline spike sorting dense multi electrode arrays meas neural signals appear across many electrodes spike sorting currently represents major computational bottleneck present several new techniques make dense mea spike sorting robust scalable pipeline based efficient multi stage triage cluster pursuit approach initially extracts clean high quality waveforms electrophysiological time series temporarily discarding noisy orcollided events representing neurons firing synchronously accomplished developing neural net detection method followed efficient outlier triaging clean waveforms used infer number neurons shapes nonparametric bayesian clustering clustering approach adapts coreset approach data reduction uses efficient inference methods dirichlet process mixture model framework dramatically improve scalability reliability entire pipeline thetriaged waveforms finally recovered matching pursuit deconvolution techniques proposed methods improve state art terms accuracy stability real biophysically realistic simulated mea data furthermore proposed pipeline efficient learning templates clustering much faster real time 512 electrode dataset using primarily single cpu core
neural system identification large populations separating neuroscientists classify neurons different types perform similar computations different locations visual field traditional neural system identification methods capitalize separation learning deep convolutional feature spaces shared among many neurons provides exciting path forward architectural design needs account data limitations new experimental techniques enable recordings thousands neurons experimental time limited sample small fraction neuron response space show major bottleneck fitting convolutional neural networks cnns neural data estimation individual receptive field locations problem scratched surface thus far propose cnn architecture sparse pooling layer factorizing spatial feature dimensions network scales well thousands neurons short recordings trained end end explore architecture ground truth data explore challenges limitations cnn based system identification moreover show network model outperforms current state art system identification model mouse primary visual cortex publicly available dataset
simple model recognition recall memory show several striking differences memory performance recognition recall tasks explained ecological bias endemic classic memory experiments experiments universally involve stimuli retrieval cues show sensible think recall simply retrieving items probed cue typically item list better think recognition retrieving cues probed items test theory manipulating number items cues memory experiment show crossover effect memory performance within subjects recognition performance superior recall performance number items greater number cues recall performance better recognition converse holds build simple computational model around theory using sampling approximate ideal bayesian observer encoding retrieving situational occurrence frequencies stimuli retrieval cues model robustly reproduces number dissociations recognition recall previously used argue dual process accounts declarative memory
gaussian process based nonlinear latent structure discovery multivariate spike train data large body recent work focused methods identifying low dimensional latent structure multi neuron spike train data methods employed either linear latent dynamics linear log linear mappings latent space spike rates propose doubly nonlinear latent variable model population spike trains identify nonlinear low dimensional structure underlying apparently high dimensional spike train data model poisson gaussian process latent variable model gplvm defined low dimensional latent variable governed gaussian process nonlinear tuning curves parametrized exponentiated samples second gaussian process poisson observations nonlinear tuning curves allow discovery low dimensional latent embeddings even spike rates span high dimensional subspace hippocampal place cell codes learn model introduce decoupled laplace approximation fast approximate inference method allows efficiently maximize marginal likelihood latent path integrating tuning curves show method outperforms previous approaches maximizing laplace approximation based marginal likelihoods convergence speed value final objective apply model spike trains recorded hippocampal place cells show outperforms variety previous methods latent structure discovery including variational auto encoder based methods parametrize nonlinear mapping latent space spike rates deep neural network
deep adversarial neural decoding present novel approach solve problem reconstructing perceived stimuli brain responses combining probabilistic inference deep learning approach first inverts linear transformation latent features brain responses maximum posteriori estimation inverts nonlinear transformation perceived stimuli latent features adversarial training convolutional neural networks test approach functional magnetic resonance imaging experiment show generate state art reconstructions perceived faces brain activations
cross spectral factor analysis neuropsychiatric disorders schizophrenia depression often disruption way different regions brain communicate another order build greater understanding neurological basis disorders introduce novel model multisite local field potentials lfps low frequency voltage oscillations measured electrodes implanted many brain regions simultaneously proposed model called cross spectral factor analysis csfa breaks observed lfps electrical functional connectomes electomes defined differing spatiotemporal properties electome defined unique frequency power phase coherence patterns many brain regions properties granted features via gaussian process formulation multiple kernel learning framework critically electomes interpretable used design follow causal studies furthermore using formulation lfp signals mapped lower dimensional space better traditional approaches remarkably addition interpretability proposed approach achieves state art predictive ability compared black box approaches looking behavioral paradigms genotype prediction tasks mouse model demonstrating feature basis capturing neural dynamics related outcomes conclude discussion csfa analysis used conjunction experiments design causal studies provide gold standard validation inferred neural relationships
cognitive impairment prediction alzheimer disease regularized modal regression accurate automatic predictions cognitive assessment via neuroimaging markers critical early detection alzheimer disease linear regression models successfully used association study neuroimaging features cognitive performance alzheimer disease study however existing methods built least squares mean square error mse criterion sensitive outliers performance degraded heavy tailed noise complex brain disorder data paper beyond criterion investigating regularized modal regression statistical learning viewpoint new regularized scheme based modal regression proposed estimation variable selection robust outliers heavy tailed noise skewed noise conduct theoretical analysis establish approximation bound learning conditional mode function sparsity analysis variable selection robustness characterization experimental evaluations simulated data adni cohort data provided support promising performance proposed algorithm
stochastic submodular maximization case coverage functions continuous optimization techniques sgd extensions main workhorse modern machine learning nevertheless variety important machine learning problems require solving discrete optimization problems submodular objectives goal paper unleash toolkit modern continuous optimization discrete problems first introduce framework emph stochastic submodular optimization instead emph oracle access underlying objective explicitly considers statistical computational aspects evaluating objective provide formalization emph stochastic submodular maximization class important discrete optimization problems show state art techniques continuous optimization lifted realm discrete optimization extensive experimental evaluation demonstrate practical impact proposed approach
gradient methods submodular maximization paper study problem maximizing continuous submodular functions naturally arise many learning applications involving utility functions active learning sensing matrix approximations network inference despite apparent lack convexity functionals prove stochastic projected gradient methods provide strong approximation guarantees maximizing continuous submodular functions convex constraints specifically prove monotone continuous submodular functions fixed points projected gradient ascent provide factor approximation global maxima also study stochastic gradient mirror methods show iterations methods reach solutions achieve expectaion objective values exceeding opt2 immediate implication result bridge discrete continuous submodular maximization finally experiments real data demonstrate projected gradient methods consistently achieve best utility compared continuous baselines remaining competitive terms computational effort
non convex finite sum optimization via scsg methods develop class algorithms variants stochastically controlled stochastic gradient scsg methods smooth nonconvex finite sum optimization problem assuming smoothness component complexity scsg reach stationary point min 1n2 strictly outperforms stochastic gradient descent moreover scsg never worse state art methods based variance reduction significantly outperforms target accuracy low similar acceleration also achieved functions satisfy polyak lojasiewicz condition empirical experiments demonstrate scsg outperforms stochastic gradient methods training multi layers neural networks terms training validation loss
influence maximization almost submodular threshold function influence maximization problem selecting nodes social network maximize influence spread problem extensively studied works focus submodular influence diffusion models paper motivated empirical evidences explore influence maximization non submodular regime particular study general threshold model fraction nodes non submodular threshold functions threshold functions closely upper lower bounded submodular functions call almost submodular first show strong hardness result nγc approximation influence maximization unless networks almost submodular nodes parameter depending although threshold function close submodular influence maximization still hard approximate provide approximation algorithms number almost submodular nodes finally conduct experiments number real world datasets results demonstrate approximation algorithms outperform benchmark algorithms
subset selection noise problem selecting best element subset universe involved many applications previous studies assumed noise free environment noisy monotone submodular objective function paper considers realistic general situation evaluation subset noisy monotone function necessarily submodular multiplicative additive noises understand impact noise firstly show approximation ratio greedy algorithm poss powerful algorithms noise free subset selection noisy environments propose incorporate noise aware strategy poss resulting new ponss algorithm better approximation ratio empirical results influence maximization sparse regression problems show superior performance ponss
polynomial time algorithms dual volume sampling study dual volume sampling method selecting columns short wide matrix probability selection proportional volume spanned rows induced submatrix method proposed avron boutsidis 2013 showed promising method column subset selection multiple applications however wider adoption hampered lack polynomial time sampling algorithms remove hindrance developing exact randomized polynomial time sampling algorithm well derandomization thereafter study dual volume sampling via theory real stable polynomials prove distribution satisfies strong rayleigh property result remarkable consequences especially implies provably fast mixing markov chain sampler makes dual volume sampling much attractive practitioners sampler closely related classical algorithms popular experimental design methods date lacking theoretical analysis known empirically work well
lookahead bayesian optimization inequality constraints consider task optimizing objective function subject inequality constraints objective constraints expensive evaluate bayesian optimization popular way tackle optimization problems expensive objective function evaluations mostly applied unconstrained problems several approaches proposed address expensive constraints limited greedy strategies maximizing immediate reward address limitation propose lookahead approach selects next evaluation order maximize long term feasible reduction objective function present numerical experiments demonstrating performance improvements lookahead approach compared greedy algorithms constrained expected improvement eic predictive entropy search constraint pesc
non monotone continuous submodular maximization structure algorithms submodular continuous functions important objectives wide real world applications spanning map inference determinantal point processes dpps mean field inference probabilistic submodular models amongst others submodularity captures subclass non convex functions enables exact minimization approximate maximization polynomial time work study problem maximizing non monotone submodular continuous functions general closed convex constraints start investigating several properties underlie objectives used devise optimization algorithms provable guarantees concretely first devise phase algorithm approximation guarantee algorithm allows use existing methods ensured find approximate stationary points subroutine thus enabling utilize recent progress non convex optimization present non monotone frank wolfe variant approximation guarantee sublinear convergence rate finally extend approach broader class generalized submodular continuous functions captures wider spectrum applications theoretical findings validated several synthetic real world problem instances
solving almost systems random quadratic equations paper deals finding dimensional solution bmx system quadratic equations bmai bmx general known hard put forth novel procedure starts emph weighted maximal correlation initialization obtainable power iterations followed successive refinements based emph iteratively reweighted gradient type iterations novel techniques distinguish prior works inclusion fresh weighting regularization certain random measurement models proposed procedure returns true solution bmx high probability time proportional reading data bmai provided number equations constant times number unknowns namely empirically upshots contribution perfect signal recovery high dimensional regime given information theoretic limit number equations near optimal statistical accuracy presence additive noise extensive numerical tests using synthetic data real images corroborate improved signal recovery performance computational efficiency relative state art approaches
learning relus via gradient descent paper study problem learning rectified linear units relus functions form vctx max vctw vctx vctw denoting weight vector study problem high dimensional regime number observations fewer dimension weight vector assume weight vector belongs closed set convex nonconvex captures known side information structure focus realizable model inputs chosen gaussian distribution labels generated according planted weight vector show projected gradient descent initialization vct0 converges linear rate planted model number samples optimal numerical constants results dynamics convergence shallow neural nets provide insights towards understanding dynamics deeper architectures
stochastic mirror descent non convex optimization paper examine class non convex stochastic programs call emph variationally coherent properly includes quasi pseudo convex optimization problems establish convergence class problems study well known smd method show algorithm last iterate converges problem global optimum probability results contribute landscape non convex optimization clarifying convexity quasi convexity essential global convergence rather variational coherence much weaker requirement suffices localize class account locally variationally coherent problems show last iterate stochastic mirror descent converges local optima high probability finally consider last iterate convergence rates problems sharp minima derive special case conclusion probability last iterate stochastic gradient descent reaches exact global optimum finite number steps result contrasted existing work linear programs exhibit asymptotic convergence rates
accelerated first order methods geodesically convex optimization riemannian manifolds paper propose accelerated first order method geodesically convex optimization generalization standard nesterov accelerated method euclidean space nonlinear riemannian space first derive equations approximate linearization gradient like updates euclidean space geodesically convex optimization particular analyze global convergence properties accelerated method geodesically strongly convex problems show method improves convergence rate sqrt moreover method also improves global convergence rate geodesically general convex problems finally give specific iterative scheme matrix karcher mean problems validate theoretical results experiments
fine grained complexity empirical risk minimization kernel methods neural networks empirical risk minimization erm ubiquitous machine learning underlies supervised learning methods large body work algorithms various erm problems exact computational complexity erm still understood address issue multiple popular erm problems including kernel svms kernel ridge regression training final layer neural network particular give conditional hardness results problems based complexity theoretic assumptions strong exponential time hypothesis assumptions show algorithms solve aforementioned erm problems high accuracy sub quadratic time also give similar hardness results computing gradient empirical loss main computational burden many non convex learning tasks
large scale quadratically constrained quadratic program via low discrepancy sequences consider problem solving large scale quadratically constrained quadratic program problems occur naturally many scientific web applications although efficient methods tackle problem mostly scalable paper develop method transforms quadratic constraint linear form sampling set low discrepancy points transformed problem solved applying state art large scale solvers show convergence approximate solution true solution well finite sample error bounds experimental results also shown prove scalability practice
new alternating direction method linear programming well known linear program constraint matrix alternating direction method multiplier converges globally linearly rate log however rate related problem dimension algorithm exhibits slow fluctuating tail convergence practice paper propose new variable splitting method prove method convergence rate 2log proof based simultaneously estimating distance pair primal dual iterates optimal primal dual solution set certain residuals practice result new first order solver exploit sparsity specific structure matrix significant speedup important problems basis pursuit inverse covariance matrix estimation svm nonnegative matrix factorization problem compared current fastest solvers
dykstra algorithm admm coordinate descent connections insights extensions study connections dykstra algorithm projecting onto intersection convex sets augmented lagrangian method multipliers admm block coordinate descent prove coordinate descent regularized regression problem separable penalty functions seminorms exactly equivalent dykstra algorithm applied dual problem admm dual problem also seen equivalent special case sets linear subspace connections aside interesting right suggest new ways analyzing extending coordinate descent example existing convergence theory dykstra algorithm polyhedra discern coordinate descent lasso problem converges asymptotically linear rate also develop parallel versions coordinate descent based dykstra admm connections
smooth primal dual coordinate descent algorithms nonsmooth convex optimization propose new randomized coordinate descent method convex optimization template broad applications analysis relies novel combination ideas applied primal dual gap function smoothing acceleration homotopy non uniform sampling result method features first convergence rate guarantees best known variety common structure assumptions template provide numerical evidence support theoretical results comparison state art algorithms
first order adaptive sample size methods reduce complexity empirical risk minimization paper studies empirical risk minimization erm problems large scale datasets incorporates idea adaptive sample size methods improve guaranteed convergence bounds first order stochastic deterministic methods contrast traditional methods attempt solve erm problem corresponding full dataset directly adaptive sample size schemes start small number samples solve corresponding erm problem statistical accuracy sample size grown geometrically scaling factor use solution previous erm warm start new erm theoretical analyses show use adaptive sample size methods reduces overall computational cost achieving statistical accuracy whole dataset broad range deterministic stochastic first order methods gains specific choice method particularized accelerated gradient descent stochastic variance reduce gradient computational cost advantage logarithm number training samples numerical experiments various datasets confirm theoretical claims showcase gains using proposed adaptive sample size scheme
accelerated consensus via min sum splitting apply min sum message passing protocol solve consensus problem distributed optimization show ordinary min sum algorithm converge modified version known splitting yields convergence problem solution prove proper choice tuning parameters allows min sum splitting yield subdiffusive accelerated convergence rates matching rates obtained shift register methods acceleration scheme embodied min sum splitting consensus problem bears similarities lifted markov chains techniques multi step first order methods convex optimization
integration methods optimization algorithms show accelerated optimization methods seen particular instances multi step integration schemes numerical analysis applied gradient flow equation compared recent advances vein differential equation considered basic gradient flow derive class multi step schemes includes accelerated algorithms using classical conditions numerical analysis multi step schemes integrate differential equation using larger step sizes intuitively explains acceleration phenomenon
efficient use limited memory resources accelerate linear learning work propose generic approach efficiently use compute accelerators gpus fpgas training large scale machine learning models training data exceeds memory capacity technique builds upon primal dual coordinate selection uses duality gaps selection criteria dynamically decide part data made available fast processing provide strong theoretical guarantees motivating gap based selection scheme provide efficient practical implementation thereof illustrate power approach demonstrate performance training generalized linear models large scale datasets exceeding memory size modern gpu showing order magnitude speedup existing approaches
screening rule regularized ising model estimation discover screening rule regularized ising model estimation simple closed form screening rule necessary sufficient condition exactly recovering blockwise structure solution given regularization parameters enough sparsity screening rule combined exact inexact optimization procedures deliver solutions efficiently practice screening rule especially suitable large scale exploratory data analysis number variables dataset thousands interested relationship among handful variables within moderate size clusters interpretability experimental results various datasets demonstrate efficiency insights gained introduction screening rule
uprooting rerooting higher order graphical models idea uprooting rerooting graphical models introduced specifically binary pairwise models weller way transform model whole equivalence class related models inference model yields inference results others helpful since inference relevant bounds much easier obtain accurate model class introduce methods extend approach models higher order potentials develop theoretical insights example demonstrate triplet consistent polytope tri unique universally rooted demonstrate empirically rerooting significantly improve accuracy methods inference higher order models negligible computational cost
concentration multilinear functions ising model applications network data prove near tight concentration measure polynomial functions ising model high temperature improving radius concentration guaranteed known results polynomial factors dimension number nodes ising model show results optimal logarithmic factors dimension obtain results extending strengthening exchangeable pairs approach used prove concentration measure setting chatterjee demonstrate efficacy functions statistics testing strength interactions social networks synthetic real world data
inference graphical models via semidefinite programming hierarchies maximum posteriori probability map inference graphical models amounts solving graph structured combinatorial optimization problem popular inference algorithms belief propagation generalized belief propagation gbp intimately related linear programming relaxation within sherali adams hierarchy despite popularity algorithms well understood sum squares sos hierarchy based semidefinite programming sdp provide superior guarantees unfortunately sos relaxations graph vertices require solving sdp variables degree hierarchy practice approach scale beyond tens variables paper propose sdp relaxations map inference using sos hierarchy innovations focused computational efficiency firstly analogy variants introduce decision variables corresponding contiguous regions graphical model secondly solve resulting sdp using non convex burer monteiro style method develop sequential rounding procedure demonstrate resulting algorithm solve problems tens thousands variables within minutes significantly outperforms gbp practical problems image denoising ising spin glasses finally specific graph types establish sufficient condition tightness proposed partial sos relaxation
beyond normality learning sparse probabilistic graphical models non gaussian setting present algorithm identify sparse dependence structure continuous non gaussian probability distributions given corresponding set data conditional independence structure arbitrary distribution represented undirected graph markov random field algorithms learning structure restricted discrete gaussian cases new approach allows realistic accurate descriptions distribution question turn better estimates sparse markov structure sparsity graph interest accelerate inference improve sampling methods reveal important dependencies variables algorithm relies exploiting connection sparsity graph sparsity transport maps deterministically couple probability measure another
dynamic importance sampling anytime bounds partition function computing partition function key inference task many graphical models paper propose dynamic importance sampling scheme provides anytime finite sample bounds partition function algorithm balances advantages major inference strategies heuristic search variational bounds monte carlo methods blending sampling search refine variationally defined proposal algorithm combines generalizes recent work anytime search probabilistic bounds partition function using intelligently chosen weighted average samples construct unbiased estimator partition function strong finite sample confidence intervals inherit rapid early improvement rate sampling long term benefits improved proposal search gives significantly improved anytime behavior flexible trade offs memory time solution quality demonstrate effectiveness approach empirically real world problem instances taken recent uai competitions
nonbacktracking bounds influence independent cascade models paper develops upper lower bounds influence measure network precisely expected number nodes seed set influence independent cascade model particular bounds exploit nonbacktracking walks fortuin kasteleyn ginibre fkg type inequalities computed message passing implementation nonbacktracking walks recently allowed headways community detection paper shows use also impact influence computation provide knob control trade efficiency accuracy bounds finally tightness bounds illustrated simulations various network models
rigorous dynamics consistent estimation arbitrarily conditioned linear systems problem estimating random vector noisy linear measurements unknown parameters distributions must also learned arises wide range statistical learning linear inverse problems show computationally simple iterative message passing algorithm provably obtain asymptotically consistent estimates certain high dimensional large system limit lsl general parameterizations previous message passing techniques required sub gaussian matrices often fail matrix ill conditioned proposed algorithm called adaptive vector approximate message passing adaptive vamp auto tuning applies right rotationally random importantly class includes matrices arbitrarily bad conditioning show parameter estimates mean squared error mse iteration converge deterministic limits precisely predicted simple set state evolution equations addition simple testable condition provided mse matches bayes optimal value predicted replica method paper thus provides computationally simple method provable guarantees optimality consistency large class linear inverse problems
learning disentangled representations semi supervised deep generative models variational autoencoders vaes learn representations data jointly training probabilistic encoder decoder network typically models encode features data single variable interested learning disentangled representations encode distinct aspects data separate variables propose learn representations using model architectures generalize standard vaes employing general graphical model structure encoder decoder allows train partially specified models make relatively strong assumptions subset interpretable variables rely flexibility neural networks learn representations remaining variables define general objective semi supervised learning model class approximated using importance sampling procedure applies generally class models evaluate framework ability learn disentangled representations qualitative exploration generative capacity quantitative evaluation discriminative ability variety models datasets
gauging variational inference computing partition function important statistical inference task arising applications graphical models since computationally intractable approximate methods used resolve issue practice mean field belief propagation arguably popular successful approaches variational type paper propose new variational schemes coined gauged gauged improving respectively provide lower bounds partition function utilizing called gauge transformation modifies factors keeping partition function invariant moreover prove exact gms single loop special structure even though bare perform badly case extensive experiments complete gms relatively small size large 300 variables confirm newly proposed algorithms outperform generalize
variational inference via upper bound minimization variational inference widely used efficient alternative mcmc posits family approximating distributions finds member closest true posterior closeness usually measured via divergence though successful approach also problems notably typically leads underestimation posterior variance paper propose chivi new black box variational inference algorithm minimizes divergence chivi minimizes upper bound model evidence term cubo minimizing cubo leads better estimates posterior used classical lower bound elbo provide sandwich estimate marginal likelihood study chivi models probit regression gaussian process classification cox process model basketball plays compared classical chivi produces better error rates accurate estimates posterior variance
collapsed variational bayes markov jump processes markov jump processes continuous time stochastic processes widely used statistical applications natural sciences recently machine learning inference models typically proceeds via markov chain monte carlo suffer various computational challenges work propose novel collapsed variational inference algorithm address issue work leverages ideas discrete time markov chains exploits connection idea called uniformization algorithm proceeds marginalizing parameters markov jump process approximating distribution trajectory factored distribution segments piecewise constant function unlike mcmc schemes marginalize transition times piecewise constant process scheme optimizes discretization time resulting significant computational savings apply ideas synthetic data well dataset check recordings demonstrate superior performance state art mcmc methods
bayesian dyadic trees histograms regression many machine learning tools regression based recursive partitioning covariate space smaller regions regression function estimated locally among regression trees ensembles demonstrated impressive empirical performance work shed light machinery behind bayesian variants methods particular study bayesian regression histograms bayesian dyadic trees simple regression case predictor focus reconstruction regression surfaces piecewise constant number jumps unknown show suitably designed priors posterior distributions concentrate around true step regression function minimax rate log factor results require knowledge true number steps width true partitioning cells thus bayesian dyadic regression trees fully adaptive recover true piecewise regression function nearly well knew exact number location jumps results constitute first step towards understanding bayesian trees ensembles worked well practice aside discuss prior distributions balanced interval partitions relate problem geometric probability namely quantify probability covering circumference circle random arcs whose endpoints confined grid new variant original problem
differentially private bayesian learning distributed data many applications machine learning example health care would benefit methods guarantee privacy data subjects differential privacy become established standard protecting learning results standard algorithms require single trusted party access entire data clear weakness consider bayesian learning distributed setting party holds single sample samples data propose learning strategy based secure multi party sum function aggregating summaries data holders gaussian mechanism method builds asymptotically optimal practically efficient bayesian inference rapidly diminishing extra cost
model powered conditional independence test consider problem non parametric conditional independence testing testing continuous random variables given samples joint distribution continuous random vectors determine whether independenty approach converting conditional independence test classification problem allows harness powerful classifiers like gradient boosted trees deep neural networks models handle complex probability distributions allow perform significantly better compared prior state art high dimensional testing main technical challenge classification problem need samples conditional product distribution fci joint distribution independenty given access samples true joint distribution tackle problem propose novel nearest neighbor bootstrap procedure theoretically show generated samples indeed close fci terms total variational distance develop theoretical results regarding generalization bounds classification problem translate error bounds testing provide novel analysis rademacher type classification bounds presence non textit near independent samples empirically validate performance algorithm simulated real datasets show performance gains previous methods
worlds collide integrating different counterfactual assumptions fairness machine learning used make crucial decisions people lives nearly decisions risk individuals certain race gender sexual orientation subpopulation unfairly discriminated recent method demonstrated use techniques counterfactual inference make predictions fair across different subpopulations method requires provides causal model generated data hand genera validating causal model impossible using observational data alone without assumptions hence desirable integrate competing causal models provide counterfactually fair decisions regardless world correct paper show possible make predictions approximately fair respect multiple possible causal models thus bypassing problem exact causal specification frame goal learning fair classifier optimization problem fairness constraints provide techniques relaxations solve optimization problem demonstrate flexibility model real world fair classification problems show model seamlessly balance fairness multiple worlds prediction accuracy
lda uncovering latent patterns text based sequential decision processes sequential decision making often important useful end users understand underlying patterns causes lead corresponding decisions however typical deep reinforcement learning algorithms seldom provide information due black box nature paper present probabilistic model lda uncover latent patterns text based sequential decision processes model understood variant latent topic models tailored maximize total rewards draw interesting connection approximate maximum likelihood estimation lda celebrated learning algorithm demonstrate text game domain proposed method provides viable mechanism uncover latent patterns decision processes also obtains state art rewards games
probabilistic models integration error assessment functional cardiac models paper studies numerical computation integrals representing estimates predictions output computational model respect distribution uncertain inputs model functional cardiac models motivate work neither possess closed form expression evaluation either requires 100 cpu hours precluding standard numerical integration methods proposal treat integration estimation problem joint model priori unknown function priori unknown distribution result posterior distribution integral explicitly accounts dual sources numerical approximation error due severely limited computational budget construction applied account statistically principled manner impact numerical errors present confounding factors functional cardiac model assessment
expectation propagation exponential family using algebra exponential family distributions highly useful machine learning since calculation performed efficiently natural parameters exponential family recently extended emph exponential family contains student distributions family members thus allows handle noisy data well however since exponential family defined emph deformed exponential cannot derive efficient learning algorithm exponential family expectation propagation paper borrow mathematical tools algebra statistical physics show pseudo additivity distributions allows perform calculation exponential family distributions natural parameters develop expectation propagation algorithm exponential family provides deterministic approximation posterior predictive distribution simple moment matching finally apply proposed algorithm bayes point machine student process classification demonstrate performance numerically
probabilistic framework nonlinearities stochastic neural networks present probabilistic framework nonlinearities based doubly truncated gaussian distributions setting truncation points appropriately able generate various types nonlinearities within unified framework including sigmoid tanh relu commonly used nonlinearities neural networks framework readily integrates existing stochastic neural networks hidden units characterized random variables allowing first time learn nonlinearities alongside model weights networks extensive experiments demonstrate performance improvements brought proposed framework integrated restricted boltzmann machine rbm temporal rbm truncated gaussian graphical model tggm
clone mcmc parallel high dimensional gaussian gibbs sampling propose generalized gibbs sampler algorithm obtaining samples approx imately distributed high dimensional gaussian distribution similarly hogwild methods approach target original gaussian distribution interest approximation contrary hogwild methods single parameter allows trade bias variance show empirically method flexible performs well compared hogwild type algorithms
learning spatiotemporal piecewise geodesic trajectories longitudinal manifold valued data introduce hierarchical model allows estimate group average piecewise geodesic trajectory riemannian space measurements individual variability model falls well defined mixed effect models subject specific trajectories defined spatial temporal transformations group average piecewise geodesic path component component thus apply model wide variety situations due non linearity model use stochastic approximation expectation maximization algorithm estimate model parameters experiments synthetic data validate choice model applied metastatic renal cancer chemotherapy monitoring run estimations recist scores treated patients estimate time escape treatment experiments highlight role different parameters response treatment
scalable levy process priors spectral kernel learning gaussian processes rich distributions functions generalisation properties determined kernel function propose distribution kernels formed modelling spectral density levy process resulting distribution support stationary covariances including popular rbf periodic matern kernels combined inductive biases enable automatic data efficient learning long range extrapolation state art predictive performance posterior inference develop reversible jump mcmc approach includes automatic selection model order exploit algebraic structure proposed process training predictions show proposed model empirically recover flexible ground truth covariances demonstrate extrapolation several benchmarks
inferring latent structure human decision making raw visual inputs goal imitation learning match example expert behavior without access reinforcement signal expert demonstrations provided humans however often show significant variability due latent factors explicitly modeled introduce extension generative adversarial imitation learning method infer latent structure human decision making unsupervised way method imitate complex behaviors also learn interpretable meaningful representations demonstrate approach applicable high dimensional environments including raw visual inputs highway driving domain show model learned demonstrations able produce different driving styles accurately anticipate human actions method surpasses various baselines terms performance functionality
hybrid reward architecture reinforcement learning main challenges reinforcement learning generalisation typical deep methods achieved approximating optimal value function low dimensional representation using deep network approach works well many domains domains optimal value function cannot easily reduced low dimensional representation learning slow unstable paper contributes towards tackling challenging domains proposing new method called hybrid reward architecture hydra hydra takes input decomposed reward function learns separate value function component reward function component typically depends subset features overall value function much smoother easier approximated low dimensional representation enabling effective learning demonstrate hydra toy problem atari game pac man hydra achieves human performance
shallow updates deep reinforcement learning deep reinforcement learning drl methods deep network dqn achieved state art results variety challenging high dimensional domains success mainly attributed power deep neural networks learn rich domain representations approximating value function policy batch reinforcement learning methods linear representations hand stable require less hyper parameter tuning yet substantial feature engineering necessary achieve good results work propose hybrid approach least squares deep network dqn combines rich feature representations learned drl algorithm stability linear least squares method periodically training last hidden layer drl network batch least squares update key approach bayesian regularization term least squares update prevents fitting recent data tested dqn atari games demonstrate significant improvement vanilla dqn double dqn also investigated reasons superior performance method interestingly found performance improvement attributed large batch size used method optimizing last layer
towards generalization simplicity continuous control remarkable successes deep learning speech recognition computer vision motivated efforts adapt similar techniques problem domains including reinforcement learning consequently methods produced rich motor behaviors simulated robot tasks success largely attributed use multi layer neural networks work among first carefully study might responsible recent advancements main result calls emerging narrative question showing much simpler architectures based linear rbf parameterizations achieve comparable performance state art results study different policy representations regard performance measures hand also towards robustness external perturbations find learned neural network policies standard training scenarios robust linear rbf policies fact remarkably brittle finally directly modify training scenarios order favor robust policies find compelling case favor multi layer architectures overall study suggests multi layer architectures default choice unless side side comparison simpler architectures shows otherwise generally hope results lead interest carefully studying architectural choices associated trade offs training generalizable robust policies
interpolated policy gradient merging policy policy gradient estimation deep reinforcement learning policy model free deep reinforcement learning methods using previously collected data improve sample efficiency policy policy gradient techniques hand policy algorithms often stable easier use paper examines theoretically empirically approaches merging policy updates deep reinforcement learning theoretical results show policy updates value function estimator interpolated policy policy gradient updates whilst still satisfying performance bounds analysis uses control variate methods produce family policy gradient algorithms several recently proposed algorithms special cases family provide empirical comparison techniques remaining algorithmic details fixed show different mixing policy gradient estimates policy samples contribute improvements empirical performance final algorithm provides generalization unification existing deep policy gradient techniques theoretical guarantees bias introduced policy updates improves state art model free deep methods number openai gym continuous control benchmarks
scalable planning tensorflow hybrid nonlinear domains given recent deep learning results demonstrate ability effectively optimize high dimensional non convex functions gradient descent optimization gpus ask paper whether symbolic gradient optimization tools tensorflow effective planning hybrid mixed discrete continuous nonlinear domains high dimensional state action spaces end demonstrate hybrid planning tensorflow rmsprop gradient descent competitive mixed integer linear program milp based optimization piecewise linear planning domains compute optimal solutions substantially outperforms state art interior point methods nonlinear planning domains furthermore remark tensorflow highly scalable converging strong policy large scale concurrent domain total 576 000 continuous actions horizon time steps minutes provide number insights clarify strong performance including observations despite long horizons rmsprop avoids vanishing exploding gradients problem together results suggest new frontier highly scalable planning nonlinear hybrid domains leveraging gpus power recent advances gradient descent highly optmized toolkits like tensorflow
task based end end model learning stochastic optimization machine learning techniques becoming widespread become common see prediction algorithms operating within larger process however criteria train algorithms often differ ultimate criteria evaluate paper proposes end end approach learning probabilistic machine learning models within context stochastic programming manner directly captures ultimate task based objective used present experimental evaluations proposed approach classical inventory stock problem real world electrical grid scheduling task cases show proposed approach outperform traditional modeling purely black box policy optimization approaches
value prediction network paper proposes novel deep reinforcement learning approach called value prediction network vpn integrates model free model based methods single neural network contrast typical model based methods vpn learns dynamics model whose abstract states trained make option conditional predictions future values rather future observations experimental results show vpn several advantages model free model based baselines stochastic environment careful planning required building accurate observation prediction model difficult furthermore vpn outperforms deep network dqn several atari games even short lookahead planning demonstrating potential new way learning good state representation
variable importance using decision trees decision trees random forests well established models offer good predictive performance also provide rich feature importance information practitioners often employ variable importance methods rely impurity based information methods remain poorly characterized theoretical perspective provide novel insights performance methods deriving finite sample performance guarantees high dimensional setting various modeling assumptions demonstrate effectiveness impurity based methods via extensive set simulations
expressive power neural networks view width expressive power neural networks important understanding deep learning existing works consider problem view depth network paper study width affects expressiveness neural networks classical results state emph depth bounded depth networks suitable activation functions universal approximators show universal approximation theorem emph width bounded relu networks width relu networks input dimension universal approximators moreover except measure set functions cannot approximated width relu networks exhibits phase transition several recent works demonstrate benefits depth proving depth efficiency neural networks classes deep networks cannot realized shallow network whose size emph exponential bound pose dual question width efficiency relu networks wide networks cannot realized narrow networks whose size substantially larger show exist classes wide networks cannot realized narrow network whose depth emph polynomial bound hand demonstrate extensive experiments narrow networks whose depth exceed polynomial bound constant factor approximate wide shallow network high accuracy results provide comprehensive evidence depth effective width expressiveness relu networks
sgd learns conjugate kernel class network show standard stochastic gradient decent sgd algorithm guaranteed learn polynomial time function competitive best function conjugate kernel space network defined daniely frostig singer result holds log depth networks rich family architectures best knowledge first polynomial time guarantee standard neural network learning algorithm networks depth corollaries follows neural networks depth log sgd guaranteed learn polynomial time constant degree polynomials polynomially bounded coefficients likewise follows sgd large enough networks learn continuous function polynomial time complementing classical expressivity results
radon machines effective parallelisation machine learning order simplify adaptation learning algorithms growing amounts data well growing need accurate confident predictions critical applications paper propose novel provably effective parallelisation scheme contrast parallelisation techniques scheme applied broad class learning algorithms without mathematical derivations without writing single line additional code achieve treating learning algorithm black box applied parallel random data subsets resulting hypotheses assigned leaves aggregation tree bottom replaces set hypotheses corresponding inner node tree radon point considering confidence parameters epsilon delta input learning algorithm efficient sample complexity polynomial epsilon delta time complexity polynomial sample complexity parallelisation scheme algorithm achieve guarantees applied polynomial number cores polylogarithmic time result allows effective parallelisation broad class learning algorithms intrinsically related nick class decision problems well learnability exact learning cost parallelisation form slightly larger sample complexity empirical study confirms potential parallisation scheme range data sets several learning algorithms
noise tolerant interactive learning using pairwise comparisons study problem interactively learning binary classifier using noisy labeling pairwise comparison oracles comparison oracle answers given instances likely positive learning oracles multiple applications obtaining direct labels harder pairwise comparisons easier algorithm leverage types oracles paper attempt characterize access easier comparison oracle helps improving label total query complexity show comparison oracle reduces learning problem learning threshold function present algorithm interactively queries label comparison oracles characterize query complexity tsybakov adversarial noise conditions comparison labeling oracles lower bounds show label total query complexity almost optimal
pac bayesian analysis randomized learning application stochastic gradient descent analyze generalization properties randomized learning algorithms focusing stochastic gradient descent sgd using novel combination pac bayes algorithmic stability importantly risk bounds hold posterior distributions algorithm hyperparameters including distributions depend training data inspires adaptive sampling algorithm sgd optimizes posterior runtime analyze algorithm context risk bounds evaluate empirically benchmark dataset
revisiting perceptron efficient label optimal learning halfspaces long standing problem efficiently learn linear separator using labels possible presence noise work propose efficient perceptron based algorithm actively learning homogeneous linear separators uniform distribution bounded noise label flipped probability eta algorithm achieves near optimal tilde frac 2eta frac epsilon label complexity time tilde frac epsilon 2eta adversarial noise tilde omega epsilon fraction labels flipped algorithm achieves near optimal tilde frac epsilon label complexity time tilde frac epsilon furthermore show active learning algorithm converted efficient passive learning algorithm near optimal sample complexity respect epsilon
sample computationally efficient learning algorithms concave distributions provide new results noise tolerant sample efficient learning algorithms concave distributions new class concave distributions broad natural generalization log concavity includes many important additional distributions pareto distribution distribution class studied context efficient sampling integration optimization much remains unknown geometry class distributions applications context learning challenge unlike commonly used distributions learning uniform generally log concave distributions broader class closed marginalization operator many distributions fat tailed work introduce new convex geometry tools study properties concave distributions use properties provide bounds quantities interest learning including probability disagreement halfspaces disagreement outside band disagreement coefficient use results significantly generalize prior results margin based active learning disagreement based active learning passively learning intersections halfspaces analysis geometric properties concave distributions might independent interest optimization broadly
nearest neighbor sample compression efficiency consistency infinite dimensions examine bayes consistency recently proposed nearest neighbor based multiclass learning algorithm algorithm derived sample compression bounds enjoys statistical advantages tight fully empirical generalization bounds well algorithmic advantages runtime memory savings prove algorithm strongly bayes consistent metric spaces finite doubling dimension first consistency result efficient nearest neighbor sample compression scheme rather surprisingly discover algorithm continues bayes consistent even certain infinite dimensional setting basic measure theoretic conditions classic consistency proofs hinge violated surprising since known bayes consistent setting pose several challenging open problems future research
learning identifiable gaussian bayesian networks polynomial time sample complexity learning directed acyclic graph dag structure bayesian network observational data notoriously difficult problem many non identifiability hardness results known paper propose provably polynomial time algorithm learning sparse gaussian bayesian networks equal noise variance class bayesian networks dag structure uniquely identified observational data high dimensional settings show k4log number samples suffices method recover true dag structure high probability number variables maximum markov blanket size obtain theoretical guarantees condition called emph restricted strong adjacency faithfulness rsaf strictly weaker strong faithfulness condition methods based conditional independence testing need success sample complexity method matches information theoretic limits terms dependence validate theoretical findings synthetic experiments
world graph discovering statistical structure links fundamental problem analysis social networks choosing misspecified model equivalently incorrect inference algorithm result invalid analysis even falsely uncover patterns fact artifacts model work focuses unifying widely used link formation models stochastic block model sbm small world latent space model swm integrating techniques kernel learning spectral graph theory nonlinear dimensionality reduction develop first statistically sound polynomial time algorithm discover latent patterns sparse graphs models network comes sbm algorithm outputs block structure swm algorithm outputs estimates node latent position
mean field residual networks edge chaos study randomly initialized residual networks using mean field theory theory difference equations classical feedforward neural networks tanh activations exhibit exponential behavior average propagating inputs forward gradients backward exponential forward dynamics causes rapid collapsing input space geometry exponential backward dynamics causes drastic vanishing exploding gradients show contrast converting residual connections activations tanh power relu unit network adopt subexponential forward backward dynamics many cases fact polynomial exponents polynomials obtained analytic methods proved verified empirically correct terms edge chaos hypothesis subexponential polynomial laws allow residual networks tohover boundary stability chaos thus preserving geometry input space gradient information flow also train grid tanh residual networks mnist observe predicted theory developed paper peak performances models determined product standard deviation weights square root depth thus addition improving understanding residual networks theoretical tools guide research toward better initialization schemes
learning uncertain curves wasserstein metric gaussian processes introduce novel framework statistical analysis populations non degenerate gaussian processes gps natural representations uncertain curves allows inherent variation uncertainty function valued data properly incorporated population analysis using wasserstein metric geometrize space gps mean covariance functions compact index spaces prove existence uniqueness barycenter population gps well convergence metric barycenter finite dimensional counterparts justifies practical computations finally demonstrate framework experimental validation datasets representing brain connectivity climate change source code released upon publication
clustering network valued data community detection focuses clustering nodes detecting communities mostly single network problem considerable practical interest received great deal attention research community able cluster within network important emerging needs able cluster multiple networks largely motivated routine collection network data generated potentially different populations networks node correspondence node correspondence present cluster summarizing network graphon estimate whereas node correspondence present propose novel solution clustering networks associating computationally feasible feature vector network based trace powers adjacency matrix illustrate methods simulated real data sets theoretical justifications given terms consistency
power truncated svd general high rank matrix estimation problems show given estimate mata close general high rank positive semi definite psd matrix mata spectral norm mata mata simple truncated singular value decomposition mata produces multiplicative approximation mata frobenius norm observation leads many interesting results general high rank matrix estimation problems high rank matrix completion show possible recover general high rank matrix mata relative error frobenius norm partial observations sample complexity independent spectral gap mata high rank matrix denoising design algorithms recovers matrix mata relative error frobenius norm noise perturbed observations without assuming mata exactly low rank low dimensional estimation high dimensional covariance given samples dimension mat0 mata show possible estimate covariance matrix mata relative error frobenius norm improving classical covariance estimation results requires
adagan boosting generative models generative adversarial networks gan effective method training generative models complex data natural images however notoriously hard train suffer problem missing modes model able produce examples certain regions space propose iterative procedure called adagan every step add new component mixture model running gan algorithm weighted sample inspired boosting algorithms many potentially weak individual predictors greedily aggregated form strong composite predictor prove analytically incremental procedure leads convergence true distribution finite number steps step optimal convergence exponential rate otherwise also illustrate experimentally procedure addresses problem missing modes
adagan boosting generative models generative adversarial networks gan effective method training generative models complex data natural images however notoriously hard train suffer problem missing modes model able produce examples certain regions space propose iterative procedure called adagan every step add new component mixture model running gan algorithm weighted sample inspired boosting algorithms many potentially weak individual predictors greedily aggregated form strong composite predictor prove analytically incremental procedure leads convergence true distribution finite number steps step optimal convergence exponential rate otherwise also illustrate experimentally procedure addresses problem missing modes
discovering potential influence via information bottleneck discovering potential influence variable another variable fundamental scientific practical interest existing correlation measures suitable discovering average influence fail discover potential influences bridge gap postulate set natural axioms expect measure potential influence satisfy show rate information bottleneck hypercontractivity coefficient satisfies proposed axioms iii provide novel estimator estimate hypercontractivity coefficient samples numerical experiments demonstrate proposed estimator discovers potential influence various indicators datasets robust discovering gene interactions gene expression time series data statistically powerful estimators correlation measures binary hypothesis testing canonical potential influences
phase transitions pooled data problem coded distributed computing inverse problems computationally intensive distributed parallel computing often bottlenecked small set slow workers known stragglers paper utilize emerging idea coded computation design novel error correcting code inspired technique solving linear inverse problems specific iterative methods parallelized implementation affected stragglers example applications include inverse problems personalized pagerank sampling graphs provably show coded computation technique reduce mean squared error computational deadline constraint fact ratio mean squared error replication based coded techniques diverges infinity deadline increases experiments personalized pagerank performed real systems real social networks show ratio large 104 unlike coded computation techniques proposed thus far strategy combines outputs workers including stragglers produce accurate estimates computational deadline also ensures accuracy degrades gracefully event number stragglers large paper study pooled data problem identifying labels associated large collection items based sequence pooled tests revealing counts label within pool noiseless setting exact recovery identify exact asymptotic threshold required number tests optimal decoding prove phase transition complete success complete failure addition present novel noisy variation problem provide information theoretic framework characterizing required number tests general noise models results reveal noise make problem considerably difficult strict increases scaling laws even low noise levels
query complexity clustering side information suppose given set elements clustered unknown clusters oracle interactively answer pair wise queries form elements belong cluster goal recover optimum clustering asking minimum number queries paper initiate rigorous theoretical study basic problem query complexity interactive clustering provide strong information theoretic lower bounds well nearly matching upper bounds clustering problems come similarity matrix used automated process cluster similar points together however obtaining ideal similarity function extremely challenging due ambiguity data representation poor data quality etc primary reasons makes clustering hard improve accuracy clustering fruitful approach recent years ask domain expert crowd obtain labeled data interactively many heuristics proposed use similarity function come querying strategy however systematic theoretical study main contribution paper show dramatic power side information aka similarity matrix reducing query complexity clustering natural model similarity matrix similarity values drawn independently arbitrary probability distribution underlying pair elements belong cluster otherwise show given similarity matrix query complexity reduces drastically similarity matrix log denotes squared hellinger divergence moreover also information theoretic optimal within logn factor algorithms efficient parameter free work without knowledge depend logarithmically
revisit fuzzy neural network demystifying batch normalization relu generalized hamming network revisit fuzzy neural network cornerstone notion textit generalized hamming distance provides novel theoretically justified approach rectifying understanding traditional neural computing turns many useful neural network methods batch normalization rectified linear units could interpreted new framework rectified generalized hamming network gnn proposed accordingly ghn lends rigiour analysis within fuzzy logics theory also demonstrates superior performances variety learning tasks terms fast learning speed well controlled behaviour simple parameter settings
posterior sampling reinforcement learning worst case regret bounds present algorithm based posterior sampling aka thompson sampling achieves near optimal worst case regret bounds underlying markov decision process mdp communicating finite though unknown diameter main result high probability regret upper bound dsat communicating mdp states actions diameter s5a regret compares total reward achieved algorithm total expected reward optimal infinite horizon undiscounted average reward policy time horizon result improves best previously known upper bound dsat achieved algorithm setting matches dependence established lower bound dsat problem
framework multi rmed andit testing online fdr control propose alternative framework existing setups controlling false alarms multiple tests run time setup arises many practical applications pharmaceutical companies test new treatment options control pills different diseases internet companies test default webpages versus various alternatives time framework proposes replace sequence tests sequence best arm mab instances continuously monitored data scientist interleaving mab tests online false discovery rate fdr algorithm obtain best worlds low sample complexity time online fdr control main contributions propose reasonable definitions null hypothesis mab instances demonstrate derive always valid sequential value allows continuous monitoring mab test iii show using rejection thresholds online fdr algorithms confidence levels mab algorithms results sample optimality high power low fdr point time run extensive simulations verify claims also report results real data collected new yorker cartoon caption contest
monte carlo tree search best arm identification recent advances bandit tools techniques sequential learning steadily enabling new applications promising resolution range challenging related problems study game tree search problem goal quickly identify optimal move given game tree sequentially sampling stochastic payoffs develop new algorithms trees arbitrary depth operate summarizing deeper levels tree confidence intervals depth applying best arm identification procedure root prove new sample complexity guarantees refined dependence problem instance show experimentally algorithms outperform existing elimination based algorithms match previous special purpose methods depth trees
minimal exploration structured stochastic bandits paper introduces addresses wide class stochastic bandit problems function mapping arm corresponding reward exhibits known structural properties existing structures linear lipschitz unimodal combinatorial dueling covered framework derive asymptotic instance specific regret lower bound problems develop ossb algorithm whose regret matches fundamental limit ossb based classical principle optimism face uncertainty thompson sampling rather aims matching minimal exploration rates sub optimal arms characterized derivation regret lower bound illustrate efficiency ossb using numerical experiments case linear bandit problem show ossb outperforms existing algorithms including thompson sampling
regret analysis continuous dueling bandit dueling bandit learning framework feedback information learning process restricted noisy comparison pair actions paper address dueling bandit problem based cost function continuous space propose stochastic mirror descent algorithm show algorithm achieves sqrtt logt regret bound strong convexity smoothness assumptions cost function clarify equivalence regret minimization dueling bandit convex optimization cost function moreover considering lower bound convex optimization turned algorithm achieves optimal convergence rate convex optimization optimal regret dueling bandit except logarithmic factor
elementary symmetric polynomials optimal experimental design revisit classical problem optimal experimental design oed new mathematical model grounded geometric motivation specifically introduce models based elementary symmetric polynomials polynomials capture partial volumes offer graded interpolation widely used optimal optimal design models obtaining special cases analyze properties models derive greedy convex relaxation algorithms computing associated designs analysis establishes approximation guarantees algorithms empirical results substantiate claims demonstrate curious phenomenon concerning greedy algorithm finally byproduct obtain new results theory elementary symmetric polynomials independent interest
online learning linear dynamical systems present efficient practical algorithm online prediction discrete time linear dynamical systems despite non convex optimization problem using improper learning convex relaxation algorithm comes provable guarantees near optimal regret bounds compared best lds hindsight overparameterizing small logarithmic factor analysis brings together ideas improper learning convex relaxations online regret minimization spectral theory hankel matrices
efficient flexible inference stochastic systems many real world dynamical systems described stochastic differential equations thus parameter inference challenging important problem many disciplines provide grid free flexible algorithm offering parameter state inference stochastic systems compare approch based variational approximations state art methods showing significant advantages runtime accuracy
group sparse additive machine family learning algorithms generated additive models attracted much attention recently flexibility interpretability high dimensional data analysis among learning models grouped variables shown competitive performance prediction variable selection however previous works mainly focus least squares regression problem classification task thus desired design new additive classification model variable selection capability many real world applications focus high dimensional data classification address challenging problem paper investigate classification group sparse additive models reproducing kernel hilbert spaces novel classification method called emph group sparse additive machine groupsam proposed explore utilize structure information among input variables generalization error bound derived proved integrating sample error analysis empirical covering numbers hypothesis error estimate stepping stone technique new bound shows groupsam achieve satisfactory learning rate polynomial decay experimental results synthetic data benchmark datasets consistently show effectiveness new approach
bregman divergence stochastic variance reduction saddle point adversarial prediction adversarial machines learner competes adversary gained much recent interest machine learning naturally form saddle point optimization often separable structure sometimes also unmanageably large dimension work show adversarial prediction multivariate losses solved much faster used first reduce problem size exponentially using appropriate sufficient statistics adapt new stochastic variance reduced algorithm balamurugan bach 2016 allow bregman divergence prove linear rate convergence retained show adversarial prediction using divergence achieve speedup example times compared euclidean alternative verify theoretical findings extensive experiments example applications adversarial prediction lpboosting
online multiclass boosting recent work extended theoretical analysis boosting algorithms multiclass problems online settings however multiclass extension batch setting online extensions consider binary classification fill gap literature defining justifying weak learning condition online multiclass boosting condition leads optimal boosting algorithm requires minimal number weak learners achieve certain accuracy additionally propose adaptive algorithm near optimal enjoys excellent performance real data due adaptive property
universal consistency minimax rates online mondrian forest establish consistency algorithm mondrian forest cite lakshminarayanan2014mondrianforests lakshminarayanan2016mondrianuncertainty randomized classification algorithm implemented online first amend original mondrian forest algorithm proposed cite lakshminarayanan2014mondrianforests considers emph fixed lifetime parameter indeed fact parameter fixed actually hinders statistical consistency original procedure modified mondrian forest algorithm grows trees increasing lifetime parameters uses alternative updating rule allowing work also online fashion second provide theoretical analysis establishing simple conditions consistency theoretical analysis also exhibits surprising fact algorithm achieves minimax rate optimal rate estimation lipschitz regression function strong extension previous results cite arlot2014purf_bias emph arbitrary dimension
mean teachers better role models weight averaged consistency targets improve semi supervised deep learning results recently proposed temporal ensembling achieved state art results several semi supervised learning benchmarks maintains exponential moving average label predictions training example penalizes predictions inconsistent target however targets change per epoch temporal ensembling becomes unwieldy learning large datasets overcome problem propose mean teacher method averages model weights instead label predictions additional benefit mean teacher improves test accuracy enables training fewer labels temporal ensembling mean teacher achieves error rate svhn 250 labels better temporal ensembling 1000 labels
learning complementary labels collecting labeled data costly thus critical bottleneck real world classification tasks mitigate problem consider complementary label specifies class pattern belong collecting complementary labels would less laborious ordinary labels since users carefully choose correct class many candidate classes however complementary labels less informative ordinary labels thus suitable approach needed better learn complementary labels paper show unbiased estimator classification risk obtained complementary labels loss function satisfies particular symmetric condition theoretically prove estimation error bounds proposed method experimentally demonstrate usefulness proposed algorithms
positive unlabeled learning non negative risk estimator emph positive emph unlabeled data binary classifier trained learning state art emph unbiased learning however model flexible empirical risk training data negative suffer serious overfitting paper propose emph non negative risk estimator learning minimized robust overfitting thus able train flexible models given limited data moreover analyze emph bias emph consistency emph mean squared error reduction proposed risk estimator emph estimation error corresponding risk minimizer experiments show proposed risk estimator successfully fixes overfitting problem unbiased counterparts
semisupervised clustering queries locally encodable source coding source coding canonical problem data compression information theory locally encodable source coding compressed bit depends bits input paper show recently popular model semisupervised clustering equivalent locally encodable source coding model task perform multiclass labeling unlabeled elements beginning ask parallel set simple queries oracle provides possibly erroneous binary answers queries queries cannot involve fixed constant number elements labeling elements clustering must done based noisy query answers goal recover correct labelings minimizing number queries equivalence locally encodable source codes leads find lower bounds number queries required variety scenarios also able show fundamental limitations pairwise cluster queries propose pairwise queries provably performs better
learning errors structured prediction approximate inference work try understand differences exact approximate inference algorithms structured prediction compare estimation approximation error underestimate overestimate models result shows perspective learning errors performances approximate inference could good exact inference error analyses also suggest new margin existing learning algorithms empirical evaluations text classification sequential labelling dependency parsing witness success approximate inference benefit proposed margin
optimal generalizability parametric learning consider parametric learning problem objective learner determined parametric loss function employing empirical risk minimization possibly regularization inferred parameter vector biased toward training samples bias measured cross validation procedure practice data set partitioned training set used training validation set used training left measure sample performance classical cross validation strategy leave cross validation loocv sample left validation training done rest samples presented learner process repeated samples loocv rarely used practice due high computational complexity paper first develop computationally efficient approximate loocv aloocv provide theoretical guarantees performance use aloocv provide optimization algorithm finding optimal regularizer empirical risk minimization framework numerical experiments illustrate accuracy efficiency aloocv well proposed framework optimal regularizer
multi objective non parametric sequential prediction online learning research mainly focusing minimizing objective function many real world applications however several objective functions considered simultaneously recently algorithm dealing several objective functions case presented paper extend multi objective framework case stationary ergodic processes thus allowing dependencies among observations first identify asymptomatic lower bound prediction strategy present algorithm whose predictions achieve optimal solution fulfilling continuous convex constraining criterion
fixed rank approximation positive semidefinite matrix streaming data several important applications streaming pca semidefinite programming involve large scale positive semidefinite psd matrix presented sequence linear updates storage limitations possible retain sketch psd matrix paper develops new algorithm fixed rank psd approximation sketch approach combines nystr approximation novel mechanism rank truncation theoretical analysis establishes proposed method achieve prescribed relative error schatten norm exploits spectral decay input matrix computer experiments show proposed method dominates alternative techniques fixed rank psd matrix approximation across wide range examples
communication efficient stochastic gradient descent applications neural networks parallel implementations stochastic gradient descent sgd received significant research attention thanks excellent scalability properties fundamental barrier parallelizing sgd high bandwidth cost communicating gradient updates nodes consequently several lossy compresion heuristics proposed nodes communicate quantized gradients although effective practice heuristics always guarantee convergence clear whether improved paper propose quantized sgd qsgd family compression schemes gradient updates provides convergence guarantees qsgd allows user smoothly trade emph communication bandwidth emph convergence time nodes adjust number bits sent per iteration cost possibly higher variance show trade inherent sense improving past threshold would violate information theoretic lower bounds qsgd guarantees convergence convex non convex objectives asynchrony extended stochastic variance reduced techniques applied training deep neural networks image classification automated speech recognition qsgd leads significant reductions end end training time example 16gpus train resnet152 network full accuracy imagenet faster full precision variant
machine learning adversaries byzantine tolerant gradient descent study resilience byzantine failures distributed implementations stochastic gradient descent sgd far distributed machine learning frameworks largely ignored possibility failures especially arbitrary byzantine ones causes failures include software bugs network asynchrony biases local datasets well attackers trying compromise entire system assuming set workers byzantine ask resilient sgd without limiting dimension size parameter space first show gradient aggregation rule based linear combination vectors proposed workers current approaches tolerates single byzantine failure formulate resilience property aggregation rule capturing basic requirements guarantee convergence despite byzantine workers propose emph krum aggregation rule satisfies resilience property argue first provably byzantine resilient algorithm distributed sgd also report experimental evaluations krum
ranking data continuous labels oriented recursive partitions formulate supervised learning problem referred continuous ranking continuous real valued label assigned observable taking values feature space goal order possible observations means scoring function tend increase decrease together highest probability problem generalizes multi partite ranking certain extent task finding optimal scoring functions naturally cast optimization dedicated functional cri terion called iroc curve maximization kendall related pair theoretical side describe optimal elements problem provide statistical guarantees empirical kendall maximiza tion appropriate conditions class scoring function candidates also propose recursive statistical learning algorithm tailored empirical iroc curve optimization producing piecewise constant scoring function fully described oriented binary tree preliminary numerical experiments highlight difference nature regression continuous ranking provide strong empirical evidence performance empirical optimizers criteria proposed
practical data dependent metric compression provable guarantees introduce new distance preserving compact representation multi dimensional point sets given points dimensional space coordinate represented using bits bits per point produces representation size log epsilon log bits per point approximate distances factor epsilon algorithm almost matches recent bound indyk 2017 much simpler compare algorithm product quantization jegou 2011 state art heuristic metric compression method evaluate algorithms several data sets sift mnist new york city taxi time series synthetic dimensional data set embedded high dimensional space algorithm produces representations comparable better produced provable guarantees performance
simple strategies recovering inner products coarsely quantized random projections random projections increasingly adopted diverse set tasks machine learning involving dimensionality reduction specific line research topic investigated use quantization subsequent projection aim additional data compression motivated applications nearest neighbor search linear learning revisit problem recovering inner products respectively cosine similarities setting show even coarse scalar quantization bits per projection loss accuracy tends range negligible tomoderate implication scenarios practical interest need sophisticated recovery approach like maximum likelihood estimation considered previous work subject propose herein also yields considerable improvements terms accuracy hamming distance based approach icml 2014 comparable terms simplicity
clustering stable instances euclidean means euclidean means problem arguably widely studied clustering problem machine learning means objective hard worst case practitioners enjoyed remarkable success applying heuristics like lloyd algorithm problem address disconnect study following question properties real world instances enable design efficient algorithms prove guarantees finding optimal clustering consider natural notion called additive perturbation stability believe captures many practical instances euclidean means clustering stable instances unique optimal means solutions change even point perturbed little euclidean distance captures property means optimal solution tolerant measurement errors uncertainty points design efficient algorithms provably recover optimal clustering instances additive perturbation stable instance additional separation design simple efficient algorithm provable guarantees also robust outliers also complement results studying amount stability real datasets demonstrating algorithm performs well benchmark datasets
distributed hierarchical clustering graph clustering fundamental task many data mining machine learning pipelines particular identifying good hierarchical structure time fundamental challenging problem several applications amount data analyze increasing astonishing rate day hence need new solutions efficiently compute effective hierarchical clusterings huge data main focus paper minimum spanning tree mst based clusterings particular propose affinity novel hierarchical clustering based boruvka mst algorithm prove certain theoretical guarantees affinity well classic algorithms show practice superior several state art clustering algorithms furthermore present mapreduce algorithms affinity first works case input graph dense takes constant rounds based mst algorithm dense graphs improves upon prior work karloff second algorithm assumption density input graph finds affinity clustering log rounds using distributed hash tables dhts show experimentally algorithms scalable huge data sets
sparse means embedding means clustering algorithm ubiquitous tool data mining machine learning shows promising performance however high computational cost hindered applications broad domains researchers successfully addressed obstacles dimensionality reduction methods recently cite dblp journals tit boutsidiszmd15 develop state art random projection method faster means clustering method delivers many improvements dimensionality reduction methods example compared advanced singular value decomposition based feature extraction approach cite dblp journals tit boutsidiszmd15 reduce running time factor min ϵ2log data matrix data points features losing factor approximation accuracy unfortunately still require ndkϵ2log matrix multiplication cost prohibitive large values break bottleneck carefully build sparse embedded means clustering algorithm requires nnz nnz denotes number non zeros fast matrix multiplication moreover proposed algorithm improves cite dblp journals tit boutsidiszmd15 results approximation accuracy factor empirical studies corroborate theoretical findings demonstrate approach able significantly accelerate means clustering achieving satisfactory clustering performance
medoids means seeding show experimentally algorithm clarans han 1994 finds better medoids solutions voronoi iteration algorithm hastie 2001 finding along similarity voronoi iteration algorithm lloyd means algorithm motivates use clarans means initializer show clarans outperforms algorithms datasets mean decrease means initialization mean squared error mse final mse introduce algorithmic improvements clarans improve complexity runtime making extremely viable initialization scheme large datasets
applied algorithmic foundation hierarchical clustering hierarchical clustering data analysis method used decades despite widespread use lack analytical foundation method foundation would support methods currently used guide future improvements paper gives applied algorithmic foundation hierarchical clustering goal paper give analytic framework supporting observations seen practice paper considers dual problem framework hierarchical clustering introduced dasgupta main results popular algorithms used practice average linkage agglomerative clustering small constant approximation ratio paper establishes using recursive means divisive clustering poor lower bound approximation ratio perhaps explaining popular practice motivated poor performance means seek find divisive algorithms perform well theoretically paper gives constant approximation algorithms paper represents first work giving foundation hierarchical clustering algorithms used practice
inhomogoenous hypergraph clustering applications hypergraph partitioning important problem machine learning computer vision network analytics widely used method hypergraph partitioning relies minimizing normalized sum costs partitioning hyperedges across clusters algorithmic solutions based approach assume different partitions hyperedge incur cost however assumption fails leverage fact different subsets vertices within hyperedge different structural importance hence propose new hypergraph clustering technique termed inhomogeneous hypergraph partitioning assigns different costs different hyperedge cuts prove inhomogeneous partitioning produces quadratic approximation optimal solution inhomogeneous costs satisfy submodularity constraints moreover demonstrate inhomogenous partitioning offers significant performance improvements applications structure learning rankings subspace segmentation motif clustering
subspace clustering via tangent cones given samples lying near number fixed subspaces subspace clustering task grouping samples based corresponding subspaces many subspace clustering methods operate assigning affinity pair points feeding affinities common clustering algorithm paper proposes new paradigm subspace clustering computes affinities based underlying conic geometry union subspaces proposed conic subspace clustering csc approach considers convex hull collection normalized data points tangent cones sample union subspaces underlying data imposes strong association tangent cone point original subspace containing addition describing novel geometric perspective paper provides practical algorithm subspace clustering leverages perspective tangent cone membership test estimate affinities algorithm accompanied deterministic stochastic guarantees properties learned affinity matrix directly translate overall clustering accuracy
tensor biclustering consider dataset data collected multiple features multiple individuals multiple times type data represented dimensional individual feature time tensor become increasingly prominent various areas science tensor biclustering problem computes subset individuals subset features whose signal trajectories time lie low dimensional subspace modeling similarity among signal trajectories allowing different scalings across different individuals different features study information theoretic limit problem generative model moreover propose efficient spectral algorithm solve tensor biclustering problem analyze achievability bound asymptotic regime finally show efficiency proposed method several synthetic real datasets
unified approach interpreting model predictions understanding model made certain prediction crucial many applications however large modern datasets best accuracy often achieved complex models even experts struggle interpret ensemble deep learning models creates tension accuracy interpretability response variety methods recently proposed help users interpret predictions complex models present unified framework interpreting predictions namely shap shapley additive explanations assigns feature importance particular prediction key components shap framework identification class additive feature importance measures theoretical results unique solution class set desired properties class unifies existing methods several recent methods class desired properties means framework inform development new methods explaining prediction models demonstrate several new methods presented paper based shap framework show better computational performance better consistency human intuition existing methods
efficient sublinear regret algorithms online sparse linear regression online sparse linear regression task applying linear regression analysis examples arriving sequentially subject resource constraint limited number features examples observed despite importance many practical applications recently shown polynomial time sublinear regret algorithm unless bpp exponential time sublinear regret algorithm known paper introduce mild assumptions solve problem assumptions present polynomial time sublinear regret algorithms online sparse linear regression addition thorough experiments publically available data demonstrate algorithms outperform known algorithms
unbiased estimates linear regression via volume sampling given full rank matrix columns rows consider task estimating pseudo inverse based pseudo inverse sampled subset columns size least number rows show possible subset columns chosen proportional squared volume spanned rows chosen submatrix volume sampling resulting estimator unbiased surprisingly covariance estimator also closed form equals specific factor times pseudo inverse plays important part solving linear least squares problem try predict label column assume labels expensive given labels small subset columns sample using methods show weight vector solution sub problem unbiased estimator optimal solution whole problem based column labels believe new formulas establish fundamental connection linear least squares volume sampling use methods obtain algorithm volume sampling faster state art obtaining bounds total loss estimated least squares solution labeled columns
separability loss functions revisiting discriminative generative models revisit classical analysis generative discriminative models general exponential families high dimensional settings towards develop novel technical machinery including notion separability general loss functions allow provide general framework obtain convergence rates general estimators use machinery analyze convergence rates generative discriminative models provide insights nuanced behaviors high dimensions results also applicable differential parameter estimation quantity interest difference generative model parameters
generalized linear model regression distance set penalties estimation generalized linear models glm complicated presence constraints handle constraints maximizing penalized log likelihood penalties lasso effective high dimensions often lead severe shrinkage paper explores instead penalizing squared distance constraint sets distance penalties flexible algebraic regularization penalties avoid drawback shrinkage optimize distance penalized objectives make use majorization minimization principle resulting algorithms constructed within framework amenable acceleration come global convergence guarantees applications shape constraints sparse regression rank restricted matrix regression synthetic real data showcase strong empirical performance distance penalization even non convex constraints
group additive structure identification kernel nonparametric regression additive model popularly used models high dimensional nonparametric regression analysis however main drawback neglects possible interactions predictor variables paper reexamine group additive model proposed literature rigorously define intrinsic group additive structure relationship response variable
predictor vector vectx develop effective structure penalized kernel method simultaneous identification intrinsic group additive structure nonparametric function estimation method utilizes novel complexity measure derive group additive structures show proposed method consistent identifying intrinsic group additive structure simulation study real data applications demonstrate effectiveness proposed method general tool high dimensional nonparametric regression
learning overcomplete hmms study basic problem learning overcomplete hmms many hidden states small output alphabet despite significant practical importance hmms poorly understood known positive negative results efficient learning paper present several new results positive negative help define boundaries tractable learning setting intractable setting show positive results large subclass hmms whose transition matrices sparse well conditioned small probability mass short cycles also show learning impossible given polynomial number samples hmms small output alphabet whose transition matrices random regular graphs large degree
matrix norm estimation entries singular values data matrix form provide insights structure data effective dimensionality choice hyper parameters higher level data analysis tools however many practical applications collaborative filtering network analysis get partial observation scenarios consider fundamental problem recovering various spectral properties underlying matrix sampling entries propose framework first estimating schatten norms matrix several values using surrogates estimating spectral properties interest spectrum rank paper focuses technical challenges accurately estimating schatten norms sampling matrix introduce novel unbiased estimator based counting small structures graph provide guarantees match empirical performances theoretical analysis shows schatten norms recovered accurately strictly smaller number samples compared needed recover underlying low rank matrix numerical experiments suggest significantly improve upon competing approach using matrix completion methods
optimal shrinkage singular values random data contamination low rank matrix contaminated uniformly distributed noise missing values outliers corrupt entries reconstruction singular values singular vectors contaminated matrix key problem machine learning computer vision data science paper show common contamination models including arbitrary combinations uniform noise missing values outliers corrupt entries described efficiently using single framework develop asymptotically optimal algorithm estimates manipulation singular values applies contamination models considered finally find explicit signal noise cutoff estimation singular value decomposition must fail well defined sense
new theory nonconvex matrix completion prevalent matrix completion theories reply assumption locations missing data distributed uniformly randomly uniform sampling nevertheless reason observations missing often depends unseen observations thus missing data practice usually occurs nonuniform fashion rather randomly break limits randomness assumption paper introduces new hypothesis called isomeric condition provably weaker randomness assumption arguably holds even missing data placed irregularly equipped new tool prove series theorems missing data recovery matrix completion particular prove exact solutions identify target matrix included critical points commonly used nonconvex programs unlike existing nonconvex theories use condition convex programs theories show nonconvex programs work much weaker condition comparing existing theories nonuniform sampling theories flexible powerful
learning low dimensional metrics paper investigates theoretical foundations metric learning focused key questions fully addressed prior work consider learning general low dimensional low rank metrics well sparse metrics develop upper lower minimax bounds generalization error quantify sample complexity metric learning terms dimension feature space dimension rank underlying metric also bound accuracy learned metric relative underlying true generative metric results involve novel mathematical approaches metric learning problem also shed new light special case ordinal embedding aka non metric multidimensional scaling
fast alternating minimization algorithms dictionary learning present theoretical guarantees alternating minimization algorithm dictionary learning sparse coding problem dictionary learning problem factorize samples appropriate basis dictionary times sparse vector algorithm simple alternating minimization procedure switching gradient descent minimization every step dictionary learning specifically alternating minimization algorithms dictionary learning well studied theoretically empirically however contrast previous theoretical analysis problem replace condition operator norm true underlying dictionary condition matrix infinity norm allows get convergence rates terms error estimated dictionary infinity norm also allows initialize randomly converge globally optimum guarantees reasonable generative model allow dictionaries growing operator norms handle arbitrary level overcompleteness sparsity information theoretically optimal incoherent dictionaries also present statistical guarantees present sample complexity guarantees algorithm
consistent robust regression present first efficient provably consistent estimator robust regression problem area robust learning optimization generated significant amount interest learning statistics communities recent years owing applicability scenarios corrupted data well handling model mis specifications particular special interest devoted fundamental problem robust linear regression estimators tolerate corruption constant fraction response variables widely studied surprisingly however date aware polynomial time estimator offers consistent estimate presence dense unbounded corruptions work present estimator called crr solves open problem put forward work bhatia 2015 consistency analysis requires novel stage proof technique involving careful analysis stability ordered lists independent interest show crr offers consistent estimates empirically far superior several recently proposed algorithms robust regression problem including extended lasso torrent algorithm comparison crr offers comparable better model recovery runtimes faster order magnitude
partial hard thresholding towards unified analysis support recovery machine learning compressed sensing central importance understand tractable algorithm recovers support sparse signal compressed measurements paper present towards principled analysis support recovery performance family hard thresholding algorithms end appeal partial hard thresholding pht operator proposed recently jain ieee trans information theory 2017 show proper conditions pht recovers arbitrary sparse signal within sκlog iterations condition number specializing pht operator obtain best known result hard thresholding pursuit orthogonal matching pursuit replacement experiments simulated data complement theoretical findings also illustrate interesting phase transition iteration number cannot significantly reduced
minimax estimation bandable precision matrices inverse covariance matrix provides considerable insight understanding statistical models multivariate setting particular distribution variables assumed multivariate normal sparsity pattern inverse covariance matrix commonly referred precision matrix corresponds adjacency matrix representation gauss markov graph encodes conditional independence statements variables minimax results spectral norm previously established covariance matrices sparse banded sparse precision matrices establish minimax estimation bounds estimating banded precision matrices spectral norm results greatly improve upon existing bounds particular find minimax rate estimating banded precision matrices matches estimating banded covariance matrices key insight analysis able obtain barely noisy estimates times subblocks precision matrix inverting slightly wider blocks empirical covariance matrix along diagonal theoretical results complemented experiments demonstrating sharpness bounds
diffusion approximations online principal component estimation global convergence paper propose adopt diffusion approximation tools study dynamics oja iteration online stochastic gradient method principal component analysis oja iteration maintains running estimate true principal component streaming data enjoys less temporal spatial complexities show oja iteration top eigenvector generates continuous state discrete time markov chain unit sphere characterize oja iteration phases using diffusion approximation weak convergence tools phase analysis provides finite sample error bound running estimate matches minimax information lower bound pca bounded noise
estimation covariance structure heavy tailed distributions propose analyze new estimator covariance matrix admits strong theoretical guarantees weak assumptions underlying distribution existence moments low order estimation covariance matrices corresponding sub gaussian distributions well understood much less known case heavy tailed data balasubramanian yuan write data real world experiments oftentimes tend corrupted outliers exhibit heavy tails cases clear covariance matrix estimators remain optimal possible strategies deal heavy tailed distributions warrant studies make step towards answering question prove tight deviation inequalities proposed estimator depend parameters controlling intrinsic dimension associated covariance matrix opposed dimension ambient space particular results applicable case high dimensional observations
learning koopman invariant subspaces dynamic mode decomposition spectral decomposition koopman operator attracting attention tool analysis nonlinear dynamical systems dynamic mode decomposition popular numerical algorithm koopman spectral analysis however often need prepare nonlinear observables manually according underlying dynamics always possible since priori knowledge paper propose fully data driven method koopman spectral analysis based principle learning koopman invariant subspaces observed data end propose minimization residual sum squares linear least squares regression estimate set functions transforms data form linear regression fits well introduce implementation neural networks evaluate performance empirically using nonlinear dynamical systems applications
tochastic approximation canonical correlation analysis propose novel first order stochastic approximation algorithms canonical correlation analysis cca algorithms presented instances noisy matrix stochastic gradient msg noisy matrix exponential gradient meg achieve suboptimality population objective time poly probability least input dimensionality also consider practical variants proposed algorithms compare methods cca theoretically empirically
diving shallows computational perspective large scale shallow learning remarkable recent success deep neural networks easy analyze theoretically particularly hard disentangle relative significance architecture optimization achieving accurate classification large datasets flip side shallow methods kernel methods encountered obstacles scaling large data practical methods variants gradient descent used successfully deep learning seem perform par applied kernel methods difficulty sometimes attributed limitations shallow architecture paper first identify basic limitation gradient descent based optimization methods used conjunctions smooth kernels analysis demonstrates vanishingly small fraction function space reachable polynomial number gradient descent iterations drastically limits approximating power gradient descent fixed computational budget leading serious regularization issue purely algorithmic persisting even limit infinite data address shortcoming practice introduce eigenpro iteration based simple direct preconditioning scheme using small number approximate eigenvectors also viewed learning new kernel optimized gradient descent turns injecting small amount approximate second order information leads major improvements convergence large data translates significant performance boost state art kernel methods particular able match improve results recently reported literature small fraction computational budget finally feel results show need broader computational perspective modern large scale learning complement traditional statistical convergence analyses
unreasonable effectiveness structured random orthogonal embeddings examine class embeddings based structured random matrices orthogonal rows applied many machine learning applications including dimensionality reduction kernel approximation johnson lindenstrauss transform angular kernel show select matrices yielding guaranteed improved performance accuracy speed compared earlier methods introduce matrices complex entries give significant accuracy improvement provide geometric markov chain based perspectives help understand benefits empirical results suggest approach helpful wider range applications
generalization properties learning random features study generalization properties ridge regression random features statistical learning framework show first time learning bounds achieved
nlog random features rather suggested previous results prove faster learning rates show might require random features unless sampled according possibly problem dependent distribution results shed light statistical computational trade offs large scale kernelized learning showing potential effectiveness random features reducing computational complexity keeping optimal generalization properties
gaussian quadrature kernel features kernel methods recently attracted resurgent interest matching performance deep neural networks tasks speech recognition random fourier features map technique commonly used scale kernel machines employing randomized feature map means samples required achieve approximation error paper investigate alternative schemes constructing feature maps deterministic rather random approximating kernel frequency domain using gaussian quadrature show deterministic feature maps constructed achieve error samples goes validate methods datasets different domains mnist timit showing deterministic features faster generate achieve comparable accuracy state art kernel methods based random fourier features
linear time kernel goodness fit test propose novel adaptive test goodness fit computational cost linear number samples learn test features best indicate differences observed samples reference model minimizing false negative rate features constructed via stein method meaning necessary compute normalising constant model analyse asymptotic bahadur efficiency new test prove mean shift alternative test always greater relative efficiency previous linear time kernel test regardless choice parameters test experiments performance method exceeds earlier linear time test matches exceeds power quadratic time kernel test high dimensions model structure exploited goodness fit test performs far better quadratic time sample test based maximum mean discrepancy samples drawn model
convergence rates partition based bayesian multivariate density estimation method study class non parametric density estimators bayesian settings estimators obtained adaptively partitioning sample space suitable prior analyze concentration rate posterior distribution demonstrate rate directly depend dimension problem several special cases another advantage class bayesian density estimators adapt unknown smoothness true density function thus achieving optimal conv
power absolute discounting dimensional distribution estimation categorical models natural fit many problems learning distribution categories samples high dimensionality dilute data minimax optimality pessimistic remedy issue serendipitously discovered estimator absolute discounting corrects empirical frequencies subtracting constant observed categories redistributes among unobserved outperforms classical estimators empirically used extensively natural language modeling paper rigorously explain prowess estimator using less pessimistic notions show absolute discounting recovers classical minimax risk rates emph adaptive effective dimension rather true dimension strongly related good turing estimator inherits emph competitive properties use power law distributions corner stone results validate theory via synthetic data application global terrorism database
optimally learning populations parameters consider following fundamental estimation problem entities unknown parameter observe independent random variables binomial accurately recover histogram cumulative density function empirical estimates would recover histogram earth mover distance equivalently distance cdfs show provided sufficiently large achieve error information theoretically optimal also extend results multi dimensional parameter case capturing settings member population multiple associated parameters beyond theoretical results demonstrate recovery algorithm performs well practice variety datasets providing illuminating insights several domains including politics sports analytics
communication efficient distributed learning discrete distributions initiate systematic study distribution learning density estimation distributed model problem data drawn unknown distribution partitioned across multiple machines machines must succinctly communicate referee end referee estimate underlying distribution data problem motivated pressing need build communication efficient protocols various distributed systems power consumption limited bandwidth impose stringent communication constraints give first upper lower bounds communication complexity nonparametric density estimation discrete probability distributions distances specifically results include following case unknown distribution arbitrary machine sample show interactive protocol learns distribution must essentially communicate entire sample case structured distributions
histograms monotone design distributed protocols achieve better communication guarantees trivial ones show tight bounds regimes
improved dynamic regret non degeneracy functions recently growing research interest analysis dynamic regret measures performance online learner sequence local minimizers exploiting strong convexity previous studies shown dynamic regret upper bounded path length comparator sequence paper illustrate dynamic regret improved allowing learner query gradient function multiple times meanwhile strong convexity weakened non degeneracy conditions specifically introduce squared path length could much smaller path length new regularity comparator sequence multiple gradients accessible learner first demonstrate dynamic regret strongly convex functions upper bounded minimum path length squared path length extend theoretical guarantee functions semi strongly convex self concordant best knowledge first time semi strong convexity self concordance utilized tighten dynamic regret
parameter free online learning via model selection introduce new framework deriving efficient algorithms obtain model selection oracle inequalities adversarial online learning setting also sometimes described parameter free online learning work area focused specific highly structured function classes nested balls hilbert space eschew approach propose generic meta algorithm framework achieves oracle inequalities minimal structural assumptions allows derive new computationally efficient algorithms oracle bounds wide range settings results previously unavailable give first computationally efficient algorithms work arbitrary banach spaces mild smoothness assumptions previous results applied hilbert case derive new oracle inequalities various matrix classes non nested convex sets generic regularizers finally generalize providing oracle inequalities arbitrary non linear classes contextual learning model particular give new algorithms learning multiple kernels results derived unified meta algorithm scheme based novel multi scale algorithm prediction expert advice based random playout independent interest
fast rates bandit optimization upper confidence frank wolfe consider problem bandit optimization inspired stochastic optimization online learning problems bandit feedback problem objective minimize global loss function actions necessarily cumulative loss framework allows study general class problems applications statistics machine learning fields solve problem analyze upper confidence frank wolfe algorithm inspired techniques bandits convex optimization give theoretical guarantees performance algorithm various classes functions discuss optimality results
online learning transductive regret study online learning general notion transductive regret regret modification rules applying expert sequences opposed single experts representable weighted finite state transducers show transductive regret generalizes existing notions regret including external regret internal regret swap regret conditional swap regret present general online learning algorithm minimizing transductive regret extend work design efficient algorithms time selection sleeping expert settings product study algorithm swap regret mild assumptions efficient existing methods
multi armed bandits metric movement costs consider non stochastic multi armed bandit problem setting fixed known metric action space determines cost switching pair actions loss online learner components first usual loss selected actions second additional loss due switching actions main contribution gives tight characterization expected minimax regret setting terms complexity measure underlying metric depends covering numbers finite metric spaces max setc1 3t2 show best possible regret bound generalizes previous known regret bounds special cases unit switching cost regret max setk1 3t2 interval metric regret max sett2 infinite metrics spaces lipschitz loss functions derive tight regret bound minkowski dimension space known tight even switching costs
differentially private empirical risk minimization revisited faster general paper study differentially private empirical risk minimization erm different settings smooth strongly convex loss function without non smooth regularization give algorithms achieve either optimal near optimal utility bound less gradient complexity compared previous work erm smooth convex loss function high dimension setting give algorithm achieves upper bound less gradient complexity previous ones last generalize expected excess empirical risk convex polyak lojasiewicz condition give tighter upper bound utility comparing result cite dblp journals corr zhangzmw17
certified defenses data poisoning attacks machine learning systems trained user provided data susceptible data poisoning attacks whereby malicious users inject data aim corrupting learned model recent work proposed number attacks defenses little understood worst case performance defense face determined attacker remedy constructing upper bounds loss across broad family attacks defenders operate via outlier removal followed empirical risk minimization bound comes paired candidate attack nearly realizes upper bound giving powerful tool quickly assessing defense given dataset empirically find even simple defense mnist dogfish datasets certifiably resilient attack contrast imdb sentiment dataset driven test error adding poisoned data
sparse approximate conic hulls consider problem computing restricted nonnegative matrix factorization nmf matrix specifically seek factorization columns subset equivalently given matrix consider problem finding small subset columns conic hull eps approximates conic hull columns distance every column conic hull columns eps fraction angular diameter size smallest eps approximation produce eps2 sized eps1 approximation yielding first provable polynomial time eps approximation class nmf problems also desirably approximation independent furthermore prove approximate conic caratheodory theorem general sparsity result shows column eps approximated eps2 sparse combination results facilitated reduction problem approximating convex hulls prove convex conic hull variants sum hard resolving open problem finally provide experimental results convex conic algorithms variety feature selection tasks
estimating high dimensional non gaussian multiple index models via stein lemma consider estimating parametric components semi parametric multiple index models high dimensional non gaussian setting estimators leverage score function based second order stein lemma require gaussian elliptical symmetry assumptions made literature show estimator achieves near optimal statistical rate convergence even score function response variable heavy tailed utilize data driven truncation argument based required concentration results established supplement theoretical results via simulation experiments confirm theory
solid harmonic wavelet scattering predicting quantum molecular energy invariant descriptors electronic densities introduce solid harmonic wavelet scattering representation invariant rigid movements stable deformations regression classification images solid harmonic wavelets computed multiplying solid harmonic functions gaussian windows dilated different scales invariant scattering coefficients obtained cascading wavelet transforms complex modulus nonlinearity study application solid harmonic scattering invariants estimation quantum molecular energies also invariant rigid movements stable respect deformations introduce neural network multiplicative non linearity regression scattering invariants provide close state art results database organic molecules
clustering billions reads dna data storage storing data synthetic dna offers possibility improving information density durability several orders magnitude compared current storage technologies however dna data storage requires computationally intensive process retrieve data particular crucial step data retrieval pipeline involves clustering billions strings respect edit distance observe datasets domain many notable properties containing large number small clusters well separated edit distance metric space regime existing algorithms unsuitable either long running time low accuracy address issue present novel distributed algorithm approximately computing underlying clusters algorithm converges efficiently dataset satisfies certain separability properties coming dna storage systems also prove assumptions algorithm robust outliers high levels noise provide empirical justification accuracy scalability convergence algorithm real synthetic data compared state art algorithm clustering dna sequences algorithm simultaneously achieves higher accuracy 1000x speedup real datasets
deep recurrent neural network based identification precursor micrornas micrornas mirnas small non coding ribonucleic acids rnas play key roles post transcriptional gene regulation direct identification mature mirnas infeasible due short lengths researchers instead aim identifying precursor mirnas pre mirnas many known pre mirnas distinctive stem loop secondary structure structure based filtering usually first step predict possibility given sequence pre mirna identify new pre mirnas often non canonical structure however need consider additional features structure obtain additional characteristics existing computational methods rely manual feature extraction inevitably limits efficiency robustness generalization computational identification address limitations existing approaches propose pre mirna identification method incorporates deep recurrent neural network rnn automated feature learning classification multimodal architecture seamless integration prior knowledge secondary structure attention mechanism improving long term dependence modeling rnn based class activation mapping highlighting learned representations contrast pre mirnas non pre mirnas experiments recent benchmarks proposed approach outperformed compared state art alternatives terms various performance metrics
decoding value networks neural machine translation neural machine translation nmt become popular technology recent years beam search facto decoding method due shrunk search space reduced computational complexity issue beam search since searches local optima time step step forward looking usually cannot output best target sentence inspired success methodology alphago paper propose using prediction network improve beam search takes source sentence currently available decoding output candidate word step inputs predicts long term value bleu score partial target sentence completed nmt model following practice reinforcement learning call prediction network emph value network specifically propose recurrent structure value network train parameters bilingual data test time choosing word decoding consider conditional probability given nmt model long term value predicted value network experiments show approach significantly improve translation accuracy translation tasks english french translation chinese english translation
towards imagenet cnn nlp pretraining sentence encoders machine translation computer vision benefited initializing multiple deep layers weights pre trained large supervised training sets like imagenet contrast deep models language tasks currently benefit transfer unsupervised word vectors randomly initialize higher layers paper use encoder attentional sequence sequence model trained machine translation initialize models different language tasks show transfer improves performance using word vectors wide variety common nlp tasks sentiment analysis sst imdb question classification entailment snli question answering squad
deep voice multi speaker neural text speech introduce technique augmenting neural text speech tts low dimensional trainable speaker embeddings generate different voices single model starting point show improvements state art approaches single speaker neural tts deep voice tacotron introduce deep voice based similar pipeline deep voice constructed higher performance building blocks demonstrates significant audio quality improvement deep voice improve tacotron introducing post processing neural vocoder demonstrate significant audio quality improvement demonstrate technique multi speaker speech synthesis deep voice tacotron multi speaker tts datasets show single neural tts system learn hundreds unique voices less half hour data per speaker achieving high audio quality synthesis preserving speaker identities almost perfectly
modulating early visual processing language commonly assumed language refers high level visual concepts leaving low level visual processing unaffected view dominates current literature computational models language vision tasks visual linguistic input mostly processed independently fused single representation paper deviate classic pipeline propose modulate emph entire visual processing linguistic input specifically condition batch normalization parameters pretrained residual network language embedding approach call modulated residual networks mrn significantly improves strong baselines visual question answering tasks ablation study shows modulating early stages visual processing beneficial
multimodal learning reasoning visual question answering reasoning entities relationships multimodal data key goal artificial general intelligence visual question answering vqa problem excellent way test reasoning capabilities model multimodal representation learning however current vqa models oversimplified deep neural networks comprised long short term memory lstm unit question comprehension convolutional neural network cnn learning single image representation argue single visual representation contains limited general information image contents thus limit model reasoning capabilities work introduce modular neural network model learns multimodal multifaceted representation image question proposed model learns use multimodal representation reason image entities achieves new state art performance vqa benchmark datasets vqa wide margin
learning model tail describe approach learning long tailed imbalanced datasets prevalent real world settings challenge learn accurate shot models classes tail little data available cast problem transfer learning knowledge data rich classes head transferred data poor classes tail key insights follows first propose transfer meta knowledge learning learn head knowledge encoded meta network operates space model parameters trained predict many shot model parameters shot model parameters second transfer meta knowledge progressive manner classes head thebody body tail transfer knowledge gradual fashion regularizing meta networks shot regression trained training data allows final network capture dynamics transferring meta knowledge data rich data poor regime demonstrate results image classification datasets sun places imagenet tuned long tailed setting significantly outperform widespread heuristics data resampling reweighting
interpretable globally optimal prediction textual grounding using image concepts textual grounding important challenging task human computer interaction robotics knowledge mining existing algorithms generally formulate task selection solution set bounding box proposals obtained deep net based systems work demonstrate cast problem textual grounding unified framework permits efficient search possible bounding boxes hence able consider significantly proposals due unified formulation approach rely successful first stage beyond demonstrate trained parameters model used word embeddings capture spatial image relationships provide interpretability lastly approach outperforms current state art methods flickr 30k entities referitgame dataset respectively
multiscale quantization fast similarity search propose multiscale quantization approach fast similarity search large high dimensional datasets key insight approach quantization methods particular product quantization perform poorly variance norm data points common scenario real world datasets especially product quantization residuals obtained coarse vector quantization address issue propose multiscale formulation learn separate scalar quantizer residual norms parameters learned jointly stochastic gradient descent framework minimize overall quantization error provide theoretical motivation proposed technique conduct comprehensive experiments large scale public datasets demonstrating substantial improvements recall existing state art methods
maskrnn instance level video object segmentation instance level video object segmentation important technique video editing compression capture temporal coherence paper develop maskrnn recurrent neural net approach fuses frame output deep nets object instance binary segmentation net providing mask localization net providing bounding box due recurrent component localization component method able take advantage long term temporal structures video data well rejecting outliers validate proposed algorithm challenging benchmark datasets davis 2016 dataset davis 2017 dataset segtrack dataset achieving state art performance
flat2sphere learning spherical convolution fast features 360 imagery 360 cameras offer tremendous new possibilities vision graphics augmented reality spherical images produce make core feature extraction non trivial convolutional neural networks cnns trained images perspective cameras yield flat filters yet 360 images cannot projected single plane without significant distortion naive solution repeatedly projects viewing sphere tangent planes accurate much computationally intensive real problems propose learn spherical convolutional network translates planar cnn process 360 imagery directly equirectangular projection approach learns reproduce flat filter outputs 360 data sensitive varying distortion effects across viewing sphere key benefits efficient feature extraction 360 images video ability leverage powerful pre trained networks researchers carefully honed together massive labeled image training sets perspective images validate approach compared several alternative methods terms raw cnn output accuracy well applying state art flat object detector 360 data method yields accurate results saving orders magnitude computation versus existing exact reprojection solution
deep mean shift priors image restoration paper introduce natural image prior directly represents gaussian smoothed version natural image distribution include prior formulation image restoration bayes estimator also allows solve noise blind image restoration problems gradient bound estimator involves gradient logarithm prior gradient corresponds mean shift vector natural image distribution learn mean shift vector field using denoising autoencoders demonstrate competitive results noise blind deblurring super resolution demosaicing
pixels graphs associative embedding graphs useful abstraction image content graphs represent details individual objects scene capture interactions pairs objects present method training convolutional neural network takes input image produces full graph definition done end end single stage use associative embeddings network learns simultaneously identify elements make graph piece together benchmark visual genome dataset demonstrate state art performance challenging task scene graph generation
shape reconstruction modeling sketch object reconstruction single image highly determined problem requiring strong prior knowledge plausible shapes introduces challenge learning based approaches object annotations real images scarce previous work chose train synthetic data ground truth information suffered domain adaptation issue tested real data work propose end end trainable framework sequentially estimating sketch object shape disentangled step formulation advantages first compared full shape sketch much easier recovered image transfer synthetic real images second reconstruction sketch easily transfer learned model synthetic data real images rendered sketches invariant object appearance variations real images including lighting texture etc relieves domain adaptation problem third derive differentiable projective functions shapes sketch making framework end end trainable real images requiring real image annotations framework achieves state art performance shape reconstruction
temporal coherency based criteria predicting video frames using deep multi stage generative adversarial networks predicting future sequence video frames recently sought yet challenging task field computer vision machine learning although efforts tracking using motion trajectories flow features complex problem generating unseen frames studied extensively paper deal problem using convolutional models within multi stage generative adversarial networks gan framework proposed method uses stages gans generate crisp clear set future frames although gans used past predicting future none works consider relation subsequent frames temporal dimension main contribution lies formulating objective functions based normalized cross correlation ncc pairwise contrastive divergence pcd solving problem method coupled traditional loss experimented real world video datasets viz sports ucf 101 kitti performance analysis reveals superior results recent state art methods
learning generalize intrinsic images structured disentangling autoencoder intrinsic decomposition single image highly challenging task due inherent ambiguity scarcity training data contrast traditional fully supervised learning approaches paper propose learning intrinsic image decomposition explaining input image model rendered intrinsics network rin joins together image decomposition pipeline predicts reflectance shape lighting conditions given single image recombination function learned shading model used recompose original input based intrinsic image predictions network use unsupervised reconstruction error additional signal improve intermediate representations allows large scale unlabeled data useful training also enables transferring learned knowledge images unseen object categories lighting conditions shapes extensive experiments demonstrate method performs well intrinsic image decomposition knowledge transfer
unsupervised object learning dense equivariant image labelling key challenges visual perception extract abstract models objects object categories visual measurements affected complex nuisance factors viewpoint occlusion motion deformations starting recent idea viewpoint factorization propose new approach given large number images object supervision extract dense object centric coordinate frame coordinate frame invariant deformations images comes dense equivariant labelling neural network map image pixels corresponding object coordinates demonstrate applicability method simple articulated objects deformable objects human faces learning embeddings random synthetic transformations optical flow correspondences without manual supervision
sided unsupervised domain mapping unsupervised domain mapping learner given unmatched datasets goal learn mapping gab translates sample analog sample recent approaches shown learning simultaneously gab inverse mapping gba convincing mappings obtained work present method learning gab without learning gba done learning mapping maintains distance pair samples moreover good mappings obtained even maintaining distance different parts sample mapping present experimental results new method allows sided mapping learning also leads preferable numerical results existing circularity based constraint entire code made publicly available
contrastive learning image captioning image captioning popular topic computer vision achieved substantial progress recent years however distinctiveness natural descriptions often overlooked previous work closely related quality captions distinctive captions likely describe images unique aspects work propose new learning method contrastive learning image captioning specifically via constraints formulated top reference model proposed method encourage distinctiveness maintaining overall quality generated captions tested method challenging datasets improves baseline model significant margins also showed studies proposed method generic used models various structures
dynamic routing capsules capsule group neurons whose activity vector represents instantiation parameters specific type entity object object part use length activity vector represent probability entity exists orientation represent instantiation paramters active capsules level make predictions via transformation matrices instantiation parameters higher level capsules multiple predictions agree higher level capsule becomes active show discrimininatively trained multi layer capsule system achieves state art performance mnist considerably better convolutional net recognizing highly overlapping digits achieve results use iterative routing agreement mechanism lower level capsule prefers send output higher level capsules whose activity vectors big scalar product prediction coming lower level capsule
uncertainties need bayesian deep learning computer vision major types uncertainty model aleatoric uncertainty captures noise inherent observations hand epistemic uncertainty accounts uncertainty model uncertainty explained away given enough data traditionally difficult model epistemic uncertainty computer vision new bayesian deep learning tools possible study benefits modeling epistemic aleatoric uncertainty bayesian deep learning models vision tasks present bayesian deep learning framework combining input dependent aleatoric uncertainty together epistemic uncertainty study models framework per pixel semantic segmentation depth regression tasks explicit uncertainty formulation leads new loss functions tasks interpreted learned attenuation makes loss robust noisy data also giving new state art results segmentation depth regression benchmarks
efficient optimization linear dynamical systems applications clustering sparse coding linear dynamical systems ldss fundamental tools modeling spatio temporal data various disciplines though rich modeling analyzing ldss free difficulty mainly ldss comply euclidean geometry hence conventional learning techniques applied directly paper propose efficient projected gradient descent method minimize general form loss function demonstrate clustering sparse coding ldss solved proposed method efficiently end first derive novel canonical form representing parameters lds show gradient descent updates projection space ldss achieved dexterously contrast previous studies solution avoids approximation lds modeling optimization process extensive experiments reveal superior performance proposed method terms convergence classification accuracy state art techniques
label distribution learning forests label distribution learning ldl general learning framework assigns instance distribution set labels rather single label multiple labels current ldl methods either restricted assumptions expression form label distribution limitations representation learning learn deep features end end manner paper presents label distribution learning forests ldlfs novel label distribution learning algorithm based differentiable decision trees several advantages decision trees potential model general form label distributions mixture leaf node predictions learning differentiable decision trees combined representation learning define distribution based loss function forest enabling trees learned jointly show update function leaf node predictions guarantees strict decrease loss function derived variational bounding effectiveness proposed ldlfs verified several ldl tasks computer vision application showing significant improvements state art ldl methods
graph matching via multiplicative update algorithm graph matching fundamental problem computer vision machine learning area problem usually formulated quadratic programming problem doubly stochastic discrete integer constraints since hard approximate algorithms required paper present new algorithm called multiplicative update graph matching mpgm develops multiplicative update technique solve matching problem mpgm main benefits theoretically mpgm solves general problem doubly stochastic constraint naturally directly whose convergence kkt optimality guaranteed empirically mpgm generally returns sparse solution thus also incorporate discrete constraint approximately optimization efficient simple implement experiments synthetic real world matching tasks show benefits mpgm algorithm
training quantized nets deeper understanding currently deep neural networks deployed low power embedded devices first training full precision model using powerful computing hardware deriving corresponding low precision model efficient inference systems however training models directly coarsely quantized weights key step towards learning embedded platforms limited computing resources memory capacity power consumption numerous recent publications studied methods training quantized network weights studies mostly empirical work investigate training methods quantized neural networks theoretical viewpoint first explore accuracy guarantees training methods convexity assumptions look behavior algorithms non convex problems show training algorithms exploit high precision representations important annealing property purely quantized training methods lack explains many observed empirical differences types algorithms
inner loop free admm using auxiliary deep neural networks propose new method uses apply deep learning techniques accelerate popular alternating direction method multipliers admm solution inverse problems admm updates consist proximity operator least squares regression includes big matrix inversion explicit solution updating dual variables typically inner loops required solve first sub minimization problems due intractability prior matrix inversion avoid drawbacks limitations propose textit inner loop free update rule pre trained deep convolutional architectures specifically learn conditional denoising auto encoder imposes implicit data dependent prior regularization ground truth first sub minimization problem design follows empirical bayesian strategy leading called amortized inference matrix inversion second sub problem learn convolutional neural network approximate matrix inversion inverse mapping learned feeding input learned forward network note training neural network require ground truth measurements data independent extensive experiments synthetic data real datasets demonstrate efficiency accuracy proposed method compared conventional admm solution using inner loops solving inverse problems
towards accurate binary convolutional neural network introduce novel scheme train binary convolutional neural networks cnns cnns weights activations constrained run time known using binary weights activations drastically reduce memory size accesses replace arithmetic operations efficient bitwise operations leading much faster test time inference lower power consumption however previous works binarizing cnns usually result severe prediction accuracy degradation paper address issue major innovations approximating full precision weights linear combination multiple binary weight bases employing multiple binary activations alleviate information loss implementation resulting binary cnn denoted abc net shown achieve much closer performance full precision counterpart even reach comparable prediction accuracy imagenet forest trail datasets given adequate binary weight bases activations
runtime neural pruning paper propose runtime neural pruning rnp framework prunes deep neural network dynamically runtime unlike existing neural pruning methods produce fixed pruned model deployment method preserves full ability original network conducts pruning according input image current feature maps adaptively pruning performed bottom layer layer manner model markov decision process use reinforcement learning training agent judges importance convolutional kernel conducts channel wise pruning conditioned different samples network pruned image easier task since ability network fully preserved balance point easily adjustable according available resources method applied shelf network structures reach better tradeoff speed accuracy especially large pruning rate
structured embedding models grouped data word embeddings powerful approach analyzing language exponential family embeddings efe extend types data develop structured exponential family embeddings efe method discovering embeddings vary across related groups data study word usage congressional speeches varies across states party affiliation words used differently across sections arxiv purchase patterns groceries vary across seasons key success method groups share statistical information develop sharing strategies hierarchical modeling amortization demonstrate benefits approach empirical studies speeches abstracts shopping baskets show sefe enables group specific interpretation word usage outperforms efe predicting held data
poincaré embeddings learning hierarchical representations representation learning become invaluable approach learning symbolic data text graphs however complex symbolic datasets often exhibit latent hierarchical structure state art methods typically learn embeddings euclidean vector spaces account property purpose introduce new approach learning hierarchical representations symbolic data embedding hyperbolic space precisely dimensional poincaré ball due underlying hyperbolic geometry allows learn parsimonious representations symbolic data simultaneously capturing hierarchy similarity introduce efficient algorithm learn embeddings based riemannian optimization show experimentally poincaré embeddings outperform euclidean embeddings significantly data latent hierarchies terms representation capacity terms generalization ability
language modeling recurrent highway hypernetworks provide extensive experimental theoretical support efficacy recurrent highway networks rhns recurrent hypernetworks complimentary original works demonstrate experimentally rhns benefit far better gradient flow lstms coupled greatly improved task accuracy raise provide solutions several theoretical issues hypernetworks believe yield gains future along dramatically reduced computational cost combining rhns hypernetworks make significant improvement current state art language modeling performance penn treebank relying much simpler regularization finally argue rhns drop replacement lstms analogous lstms vanilla rnns hypernetworks facto augmentation analogous attention recurrent architectures
preventing gradient explosions gated recurrent units gated recurrent unit gru successful recurrent neural network architecture time series data gru typically trained using gradient based method subject exploding gradient problem gradient increases significantly problem caused abrupt change dynamics gru due small variation parameters paper find condition dynamics gru changes drastically propose learning method address exploding gradient problem method constrains dynamics gru drastically change evaluated method experiments language modeling polyphonic music modeling experiments showed method prevent exploding gradient problem improve modeling accuracy
wider deeper cheaper faster tensorized lstms sequence learning long short term memory lstm popular approach boosting ability recurrent neural networks store longer term temporal information capacity lstm network increased widening adding layers however former introduces additional parameters latter increases runtime alternative propose tensorized lstm hidden states represented tensors updated via cross layer convolution increasing tensor size network widened efficiently without additional parameters since parameters shared across different locations tensor delaying output network deepened implicitly little additional runtime since deep computations timestep merged temporal computations sequence experiments conducted challenging sequence learning tasks show potential proposed model
fast slow recurrent neural networks processing sequential data variable length major challenge wide range applications speech recognition language modeling generative image modeling machine translation address challenge proposing novel recurrent neural network rnn architecture fast slow rnn rnn rnn incorporates strengths multiscale rnns deep transition rnns processes sequential data different timescales learns complex transition functions time step next evaluate rnn character based language modeling data sets penn treebank hutter prize wikipedia improve state art results bits per character bpc respectively addition ensemble rnns achieves bpc hutter prize wikipedia outperforming best known compression algorithm respect bpc measure also present empirical investigation learning network dynamics rnn explains improved performance compared rnn architectures approach general kind rnn cell possible building block rnn architecture thus flexibly applied different tasks
cold start reinforcement learning softmax policy gradients present learning algorithm targeted efficiently solving fundamental problems structured output prediction exposure bias problem model exposed training data distribution fail exposed predictions wrong objective problem training model convenient objective functions gives suboptimal performance method based policy gradient approach reinforcement learning succeeds avoiding common overhead procedures associated approaches namely warm start training variance reduction policy updates proposed cold start reinforcement learning method based new softmax policy gradient softmax policy combines efficiency simplicity maximum likelihood approach effectiveness reward based signal empirical evidence validates method structured output predictions automatic summarization image captioning tasks
deep learning precipitation nowcasting benchmark new model goal making high resolution forecasts regional rainfall precipitation nowcasting become important fundamental technology underlying various public services ranging rainfall alerts flight safety recently convolutional lstm convlstm model shown outperform traditional optical flow based methods precipitation nowcasting suggesting deep learning models huge potential solving problem however convolutional recurrence structure convlstm based models location invariant natural motion transformation rotation location variant general furthermore since deep learning based precipitation nowcasting newly emerging area clear evaluation protocols yet established address problems propose new model benchmark precipitation nowcasting specifically beyond convlstm propose trajectory gru trajgru model actively learn location variant structure recurrent connections besides provide benchmark includes real world large scale dataset hong kong observatory new training loss comprehensive evaluation protocol facilitate future research gauge state art
recurrent ladder networks propose recurrent extension ladder network cite ladder motivated inference required hierarchical latent variable models demonstrate recurrent ladder able handle wide variety complex learning tasks need iterative inference temporal modeling architecture shows close optimal results temporal modeling video data competitive results music modeling improved perceptual grouping based higher order abstractions stochastic textures motion cues present results fully supervised semi supervised unsupervised tasks results suggest proposed architecture principles powerful tools learning hierarchy abstractions handling temporal information modeling relations interactions objects
predictive state decoders encoding future recurrent networks recurrent neural networks rnns vital modeling technique rely internal states learned indirectly optimization supervised unsupervised reinforcement training loss rnns used model dynamic processes characterized underlying latent states whose form often unknown precluding analytic representation inside rnn predictive state representation psr literature latent state processes modeled internal state representation directly models distribution future observations recent work area relied explicitly representing targeting sufficient statistics probability distribution seek combine advantages rnns psrs augmenting existing state art recurrent neural networks predictive state decoders psds add supervision network internal state representation target predicting future observations psds simple implement easily incorporated existing training pipelines via additional loss regularization demonstrate effectiveness psds experimental results different domains probabilistic filtering imitation learning reinforcement learning method improves statistical performance state art recurrent baselines fewer iterations less data
qmdp net deep learning planning partial observability paper introduces qmdp net neural network architecture planning partial observability qmdp net combines strengths model free learning model based planning recurrent policy network represents policy connecting model planning algorithm solves model thus embedding solution structure planning network learning architecture qmdp net fully differentiable allows end end training train qmdp net set different environments generalize new ones transfer larger environments well preliminary experiments qmdp net showed strong performance several robotic tasks simulation interestingly qmdp net encodes qmdp algorithm sometimes outperforms qmdp algorithm experiments qmdp net increased robustness end end learning
filtering variational objectives evidence lower bound elbo appears many algorithms maximum likelihood estimation mle latent variables sharp lower bound marginal log likelihood neural latent variable models optimizing elbo jointly variational posterior model parameters produces state art results inspired success elbo surrogate mle objective consider extension elbo family lower bounds defined monte carlo estimator marginal likelihood show tightness bounds asymptotically related variance underlying estimator introduce special case filtering variational objectives takes arguments elbo passes particle filter form tighter bound filtering variational objectives optimized tractably stochastic gradients particularly suited mle sequential latent variable models standard sequential generative modeling tasks present uniform improvements computational budget models trained elbo iwae objectives include whole nat per timestep improvements
unsupervised learning disentangled latent representations sequential data present factorized hierarchical variational autoencoder learns disentangled representations sequential data without supervision specifically exploit multi scale nature information sequential data formulating explicitly within factorized hierarchical graphical model imposes sequence specific priors global priors different sets latent variables model evaluated speech corpora demonstrate qualitatively ability transform speakers linguistic content manipulating different sets latent variables quantitatively ability outperform vector baseline speaker verification reduce word error rate much mismatched train test scenarios automatic speech recognition tasks
neural discrete representation learning learning useful representations without supervision remains key challenge machine learning paper propose simple yet powerful generative model learns discrete representations model vector quantised variational autoencoder vae differs vaes key ways encoder network outputs discrete rather continuous codes prior learnt rather static order learn discrete latent representation incorporate ideas vector quantisation using method allows model circumvent issues posterior collapse latents ignored paired powerful autoregressive decoder typically observed vae framework pairing representations autoregressive prior model generate high quality images videos speech well high quality speaker inpainting providing evidence utility learnt representations
variational memory addressing generative models aiming augment generative models external memory interpret output memory module stochastic addressing conditional mixture distribution read operation corresponds sampling discrete memory address retrieving corresponding content memory perspective allows apply variational inference memory addressing enables effective training memory module using target information guide memory lookups stochastic addressing particularly well suited generative models naturally encourages multimodality prominent aspect high dimensional datasets treating chosen address latent variable also allows quantify amount information gained memory lookup measure contribution memory module generative process illustrate advantages approach incorporate variational autoencoder apply resulting model task generative shot learning intuition behind architecture memory module pick relevant template memory continuous part model concentrate modeling remaining variations demonstrate empirically model able identify access relevant memory contents even hundreds unseen omniglot characters memory
cortical microcircuits gated recurrent neural networks cortical circuits exhibit intricate recurrent architectures remarkably similar across different brain areas stereotyped structure suggests existence common computational principles remained largely elusive inspired gated memory networks namely long short term memory lstm nets introduce recurrent neural network rnn information gated inhibitory units subtractive balanced subrnn propose subrnns natural mapping onto known canonical excitatory inhibitory cortical microcircuits show networks subtractive gating easier optimise standard multiplicative gates moreover subrnns yield near exact solution standard long term dependency task temporal addition task empirical results across several long term dependency tasks generalised temporal addition multiplication temporal mnist word level language modelling show subrnns outperform achieve similar performance lstm networks tested work suggests novel view cortex solves complex contextual problems provides first step towards unifying machine learning recurrent networks biological counterparts
continual learning deep generative replay attempts train comprehensive artificial intelligence capable solving multiple tasks impeded chronic problem called catastrophic forgetting although simply replaying previous data alleviates problem requires large memory even worse often infeasible real world applications access past data limited inspired generative nature hippocampus short term memory system primate brain propose deep generative replay novel framework cooperative dual model architecture consisting deep generative model generator task solving model solver models training data previous tasks easily sampled interleaved new task test methods several sequential learning settings involving image classification tasks
hierarchical attentive recurrent tracking class agnostic object tracking particularly difficult cluttered environments target specific discriminative models cannot learned priori inspired human visual cortex employs spatial attention separate andwhat processing pathways actively suppress irrelevant visual features work develops hierarchical attentive recurrent model single object tracking videos first layer attention discards majority background selecting region containing object interest subsequent layers tune visual features particular tracked object framework fully differentiable trained purely data driven fashion gradient methods improve training convergence augment loss function terms number auxiliary tasks relevant tracking evaluation proposed model performed datasets increasing difficulty pedestrian tracking kth activity recognition dataset kitti object tracking dataset
vae learning via stein variational gradient descent new method learning variational autoencoders vaes developed based stein variational gradient descent key advantage approach need make parametric assumptions form encoder distribution performance enhanced integrating proposed encoder importance sampling excellent performance demonstrated across multiple unsupervised semi supervised problems including semi supervised analysis imagenet data demonstrating scalability model large datasets
learning inpaint image compression study design deep architectures lossy image compression present architectural recipes context multi stage progressive encoders empirically demonstrate importance compression performance specifically show predicting original image data residuals multi stage progressive architecture facilitates learning leads improved performance approximating original content learning inpaint neighboring image pixels performing compression reduces amount information must stored achieve high quality approximation incorporating design choices baseline progressive encoder yields average reduction file size similar quality compared original residual encoder
visual interaction networks glance humans make rich predictions future state wide range physical systems modern approaches engineering robotics graphics often restricted narrow domains require direct measurements underlying states introduce visual interaction network general purpose model learning dynamics physical system raw visual observations predicting future states model consists perceptual front end based convolutional neural networks dynamics predictor based interaction networks joint training perceptual front end learns parse dynamic visual scene set factored latent object representations dynamics predictor learns roll states forward time computing interactions dynamics producing predicted physical trajectory arbitrary length found input video frames visual interaction network generate accurate future trajectories hundreds time steps wide range physical systems model also applied scenes invisible objects inferring future states effects visible objects implicitly infer unknown mass objects results demonstrate perceptual module object based dynamics predictor module induce factored latent representations support accurate dynamical predictions work opens new opportunities model based decision making planning raw sensory observations complex physical environments
neuralfdr learning discovery thresholds hypothesis features datasets grow richer important challenge leverage full features data maximize number useful discoveries controlling false positives address problem context multiple hypotheses testing hypothesis observe value along set features specific hypothesis example genetic association studies hypothesis tests correlation variant trait rich set features variant location conservation epigenetics etc could inform likely variant true association however popular testing approaches benjamini hochberg procedure independent hypothesis weighting ihw either ignore features assume features categorical propose new algorithm neuralfdr automatically learns discovery threshold function hypothesis features parametrize discovery threshold neural network enables flexible handling multi dimensional discrete continuous features well efficient end end optimization prove neuralfdr strong false discovery rate fdr guarantees show makes substantially discoveries synthetic real datasets moreover demonstrate learned discovery threshold directly interpretable
eigen distortions hierarchical representations develop method comparing hierarchical image representations terms ability explain perceptual sensitivity humans specifically utilize fisher information establish model derived prediction local sensitivity perturbations around given natural image given image compute eigenvectors fisher information matrix largest smallest eigenvalues corresponding model predicted least noticeable image distortions respectively human subjects measure amount distortion reliably detected added image compare thresholds predictions corresponding model use method test ability variety representations mimic human perceptual sensitivity find early layers vgg16 deep neural network optimized object recognition provide better match human perception later layers better match stage convolutional neural network cnn trained database human ratings distorted image quality hand find simple models early visual processing incorporating stages local gain control trained database distortion ratings predict human sensitivity significantly better cnn layers vgg16
fly operation batching dynamic computation graphs dynamic neural networks toolkits pytorch dynet chainer offer flexibility implementing models cope data varying dimensions structure relative toolkits operate statically declared computations tensorflow cntk theano however existing toolkits static dynamic require developer organize computations batches necessary exploiting high performance data parallel algorithms hardware batching task generally difficult becomes major hurdle architectures become complex paper present algorithm implementation dynet toolkit automatically batching operations developers simply write minibatch computations aggregations single instance computations batching algorithm seamlessly executes fly computationally efficient batches variety tasks obtain throughput similar manual batches well comparable speedups single instance learning architectures impractical batch manually
learning affinity via spatial propagation networks paper propose spatial propagation networks learning affinity matrix show constructing row column linear propagation model spatially variant transformation matrix constitutes affinity matrix models dense global pairwise similarities image specifically develop way connection linear propagation model formulates sparse transformation matrix elements output deep cnn results dense affinity matrix effective model task specific pairwise similarity instead designing similarity kernels according image features points directly output similarities pure data driven manner spatial propagation network generic framework applied numerous tasks traditionally benefit designed affinity image matting colorization guided filtering name furthermore model also learn semantic aware affinity high level vision tasks due learning capability deep model validate proposed framework refinement object segmentation experiments helen face parsing pascal voc 2012 semantic segmentation tasks show spatial propagation network provides general effective efficient solutions generating high quality segmentation results
supervised adversarial domain adaptation work provides framework addressing problem supervised domain adaptation deep models main idea exploit adversarial learning learn embedded subspace simultaneously maximizes confusion domains semantically aligning embedded versions supervised setting becomes attractive especially target data samples need labeled scenario alignment separation semantic probability distributions difficult lack data found carefully designing training scheme whereby typical binary adversarial discriminator augmented distinguish different classes possible effectively address supervised adaptation problem addition approach high speed adaptation requires extremely low number labeled target training samples even per category effective extensively compare approach state art domain adaptation experiments using datasets handwritten digit recognition using datasets visual object recognition
deep hyperspherical learning convolution inner product founding basis convolutional neural networks cnns key end end visual representation learning benefiting deeper architectures recent cnns demonstrated increasingly strong representation abilities despite improvement increased depth larger parameter space also led challenges properly training network light challenges propose hyperspherical convolution sphereconv novel learning framework gives angular representations hyperspheres introduce spherenet deep hyperspherical convolution networks distinct conventional inner product based convolutional networks particular spherenet adopts sphereconv basic convolution operator supervised generalized angular softmax loss natural loss formulation sphereconv show spherenet effectively encode discriminative representation alleviate training difficulty leading easier optimization faster convergence better classification performance convolutional counterparts also provide theoretical justifications advantages hyperspherical optimization experiments ablation studies verified conclusion
riemannian approach batch normalization batch normalization proven effective algorithm deep neural network training normalizing input neuron reducing internal covariate shift space weight vectors layer naturally interpreted riemannian manifold invariant linear scaling weights following intrinsic geometry manifold provides new learning rule efficient easier analyze also propose intuitive effective gradient clipping regularization methods proposed algorithm utilizing geometry manifold resulting algorithm consistently outperforms original various types network architectures datasets
backprop without learning rates coin betting deep learning methods achieve state art performance many application scenarios yet methods require significant amount hyperparameters tuning order achieve best results particular tuning learning rates stochastic optimization process still main bottlenecks paper propose new stochastic gradient descent procedure deep networks require learning rate setting contrary previous methods adapt learning rates make use assumed curvature objective function instead reduce optimization process game betting coin propose learning rate free optimal algorithm scenario theoretical convergence proven convex quasi convex functions empirical evidences show advantage algorithm popular stochastic gradient algorithms
convergence block coordinate descent training dnns tikhonov regularization lifting relu function higher dimensional space develop smooth multi convex formulation training feed forward deep neural networks dnns allows develop block coordinate descent bcd training algorithm consisting sequence numerically well behaved convex optimizations using ideas proximal point methods convex analysis prove bcd algorithm converge globally stationary point linear convergence rate order experiments mnist database dnns trained bcd algorithm consistently yielded better test set error rates identical dnn architectures tarined via stochastic gradient descent sgd variants caffe toolbox
collaborative deep learning fixed topology networks significant recent interest parallelize deep learning algorithms order handle enormous growth data model sizes advances focus model parallelization engaging multiple computing agents via using central parameter server aspect data parallelization along decentralized computation explored sufficiently context paper presents new consensus based distributed sgd cdsgd momentum variant cdmsgd algorithm collaborative deep learning fixed topology networks enables data parallelization well decentralized computation framework extremely useful learning agents access local private data communication constrained environment analyze convergence properties proposed algorithm strongly convex nonconvex objective functions fixed diminishing step sizes using concepts lyapunov function construction demonstrate efficacy algorithms comparison baseline centralized sgd recently proposed federated averaging algorithm also enables data parallelism based benchmark datasets mnist cifar cifar 100
regularization affects critical points linear networks paper concerned problem representing learning linear transformation using linear neural network recent years growing interest study networks part due successes deep learning main question body research also paper pertains existence optimality properties critical points mean squared loss function primary concern robustness critical points regularization loss function optimal control model introduced purpose learning algorithm regularized form backprop derived using hamilton formulation optimal control formulation used provide complete characterization critical points terms solutions nonlinear matrix valued equation referred characteristic equation analytical numerical tools bifurcation theory used compute critical points via solutions characteristic equation main conclusion critical point diagram fundamentally different even arbitrary small amounts regularization
predicting organic reaction outcomes weisfeiler lehman network prediction organic reaction outcomes fundamental problem computational chemistry since reaction involve hundreds atoms fully exploring space possible transformations intractable current solution utilizes reaction templates limit space suffers coverage efficiency issues paper propose template free approach efficiently explore space product molecules first pinpointing reaction center set nodes edges graph edits occur since small number atoms contribute reaction center directly enumerate candidate products generated candidates scored weisfeiler lehman difference network models high order interactions changes occurring nodes across molecule framework outperforms top performing template based approach margin running orders magnitude faster finally demonstrate model accuracy rivals performance domain experts
predicting scene parsing motion dynamics future important intelligent systems textit autonomous vehicles robotics anticipate future order plan early make decisions accordingly predicting future scene parsing motion dynamics helps agents understand visual environment better former provides dense semantic segmentations textit objects present later provides dense motion information textit objects move future paper propose novel model predict future scene parsing motion dynamics unobserved video frames simultaneously using history information preceding frames corresponding scene parsing results input model able predict scene parsing motion arbitrary time steps ahead importantly model superior compared methods predict parsing motion individually solve prediction tasks jointly fully exploit complementary relationship best knowledge paper first aiming learn predict future scene parsing motion dynamics simultaneously large scale cityscapes dataset demonstrated model produces significantly better parsing motion prediction compared well established baselines addition also present predict steering angle vehicles using model good results verify capability model learn underlying latent parameters
houdini democratizing adversarial examples generating adversarial examples critical step evaluating improving robustness learning machines far existing methods work classification designed alter true performance measure problem hand introduce novel flexible approach named houdini generating adversarial specifically tailored final performance measure task considered successfully apply houdini range applications speech recognition pose estimation
geometric matrix completion recurrent multi graph neural networks matrix completion models among common formulations recommender systems recent works showed boost performance techniques introducing pairwise relationships users items form graphs imposing smoothness priors graphs however techniques fully exploit local stationary structures user item graphs number parameters learn linear number users items propose novel approach overcome limitations using geometric deep learning graphs matrix completion architecture combines novel multi graph convolutional neural network learn meaningful statistical graph structured patterns users items recurrent neural network applies learnable diffusion score matrix neural network system computationally attractive requires constant number parameters independent matrix size apply method several standard datasets showing outperforms state art matrix completion techniques
compression aware training deep neural networks recent years great progress made variety application domains thanks development increasingly deeper neural networks unfortunately huge number units networks makes expensive computationally memory wise overcome exploiting fact deep networks parametrized several compression strategies proposed methods however typically start network trained standard manner without considering future compression paper propose explicitly account compression training process end introduce regularizer encourages parameter matrix layer low rank training show allows learn much compact yet least effective models state art compression techniques
non parametric neural networks deep neural networks dnns probabilistic graphical models pgms main tools statistical modeling dnns provide ability model rich complex relationships input independent output variables pgms provide ability encode dependencies among output variables end end training models structured graphical dependencies top independent neural predictions recently emerged principled ways combining paradigms types models proven powerful discriminative settings discrete outputs extensions structured continuous spaces well performing efficient inference spaces lacking propose non parametric neural networks n3s modular approach cleanly separates non parametric structured posterior representation discriminative inference scheme allows end end training components experiments evaluate ability n3s capture structured posterior densities modeling compute complex statistics densities inference compare model number baselines including popular variational sampling based inference schemes terms accuracy speed
gibbsnet iterative adversarial inference deep graphical models directed latent variable models formulate joint distribution advantage sampling fast exact yet weakness need specify often simple fixed prior limits expressiveness model undirected latent variable models discard requirement specified prior yet sampling generally requires iterative procedure blocked gibbs sampling require many steps achieve samples joint distribution propose novel approach learning joint distribution data latent code uses adversarially learned iterative procedure gradually refine joint distribution better match data distribution step gibbsnet best worlds theory practice achieving speed simplicity directed latent variable model guaranteed assuming adversarial game reaches virtual training criteria global minimum produce samples sampling iterations achieving expressiveness flexibility undirected latent variable model gibbsnet away need explicit ability classification class conditional generation joint image attribute modeling single model trained specific tasks show empirically gibbsnet able learn complex show leads improved inpainting iterative refinement dozens steps stable generation without collapse thousands steps despite trained steps
exploring generalization deep learning goal understanding drives generalization deep networks consider several recently suggested explanations including norm based control sharpness robustness study measures ensure generalization highlighting importance scale normalization making connection sharpness pac bayes theory investigate well measures explain different observed phenomena
regularizing deep neural networks noise interpretation optimization overfitting critical challenges deep neural networks various types regularization methods improve generalization performance injecting noises hidden units training dropout known successful regularizer still clear enough training techniques work well practice maximize benefit presence conflicting objectives optimizing true data distribution preventing overfitting regularization paper addresses issues interpreting conventional training methods regularization noise injection optimize lower bound true objective proposing technique achieve tighter lower bound using multiple noise samples per mini batch demonstrate effectiveness idea several computer vision applications
extracting low dimensional dynamics multiple large scale neural population recordings learning predict correlations powerful approach understanding neural population dynamics extract low dimensional trajectories population recordings using dimensionality reduction methods current approaches dimensionality reduction neural data limited single population recordings identify dynamics embedded across multiple measurements propose approach extracting low dimensional dynamics multiple sequential recordings algorithm scales data comprising millions observed dimensions making possible access dynamics distributed across large populations multiple brain areas building subspace identification approaches dynamical systems perform parameter estimation minimizing moment matching objective using scalable stochastic gradient descent algorithm model optimized predict temporal covariations across neurons across time show approach naturally handles missing data multiple partial recordings identify dynamics predict correlations even presence severe subsampling small overlap recordings demonstrate effectiveness approach simulated data whole brain larval zebrafish imaging dataset
adaptive sampling population neurons adaptive sampling methods neuroscience primarily focused maximizing firing rate single recorded neuron recording neurons usually possible find single stimulus maximizes firing rates neurons motivates objective function takes account recorded population neurons together propose adept adaptive sampling method optimize population objective functions simulated experiments first confirmed population objective functions elicited varied stimulus responses single neuron objective functions tested adept closed loop electrophysiological experiment population activity recorded macaque cortical area known mid level visual processing adept uses outputs deep convolutional neural network model feature embeddings predict neural responses adept elicited mean stimulus responses larger randomly chosen natural images well larger scatter stimulus responses adaptive sampling methods enable new scientific discoveries recording population neurons heterogeneous response properties
onacid online analysis calcium imaging data real time optical imaging methods using calcium indicators critical monitoring activity large neuronal populations vivo imaging experiments typically generate large amount data needs processed extract activity imaged neuronal sources deriving processing algorithms active area research existing methods require processing large amounts data time rendering vulnerable volume recorded data preventing real time experimental interrogation introduce onacid online framework analysis streaming calcium imaging data including motion artifact correction neuronal source extraction iii activity denoising deconvolution approach combines extends previous work online dictionary learning calcium imaging data analysis deliver automated pipeline discover track activity hundreds cells real time thereby enabling new types closed loop experiments apply algorithm large scale experimental datasets benchmark performance manually annotated data show outperforms popular offline approach
detrended partial cross correlation brain connectivity analysis brain connectivity analysis critical component ongoing human connectome projects decipher healthy diseased brain recent work highlighted power law multi time scale properties brain signals however remains lack methods specifically quantify short long range brain connections paper using detrended partial cross correlation analysis dpcca propose novel functional connectivity measure delineate brain interactions multiple time scales controlling covariates use rich simulated fmri data validate proposed method apply real fmri data cocaine dependence prediction task show compared extant methods dpcca based approach distinguishes short long range functional connectivity also improves feature extraction subsequently increasing classification accuracy together paper contributes broadly new computational methodologies understand neural information processing
practical bayesian optimization model fitting bayesian adaptive direct search computational models fields computational neuroscience often evaluated via stochastic simulation numerical approximation fitting models implies difficult optimization problem complex possibly noisy parameter landscapes bayesian optimization successfully applied solving expensive black box problems engineering machine learning explore whether applied general tool model fitting first present novel algorithm bayesian adaptive direct search bads achieves competitive performance affordable computational overhead running time typical models perform extensive benchmark bads many common state art nonconvex derivative free optimizers set model fitting problems real data models studies behavioral cognitive computational neuroscience default settings bads consistently finds comparable better solutions methods showing great promise bads particular general model fitting tool
error detection correction framework connectomics significant advances made recent years problem neural circuit reconstruction electron microscopic imagery improvements image acquisition image alignment boundary detection greatly reduced achievable error rate order make progress argue automated error detection essential focussing effort attention human machine paper report use automated error detection attention signal flood filling error correction module demonstrate significant improvements upon state art segmentation performance
cake effective brain connectivity causal kernels fundamental goal network neuroscience understand activity region drives activity elsewhere process referred effective connectivity propose model causal interaction using integro differential equations causal kernels allow rich analysis effective connectivity approach combines tractability flexibility autoregressive modeling biophysical interpretability dynamic causal modeling causal kernels learned nonparametrically using gaussian process regression yielding efficient framework causal inference construct novel class causal covariance functions enforce desired properties causal kernels approach call cake construction model hyperparameters biophysical meaning therefore easily interpretable demonstrate efficacy cake number simulations give example realistic application magnetoencephalography meg data
learning neural representations human cognition across many fmri studies cognitive neuroscience enjoying rapid increase extensive public brain imaging datasets opening door design deploy large scale statistical models targeting unified perspective available data implies finding scalable automated solutions old challenge aggregate heterogeneous information brain function universal cognitive system relates psychological behavior brain networks cast challenge machine learning approach predict conditions statistical brain maps across different studies leverage multi task learning multi scale dimension reduction learn low dimensional representations brain images carry robust cognitive information robustly associated psychological stimuli multi dataset classification model achieves best prediction performance several large reference datasets compared models forgo learning cognitive aware low dimension representation brings substantial performance boost analysis small datasets introspected identify universal template cognitive concepts
mapping distinct timescales functional interactions among brain networks brain processes occur various timescales ranging milliseconds neurons minutes hours behavior characterizing functional coupling among brain regions diverse timescales key understanding brain produces behavior apply instantaneous lag based measures conditional linear dependence based granger geweke causality infer network connections distinct timescales functional magnetic resonance imaging fmri data due slow sampling rate fmri widely held produces spurious unreliable estimates functional connectivity applied fmri data challenge claim combining simulations novel machine learning approach first show simulated fmri data instantaneous lag based identify distinct timescales complementary patterns functional connectivity next analyzing fmri recordings 500 human subjects show linear classifier trained either instantaneous lag based connectivity reliably distinguishes task versus rest brain states cross validation accuracy importantly instantaneous lag based exploit markedly different spatial temporal patterns connectivity achieve robust classification approach provides novel framework uncovering validating functionally connected networks operate distinct timescales brain
robust estimation neural signals calcium imaging calcium imaging prominent technology neuroscience research allows simultaneous recording large numbers neurons awake animals automated extraction neurons temporal activity imaging datasets important step path producing neuroscience results however nearly imaging datasets typically contain gross contaminating sources could contributed technology used underlying biological tissue although attempts made better extract neural signals limited gross contamination scenarios effort address contamination full generality statistical estimation work proceed new direction propose extract cells activity using robust estimation derive optimal robust loss based simple abstraction calcium imaging data also find simple practical optimization routine loss provably fast convergence use proposed robust loss matrix factorization framework extract neurons temporal activity calcium imaging datasets demonstrate superiority robust estimation approach existing methods simulated real datasets
learning morphology brain signals using alpha stable convolutional sparse coding neural time series data contain wide variety prototypical signal waveforms atoms significant importance clinical cognitive research goals analyzing data hence extract shift invariant atoms even though success reported existing algorithms limited applicability due heuristic nature moreover often vulnerable artifacts impulsive noise typically present raw neural recordings study address issues propose novel probabilistic convolutional sparse coding csc model learning shift invariant atoms raw neural signals containing potentially severe artifacts core model call αcsc lies family heavy tailed distributions called stable distributions develop novel computationally efficient monte carlo expectation maximization algorithm inference maximization step boils weighted csc problem develop computationally efficient optimization algorithm results show proposed algorithm achieves state art convergence speeds besides αcsc significantly robust artifacts compared competing algorithms extract spike bursts oscillations even reveal subtle phenomena cross frequency coupling applied noisy neural time series
streaming weak submodularity interpreting neural networks fly many machine learning applications important explain predictions black box classifier example deep neural network assign image particular class cast interpretability black box classifiers combinatorial maximization problem propose efficient streaming algorithm solve subject cardinality constraints extending ideas badanidiyuru 2014 provide constant factor approximation guarantee algorithm case random stream order weakly submodular objective function first theoretical guarantee general class functions also show algorithm exists worst case stream order algorithm obtains similar explanations inception predictions times faster state art lime framework ribeiro 2016
decomposable submodular function minimization discrete continuous paper investigates connections discrete continuous approaches decomposable submodular function minimization provide improved running time estimates state art continuous algorithms problem using combinatorial arguments also provide systematic experimental comparison types methods based clear distinction level level algorithms
differentiable learning submodular functions incorporate discrete optimization algorithms within modern machine learning models example possible use deep architectures layer whose output minimal cut parametrized graph given models trained end end leveraging gradient information introduction layers seems challenging due non continuous output paper focus problem submodular minimization show layers indeed possible key idea continuously relax output without sacrificing guarantees provide easily computable approximation jacobian complemented complete theoretical analysis finally contributions let experimentally learn probabilistic log supermodular models via level variational inference formulation
robust optimization non convex objectives consider robust optimization problems goal optimize worst case class objective functions develop reduction robust improper optimization bayesian optimization given oracle returns approximate solutions distributions objectives compute distribution solutions approximate worst case show derandomizing solution hard general done broad class statistical learning tasks apply results robust neural network training submodular optimization evaluate approach experimentally character classification task subject adversarial distortion robust influence maximization large networks
optimization landscape tensor decompositions non convex optimization local search heuristics widely used machine learning achieving many state art results becomes increasingly important understand work hard problems typical data landscape many objective functions learning conjectured geometric property local optima approximately global optima thus solved efficiently local search algorithms however establishing property difficult paper analyze optimization landscape random complete tensor decomposition problem many applications unsupervised leaning especially learning latent variable models practice efficiently solved gradient ascent non convex objective show small constant among set points function values factor larger expectation function local maxima approximate global maxima previously best known result characterizes geometry small neighborhoods around true components result implies even initialization barely better random guess gradient ascent algorithm guaranteed solve problem main technique uses kac rice formula random matrix theory best knowledge first time kac rice formula successfully applied counting number local minima highly structured random polynomial dependent coefficients
gradient descent take exponential time escape saddle points although gradient descent almost always escapes saddle points asymptotically lee 2016 paper shows even fairly natural random initialization schemes non pathological functions significantly slowed saddle points take exponential time escape hand gradient descent perturbations 2015 jin 2017 slowed saddle points find approximate local minimizer polynomial time result concludes gradient descent inherently slower justifies importance adding perturbations efficient non convex optimization experiments also provided demonstrate theoretical findings
convolutional phase retrieval study convolutional phase retrieval problem asks recover unknown signal length measurements consisting magnitude cyclic convolution known kernel length model motivated applications channel estimation optics underwater acoustic communication signal interest acted given channel filter phase information difficult impossible acquire show random mathbf efficiently recovered global phase using combination spectral initialization generalized gradient descent main challenge coping dependencies measurement operator overcome challenge using ideas decoupling theory suprema chaos processes restricted isometry property random circulant matrices recent analysis alternating minimizing methods
implicit regularization matrix factorization study implicit regularization optimizing underdetermined quadratic objective matrix gradient descent factorization conjecture provide empirical theoretical evidence small enough step sizes initialization close enough origin gradient descent full dimensional factorization converges minimum nuclear norm solution
near linear time approximation algorithms optimal transport via sinkhorn iteration computing optimal transport distances earth mover distance fundamental problem machine learning statistics computer vision despite recent introduction several algorithms good empirical performance unknown whether general optimal transport distances approximated near linear time paper demonstrates ambitious goal fact achieved cuturi sinkhorn distances provides guidance towards parameter tuning algorithm result relies new analysis sinkhorn iterations also directly suggests new algorithm greenkhorn theoretical guarantees numerical simulations clearly illustrate greenkhorn significantly outperforms classical sinkhorn algorithm practice
frank wolfe equilibrium computation consider frank wolfe method constrained convex optimization first order projection free procedure show algorithm recast different light emerging special case particular meta algorithm computing equilibria saddle points convex concave sum games equilibrium computation trick relies existence regret online learning generate sequence iterates also provide proof convergence vanishing regret show stated equivalence several nice properties particularly exhibits modularity gives rise various old new algorithms explore resulting methods provide experimental results demonstrate correctness efficiency
greedy algorithms cone constrained optimization convergence guarantees greedy optimization methods matching pursuit frank wolfe algorithms regained popularity recent years due simplicity effectiveness theoretical guarantees address optimization textit linear span textit convex hull set atoms respectively paper consider intermediate case optimization textit convex cone parametrized conic hull generic atom set leading first principled definitions non negative algorithms give explicit convergence rates demonstrate excellent empirical performance novel algorithms analysis tailored particular function atom set particular derive sublinear convergence general smooth convex objectives linear convergence strongly convex objectives cases general sets atoms furthermore establish clear correspondence algorithms known algorithms literature novel algorithms analyses target general atom sets general objective functions hence directly applicable large variety learning settings
cyclic coordinate descent beats randomized coordinate descent coordinate descent methods seen resurgence recent interest applicability machine learning well large scale data analysis superior empirical performance methods variants cyclic coordinate descent ccd randomized coordinate descent rcd deterministic randomized versions methods light recent results literature common perception rcd always dominates ccd terms performance paper question perception provide examples generally problem classes ccd deterministic order faster rcd terms asymptotic worst case convergence furthermore provide lower upper bounds amount improvement rate deterministic relative rcd amount improvement depend deterministic order used also provide characterization best deterministic order leads maximum improvement convergence rate terms combinatorial properties hessian matrix objective function
linear convergence frank wolfe type algorithm trace norm balls propose rank variant classical frank wolfe algorithm solve convex minimization trace norm ball algorithm replaces top singular vector computation svd frank wolfe top singular vector computation svd done repeatedly applying svd times algorithm linear convergence rate objective function smooth strongly convex optimal solution rank improves convergence rate total complexity frank wolfe method variants
adaptive accelerated gradient converging method lderian error bound condition recent studies shown proximal gradient method accelerated gradient method apg restarting enjoy linear convergence weaker condition strong convexity namely quadratic growth condition qgc however faster convergence restarting apg method relies potentially unknown constant qgc appropriately restart apg restricts applicability address issue developing novel adaptive gradient converging methods leveraging magnitude proximal gradient criterion restart termination analysis extends much general condition beyond qgc namely lderian error bound heb condition key technique development novel synthesis adaptive regularization conditional restarting scheme extends previous work focusing strongly convex problems much broader family problems furthermore demonstrate results important implication applications machine learning objective function coercive semi algebraic convergence speed essentially total number iterations objective function consists huber norm regularization convex smooth piecewise quadratic loss square loss squared hinge loss huber loss proposed algorithm parameter free enjoys faster linear convergence without assumptions restricted eigen value condition notable linear convergence results aforementioned problems global instead local best knowledge improved results first shown work
searching dark practical svrg methods error bound conditions guarantee paper develops practical stochastic variance reduced gradient svrg methods error bound conditions theoretical guarantee error bound conditions inherent property optimization problem recently revived optimization developing fast algorithms improved global convergence without strong convexity particular condition interest quadratic error bound aka second order growth condition weaker strong convexity leveraged developing linear convergence many gradient proximal gradient methods several recent studies also derived linear convergence quadratic error bound condition stochastic variance reduced gradient method important milestone stochastic optimization solving machine learning problems however studies overlooked critical issue algorithmic dependence unknown parameter analogous strong convexity modulus error bound conditions usually difficult estimate therefore makes algorithm practical solving many interesting machine learning problems address issue propose novel techniques automatically search unknown parameter fly optimization maintaining almost convergence rate oracle setting assuming involved parameter given
geometric descent method convex composite minimization paper extend geometric descent method recently proposed bubeck lee singh tackle nonsmooth strongly convex composite problems prove proposed algorithm dubbed geometric proximal gradient method geopg converges linear rate
condition number problem numerical results linear regression logistic regression elastic net regularization show geopg compares favorably nesterov accelerated proximal gradient method especially problem ill conditioned
faster non ergodic stochastic alternating direction method multipliers study stochastic convex optimization subjected linear equality constraints traditional stochastic alternating direction method multipliers nesterov acceleration scheme achieve ergodic sqrt convergence rates number iteration introducing variance reduction techniques convergence rates improve ergodic paper propose new stochastic admm elaborately integrates nesterov extrapolation techniques nesterov extrapolation algorithm achieve non ergodic convergence rate optimal separable linearly constrained non smooth convex problems convergence rates based admm methods actually tight sqrt non ergodic sense best knowledge first work achieves truly accelerated stochastic convergence rate constrained convex problems experimental results demonstrate algorithm significantly faster existing state art stochastic admm methods
doubly accelerated stochastic variance reduced dual averaging method regularized empirical risk minimization develop new accelerated stochastic gradient method efficiently solving convex regularized empirical risk minimization problem mini batch settings use mini batches becoming golden standard machine learning community mini batch settings stabilize gradient estimate easily make good use parallel computing core proposed method incorporation new double acceleration technique variance reduction technique theoretically analyze proposed method show method much improves mini batch efficiencies previous accelerated stochastic methods essentially needs mini batches achieving optimal iteration complexities non strongly strongly convex objectives training set size show even non mini batch settings method surpasses best known convergence rate non strongly convex objectives achieves strongly convex objectives
limitations variance reduction acceleration schemes finite sums optimization study conditions able efficiently apply variance reduction acceleration schemes finite sums problems first show perhaps surprisingly finite sum structure sufficient obtaining complexity bound
smooth strongly convex finite sums must also know exactly individual function referred oracle iteration next show broad class first order coordinate descent finite sums algorithms including sdca svrg sag possible get accelerated complexity bound unless strong convexity parameter given explicitly lastly show class algorithms used minimizing
smooth non strongly convex finite sums optimal complexity bound
nonlinear acceleration stochastic algorithms extrapolation methods use last iterates optimization algorithm produce better estimate optimum shown achieve optimal convergence rates deterministic setting using simple gradient iterates study extrapolation methods stochastic setting iterates produced either simple accelerated stochastic gradient algorithm first derive convergence bounds arbitrary potentially biased perturbations produce asymptotic bounds using ratio variance noise accuracy current point finally apply acceleration technique stochastic algorithms sgd saga svrg katyusha different settings show significant performance gains
acceleration averaging stochastic descent dynamics formulate study general family continuous time stochastic dynamics accelerated first order minimization smooth convex functions building averaging formulation accelerated mirror descent propose stochastic variant gradient contaminated noise study resulting stochastic differential equation prove bound rate change energy function associated problem use derive estimates convergence rates function values expectation persistent asymptotically vanishing noise discuss interaction parameters dynamics learning rate averaging weights variation noise process show particular asymptotic rate variation affects choice parameters ultimately convergence rate
multiscale semi markov dynamics intracortical brain computer interfaces intracortical brain computer interfaces allow people tetraplegia control computer cursor imagining motion paralyzed limbs standard decoders derived kalman filter assumes markov dynamics angle intended movement unimodal likelihood channel neural activity due errors made decoding noisy neural data user attempts move cursor goal angle cursor goal positions change rapidly propose dynamic bayesian network includes screen goal position part latent state thus allows motion cues aggregated much longer history neural activity multiscale model explicitly captures relationship instantaneous angles motion long term goals incorporates semi markov dynamics motion trajectories also propose flexible likelihood model recordings neural populations offline experiments recorded neural data demonstrate significantly improved prediction motion directions compared kalman filter baselines derive efficient online inference algorithm enabling clinical trial participant tetraplegia control computer cursor neural activity real time
eeg graph factor graph based model capturing spatial temporal observational relationships electroencephalograms paper reports factor graph based model brain activity jointly describes instantaneous observation based temporal spatial dependencies factor functions represent dependencies defined manually based domain knowledge model validated using clinically collected intracranial electroencephalogram eeg data epilepsy patients application seizure onset localization results indicate model outperforms conventional approaches devised using observational dependency alone better auc furthermore also show manual definitions factor functions allow solve graph inference exactly using graph cut algorithm experiments show proposed inference technique provides gain auc compared sampling based alternatives
asynchronous parallel coordinate minimization map inference finding maximum posteriori map assignment central task graphical models since modern applications give rise large problem instances increasing need efficient solvers work propose improve efficiency coordinate minimization based dual decomposition solvers running updates asynchronously parallel case message passing inference performed multiple processing units simultaneously without coordination reading writing shared memory analyze convergence properties resulting algorithms identify settings speedup gains expected numerical evaluations show approach indeed achieves significant speedups common computer vision tasks
speeding latent variable gaussian graphical model estimation via nonconvex optimization study estimation latent variable gaussian graphical model lvggm precision matrix superposition sparse matrix low rank matrix order speed estimation sparse plus low rank components propose sparsity constrained maximum likelihood estimator based matrix factorization efficient alternating gradient descent algorithm hard thresholding solve algorithm orders magnitude faster convex relaxation based methods lvggm addition prove algorithm guaranteed linearly converge unknown sparse low rank components optimal statistical precision experiments synthetic genomic data demonstrate superiority algorithm state art algorithms corroborate theory
expxorcist nonparametric graphical models via conditional exponential densities non parametric multivariate density estimation faces strong statistical computational bottlenecks practical approaches impose near parametric assumptions form density functions paper leverage recent developments propose class non parametric models attractive computational statistical properties approach relies simple function space assumption conditional distribution variable conditioned variables non parametric exponential family form
reducing reparameterization gradient variance optimization noisy gradients become ubiquitous statistics machine learning reparameterization gradients gradient estimates computed via reparameterization trick represent class noisy gradients often used monte carlo variational inference mcvi however gradient estimators noisy optimization procedure slow fail converge way reduce noise use samples gradient estimate computationally expensive instead view noisy gradient random variable form inexpensive approximation generating procedure gradient sample approximation high correlation noisy gradient construction making useful control variate variance reduction demonstrate approach non conjugate multi level hierarchical models bayesian neural net observed gradient variance reductions multiple orders magnitude 000
robust conditional probabilities conditional probabilities core concept machine learning example optimal prediction label given input corresponds maximizing conditional probability common approach inference tasks learning model conditional probabilities however models often based strong assumptions log linear models hence estimate conditional probabilities robust highly dependent validity assumptions propose framework reasoning conditional probabilities without assuming anything underlying distributions except knowledge second order marginals estimated data show setting leads guaranteed bounds conditional probabilities calculated efficiently variety settings including structured prediction finally apply semi supervised deep learning obtaining results competitive variational autoencoders
stein variational gradient descent gradient flow stein variational gradient descent svgd deterministic sampling algorithm iteratively transports set particles approximate given distributions based efficient gradient based update guarantees optimally decrease divergence within function space paper develops first theoretical analysis svgd establish empirical measures svgd samples weakly converges target distribution show asymptotic behavior svgd characterized nonlinear fokker planck equation known vlasov equation physics develop geometric perspective views svgd gradient flow divergence functional new metric structure space distributions induced stein operator
parallel streaming wasserstein barycenters efficiently aggregating data different sources challenging problem particularly samples source distributed differently differences inherent inference task present reasons sensors sensor network placed far apart affecting individual measurements conversely computationally advantageous split bayesian inference tasks across subsets data data need identically distributed across subsets principled way fuse probability distributions via lens optimal transport wasserstein barycenter single distribution summarizes collection input measures respecting geometry however computing barycenter scales poorly requires discretization input distributions barycenter improving situation present scalable communication efficient parallel algorithm computing wasserstein barycenter arbitrary distributions algorithm operate directly continuous input distributions optimized streaming data method even robust nonstationary input distributions produces barycenter estimate tracks input measures time algorithm semi discrete needing discretize barycenter estimate best knowledge also provide first bounds quality approximate barycenter discretization becomes finer finally demonstrate practical effectiveness method tracking moving distributions sphere well large scale bayesian inference task
aide algorithm measuring accuracy probabilistic inference algorithms approximate probabilistic inference algorithms central many fields examples include sequential monte carlo inference robotics variational inference machine learning markov chain monte carlo inference statistics key problem faced practitioners measuring accuracy approximate inference algorithm specific dataset existing techniques measuring inference accuracy often brittle specialized type inference algorithm paper introduces auxiliary inference divergence estimator aide algorithm measuring accuracy approximate inference algorithms aide based observation inference algorithms treated probabilistic models random variables used within inference algorithm viewed auxiliary variables view leads new estimator symmetric divergence output distributions inference algorithms paper illustrates application aide algorithms inference regression hidden markov dirichlet process mixture models experiments show aide captures qualitative behavior broad class inference algorithms detect failure modes inference algorithms missed standard heuristics
deep dynamic poisson factorization model new model named deep dynamic poisson factorization model analyzing sequential count vectors proposed paper model based poisson factor analysis method captures dependence among time steps neural networks representing implicit distributions local complicated relationship obtained local implicit distribution deep latent structure exploited get long time dependence variational inference latent variables gradient descent based loss functions derived variational distribution performed inference synthetic dataset real world dataset applied proposed model results show good predicting fitting performance interpretable latent structure
model shrinkage effect gamma process edge partition models edge partition model epm fundamental bayesian nonparametric model extracting overlapping structure binary matrix epm adopts gamma proces prior automatically shrink number active atoms however empirically found model shrinkage epm typically work appropriately leads overfitted solution analysis expectation epm intensity function suggested gamma priors epm hyperparameters disturb model shrinkage effect internal order ensure model shrinkage effect epm works appropriate manner proposed novel generative constructions epm cepm incorporating constrained gamma priors depm incorporating dirichlet priors instead gamma priors furthermore depm model parameters including infinite atoms prior could marginalized thus possible derive truly infinite depm idepm efficiently inferred using collapsed gibbs sampler experimentally confirmed model shrinkage proposed models works well idepm indicated state art performance generalization ability link prediction accuracy mixing efficiency convergence speed
model evidence nonequilibrium simulations marginal likelihood model evidence key quantity bayesian parameter estimation model comparison many probabilistic models computation marginal likelihood challenging involves sum integral enormous parameter space markov chain monte carlo mcmc powerful approach compute marginal likelihoods various mcmc algorithms evidence estimators proposed literature discuss use nonequilibrium techniques estimating marginal likelihood nonequilibrium estimators build recent developments statistical physics known annealed importance sampling ais reverse ais probabilistic machine learning introduce new estimators model evidence combine forward backward simulations show various challenging models new evidence estimators outperform forward reverse ais
nice adversarial training mcmc existing markov chain monte carlo mcmc methods either based general purpose domain agnostic schemes lead slow convergence require hand crafting problem specific proposals expert propose nice novel method train flexible parametric markov chain kernels produce samples desired properties first propose efficient likelihood free adversarial training method train markov chain mimic given data distribution leverage flexible volume preserving flows obtain parametric kernels mcmc using bootstrap approach show train efficient markov chains sample prescribed posterior distribution iteratively improving quality model samples nice provides first framework automatically design efficient domain specific mcmc proposals empirical results demonstrate nice combines strong guarantees mcmc expressiveness deep neural networks able significantly outperform competing methods hamiltonian monte carlo
identification gaussian process state space models gaussian process state space model gpssm non linear dynamical system unknown transition measurement mappings described gps research gpssms focussed state estimation problem however key challenge gpssms satisfactorily addressed yet system identification address challenge impose structured gaussian variational posterior distribution latent states parameterised recognition model form directional recurrent neural network inference structure allows recover posterior smoothed entire sequence data provide practical algorithm efficiently computing lower bound marginal likelihood using reparameterisation trick additionally allows arbitrary kernels used within gpssm demonstrate efficiently generate plausible future trajectories system seek model gpssm requiring small number interactions true system
streaming sparse gaussian process approximations sparse approximations gaussian process models provide suite methods enable models deployed large data regime enable analytic intractabilities sidestepped however field lacks principled method handle streaming data posterior distribution function values hyperparameters updated online fashion small number existing approaches either use suboptimal hand crafted heuristics hyperparameter learning suffer catastrophic forgetting slow updating new data arrive paper develops new principled framework deploying gaussian process probabilistic models streaming setting providing principled methods learning hyperparameters optimising pseudo input locations proposed framework experimentally validated using synthetic real world datasets
bayesian optimization gradients bayesian optimization shown success global optimization expensive evaluate multimodal objective functions however unlike optimization methods bayesian optimization typically use derivative information paper show bayesian optimization exploit derivative information find good solutions fewer objective function evaluations particular develop novel bayesian optimization algorithm derivative enabled knowledge gradient dkg step bayes optimal asymptotically consistent provides greater step value information derivative free setting dkg accommodates noisy incomplete derivative information comes sequential batch forms optionally reduce computational cost inference automatically selected retention single directional derivative also compute dkg acquisition function gradient using novel fast discretization free technique show dkg provides state art performance compared wide range optimization procedures without gradients benchmarks including logistic regression deep learning kernel learning nearest neighbors
variational inference gaussian process models linear complexity large scale gaussian process inference long faced practical challenges due time space complexity superlinear dataset size sparse variational gaussian process models capable learning large scale data standard strategies sparsifying model prevent approximation complex functions work propose novel variational gaussian process model decouples representation mean covariance functions reproducing kernel hilbert space show new parametrization generalizes previous models yields variational inference problem solved stochastic gradient ascent time space complexity linear number mean function parameters strategy makes adoption large scale expressive gaussian process models possible run several experiments regression tasks show decoupled approach greatly outperforms previous sparse variational gaussian process inference procedures
efficient modeling latent information supervised learning using gaussian processes often machine learning data collected combination multiple conditions voice recordings multiple persons labeled could build model captures latent information related conditions generalize new data present new model called latent variable multiple output gaussian processes lvmogp allows jointly model multiple conditions regression generalize new condition data points test time lvmogp infers posteriors gaussian processes together latent space representing information different conditions derive efficient variational inference method lvmogp computational complexity low sparse gaussian processes show lvmogp significantly outperforms related gaussian process methods various tasks synthetic real data
non stationary spectral kernels propose non stationary spectral kernels gaussian process regression propose model spectral density non stationary kernel function mixture input dependent gaussian process frequency density surfaces solve generalised fourier transform model present family non stationary non monotonic kernels learn input dependent potentially long range non monotonic covariances inputs derive efficient inference using model whitening marginalized posterior show case studies kernels necessary modelling even rather simple time series image geospatial data non stationary characteristics
scalable log determinants gaussian process kernel learning applications varied bayesian neural networks determinantal point processes elliptical graphical models kernel learning gaussian processes gps must compute log determinant positive definite matrix derivatives leading prohibitive computations propose novel approaches estimating quantities fast matrix vector multiplications mvms stochastic approximations based chebyshev lanczos surrogate models converge quickly even kernel matrices challenging spectra leverage approximations develop scalable gaussian process approach kernel learning find lanczos generally superior chebyshev kernel learning surrogate approach highly efficient accurate popular kernels
spectral mixture kernels multi output gaussian processes initially multiple output gaussian processes mogps models relied linear transformations independent latent single output gaussian processes gps resulted cross covariance functions limited parametric interpretation thus conflicting single output gps intuitive understanding lengthscales frequencies magnitudes name contrary current approaches mogp able better interpret relationship different channels directly modelling cross covariances spectral mixture kernel phase shift propose parametric family complex valued cross spectral densities build cramer theorem multivariate version bochner theorem provide principled approach design multivariate covariance functions constructed kernels able model delays among channels addition phase differences thus expressive previous methods also providing full parametric interpretation relationship across channels proposed method validated synthetic data compared existing mogp methods real world examples
linearly constrained gaussian processes consider modification covariance function gaussian processes correctly account known linear constraints modelling target function transformation underlying function constraints explicitly incorporated model guaranteed fulfilled sample drawn prediction made also propose constructive procedure designing transformation operator illustrate result simulated real data examples
hindsight experience replay pieter abbeel wojciech zaremba
dealing sparse rewards biggest challenges reinforcement learning present novel technique called hindsight experience replay allows sample efficient learning rewards sparse binary therefore avoid need complicated reward engineering combined arbitrary policy algorithm seen form implicit curriculum demonstrate approach task manipulating objects robotic arm particular run experiments different tasks pushing sliding pick place case using binary rewards indicating whether task completed ablation studies show hindsight experience replay crucial ingredient makes training possible challenging environments show policies trained physics simulation deployed physical robot successfully complete task video presenting experiments available https goo smrqni
log normality skewness estimated state action values reinforcement learning overestimation state action values harmful reinforcement learning agents paper show state action value estimated using bellman equation decomposed weighted sum path wise values follow log normal distributions since log normal distributions skewed distribution estimated values also skewed leading imbalanced likelihood overestimation degree imbalance vary greatly among actions policies within problem instance making agent prone select actions policies inferior expected return higher likelihood overestimation present comprehensive analysis skewness examine factors impacts theoretical empirical results discuss possible ways reduce undesirable effect skewness
finite sample analysis gtd policy evaluation algorithms markov setting reinforcement learning key component policy evaluation aims estimate value function expected long term accumulated reward starting state given policy good policy evaluation method algorithms estimate value functions given policy accurately find better policy state space large continuous emph gradient based temporal difference gtd algorithms linear function approximation value function widely used considering collection evaluation data likely time reward consuming get clear understanding finite sample performance gtd algorithms important efficiency policy evaluation entire algorithms previous work converted gtd algorithms convex concave saddle point problem provided finite sample analysis gtd algorithms constant step size assumption data generated however know problems data generated markov processes rather step size set different ways paper realistic markov setting derive finite sample bounds expectation high probability general convex concave saddle point problem hence gtd algorithms bounds show markov setting variants step size gtd algorithms converge convergence rate determined step size related mixing time markov process explain experience reply trick effective since improve mixing property markov process best knowledge analysis first provide finite sample bounds gtd algorithms markov setting
inverse filtering hidden markov models paper considers related inverse filtering problems hidden markov models hmms given sequence state posteriors system dynamics estimate corresponding sequence observations estimate observation likelihoods iii jointly estimate observation likelihoods observation sequence problems motivated challenges reverse engineering sensors including calibration diagnostics show avoid computationally expensive mixed integer linear program milp exploiting structure hmm filter provide conditions quantities uniquely recovered finally also consider case posteriors corrupted noise shown problem naturally posed clustering problem proposed algorithm evaluated real world polysomnographic data used automatic sleep staging
safe model based reinforcement learning stability guarantees reinforcement learning powerful paradigm learning optimal policies experimental data however find optimal policies reinforcement learning algorithms explore possible actions harmful real world systems consequence learning algorithms rarely applied safety critical systems real world paper present learning algorithm explicitly considers safety terms stability guarantees specifically extend control theoretic results lyapunov stability verification show use statistical models dynamics obtain high performance control policies provable stability certificates moreover additional regularity assumptions terms gaussian process prior prove effectively safely collect data order learn dynamics thus improve control performance expand safe region state space experiments show resulting algorithm safely optimize neural network policy simulated inverted pendulum without pendulum ever falling
data efficient reinforcement learning continuous state action gaussian pomdps present data efficient reinforcement learning method continuous state action systems significant observation noise data efficient solutions small noise exist pilco learns cartpole swing task 30s pilco evaluates policies planning state trajectories using dynamics model however pilco applies policies observed state therefore planning observation space extend pilco filtering instead plan belief space consistent partially observable markov decisions process pomdp planning enables data efficient learning significant observation noise outperforming naive methods post hoc application filter policies optimised original unfiltered pilco algorithm test method cartpole swing task involves nonlinear dynamics requires nonlinear control
linear regression without correspondence article considers algorithmic statistical aspects linear regression correspondence covariates responses unknown first fully polynomial time approximation scheme given natural least squares optimization problem constant dimension next average case noise free setting responses exactly correspond linear function draws standard multivariate normal distribution efficient algorithm based lattice basis reduction shown exactly recover unknown linear function arbitrary dimension finally lower bounds signal noise ratio established approximate recovery unknown linear function
complexity learning neural networks stunning empirical successes neural networks currently lack rigorous theoretical eplanation form would explanation take face existing complexity theoretic lower bounds first step might show data generated neural networks single hidden layer smooth activation functions benign input distributions learned efficiently demonstrate comprehensive lower bound ruling possibility wide class activation functions including currently used inputs drawn logconcave distribution family hidden layer functions whose output sum gate hard learn precise sense statistical query algorithm includes known variants stochastic gradient descent loss function needs exponential number queries even using tolerance inversely proportional input dimensionality moreover hard family functions realizable small sublinear dimension number activation units single hidden layer lower bound also robust small perturbations true weights systematic experiments illustrate phase transition training error predicted analysis
near optimal sketching low rank tensor regression study least squares regression problem low rank tensor defined 1rθ1 vectors rpd small compared denotes outer product vectors linear function problem motivated fact number parameters 1dpd significantly smaller 1dpd number parameters ordinary least squares regression consider decomposition model tensors well tucker decomposition models show apply data dimensionality reduction techniques based sparse random projections reduce problem much smaller problem minθ holds simultaneously obtain significantly smaller dimension sparsity randomized linear mapping possible ordinary least squares regression finally give number numerical simulations supporting theory
input sparsity time possible kernel low rank approximation low rank approximation common tool used accelerate kernel methods kernel matrix approximated via rank matrix stored much less space processed quickly work study limits computationally efficient low rank kernel approximation show broad class kernels including popular gaussian polynomial kernels computing relative error rank approximation least difficult multiplying input data matrix arbitrary matrix barring breakthrough fast matrix multiplication large requires nnz time nnz number non zeros lower bound matches many parameter regimes recent work subquadratic time algorithms low rank approximation general kernels mm16 mm17 demonstrating algorithms unlikely significantly improved particular nnz input sparsity runtimes time hope show first time nnz time approximation possible general radial basis function kernels gaussian kernel closely related problem low rank approximation kernelized dataset
higher order total variation classes grids minimax theory trend filtering methods consider problem estimating values function nodes dimensional grid graph equal side lengths smash noisy observations function assumed smooth allowed exhibit different amounts smoothness different regions grid heterogeneity eludes classical measures smoothness nonparametric statistics holder smoothness meanwhile total variation smoothness classes allow heterogeneity restrictive another sense constant functions counted perfectly smooth achieve move past consider higher order classes based ways compiling discrete derivatives parameter across nodes relate classes holder classes derive minimax error rates higher order classes analyze naturally associated trend filtering methods seen optimal appropriate class
adaptive clustering semidefinite programming analyze clustering problem flexible probabilistic model aims identify optimal partition sample perform exact clustering high probability using convex semidefinite estimator interprets corrected relaxed version means estimator analyzed non asymptotic framework showed optimal near optimal recovering partition furthermore performances shown adaptive problem effective dimension well unknown number groups partition illustrate method performances comparison classical clustering algorithms numerical experiments simulated data
compressing gram matrix learning neural networks polynomial time consider problem learning function classes computed neural networks various activations relu sigmoid task believed intractable worst case major open problem understand minimal assumptions classes admit efficient algorithms work show natural distributional assumption eigenvalue decay gram matrix yields polynomial time algorithms non realizable setting expressive classes networks feed forward networks relus make assumptions network architecture labels given sufficiently strong polynomial eigenvalue decay obtain fully polynomial time algorithms parameters respect square loss milder decay also leads improved algorithms aware prior work assumption marginal distribution alone leads polynomial time algorithms networks relus even hidden layer unlike prior assumptions marginal distribution gaussian eigenvalue decay observed practice common data sets algorithm applies function class embedded suitable rkhs main technical contribution new approach proving generalization bounds kernelized regression using compression schemes opposed rademacher bounds general known sample complexity bounds kernel methods must depend norm corresponding rkhs quickly become large depending kernel function employed sidestep worst case bounds sparsifying gram matrix using recent work recursive nystrom sampling due musco musco prove approximate sparse hypothesis admits compression scheme whose true error depends rate eigenvalue decay
learning average top loss work introduce average top atk loss new ensemble loss supervised learning atk loss provides natural generalization widely used ensemble losses namely average loss maximum loss furthermore atk loss combines advantages alleviate corresponding drawbacks better adapt different data distributions show atk loss affords intuitive interpretation reduces penalty continuous convex individual losses correctly classified data atk loss lead convex optimization problems solved effectively conventional sub gradient based method study statistical learning theory matk establishing classification calibration statistical consistency matk provide useful insights practical choice parameter demonstrate applicability matk learning combined different individual loss functions binary multi class classification regression using synthetic real datasets
hierarchical clustering beyond worst case hiererachical clustering computing recursive partitioning dataset obtain clusters increasingly finer granularity fundamental problem data analysis although hierarchical clustering mostly studied procedures linkage algorithms top heuristics rather optimization problems recently dasgupta proposed objective function hierarchical clustering initiated line work developing algorithms explicitly optimize objective see also paper consider fairly general random graph model hierarchical clustering called hierarchical stochastic blockmodel hsbm show certain regimes svd approach mcsherry combined specific linkage methods results clustering give approximation dasgupta cost function also show approach based sdp relaxations balanced cuts based work makarychev combined recursive sparsest cut algorithm dasgupta yields approximation slightly larger regimes also semi random setting adversary remove edges random graph generated according hsbm finally report empirical evaluation synthetic real world data showing proposed svd based method indeed achieve better cost widely used heurstics also results better classification accuracy underlying problem multi class classification
net trim convex pruning deep neural networks performance guarantee introduce analyze new technique model reduction deep neural networks large networks theoretically capable learning arbitrarily complex models overfitting model redundancy negatively affects prediction accuracy model variance net trim algorithm prunes sparsifies trained network layer wise removing connections layer solving convex optimization program program seeks sparse set weights layer keeps layer inputs outputs consistent originally trained model algorithms associated analysis applicable neural networks operating rectified linear unit relu nonlinear activation present parallel cascade versions algorithm latter achieve slightly simpler models generalization performance former computed distributed manner cases net trim significantly reduces number connections network also providing enough regularization slightly reduce generalization error also provide mathematical analysis consistency initial network retrained model analyze model sample complexity derive general sufficient conditions recovery sparse transform matrix single layer taking independent gaussian random vectors inputs show network response described using maximum number non weights per node weights learned slog samples
graph theoretic approach multitasking key feature neural network architectures ability support simultaneous interaction among large numbers units learning processing representations however richness interactions trades ability network simultaneously carry multiple independent processes salient limitation many domains human cognition remains largely unexplored paper use graph theoretic analysis network architecture address question tasks represented edges bipartite graph define new measure multitasking capacity networks based assumptions tasks emph need multitasked rely independent resources form matching tasks emph performed without interference form induced matching main result inherent tradeoff multitasking capacity average degree network holds emph regardless network architecture results also extended networks depth greater
positive side demonstrate networks random like locally sparse desirable multitasking properties results shed light parallel processing limitations neural systems provide insights useful analysis design parallel architectures
information theoretic analysis generalization capability learning algorithms derive upper bounds generalization error learning algorithm terms mutual information input output upper bounds provide theoretical guidelines striking right balance data fit generalization controlling input output mutual information learning algorithm results also used analyze generalization capability learning algorithms adaptive composition bias accuracy tradeoffs adaptive data analytics work extends leads nontrivial improvements recent results russo zou
independence clustering without matrix independence clustering problem considered following formulation given set random variables required find finest partitioning clusters clusters mutually independent since mutual independence target pairwise similarity measurements use thus traditional clustering algorithms inapplicable distribution random variables general unknown sample available thus problem cast terms time series forms sampling considered stationary time series main emphasis latter general case consistent computationally tractable algorithm settings proposed number fascinating open directions research outlined
polynomial codes optimal design high dimensional coded matrix multiplication consider large scale matrix multiplication problem computation carried using distributed system master node multiple worker nodes worker store parts input matrices propose computation strategy leverages ideas coding theory design intermediate computations worker nodes order efficiently deal straggling workers proposed strategy named emph polynomial codes achieves optimum recovery threshold defined minimum number workers master needs wait order compute output furthermore leveraging algebraic structure polynomial codes map reconstruction problem final output polynomial interpolation problem solved efficiently polynomial codes provide order wise improvement state art terms recovery threshold also optimal terms several metrics furthermore extend code distributed convolution show order wise optimality
estimating mutual information discrete continuous mixtures estimation mutual information observed samples basic primitive machine learning useful several learning tasks including correlation mining information bottleneck chow liu tree conditional independence testing causal graphical models mutual information quantity well defined general probability spaces estimators developed special case discrete continuous pairs random variables estimators operate using principle calculating differential entropies pair however general mixture spaces individual entropies well defined even though mutual information paper develop novel estimator estimating mutual information discrete continuous mixtures prove consistency estimator theoretically well demonstrate excellent empirical performance problem relevant wide array applications variables discrete continuous others mixture continuous discrete components
best response regression regression task predictor given set instances along real value point subsequently identify value new instance accurately possible work initiate study strategic predictions machine learning consider regression task tackled players payoff player proportion points predicts accurately player first revise probably approximately correct learning framework deal case duel predictors devise algorithm finds linear regression predictor best response necessarily linear regression algorithm show linearithmic sample complexity polynomial time complexity dimension instances domain fixed also test approach high dimensional setting show significantly defeats classical regression algorithms prediction duel together work introduces novel machine learning task lends well current competitive online settings provides theoretical foundations illustrates applicability
statistical cost sharing study cost sharing problem cooperative games situations cost function available via oracle queries must instead learned samples drawn distribution represented tuples different subsets players formalize approach call statistical cost sharing consider computation core shapley value expanding work balcan 2015 give precise sample complexity bounds computing cost shares satisfy core property high probability function non empty core shapley value never studied setting show submodular cost functions curvature bounded curvature approximated samples uniform distribution factor bound tight define statistical analogues shapley axioms derive notion statistical shapley value approximated arbitrarily well samples distribution function
sample complexity measure applications learning optimal auctions introduce new sample complexity measure refer split sample growth rate hypothesis sample size split sample growth rate counts many different hypotheses empirical risk minimization output sub sample size show expected generalization error upper bounded log result enabled strengthening rademacher complexity analysis expected generalization error show sample complexity measure greatly simplifies analysis sample complexity optimal auction design many auction classes studied literature sample complexity derived solely noticing auction classes erm sample sub sample pick parameters equal points sample
multiplicative weights update constant step size congestion games convergence limit cycles chaos multiplicative weights update mwu method ubiquitous meta algorithm works follows distribution maintained certain set step probability assigned action multiplied cost action rescaled ensure new values form distribution analyze mwu congestion games agents use textit arbitrary admissible constants learning rates prove convergence textit exact nash equilibria interestingly convergence result carry nearly homologous mwu variant step probability assigned action multiplied even innocuous case agent strategy load balancing games dynamics provably lead limit cycles even chaotic behavior
efficiency guarantees data analysis efficiency outcomes game theoretic settings main item study intersection economics computer science notion price anarchy takes worst case stance efficiency analysis considering instance independent guarantees efficiency propose data dependent analog price anarchy refines worst case assuming access samples strategic behavior focus auction settings latter non trivial due private information held participants approach bounding efficiency data robust statistical errors mis specification unlike traditional econometrics seek learn private information players observed behavior analyze properties outcome directly quantify inefficiency without going private information apply approach datasets sponsored search auction system find empirical results significant improvement bounds worst case analysis
safe nested subgame solving imperfect information games unlike perfect information games imperfect information games cannot solved decomposing game subgames solved independently thus computationally intensive equilibrium finding techniques used decisions must consider strategy game whole possible solve imperfect information game exactly decomposition possible approximate solutions improve existing solutions solving disjoint subgames process referred subgame solving introduce subgame solving techniques outperform prior methods theory practice also show adapt past subgame solving techniques respond opponent actions outside original action abstraction significantly outperforms prior state art approach action translation finally show subgame solving repeated game progresses tree leading significantly lower exploitability applied techniques develop first defeat top humans heads limit texas hold poker
primer optimal transport optimal transport provides powerful flexible way compare probability measures discrete continuous includes therefore point clouds histograms datasets parametric generative models originally proposed eighteenth century theory later led nobel prizes koopmans kantorovich well villani fields medal 2010 recently reached machine learning community tackle challenging learning scenarios including dimensionality reduction structured prediction problems involve histogram outputs estimation generative models gans highly degenerate high dimensional problems despite recent successes bringing theory practice remains challenging machine learning community mathematical formality tutorial introduce approachable way crucial theoretical computational algorithmic practical aspects needed machine learning applications
fast black box variational inference stochastic trust region optimization introduce trustvi fast second order algorithm black box variational inference based trust region optimization reparameterization trick iteration trustvi proposes assesses step based minibatches draws variational distribution algorithm provably converges stationary point implement trustvi stan framework compare advi trustvi typically converges tens iterations solution least good advi reaches thousands iterations trustvi iterations computationally expensive total computation typically order magnitude less experiments
optimal transport machine learning optimal transport gradually establishing powerful essential tool compare probability measures machine learning take form point clouds histograms bags features generally datasets compared probability densities generative models traced back early work monge later kantorovich dantzig birth linear programming mathematical theory produced several important developments since crowned cédric villani fields medal 2010 transitioning applied spheres including recent applications machine learning tackle challenging learning scenarios including dimensionality reduction structured prediction problems involve histograms estimation generative models highly degenerate high dimensional problems workshop follow organized years ago nips 2014 seek amplify trend provide audience update recent successes brought forward efficient solvers innovative applications long list invited talks add contributed presentations oral needed posters finally panel invited speakers take questions audience formulate nuanced opinions nascent field
bayesian optimization science engineering ruben martinez cantin jose miguel hernández lobato javier gonzalez
bayesian optimization recent subfield machine learning comprising collection methodologies efficient optimization expensive black box functions techniques work fitting model black box function data using model predictions decide collect data next optimization problem solved using small number function evaluations resulting methods characterized high sample efficiency compared alternative black box optimization algorithms enabling solution new challenging problems example recent years become popular tool machine learning community excellent performance attained problem hyperparameter tuning important results academia industry success made crucial player current trend automatic machine learning new methods developed area applicability continuously expanding problem hyperparameter tuning permeates disciplines field moved towards specific problems science engineering requiring new advanced methodology today bayesian optimization promising approach accelerating automating science engineering therefore chosen year theme workshop bayesian optimization science engineering
opt 2017 optimization machine learning year marks major milestone history opt 10th anniversary edition long running nips workshop
previous opt workshops enjoyed packed overpacked attendance huge interest surprise optimization 2nd largest topic nips indeed foundational wider community
looking back past decade strong trend apparent intersection opt grown monotonically point several cutting edge advances optimization arise community distinctive feature optimization within departure textbook approaches particular different set goals driven big data models practical implementation crucial
revenue optimization approximate bid predictions context advertising auctions finding good reserve prices notoriously challenging learning problem due heterogeneity opportunity types non convexity objective function work show reduce reserve price optimization standard setting prediction squared loss well understood problem learning community bound gap expected bid revenue terms average loss predictor first result formally relates revenue gained quality standard machine learned model
multi information source optimization consider bayesian methods multi information source optimization miso seek optimize expensive evaluate black box objective function also accessing cheaper biased noisy approximations information sources present novel algorithm outperforms state art problem using joint statistical model information sources better suited miso used previous approaches novel acquisition function based step optimality analysis supported efficient parallelization provide guarantee asymptotic quality solution provided algorithm experimental evaluations demonstrate algorithm consistently finds designs higher value less cost previous approaches
straggler mitigation distributed optimization data encoding slow running straggler tasks significantly reduce computation speed distributed computation recently coding theory inspired approaches applied mitigate effect straggling embedding redundancy certain linear computational steps optimization algorithm thus completing computation without waiting stragglers paper propose alternate approach embed redundancy data instead computation allow nodes operate completely oblivious encoding propose several encoding schemes demonstrate popular batch algorithms gradient descent bfgs applied coding oblivious manner deterministically achieve sample path linear convergence approximate solution original problem using arbitrarily varying subset nodes iteration moreover approximation controlled choice encoding matrix number nodes used iteration provide experimental results demonstrating advantage approach uncoded replication strategies
beyond worst case probabilistic analysis affine policies dynamic optimization affine policies control widely used solution approach dynamic optimization computing optimal adjustable solution usually intractable worst case performance affine policies significantly bad empirical performance observed near optimal large class problem instances instance stage dynamic robust optimization problem linear covering constraints uncertain right hand side worst case approximation bound affine policies also tight see bertsimas goyal 2012 whereas observed empirical performance near optimal paper optimization aim address stark contrast worst case empirical performance affine policies particular show affine policies give good approximation stage adjustable robust optimization problem high probability random instances constraint coefficients generated large class distributions thereby providing theoretical justification observed empirical performance hand also present distribution performance bound affine policies instances generated according distribution high probability however constraint coefficients demonstrates empirical performance affine policies depend generative model instances
breaking nonsmooth barrier scalable parallel method composite optimization due simplicity excellent performance parallel asynchronous variants stochastic gradient descent become popular methods solve wide range large scale optimization problems multi core architectures yet despite practical success support nonsmooth objectives still lacking making unsuitable many problems interest machine learning lasso group lasso empirical risk minimization box constraints key technical issues explain paucity design algorithms asynchronous analysis work propose analyze proxasaga fully asynchronous sparse method inspired saga variance reduced incremental gradient algorithm proposed method easy implement significantly outperforms state art several nonsmooth large scale problems prove method achieves theoretical linear speedup respect sequential version assumptions sparsity gradients block separability proximal term empirical benchmarks multi core architecture illustrate practical speedups 13x core machine
structured prediction theory calibrated convex surrogate losses provide novel theoretical insights structured prediction context efficient convex surrogate loss minimization consistency guarantees task loss construct convex surrogate optimized via stochastic gradient descent prove tight bounds called calibration function relating excess surrogate risk actual risk contrast prior related work carefully monitor effect exponential number classes learning guarantees well optimization complexity interesting consequence formalize intuition task losses make learning harder others classical loss ill suited structured prediction
terngrad ternary gradients reduce communication distributed deep learning high network communication cost synchronizing gradients parameters well known bottleneck distributed training work propose terngrad uses ternary gradients accelerate distributed deep learning data parallelism approach requires numerical levels aggressively reduce communication time mathematically prove convergence terngrad assumption bound gradients guided bound propose layer wise ternarizing gradient clipping improve convergence experiments show applying terngrad alexnet incur accuracy loss even improve accuracy accuracy loss googlenet induced terngrad less average finally performance model proposed study scalability terngrad experiments show significant speed gains various deep neural networks
rebar low variance unbiased gradient estimates discrete latent variable models learning models discrete latent variables challenging due high variance gradient estimators generally approaches relied control variates reduce variance reinforce estimator recent work citep jang2016categorical maddison2016concrete taken different approach introducing continuous relaxation discrete variables produce low variance biased gradient estimates work combine approaches novel control variate produces low variance emph unbiased gradient estimates introduce novel continuous relaxation show tightness relaxation adapted online removing hyperparameter show state art variance reduction several benchmark generative modeling tasks generally leading faster convergence better final log likelihood
train longer generalize better closing generalization gap large batch training neural networks background deep learning models typically trained using stochastic gradient descent variants methods update weights using gradient estimated small fraction training data observed using large batch sizes persistent degradation generalization performance known generalization gap phenomena identifying origin gap closing remained open problem contributions examine initial high learning rate training phase find weight distance initialization grows logarithmicaly number weight updates therefore propose random walk random landscape statistical model known exhibit similar ultra slow diffusion behavior following hypothesis conducted experiments show empirically generalization gap stems relatively small number updates rather batch size completely eliminated adapting training regime used investigate different techniques train models large batch regime present novel algorithm named ghost batch normalization enables significant decrease generalization gap without increasing number updates validate findings conduct several additional experiments mnist cifar cifar 100 imagenet finally reassess common practices beliefs concerning training deep models suggest optimal achieve good generalization
unsupervised image image translation networks existing image image translation frameworks mapping image domain corresponding image another based supervised learning pairs corresponding images domains required learning translation function largely limits applications capturing corresponding images different domains often difficult task address issue propose unsupervised image image translation unit framework proposed framework based variational autoencoders generative adversarial networks learn translation function without corresponding images show learning capability enabled combining weight sharing constraint adversarial objective verify effectiveness proposed framework extensive experiment results
bayesian gans generative adversarial networks gans implicitly learn rich distributions images audio data hard model explicit likelihood present practical bayesian formulation unsupervised semi supervised learning gans use stochastic gradient hamiltonian monte carlo marginalize weights generator discriminator networks resulting approach straightforward obtains good performance without standard interventions feature matching mini batch discrimination exploring expressive posterior parameters generator bayesian gan avoids mode collapse produces interpretable candidate samples notable variability particular provides state art quantitative results semi supervised learning benchmarks including svhn celeba cifar outperforming dcgan wasserstein gans dcgan ensembles
imagination augmented agents deep reinforcement learning
reinforcement learning deep learning introduce imagination augmented agents i2as novel architecture deep reinforcement learning combining model free model based aspects contrast existing model based reinforcement learning planning methods prescribe model used arrive policy i2as learn interpret predictions trained environment model construct implicit plans arbitrary ways using predictions additional context deep policy networks i2as show improved data efficiency performance robustness model misspecification compared several strong baselines
multi information source optimization consider bayesian methods multi information source optimization miso seek optimize expensive evaluate black box objective function also accessing cheaper biased noisy approximations information sources present novel algorithm outperforms state art problem using joint statistical model information sources better suited miso used previous approaches novel acquisition function based step optimality analysis supported efficient parallelization provide guarantee asymptotic quality solution provided algorithm experimental evaluations demonstrate algorithm consistently finds designs higher value less cost previous approaches
chiru identifying translations comparable corpora well known problem several applications existing methods rely linguistic tools high quality corpora absence resources especially indian languages makes problem hard example state art techniques achieve mean reciprocal rank english italian mere 187 telugu kannada work address problem comparable corpora based translation correspondence induction resources available small noisy comparable corpora extracted wikipedia observe translations source target languages many topically related words common auxiliary languages model define notion translingual theme set topically related words auxiliary language corpora present probabilistic framework extensive experiments comparable corpora showed dramatic improvements performance extend ideas propose method measuring cross lingual semantic relatedness words stimulate research area make publicly available new high quality human annotated datasets clsr experiments clsr datasets show improvement correlation clsr task apply method real world problem cross lingual wikipedia title suggestion build wikitsu system user study wikitsu shows improvement quality titles suggested ccs concepts computing methodologies natural language processing additional key words phrases comparable corpora translation correspondence induction bilingual lexicon cross lingual semantic relatedness auxiliary language wikipedia title suggestion
chiru identifying groups strongly correlated variables smoothed ordered weighted norms failure lasso identify groups correlated predictors linear regression sparked significant research interest recently various norms proposed best described instances ordered weighted norms owl alternative regularization used lasso owl identify groups correlated variables forces model tobe constant within group artifactinduces unnecessary bias model estimation paper take submodular perspective show owl posed lov asz extension suitably defined submodular function submodular perspective explains groupwise constant behavior owl also suggests alternatives main contribution paper smoothed owl sowl new family norms identifies groups also allows model flexible inside group establish several algorithmic theoretical properties sowl including group identification model consistency also provide algorithmic toolsto compute sowl norm proximaloperator whose computational complexityo log significantly better thatof general purpose solvers experiments sowl compares favorably respect owl regimes interest
chiru vine copulas mixed data multi view clusteringfor mixed data beyond meta gaussian dependencies copulas enable ﬂexible parameterization multivariate distributions termsof constituent marginals dependence families vine copulas hierarchical collections ofbivariate copulas model wide variety dependencies multivariate data includingasymmetric tail dependencies widely used gaussian copulas used inmeta gaussian distributions cannot however current inference algorithms vines cannotﬁt data mixed combination continuous binary ordinal features arecommon many domains design new inference algorithm vines mixed datathereby extending use severalapplications illustrate algorithm developing adependency seeking multi view clustering model based dirichlet process mixture vinesthat generalizes previous models arbitrary dependencies well mixed marginals empirical results synthetic real datasets demonstrate performance clusteringsingle view multi view data asymmetric tail dependencies mixedmarginals vine copula mixed data multi view dependency seeking clustering
chiru clustering sum norms stochastic incremental algorithm convergence cluster recovery standard clustering methods means gaussian mixture models hierarchical clustering beset local minima sometimes drastically suboptimal moreover number clusters must known advance recently introduced sum norms son clusterpath convex relaxation means hierarchical clustering shrinks cluster centroids toward another ensure unique global minimizer give scalable stochastic incremental algorithm based proximal iterations solve son problem convergence guarantees also show algorithm recovers clusters quite general conditions similar form unifying proximity condition introduced approximation algorithms community covers paradigm cases gaussian mixtures planted partition models give experimental results confirm algorithm scales much better previous methods producing clusters comparable quality
chiru bayesian modeling temporal coherence videos entity discovery summarization video understood users terms entities present entity discovery task building appearance model entity person finding occurrences video represent video sequence tracklets spanning frames associated entity pose entity discovery tracklet clustering approach leveraging temporalcoherence property temporally neighboring tracklets likely associated entity major contributions first bayesian nonparametric models tracklet level extend chinese restaurant process crp temporally coherent chinese restaurant franchise crf jointly model entities temporal segments using mixture components sparse distributions discovering persons serial videos without meta data like scripts methods show considerable improvement state art approaches tracklet clustering terms clustering accuracy cluster purity entity coverage proposed methods perform online tracklet clustering streaming videos unlike existing approaches automatically reject false tracklets finally discuss entity driven video summarization temporal segments video selected based discovered entities create semantically meaningful summary index terms bayesian nonparametrics chinese restaurant process temporal coherence temporal segmentation tracklet clustering entity discovery entity driven video summarization
chiru non negative matrix factorization heavy noise noisy non negative matrix factorization given data matrix find non negative matrices noise matrix existing polynomial time algorithms proven error guarantees require column norm much smaller could restrictive important applications nmf topic modeling well theoretical noise models gaussian high almost every column violates condition introduce heavy noise model requires average noise large subsets columns small initiate study noisy nmf heavy noise model show noise model subsumes noise models theoretical practical interest gaussian noise maximum possible devise algorithm certain assumptions solves problem heavy noise error guarantees match previous algorithms running time substantially better previous best assumption weaker separability assumption made previous results provide empirical justification assumptions also provide first proof identifiability uniqueness noisy nmf based separability use hard check geometric conditions algorithm outperforms earlier polynomial time algorithms time error particularly presence high noise
chiru copula hdp hmm non parametric modeling temporal multivariate data efficient bulk cache preloading caching important determinant storage system performance bulk cache preloading process preloading large batches relevant data cache minutes hours advance actual requests application address bulk preloading analyzing high level spatio temporal motifs raw noisy traces aggregating trace temporal sequence correlated count vectors temporal multivariate data trace aggregation arise diverse
set workloads leading diverse data distributions complex spatio temporal dependencies motivated propose copula hdphmm new bayesian non parametric modeling technique based gaussian copula suitable temporal multivariate data arbitrary marginals avoiding limiting assumptions marginal distributions aware prior work copula based extensions
bayesian non parametric modeling algorithms discrete data inference copulas hard data continuous propose semi parametric inference technique based extended rank likelihood circumvents specifying marginals making inference suitable count data even data combination discrete continuous marginals enabling use bayesian non parametric modeling several data types without assumptions marginals finally propose efficient bulk cache preloading using copula model leverage high level spatio temporal motifs block traces experiments benchmark traces show near perfect hitrate using hulk tremendous improvement baseline using multivariate poisson fourth overhead
chiru relating romanized comments news articles inferring multi glyphic topical correspondence commenting popular facility provided news sites analyzing user generated content recently attracted research interest however multilingual societies india analyzing user generated content hard due several reasons official languages linguistic resources available mainly hindi observed people frequently use romanized text easy quick using english keyboard resulting multiglyphic comments texts language different scripts romanized texts almost unexplored machine learning far many cases comments made specific part article rather topic entire article shelf methods correspondence lda insufficient model relationships articles comments paper extend notion correspondence model multi lingual multi script inter lingual topics unified probabilistic model called multi glyphic correspondence topic model mctm using several metrics verify approach show improves state art
chiru weighted theta functions embeddings applications max cut clustering summarization introduce unifying generalization lovasz theta function sociated geometric embedding graphs weights nodes edges show computed exactly semidefinite programming approximate using svm computations show theta function interpreted measure diversity graphs use idea graph embedding algorithms max cut correlation clustering document summarization well represented problems weighted graphs
>>>>>>> 83cbb5463fdfdb399f239c70e1e0c0c88ffcad4f
